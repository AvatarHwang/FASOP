{"id": "5218", "revid": "2300266", "url": "https://en.wikipedia.org/wiki?curid=5218", "title": "Central processing unit", "text": "Central computer component which executes instructions\nA central processing unit (CPU)\u2014also called a central processor or main processor\u2014is the most important processor in a given computer. Its electronic circuitry executes instructions of a computer program, such as arithmetic, logic, controlling, and input/output (I/O) operations. This role contrasts with that of external components, such as main memory and I/O circuitry, and specialized coprocessors such as graphics processing units (GPUs).\nThe form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic\u2013logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers, and other components.\nMost modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.\nAn IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).\nArray processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamical aggregated computational resources.\nHistory.\nEarly computers such as the ENIAC had to be physically rewired to perform different tasks, which caused these machines to be called \"fixed-program computers\". The \"central processing unit\" term has been in use since as early as 1955. Since the term \"CPU\" is generally defined as a device for software (computer program) execution, the earliest devices that could rightly be called CPUs came with the advent of the stored-program computer.\nThe idea of a stored-program computer had been already present in the design of J. Presper Eckert and John William Mauchly's ENIAC, but was initially omitted so that it could be finished sooner. On June 30, 1945, before ENIAC was made, mathematician John von Neumann distributed a paper entitled \"First Draft of a Report on the EDVAC\". It was the outline of a stored-program computer that would eventually be completed in August 1949. EDVAC was designed to perform a certain number of instructions (or operations) of various types. Significantly, the programs written for EDVAC were to be stored in high-speed computer memory rather than specified by the physical wiring of the computer. This overcame a severe limitation of ENIAC, which was the considerable time and effort required to reconfigure the computer to perform a new task. With von Neumann's design, the program that EDVAC ran could be changed simply by changing the contents of the memory. EDVAC was not the first stored-program computer; the Manchester Baby, which was a small-scale experimental stored-program computer, ran its first program on 21 June 1948 and the Manchester Mark 1 ran its first program during the night of 16\u201317 June 1949.\nEarly CPUs were custom designs used as part of a larger and sometimes distinctive computer. However, this method of designing custom CPUs for a particular application has largely given way to the development of multi-purpose processors produced in large quantities. This standardization began in the era of discrete transistor mainframes and minicomputers, and has rapidly accelerated with the popularization of the integrated circuit (IC). The IC has allowed increasingly complex CPUs to be designed and manufactured to tolerances on the order of nanometers. Both the miniaturization and standardization of CPUs have increased the presence of digital devices in modern life far beyond the limited application of dedicated computing machines. Modern microprocessors appear in electronic devices ranging from automobiles to cellphones, and sometimes even in toys.\nWhile von Neumann is most often credited with the design of the stored-program computer because of his design of EDVAC, and the design became known as the von Neumann architecture, others before him, such as Konrad Zuse, had suggested and implemented similar ideas. The so-called Harvard architecture of the Harvard Mark I, which was completed before EDVAC, also used a stored-program design using punched paper tape rather than electronic memory. The key difference between the von Neumann and Harvard architectures is that the latter separates the storage and treatment of CPU instructions and data, while the former uses the same memory space for both. Most modern CPUs are primarily von Neumann in design, but CPUs with the Harvard architecture are seen as well, especially in embedded applications; for instance, the Atmel AVR microcontrollers are Harvard-architecture processors.\nRelays and vacuum tubes (thermionic tubes) were commonly used as switching elements; a useful computer requires thousands or tens of thousands of switching devices. The overall speed of a system is dependent on the speed of the switches. Vacuum-tube computers such as EDVAC tended to average eight hours between failures, whereas relay computers\u2014such as the slower but earlier Harvard Mark I\u2014failed very rarely. In the end, tube-based CPUs became dominant because the significant speed advantages afforded generally outweighed the reliability problems. Most of these early synchronous CPUs ran at low clock rates compared to modern microelectronic designs. Clock signal frequencies ranging from 100 kHz to 4\u00a0MHz were very common at this time, limited largely by the speed of the switching devices they were built with.\nTransistor CPUs.\nThe design complexity of CPUs increased as various technologies facilitated building smaller and more reliable electronic devices. The first such improvement came with the advent of the transistor. Transistorized CPUs during the 1950s and 1960s no longer had to be built out of bulky, unreliable, and fragile switching elements, like vacuum tubes and relays. With this improvement, more complex and reliable CPUs were built onto one or several printed circuit boards containing discrete (individual) components.\nIn 1964, IBM introduced its IBM System/360 computer architecture that was used in a series of computers capable of running the same programs with different speed and performance. This was significant at a time when most electronic computers were incompatible with one another, even those made by the same manufacturer. To facilitate this improvement, IBM used the concept of a microprogram (often called \"microcode\"), which still sees widespread usage in modern CPUs. The System/360 architecture was so popular that it dominated the mainframe computer market for decades and left a legacy that is still continued by similar modern computers like the IBM zSeries. In 1965, Digital Equipment Corporation (DEC) introduced another influential computer aimed at the scientific and research markets\u2014the PDP-8.\nTransistor-based computers had several distinct advantages over their predecessors. Aside from facilitating increased reliability and lower power consumption, transistors also allowed CPUs to operate at much higher speeds because of the short switching time of a transistor in comparison to a tube or relay. The increased reliability and dramatically increased speed of the switching elements, which were almost exclusively transistors by this time; CPU clock rates in the tens of megahertz were easily obtained during this period. Additionally, while discrete transistor and IC CPUs were in heavy usage, new high-performance designs like single instruction, multiple data (SIMD) vector processors began to appear. These early experimental designs later gave rise to the era of specialized supercomputers like those made by Cray Inc and Fujitsu Ltd.\nSmall-scale integration CPUs.\nDuring this period, a method of manufacturing many interconnected transistors in a compact space was developed. The integrated circuit (IC) allowed a large number of transistors to be manufactured on a single semiconductor-based die, or \"chip\". At first, only very basic non-specialized digital circuits such as NOR gates were miniaturized into ICs. CPUs based on these \"building block\" ICs are generally referred to as \"small-scale integration\" (SSI) devices. SSI ICs, such as the ones used in the Apollo Guidance Computer, usually contained up to a few dozen transistors. To build an entire CPU out of SSI ICs required thousands of individual chips, but still consumed much less space and power than earlier discrete transistor designs.\nIBM's System/370, follow-on to the System/360, used SSI ICs rather than Solid Logic Technology discrete-transistor modules. DEC's PDP-8/I and KI10 PDP-10 also switched from the individual transistors used by the PDP-8 and PDP-10 to SSI ICs, and their extremely popular PDP-11 line was originally built with SSI ICs, but was eventually implemented with LSI components once these became practical.\nLarge-scale integration CPUs.\nLee Boysel published influential articles, including a 1967 \"manifesto\", which described how to build the equivalent of a 32-bit mainframe computer from a relatively small number of large-scale integration circuits (LSI). The only way to build LSI chips, which are chips with a hundred or more gates, was to build them using a metal\u2013oxide\u2013semiconductor (MOS) semiconductor manufacturing process (either PMOS logic, NMOS logic, or CMOS logic). However, some companies continued to build processors out of bipolar transistor\u2013transistor logic (TTL) chips because bipolar junction transistors were faster than MOS chips up until the 1970s (a few companies such as Datapoint continued to build processors out of TTL chips until the early 1980s). In the 1960s, MOS ICs were slower and initially considered useful only in applications that required low power. Following the development of silicon-gate MOS technology by Federico Faggin at Fairchild Semiconductor in 1968, MOS ICs largely replaced bipolar TTL as the standard chip technology in the early 1970s.\nAs the microelectronic technology advanced, an increasing number of transistors were placed on ICs, decreasing the number of individual ICs needed for a complete CPU. MSI and LSI ICs increased transistor counts to hundreds, and then thousands. By 1968, the number of ICs required to build a complete CPU had been reduced to 24 ICs of eight different types, with each IC containing roughly 1000 MOSFETs. In stark contrast with its SSI and MSI predecessors, the first LSI implementation of the PDP-11 contained a CPU composed of only four LSI integrated circuits.\nMicroprocessors.\nSince microprocessors were first introduced they have almost completely overtaken all other central processing unit implementation methods. The first commercially available microprocessor, made in 1971, was the Intel 4004, and the first widely used microprocessor, made in 1974, was the Intel 8080. Mainframe and minicomputer manufacturers of the time launched proprietary IC development programs to upgrade their older computer architectures, and eventually produced instruction set compatible microprocessors that were backward-compatible with their older hardware and software. Combined with the advent and eventual success of the ubiquitous personal computer, the term \"CPU\" is now applied almost exclusively to microprocessors. Several CPUs (denoted \"cores\") can be combined in a single processing chip.\nPrevious generations of CPUs were implemented as discrete components and numerous small integrated circuits (ICs) on one or more circuit boards. Microprocessors, on the other hand, are CPUs manufactured on a very small number of ICs; usually just one. The overall smaller CPU size, as a result of being implemented on a single die, means faster switching time because of physical factors like decreased gate parasitic capacitance. This has allowed synchronous microprocessors to have clock rates ranging from tens of megahertz to several gigahertz. Additionally, the ability to construct exceedingly small transistors on an IC has increased the complexity and number of transistors in a single CPU many fold. This widely observed trend is described by Moore's law, which had proven to be a fairly accurate predictor of the growth of CPU (and other IC) complexity until 2016.\nWhile the complexity, size, construction and general form of CPUs have changed enormously since 1950, the basic design and function has not changed much at all. Almost all common CPUs today can be very accurately described as von Neumann stored-program machines. As Moore's law no longer holds, concerns have arisen about the limits of integrated circuit transistor technology. Extreme miniaturization of electronic gates is causing the effects of phenomena like electromigration and subthreshold leakage to become much more significant. These newer concerns are among the many factors causing researchers to investigate new methods of computing such as the quantum computer, as well as to expand the usage of parallelism and other methods that extend the usefulness of the classical von Neumann model.\nOperation.\nThe fundamental operation of most CPUs, regardless of the physical form they take, is to execute a sequence of stored instructions that is called a program. The instructions to be executed are kept in some kind of computer memory. Nearly all CPUs follow the fetch, decode and execute steps in their operation, which are collectively known as the instruction cycle.\nAfter the execution of an instruction, the entire process repeats, with the next instruction cycle normally fetching the next-in-sequence instruction because of the incremented value in the program counter. If a jump instruction was executed, the program counter will be modified to contain the address of the instruction that was jumped to and program execution continues normally. In more complex CPUs, multiple instructions can be fetched, decoded and executed simultaneously. This section describes what is generally referred to as the \"classic RISC pipeline\", which is quite common among the simple CPUs used in many electronic devices (often called microcontrollers). It largely ignores the important role of CPU cache, and therefore the access stage of the pipeline.\nSome instructions manipulate the program counter rather than producing result data directly; such instructions are generally called \"jumps\" and facilitate program behavior like loops, conditional program execution (through the use of a conditional jump), and existence of functions. In some processors, some other instructions change the state of bits in a \"flags\" register. These flags can be used to influence how a program behaves, since they often indicate the outcome of various operations. For example, in such processors a \"compare\" instruction evaluates two values and sets or clears bits in the flags register to indicate which one is greater or whether they are equal; one of these flags could then be used by a later jump instruction to determine program flow.\nFetch.\nFetch involves retrieving an instruction (which is represented by a number or sequence of numbers) from program memory. The instruction's location (address) in program memory is determined by the program counter (PC; called the \"instruction pointer\" in Intel x86 microprocessors), which stores a number that identifies the address of the next instruction to be fetched. After an instruction is fetched, the PC is incremented by the length of the instruction so that it will contain the address of the next instruction in the sequence. Often, the instruction to be fetched must be retrieved from relatively slow memory, causing the CPU to stall while waiting for the instruction to be returned. This issue is largely addressed in modern processors by caches and pipeline architectures (see below).\nDecode.\nThe instruction that the CPU fetches from memory determines what the CPU will do. In the decode step, performed by binary decoder circuitry known as the \"instruction decoder\", the instruction is converted into signals that control other parts of the CPU.\nThe way in which the instruction is interpreted is defined by the CPU's instruction set architecture (ISA). Often, one group of bits (that is, a \"field\") within the instruction, called the opcode, indicates which operation is to be performed, while the remaining fields usually provide supplemental information required for the operation, such as the operands. Those operands may be specified as a constant value (called an immediate value), or as the location of a value that may be a processor register or a memory address, as determined by some addressing mode.\nIn some CPU designs the instruction decoder is implemented as a hardwired, unchangeable binary decoder circuit. In others, a microprogram is used to translate instructions into sets of CPU configuration signals that are applied sequentially over multiple clock pulses. In some cases the memory that stores the microprogram is rewritable, making it possible to change the way in which the CPU decodes instructions.\nExecute.\nAfter the fetch and decode steps, the execute step is performed. Depending on the CPU architecture, this may consist of a single action or a sequence of actions. During each action, control signals electrically enable or disable various parts of the CPU so they can perform all or part of the desired operation. The action is then completed, typically in response to a clock pulse. Very often the results are written to an internal CPU register for quick access by subsequent instructions. In other cases results may be written to slower, but less expensive and higher capacity main memory.\nFor example, if an addition instruction is to be executed, registers containing operands (numbers to be summed) are activated, as are the parts of the arithmetic logic unit (ALU) that perform addition. When the clock pulse occurs, the operands flow from the source registers into the ALU, and the sum appears at its output. On subsequent clock pulses, other components are enabled (and disabled) to move the output (the sum of the operation) to storage (e.g., a register or memory). If the resulting sum is too large (i.e., it is larger than the ALU's output word size), an arithmetic overflow flag will be set, influencing the next operation.\nStructure and implementation.\nHardwired into a CPU's circuitry is a set of basic operations it can perform, called an instruction set. Such operations may involve, for example, adding or subtracting two numbers, comparing two numbers, or jumping to a different part of a program. Each instruction is represented by a unique combination of bits, known as the machine language opcode. While processing an instruction, the CPU decodes the opcode (via a binary decoder) into control signals, which orchestrate the behavior of the CPU. A complete machine language instruction consists of an opcode and, in many cases, additional bits that specify arguments for the operation (for example, the numbers to be summed in the case of an addition operation). Going up the complexity scale, a machine language program is a collection of machine language instructions that the CPU executes.\nThe actual mathematical operation for each instruction is performed by a combinational logic circuit within the CPU's processor known as the arithmetic\u2013logic unit or ALU. In general, a CPU executes an instruction by fetching it from memory, using its ALU to perform an operation, and then storing the result to memory. Beside the instructions for integer mathematics and logic operations, various other machine instructions exist, such as those for loading data from memory and storing it back, branching operations, and mathematical operations on floating-point numbers performed by the CPU's floating-point unit (FPU).\nControl unit.\nThe control unit (CU) is a component of the CPU that directs the operation of the processor. It tells the computer's memory, arithmetic and logic unit and input and output devices how to respond to the instructions that have been sent to the processor.\nIt directs the operation of the other units by providing timing and control signals. Most computer resources are managed by the CU. It directs the flow of data between the CPU and the other devices. John von Neumann included the control unit as part of the von Neumann architecture. In modern computer designs, the control unit is typically an internal part of the CPU with its overall role and operation unchanged since its introduction.\nArithmetic logic unit.\nThe arithmetic logic unit (ALU) is a digital circuit within the processor that performs integer arithmetic and bitwise logic operations. The inputs to the ALU are the data words to be operated on (called operands), status information from previous operations, and a code from the control unit indicating which operation to perform. Depending on the instruction being executed, the operands may come from internal CPU registers, external memory, or constants generated by the ALU itself.\nWhen all input signals have settled and propagated through the ALU circuitry, the result of the performed operation appears at the ALU's outputs. The result consists of both a data word, which may be stored in a register or memory, and status information that is typically stored in a special, internal CPU register reserved for this purpose.\nAddress generation unit.\nThe address generation unit (AGU), sometimes also called the address computation unit (ACU), is an execution unit inside the CPU that calculates addresses used by the CPU to access main memory. By having address calculations handled by separate circuitry that operates in parallel with the rest of the CPU, the number of CPU cycles required for executing various machine instructions can be reduced, bringing performance improvements.\nWhile performing various operations, CPUs need to calculate memory addresses required for fetching data from the memory; for example, in-memory positions of array elements must be calculated before the CPU can fetch the data from actual memory locations. Those address-generation calculations involve different integer arithmetic operations, such as addition, subtraction, modulo operations, or bit shifts. Often, calculating a memory address involves more than one general-purpose machine instruction, which do not necessarily decode and execute quickly. By incorporating an AGU into a CPU design, together with introducing specialized instructions that use the AGU, various address-generation calculations can be offloaded from the rest of the CPU, and can often be executed quickly in a single CPU cycle.\nCapabilities of an AGU depend on a particular CPU and its architecture. Thus, some AGUs implement and expose more address-calculation operations, while some also include more advanced specialized instructions that can operate on multiple operands at a time. Some CPU architectures include multiple AGUs so more than one address-calculation operation can be executed simultaneously, which brings further performance improvements due to the superscalar nature of advanced CPU designs. For example, Intel incorporates multiple AGUs into its Sandy Bridge and Haswell microarchitectures, which increase bandwidth of the CPU memory subsystem by allowing multiple memory-access instructions to be executed in parallel.\nMemory management unit (MMU).\nMany microprocessors (in smartphones and desktop, laptop, server computers) have a memory management unit, translating logical addresses into physical RAM addresses, providing memory protection and paging abilities, useful for virtual memory. Simpler processors, especially microcontrollers, usually don't include an MMU.\nCache.\nA CPU cache is a hardware cache used by the central processing unit (CPU) of a computer to reduce the average cost (time or energy) to access data from the main memory. A cache is a smaller, faster memory, closer to a processor core, which stores copies of the data from frequently used main memory locations. Most CPUs have different independent caches, including instruction and data caches, where the data cache is usually organized as a hierarchy of more cache levels (L1, L2, L3, L4, etc.).\nAll modern (fast) CPUs (with few specialized exceptions) have multiple levels of CPU caches. The first CPUs that used a cache had only one level of cache; unlike later level 1 caches, it was not split into L1d (for data) and L1i (for instructions). Almost all current CPUs with caches have a split L1 cache. They also have L2 caches and, for larger processors, L3 caches as well. The L2 cache is usually not split and acts as a common repository for the already split L1 cache. Every core of a multi-core processor has a dedicated L2 cache and is usually not shared between the cores. The L3 cache, and higher-level caches, are shared between the cores and are not split. An L4 cache is currently uncommon, and is generally on dynamic random-access memory (DRAM), rather than on static random-access memory (SRAM), on a separate die or chip. That was also the case historically with L1, while bigger chips have allowed integration of it and generally all cache levels, with the possible exception of the last level. Each extra level of cache tends to be bigger and be optimized differently.\nOther types of caches exist (that are not counted towards the \"cache size\" of the most important caches mentioned above), such as the translation lookaside buffer (TLB) that is part of the memory management unit (MMU) that most CPUs have.\nCaches are generally sized in powers of two: 2, 8, 16 etc. KiB or MiB (for larger non-L1) sizes, although the IBM z13 has a 96 KiB L1 instruction cache.\nClock rate.\nMost CPUs are synchronous circuits, which means they employ a clock signal to pace their sequential operations. The clock signal is produced by an external oscillator circuit that generates a consistent number of pulses each second in the form of a periodic square wave. The frequency of the clock pulses determines the rate at which a CPU executes instructions and, consequently, the faster the clock, the more instructions the CPU will execute each second.\nTo ensure proper operation of the CPU, the clock period is longer than the maximum time needed for all signals to propagate (move) through the CPU. In setting the clock period to a value well above the worst-case propagation delay, it is possible to design the entire CPU and the way it moves data around the \"edges\" of the rising and falling clock signal. This has the advantage of simplifying the CPU significantly, both from a design perspective and a component-count perspective. However, it also carries the disadvantage that the entire CPU must wait on its slowest elements, even though some portions of it are much faster. This limitation has largely been compensated for by various methods of increasing CPU parallelism (see below).\nHowever, architectural improvements alone do not solve all of the drawbacks of globally synchronous CPUs. For example, a clock signal is subject to the delays of any other electrical signal. Higher clock rates in increasingly complex CPUs make it more difficult to keep the clock signal in phase (synchronized) throughout the entire unit. This has led many modern CPUs to require multiple identical clock signals to be provided to avoid delaying a single signal significantly enough to cause the CPU to malfunction. Another major issue, as clock rates increase dramatically, is the amount of heat that is dissipated by the CPU. The constantly changing clock causes many components to switch regardless of whether they are being used at that time. In general, a component that is switching uses more energy than an element in a static state. Therefore, as clock rate increases, so does energy consumption, causing the CPU to require more heat dissipation in the form of CPU cooling solutions.\nOne method of dealing with the switching of unneeded components is called clock gating, which involves turning off the clock signal to unneeded components (effectively disabling them). However, this is often regarded as difficult to implement and therefore does not see common usage outside of very low-power designs. One notable recent CPU design that uses extensive clock gating is the IBM PowerPC-based Xenon used in the Xbox 360; that way, power requirements of the Xbox 360 are greatly reduced.\nClockless CPUs.\nAnother method of addressing some of the problems with a global clock signal is the removal of the clock signal altogether. While removing the global clock signal makes the design process considerably more complex in many ways, asynchronous (or clockless) designs carry marked advantages in power consumption and heat dissipation in comparison with similar synchronous designs. While somewhat uncommon, entire asynchronous CPUs have been built without using a global clock signal. Two notable examples of this are the ARM compliant AMULET and the MIPS R3000 compatible MiniMIPS.\nRather than totally removing the clock signal, some CPU designs allow certain portions of the device to be asynchronous, such as using asynchronous ALUs in conjunction with superscalar pipelining to achieve some arithmetic performance gains. While it is not altogether clear whether totally asynchronous designs can perform at a comparable or better level than their synchronous counterparts, it is evident that they do at least excel in simpler math operations. This, combined with their excellent power consumption and heat dissipation properties, makes them very suitable for embedded computers.\nVoltage regulator module.\nMany modern CPUs have a die-integrated power managing module which regulates on-demand voltage supply to the CPU circuitry allowing it to keep balance between performance and power consumption.\nInteger range.\nEvery CPU represents numerical values in a specific way. For example, some early digital computers represented numbers as familiar decimal (base 10) numeral system values, and others have employed more unusual representations such as ternary (base three). Nearly all modern CPUs represent numbers in binary form, with each digit being represented by some two-valued physical quantity such as a \"high\" or \"low\" voltage.\nRelated to numeric representation is the size and precision of integer numbers that a CPU can represent. In the case of a binary CPU, this is measured by the number of bits (significant digits of a binary encoded integer) that the CPU can process in one operation, which is commonly called \"word size\", \"bit width\", \"data path width\", \"integer precision\", or \"integer size\". A CPU's integer size determines the range of integer values it can directly operate on. For example, an 8-bit CPU can directly manipulate integers represented by eight bits, which have a range of 256 (28) discrete integer values.\nInteger range can also affect the number of memory locations the CPU can directly address (an address is an integer value representing a specific memory location). For example, if a binary CPU uses 32 bits to represent a memory address then it can directly address 232 memory locations. To circumvent this limitation and for various other reasons, some CPUs use mechanisms (such as bank switching) that allow additional memory to be addressed.\nCPUs with larger word sizes require more circuitry and consequently are physically larger, cost more and consume more power (and therefore generate more heat). As a result, smaller 4- or 8-bit microcontrollers are commonly used in modern applications even though CPUs with much larger word sizes (such as 16, 32, 64, even 128-bit) are available. When higher performance is required, however, the benefits of a larger word size (larger data ranges and address spaces) may outweigh the disadvantages. A CPU can have internal data paths shorter than the word size to reduce size and cost. For example, even though the IBM System/360 instruction set was a 32-bit instruction set, the System/360 Model 30 and Model 40 had 8-bit data paths in the arithmetic logical unit, so that a 32-bit add required four cycles, one for each 8 bits of the operands, and, even though the Motorola 68000 series instruction set was a 32-bit instruction set, the Motorola 68000 and Motorola 68010 had 16-bit data paths in the arithmetic logical unit, so that a 32-bit add required two cycles.\nTo gain some of the advantages afforded by both lower and higher bit lengths, many instruction sets have different bit widths for integer and floating-point data, allowing CPUs implementing that instruction set to have different bit widths for different portions of the device. For example, the IBM System/360 instruction set was primarily 32 bit, but supported 64-bit floating-point values to facilitate greater accuracy and range in floating-point numbers. The System/360 Model 65 had an 8-bit adder for decimal and fixed-point binary arithmetic and a 60-bit adder for floating-point arithmetic. Many later CPU designs use similar mixed bit width, especially when the processor is meant for general-purpose usage where a reasonable balance of integer and floating-point capability is required.\nParallelism.\nThe description of the basic operation of a CPU offered in the previous section describes the simplest form that a CPU can take. This type of CPU, usually referred to as \"subscalar\", operates on and executes one instruction on one or two pieces of data at a time, that is less than one instruction per clock cycle (IPC &lt; 1).\nThis process gives rise to an inherent inefficiency in subscalar CPUs. Since only one instruction is executed at a time, the entire CPU must wait for that instruction to complete before proceeding to the next instruction. As a result, the subscalar CPU gets \"hung up\" on instructions which take more than one clock cycle to complete execution. Even adding a second execution unit (see below) does not improve performance much; rather than one pathway being hung up, now two pathways are hung up and the number of unused transistors is increased. This design, wherein the CPU's execution resources can operate on only one instruction at a time, can only possibly reach \"scalar\" performance (one instruction per clock cycle, IPC = 1). However, the performance is nearly always subscalar (less than one instruction per clock cycle, IPC &lt; 1).\nAttempts to achieve scalar and better performance have resulted in a variety of design methodologies that cause the CPU to behave less linearly and more in parallel. When referring to parallelism in CPUs, two terms are generally used to classify these design techniques:\nEach methodology differs both in the ways in which they are implemented, as well as the relative effectiveness they afford in increasing the CPU's performance for an application.\nInstruction-level parallelism.\nOne of the simplest methods for increased parallelism is to begin the first steps of instruction fetching and decoding before the prior instruction finishes executing. This is a technique known as instruction pipelining, and is used in almost all modern general-purpose CPUs. Pipelining allows multiple instruction to be executed at a time by breaking the execution pathway into discrete stages. This separation can be compared to an assembly line, in which an instruction is made more complete at each stage until it exits the execution pipeline and is retired.\nPipelining does, however, introduce the possibility for a situation where the result of the previous operation is needed to complete the next operation; a condition often termed data dependency conflict. Therefore pipelined processors must check for these sorts of conditions and delay a portion of the pipeline if necessary. A pipelined processor can become very nearly scalar, inhibited only by pipeline stalls (an instruction spending more than one clock cycle in a stage).\nImprovements in instruction pipelining led to further decreases in the idle time of CPU components. Designs that are said to be superscalar include a long instruction pipeline and multiple identical execution units, such as load\u2013store units, arithmetic\u2013logic units, floating-point units and address generation units. In a superscalar pipeline, instructions are read and passed to a dispatcher, which decides whether or not the instructions can be executed in parallel (simultaneously). If so, they are dispatched to execution units, resulting in their simultaneous execution. In general, the number of instructions that a superscalar CPU will complete in a cycle is dependent on the number of instructions it is able to dispatch simultaneously to execution units.\nMost of the difficulty in the design of a superscalar CPU architecture lies in creating an effective dispatcher. The dispatcher needs to be able to quickly determine whether instructions can be executed in parallel, as well as dispatch them in such a way as to keep as many execution units busy as possible. This requires that the instruction pipeline is filled as often as possible and requires significant amounts of CPU cache. It also makes hazard-avoiding techniques like branch prediction, speculative execution, register renaming, out-of-order execution and transactional memory crucial to maintaining high levels of performance. By attempting to predict which branch (or path) a conditional instruction will take, the CPU can minimize the number of times that the entire pipeline must wait until a conditional instruction is completed. Speculative execution often provides modest performance increases by executing portions of code that may not be needed after a conditional operation completes. Out-of-order execution somewhat rearranges the order in which instructions are executed to reduce delays due to data dependencies. Also in case of single instruction stream, multiple data stream\u2014a case when a lot of data from the same type has to be processed\u2014, modern processors can disable parts of the pipeline so that when a single instruction is executed many times, the CPU skips the fetch and decode phases and thus greatly increases performance on certain occasions, especially in highly monotonous program engines such as video creation software and photo processing.\nWhen just a fraction of the CPU is superscalar, the part that is not suffers a performance penalty due to scheduling stalls. The Intel P5 Pentium had two superscalar ALUs which could accept one instruction per clock cycle each, but its FPU could not. Thus the P5 was integer superscalar but not floating point superscalar. Intel's successor to the P5 architecture, P6, added superscalar abilities to its floating-point features.\nSimple pipelining and superscalar design increase a CPU's ILP by allowing it to execute instructions at rates surpassing one instruction per clock cycle. Most modern CPU designs are at least somewhat superscalar, and nearly all general purpose CPUs designed in the last decade are superscalar. In later years some of the emphasis in designing high-ILP computers has been moved out of the CPU's hardware and into its software interface, or instruction set architecture (ISA). The strategy of the very long instruction word (VLIW) causes some ILP to become implied directly by the software, reducing the CPU\u2019s work in boosting ILP and thereby reducing design complexity.\nTask-level parallelism.\nAnother strategy of achieving performance is to execute multiple threads or processes in parallel. This area of research is known as parallel computing. In Flynn's taxonomy, this strategy is known as multiple instruction stream, multiple data stream (MIMD).\nOne technology used for this purpose was multiprocessing (MP). The initial flavor of this technology is known as symmetric multiprocessing (SMP), where a small number of CPUs share a coherent view of their memory system. In this scheme, each CPU has additional hardware to maintain a constantly up-to-date view of memory. By avoiding stale views of memory, the CPUs can cooperate on the same program and programs can migrate from one CPU to another. To increase the number of cooperating CPUs beyond a handful, schemes such as non-uniform memory access (NUMA) and directory-based coherence protocols were introduced in the 1990s. SMP systems are limited to a small number of CPUs while NUMA systems have been built with thousands of processors. Initially, multiprocessing was built using multiple discrete CPUs and boards to implement the interconnect between the processors. When the processors and their interconnect are all implemented on a single chip, the technology is known as chip-level multiprocessing (CMP) and the single chip as a multi-core processor.\nIt was later recognized that finer-grain parallelism existed with a single program. A single program might have several threads (or functions) that could be executed separately or in parallel. Some of the earliest examples of this technology implemented input/output processing such as direct memory access as a separate thread from the computation thread. A more general approach to this technology was introduced in the 1970s when systems were designed to run multiple computation threads in parallel. This technology is known as multi-threading (MT). This approach is considered more cost-effective than multiprocessing, as only a small number of components within a CPU is replicated to support MT as opposed to the entire CPU in the case of MP. In MT, the execution units and the memory system including the caches are shared among multiple threads. The downside of MT is that the hardware support for multithreading is more visible to software than that of MP and thus supervisor software like operating systems have to undergo larger changes to support MT. One type of MT that was implemented is known as temporal multithreading, where one thread is executed until it is stalled waiting for data to return from external memory. In this scheme, the CPU would then quickly context switch to another thread which is ready to run, the switch often done in one CPU clock cycle, such as the UltraSPARC T1. Another type of MT is simultaneous multithreading, where instructions from multiple threads are executed in parallel within one CPU clock cycle.\nFor several decades from the 1970s to early 2000s, the focus in designing high performance general purpose CPUs was largely on achieving high ILP through technologies such as pipelining, caches, superscalar execution, out-of-order execution, etc. This trend culminated in large, power-hungry CPUs such as the Intel Pentium 4. By the early 2000s, CPU designers were thwarted from achieving higher performance from ILP techniques due to the growing disparity between CPU operating frequencies and main memory operating frequencies as well as escalating CPU power dissipation owing to more esoteric ILP techniques.\nCPU designers then borrowed ideas from commercial computing markets such as transaction processing, where the aggregate performance of multiple programs, also known as throughput computing, was more important than the performance of a single thread or process.\nThis reversal of emphasis is evidenced by the proliferation of dual and more core processor designs and notably, Intel's newer designs resembling its less superscalar P6 architecture. Late designs in several processor families exhibit CMP, including the x86-64 Opteron and Athlon 64 X2, the SPARC UltraSPARC T1, IBM POWER4 and POWER5, as well as several video game console CPUs like the Xbox 360's triple-core PowerPC design, and the PlayStation 3's 7-core Cell microprocessor.\nData parallelism.\nA less common but increasingly important paradigm of processors (and indeed, computing in general) deals with data parallelism. The processors discussed earlier are all referred to as some type of scalar device. As the name implies, vector processors deal with multiple pieces of data in the context of one instruction. This contrasts with scalar processors, which deal with one piece of data for every instruction. Using Flynn's taxonomy, these two schemes of dealing with data are generally referred to as \"single instruction\" stream, \"multiple data\" stream (SIMD) and \"single instruction\" stream, \"single data\" stream (SISD), respectively. The great utility in creating processors that deal with vectors of data lies in optimizing tasks that tend to require the same operation (for example, a sum or a dot product) to be performed on a large set of data. Some classic examples of these types of tasks include multimedia applications (images, video and sound), as well as many types of scientific and engineering tasks. Whereas a scalar processor must complete the entire process of fetching, decoding and executing each instruction and value in a set of data, a vector processor can perform a single operation on a comparatively large set of data with one instruction. This is only possible when the application tends to require many steps which apply one operation to a large set of data.\nMost early vector processors, such as the Cray-1, were associated almost exclusively with scientific research and cryptography applications. However, as multimedia has largely shifted to digital media, the need for some form of SIMD in general-purpose processors has become significant. Shortly after inclusion of floating-point units started to become commonplace in general-purpose processors, specifications for and implementations of SIMD execution units also began to appear for general-purpose processors. Some of these early SIMD specifications \u2013 like HP's Multimedia Acceleration eXtensions (MAX) and Intel's MMX \u2013 were integer-only. This proved to be a significant impediment for some software developers, since many of the applications that benefit from SIMD primarily deal with floating-point numbers. Progressively, developers refined and remade these early designs into some of the common modern SIMD specifications, which are usually associated with one instruction set architecture (ISA). Some notable modern examples include Intel's Streaming SIMD Extensions (SSE) and the PowerPC-related AltiVec (also known as VMX).\nHardware performance counter.\nMany modern architectures (including embedded ones) often include hardware performance counters (HPC), which enables low-level (instruction-level) collection, benchmarking, debugging or analysis of running software metrics. HPC may also be used to discover and analyze unusual or suspicious activity of the software, such as return-oriented programming (ROP) or sigreturn-oriented programming (SROP) exploits etc. This is usually done by software-security teams to assess and find malicious binary programs.\nMany major vendors (such as IBM, Intel, AMD, and Arm etc.) provide software interfaces (usually written in C/C++) that can be used to collected data from CPUs registers in order to get metrics. Operating system vendors also provide software like codice_1 (Linux) to record, benchmark, or trace CPU events running kernels and applications.\nVirtual CPUs.\nCloud computing can involve subdividing CPU operation into virtual central processing units (vCPUs).\nA host is the virtual equivalent of a physical machine, on which a virtual system is operating. When there are several physical machines operating in tandem and managed as a whole, the grouped computing and memory resources form a cluster. In some systems, it is possible to dynamically add and remove from a cluster. Resources available at a host and cluster level can be partitioned out into resources pools with fine granularity.\nPerformance.\nThe \"performance\" or \"speed\" of a processor depends on, among many other factors, the clock rate (generally given in multiples of hertz) and the instructions per clock (IPC), which together are the factors for the instructions per second (IPS) that the CPU can perform.\nMany reported IPS values have represented \"peak\" execution rates on artificial instruction sequences with few branches, whereas realistic workloads consist of a mix of instructions and applications, some of which take longer to execute than others. The performance of the memory hierarchy also greatly affects processor performance, an issue barely considered in MIPS calculations. Because of these problems, various standardized tests, often called \"benchmarks\" for this purpose\u200d\u2014\u200csuch as SPECint\u200d\u2014\u200chave been developed to attempt to measure the real effective performance in commonly used applications.\nProcessing performance of computers is increased by using multi-core processors, which essentially is plugging two or more individual processors (called \"cores\" in this sense) into one integrated circuit. Ideally, a dual core processor would be nearly twice as powerful as a single core processor. In practice, the performance gain is far smaller, only about 50%, due to imperfect software algorithms and implementation. Increasing the number of cores in a processor (i.e. dual-core, quad-core, etc.) increases the workload that can be handled. This means that the processor can now handle numerous asynchronous events, interrupts, etc. which can take a toll on the CPU when overwhelmed. These cores can be thought of as different floors in a processing plant, with each floor handling a different task. Sometimes, these cores will handle the same tasks as cores adjacent to them if a single core is not enough to handle the information.\nDue to specific capabilities of modern CPUs, such as simultaneous multithreading and uncore, which involve sharing of actual CPU resources while aiming at increased utilization, monitoring performance levels and hardware use gradually became a more complex task. As a response, some CPUs implement additional hardware logic that monitors actual use of various parts of a CPU and provides various counters accessible to software; an example is Intel's \"Performance Counter Monitor\" technology.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5220", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5220", "title": "Complex numbers", "text": ""}
{"id": "5221", "revid": "16420254", "url": "https://en.wikipedia.org/wiki?curid=5221", "title": "Carnivora", "text": "Order of mammals\nCarnivora is an order of placental mammals that have specialized in primarily eating flesh, whose members are formally referred to as carnivorans. The order Carnivora is the fifth largest order of mammals, comprising at least 279 species.\nCarnivorans live on every major landmass and in a variety of habitats, ranging from the cold polar regions to the hyper-arid region of the Sahara Desert to the open seas. They come in a very large array of different body plans in contrasting shapes and sizes.\nCarnivora can be divided into two suborders: the cat-like Feliformia and the dog-like Caniformia, which are differentiated based on the structure of their ear bones and cranial features. The feliforms include families such as the cats, the hyenas, the mongooses and the civets. The majority of feliform species are found in the Old World, though the cats and one extinct genus of hyena have successfully diversified into the Americas. The caniforms include the dogs, bears, raccoons, weasels, and seals. Members of this group are found worldwide and with immense diversity in their diet, behavior, and morphology.\nEtymology.\nThe word \"carnivore\" is derived from Latin \"car\u014d\" (stem \"carn-\") 'flesh' and \"vor\u0101re\" 'to devour', and refers to any meat-eating organism.\nPhylogeny.\nThe oldest known carnivoran line mammals (Carnivoramorpha) appeared in North America 6\u00a0million years after the Cretaceous\u2013Paleogene extinction event. These early ancestors of carnivorans would have resembled small weasel or genet-like mammals, occupying a nocturnal shift on the forest floor or in the trees, as other groups of mammals like the mesonychians and later the creodonts were occupying the megafaunal faunivorous niche. However, following the extinction of mesonychians and the oxyaenid creodonts at the end of the Eocene, carnivorans quickly moved into this niche, with forms like the nimravids being the dominant large-bodied ambush predators during the Oligocene alongside the hyaenodont creodonts (which similarly produced larger, more open-country forms at the start of the Oligocene). By the time Miocene epoch appeared, most if not all of the major lineages and families of carnivorans had diversified and become the most dominant group of large terrestrial predators in Eurasia and North America, with various lineages being successful in megafaunal faunivorous niches at different intervals during the Miocene and later epochs.\nSystematics.\nEvolution.\nThe order Carnivora belongs to a group of mammals known as Laurasiatheria, which also includes other groups such as bats and ungulates. Within this group the carnivorans are placed in the clade Ferae. Ferae includes the closest extant relative of carnivorans, the pangolins, as well as several extinct groups of mostly Paleogene carnivorous placentals such as the creodonts, the arctocyonians, and mesonychians. The creodonts were originally thought of as the sister taxon to the carnivorans, perhaps even ancestral to, based on the presence of the carnassial teeth, but the nature of the carnassial teeth is different between the two groups. In carnivorans the carnassials are positioned near the front of the molar row, while in the creodonts they are positioned near the back of the molar row, and this suggests a separate evolutionary history and an order-level distinction. In addition recent phylogenetic analysis suggests that creodonts are more closely related to pangolins while mesonychians might be the sister group to carnivorans and their stem-relatives.\nThe closest stem-carnivorans are the miacoids. The miacoids include the families Viverravidae and Miacidae, and together the Carnivora and Miacoidea form the stem-clade Carnivoramorpha. The miacoids were small, genet-like carnivoramorphs that occupy a variety of niches such as terrestrial and arboreal habitats. Recent studies have shown a supporting amount of evidence that Miacoidea is an evolutionary grade of carnivoramorphs that, while viverravids are monophyletic basal group, the miacids are paraphyletic in respect to Carnivora (as shown in the phylogeny below).\nCarnivoramorpha as a whole first appeared in the Paleocene of North America about 60\u00a0million years ago. Crown carnivorans first appeared around 42\u00a0million years ago in the Middle Eocene. Their molecular phylogeny shows the extant Carnivora are a monophyletic group, the crown group of the Carnivoramorpha. From there carnivorans have split into two clades based on the composition of the bony structures that surround the middle ear of the skull, the cat-like feliforms and the dog-like caniforms. In feliforms, the auditory bullae are double-chambered, composed of two bones joined by a septum. Caniforms have single-chambered or partially divided auditory bullae, composed of a single bone. Initially the early representatives of carnivorans were small as the creodonts (specifically, the oxyaenids) and mesonychians dominated the apex predator niches during the Eocene, but in the Oligocene carnivorans became a dominant group of apex predators with the nimravids, and by the Miocene most of the extant carnivoran families have diversified and become the primary terrestrial predators in the Northern Hemisphere.\nThe phylogenetic relationships of the carnivorans are shown in the following cladogram:\nClassification of the extant carnivorans.\nIn 1758 the Swedish botanist Carl Linnaeus placed all carnivorans known at the time into the group Ferae (not to be confused with the modern concept of Ferae which also includes pangolins) in the tenth edition of his book \"Systema Naturae\". He recognized six genera: \"Canis\" (canids and hyaenids), \"Phoca\" (pinnipeds), \"Felis\" (felids), \"Viverra\" (viverrids, herpestids, and mephitids), \"Mustela\" (non-badger mustelids), \"Ursus\" (ursids, large species of mustelids, and procyonids). It wasn't until 1821 that the English writer and traveler Thomas Edward Bowdich gave the group its modern and accepted name.\nInitially the modern concept of Carnivora was divided into two suborders: the terrestrial Fissipedia and the marine Pinnipedia. Below is the classification of how the extant families were related to each other after American paleontologist George Gaylord Simpson in 1945:\nSince then, however, the methods in which mammalogists use to assess the phylogenetic relationships among the carnivoran families has been improved with using more complicated and intensive incorporation of genetics, morphology and the fossil record. Research into Carnivora phylogeny since 1945 has found Fisspedia to be paraphyletic in respect to Pinnipedia, with pinnipeds being either more closely related to bears or to weasels. The small carnivoran families Viverridae, Procyonidae, and Mustelidae have been found to be polyphyletic: \nBelow is a table chart of the extant carnivoran families and number of extant species recognized by various authors of the first and fourth volumes of \"Handbook of the Mammals of the World\" published in 2009 and 2014 respectively:\nAnatomy.\nSkull.\nThe canine teeth are usually large and conical. The canines are thick and stress resistant. All of the terrestrial species of carnivorans have three incisors on each side of each jaw (the exception is the sea otter (\"Enhydra lutris\") which only has two lower incisor teeth). The third molar has been lost. The carnassial pair is made up of the fourth upper premolar and the first lower molar teeth. Like most mammals, the dentition is heterodont, though in some species, such as the aardwolf (\"Proteles cristata\"), the teeth have been greatly reduced and the cheek teeth are specialised for eating insects. In pinnipeds, the teeth are homodont as they have evolved to grasp or catch fish, and the cheek teeth are often lost. In bears and raccoons the carnassial pair is secondarily reduced. The skulls are heavily built with a strong zygomatic arch. Often a sagittal crest is present, sometimes more evident in sexually dimorphic species such as sea lions and fur seals, though it has also been greatly reduced in some small carnivorans. The braincase is enlarged with the frontoparietal bone at the front. In most species, the eyes are at the front of the face. In caniforms, the rostrum is usually long with many teeth, while in feliforms it is shorter with fewer teeth. The carnassial teeth of feliforms are generally more sectional than those of caniforms. The turbinates are large and complex in comparison to other mammals, providing a large surface area for olfactory receptors.\nPostcranial region.\nAside from an accumulation of characteristics in the dental and cranial features, not much of their overall anatomy unites carnivorans as a group. All species of carnivorans are quadrupedal and most have five digits on the front feet and four digits on the back feet. In terrestrial carnivorans, the feet have soft pads. The feet can either be digitigrade as seen in cats, hyenas and dogs or plantigrade as seen in bears, skunks, raccoons, weasels, civets and mongooses. In pinnipeds, the limbs have been modified into flippers.\nUnlike cetaceans and sirenians, which have fully functional tails to help them swim, pinnipeds use their limbs underwater to swim. Earless seals use their back flippers; sea lions and fur seals use their front flippers, and the walrus use all of their limbs. As a result, pinnipeds have significantly shorter tails than other carnivorans.\nAside from the pinnipeds, dogs, bears, hyenas, and cats all have distinct and recognizable appearances. Dogs are usually cursorial mammals and are gracile in appearance, often relying on their teeth to hold prey; bears are much larger and rely on their physical strength to forage for food. Compared to dogs and bears, cats have longer and stronger forelimbs armed with retractable claws to hold on to prey. Hyenas are dog-like feliforms that have sloping backs due to their front legs being longer than their hind legs. The raccoon family and red panda are small, bear-like carnivorans with long tails. The other small carnivoran families Nandiniidae, Prionodontidae, Viverridae, Herpestidae, Eupleridae, Mephitidae and Mustelidae have through convergent evolution maintained the small, ancestral appearance of the miacoids, though there is some variation seen such as the robust and stout physicality of badgers and the wolverine (\"Gulo gulo\"). Male carnivorans usually have bacula, though they are absent in hyenas and binturongs.\nThe length and density of the fur vary depending on the environment that the species inhabits. In warm climate species, the fur is often short in length and lighter. In cold climate species, the fur is either dense or long, often with an oily substance that helps to retain heat. The pelage coloration differs between species, often including black, white, orange, yellow, red, and many shades of grey and brown. Some are striped, spotted, blotched, banded, or otherwise boldly patterned. There seems to be a correlation between habitat and color pattern; for example spotted or banded species tend to be found in heavily forested environments. Some species like the grey wolf are polymorphic with different individual having different coat colors. The arctic fox (\"Vulpes lagopus\") and the stoat (\"Mustela erminea\") have fur that changes from white and dense in the winter to brown and sparse in the summer. In pinnipeds, polar bears, and sea otters a thick insulating layer of blubber helps maintain their body temperature.\nRelationship with humans.\nCarnivorans are arguably the group of mammals of most interest to humans. The dog is noteworthy for not only being the first species of carnivoran to be domesticated, but also the first species of any taxon. In the last 10,000 to 12,000 years humans have selectively bred dogs for a variety of different tasks and today there are well over 400 breeds. The cat is another domesticated carnivoran and it is today considered one of the most successful species on the planet, due to their close proximity to humans and the popularity of cats as pets. Many other species are popular, and they are often charismatic megafauna. Many civilizations have incorporated a species of carnivoran into their culture such as the lion, viewed as royalty. Yet many species such as wolves and the big cats have been broadly hunted, resulting in extirpation in some areas. Habitat loss and human encroachment as well as climate change have been the primary cause of many species going into decline. Four species of carnivorans have gone extinct since the 1600s: Falkland Island wolf (\"Dusicyon australis\") in 1876; the sea mink (\"Neogale macrodon\") in 1894; the Japanese sea lion (\"Zalophus japonicus\") in 1951 and the Caribbean monk seal (\"Neomonachus tropicalis\") in 1952. Some species such as the red fox (\"Vulpes vulpes\") and stoat (\"Mustela erminea\") have been introduced to Australasia and have caused many native species to become endangered or even extinct.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5222", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=5222", "title": "Colombia", "text": "Country in South America\nColombia (, ; ] ()), officially the Republic of Colombia, is a country mostly in South America with insular regions in North America. The Colombian mainland is bordered by the Caribbean Sea to the north, Venezuela to the east and northeast, Brazil to the southeast, Ecuador and Peru to the south and southwest, the Pacific Ocean to the west, and Panama to the northwest. Colombia is divided into 32 departments. The Capital District of Bogot\u00e1 is also the country's largest city hosting the main financial and cultural hub, and other major urbes include Medell\u00edn, Cali, Barranquilla, Cartagena, Santa Marta, C\u00facuta, Ibagu\u00e9, Villavicencio and Bucaramanga. It covers an area of 1,141,748 square kilometers (440,831 sq mi), and has a population of around 52 million. Colombia is the largest Spanish-speaking country in South America. Its cultural heritage\u2014including language, religion, cuisine, and art\u2014reflects its history as a colony, fusing cultural elements brought by immigration from Europe and the Middle East, with those brought by the African diaspora, as well as with those of the various Indigenous civilizations that predate colonization. Spanish is the Official language, although English and 64 other languages are recognized regionally.\nColombia has been home to many indigenous peoples and cultures since at least 12,000 BCE. The Spanish first landed in La Guajira in 1499, and by the mid-16th century they had colonized much of present-day Colombia, and established the New Kingdom of Granada, with Santa F\u00e9 de Bogot\u00e1 as its capital. Independence from the Spanish Empire was achieved in 1819, with what is now Colombia emerging as the United Provinces of New Granada. The new polity experimented with federalism as the Granadine Confederation (1858) and then the United States of Colombia (1863), before becoming a republic\u2014the current Republic of Colombia\u2014in 1886. With the backing of the United States and France, Panama seceded from Colombia in 1903, resulting in Colombia's present borders. Beginning in the 1960s, the country has suffered from an asymmetric low-intensity armed conflict and political violence, both of which escalated in the 1990s. Since 2005, there has been significant improvement in security, stability and rule of law, as well as unprecedented economic growth and development. Colombia is recognized for its health system, being the best healthcare in the Americas according to The World Health Organization and 22nd on the planet, In 2022, 26 Colombian hospitals were among the 61 best in Latin America (42% total). Also in 2023, two Colombian hospitals were among the Top 75 of the world.\nColombia is one of the world's seventeen megadiverse countries; it has the second-highest level of biodiversity in the world. Its territory encompasses Amazon rainforest, highlands, grasslands and deserts. It is the only country in South America with coastlines (and islands) along both the Atlantic and Pacific oceans. Colombia is a key member of major global and regional organizations including the UN, the WTO, the OECD, the OAS, the Pacific Alliance and the Andean Community; it is also a NATO Global Partner. Its diversified economy is the third-largest in South America, with macroeconomic stability and favorable long-term growth prospects.\nEtymology.\nThe name \"Colombia\" is derived from the last name of the Italian navigator Christopher Columbus (, , ). It was conceived as a reference to all of the New World. The name was later adopted by the Republic of Colombia of 1819, formed from the territories of the old Viceroyalty of New Granada (modern-day Colombia, Panama, Venezuela, Ecuador, and northwest Brazil).\nWhen Venezuela, Ecuador, and Cundinamarca came to exist as independent states, the former Department of Cundinamarca adopted the name \"Republic of New Granada\". New Granada officially changed its name in 1858 to the Granadine Confederation. In 1863 the name was again changed, this time to United States of Colombia, before finally adopting its present name \u2013 the Republic of Colombia \u2013 in 1886.\nTo refer to this country, the Colombian government uses the terms and .\nHistory.\nPre-Columbian era.\nOwing to its location, the present territory of Colombia was a corridor of early human civilization from Mesoamerica and the Caribbean to the Andes and Amazon basin. The oldest archaeological finds are from the Pubenza and El Totumo sites in the Magdalena Valley southwest of Bogot\u00e1. These sites date from the Paleoindian period (18,000\u20138000\u00a0BCE). At Puerto Hormiga and other sites, traces from the Archaic Period (~8000\u20132000\u00a0BCE) have been found. Vestiges indicate that there was also early occupation in the regions of El Abra and Tequendama in Cundinamarca. The oldest pottery discovered in the Americas, found at San Jacinto, dates to 5000\u20134000\u00a0BCE.\nIndigenous people inhabited the territory that is now Colombia by 12,500\u00a0BCE. Nomadic hunter-gatherer tribes at the El Abra, Tibit\u00f3 and Tequendama sites near present-day Bogot\u00e1 traded with one another and with other cultures from the Magdalena River Valley. A site including of pictographs that is under study at Serran\u00eda de la Lindosa was revealed in November 2020. Their age is suggested as being 12,500 years old (c. 10,480 B.C.) by the anthropologists working on the site because of extinct fauna depicted. That would have been during the earliest known human occupation of the area now known as Colombia.\nBetween 5000 and 1000\u00a0BCE, hunter-gatherer tribes transitioned to agrarian societies; fixed settlements were established, and pottery appeared. Beginning in the 1st millennium BCE, groups of Amerindians including the Muisca, Zen\u00fa, Quimbaya, and Tairona developed the political system of \"cacicazgos\" with a pyramidal structure of power headed by \"caciques\". The Muisca inhabited mainly the area of what is now the Departments of Boyac\u00e1 and Cundinamarca high plateau (\"Altiplano Cundiboyacense\") where they formed the Muisca Confederation. They farmed maize, potato, quinoa, and cotton, and traded gold, emeralds, blankets, ceramic handicrafts, coca and especially rock salt with neighboring nations. The Tairona inhabited northern Colombia in the isolated mountain range of Sierra Nevada de Santa Marta. The Quimbaya inhabited regions of the Cauca River Valley between the Western and Central Ranges of the Colombian Andes. Most of the Amerindians practiced agriculture and the social structure of each indigenous community was different. Some groups of indigenous people such as the Caribs lived in a state of permanent war, but others had less bellicose attitudes.\nColonial period.\nAlonso de Ojeda (who had sailed with Columbus) reached the Guajira Peninsula in 1499. Spanish explorers, led by Rodrigo de Bastidas, made the first exploration of the Caribbean coast in 1500. Christopher Columbus navigated near the Caribbean in 1502. In 1508, Vasco N\u00fa\u00f1ez de Balboa accompanied an expedition to the territory through the region of Gulf of Urab\u00e1 and they founded the town of Santa Mar\u00eda la Antigua del Dari\u00e9n in 1510, the first stable settlement on the continent. Santa Marta was founded in 1525, and Cartagena in 1533. Spanish conquistador Gonzalo Jim\u00e9nez de Quesada led an expedition to the interior in April 1536, and christened the districts through which he passed \"New Kingdom of Granada\". In August 1538, he founded provisionally its capital near the Muisca cacicazgo of Muyquyt\u00e1, and named it \"Santa Fe\". The name soon acquired a suffix and was called Santa Fe de Bogot\u00e1. Two other notable journeys by early conquistadors to the interior took place in the same period. Sebasti\u00e1n de Belalc\u00e1zar, conqueror of Quito, traveled north and founded Cali, in 1536, and Popay\u00e1n, in 1537; from 1536 to 1539, German conquistador Nikolaus Federmann crossed the Llanos Orientales and went over the Cordillera Oriental in a search for El Dorado, the \"city of gold\". The legend and the gold would play a pivotal role in luring the Spanish and other Europeans to New Granada during the 16th and 17th centuries.\nThe conquistadors made frequent alliances with the enemies of different indigenous communities. Indigenous allies were crucial to conquest, as well as to creating and maintaining empire. Indigenous peoples in New Granada experienced a decline in population due to conquest as well as Eurasian diseases, such as smallpox, to which they had no immunity. Regarding the land as deserted, the Spanish Crown sold properties to all persons interested in colonized territories, creating large farms and possession of mines. In the 16th century, the nautical science in Spain reached a great development thanks to numerous scientific figures of the Casa de Contrataci\u00f3n and nautical science was an essential pillar of the Iberian expansion. In 1542, the region of New Granada, along with all other Spanish possessions in South America, became part of the Viceroyalty of Peru, with its capital in Lima. In 1547, New Granada became a separate captaincy-general within the viceroyalty, with its capital at Santa Fe de Bogota. In 1549, the Royal Audiencia was created by a royal decree, and New Granada was ruled by the Royal Audience of Santa Fe de Bogot\u00e1, which at that time comprised the provinces of Santa Marta, Rio de San Juan, Popay\u00e1n, Guayana and Cartagena. But important decisions were taken from the colony to Spain by the Council of the Indies.\nIn the 16th century, European slave traders had begun to bring enslaved Africans to the Americas. Spain was the only European power that did not establish factories in Africa to purchase slaves; the Spanish Empire instead relied on the asiento system, awarding merchants from other European nations the license to trade enslaved peoples to their overseas territories. This system brought Africans to Colombia, although many spoke out against the institution. The indigenous peoples could not be enslaved because they were legally subjects of the Spanish Crown. To protect the indigenous peoples, several forms of land ownership and regulation were established by the Spanish colonial authorities: \"resguardos\", \"encomiendas\" and \"haciendas\".\nHowever, secret anti-Spanish discontentment was already brewing for Colombians since Spain prohibited direct trade between the Viceroyalty of Peru, which included Colombia, and the Viceroyalty of New Spain, which included the Philippines, the source of Asian products like silk and porcelain which was in demand in the Americas. Illegal trade between Peruvians, Filipinos, and Mexicans continued in secret, as smuggled Asian goods ended up in C\u00f3rdoba, Colombia, the distribution center for illegal Asian imports, due to the collusion between these peoples against the authorities in Spain. They settled and traded with each other while disobeying the forced Spanish monopoly.\nThe Viceroyalty of New Granada was established in 1717, then temporarily removed, and then re-established in 1739. Its capital was Santa F\u00e9 de Bogot\u00e1. This Viceroyalty included some other provinces of northwestern South America that had previously been under the jurisdiction of the Viceroyalties of New Spain or Peru and correspond mainly to today's Venezuela, Ecuador, and Panama. So, Bogot\u00e1 became one of the principal administrative centers of the Spanish possessions in the New World, along with Lima and Mexico City, though it remained somewhat backward compared to those two cities in several economic and logistical ways.\nGreat Britain declared war on Spain in 1739, and the city of Cartagena quickly became a top target for the British. A massive British expeditionary force was dispatched to capture the city, but after initial inroads devastating outbreaks of disease crippled their numbers and the British were forced to withdraw. The battle became one of Spain's most decisive victories in the conflict, and secured Spanish dominance in the Caribbean until the Seven Years' War. The 18th-century priest, botanist and mathematician Jos\u00e9 Celestino Mutis was delegated by Viceroy Antonio Caballero y G\u00f3ngora to conduct an inventory of the nature of New Granada. Started in 1783, this became known as the Royal Botanical Expedition to New Granada. It classified plants and wildlife, and founded the first astronomical observatory in the city of Santa Fe de Bogot\u00e1. In July 1801 the Prussian scientist Alexander von Humboldt reached Santa Fe de Bogot\u00e1 where he met with Mutis. In addition, historical figures in the process of independence in New Granada emerged from the expedition as the astronomer Francisco Jos\u00e9 de Caldas, the scientist Francisco Antonio Zea, the zoologist Jorge Tadeo Lozano and the painter Salvador Rizo.\nIndependence.\nSince the beginning of the periods of conquest and colonization, there were several rebel movements against Spanish rule, but most were either crushed or remained too weak to change the overall situation. The last one that sought outright independence from Spain sprang up around 1810 and culminated in the Colombian Declaration of Independence, issued on 20 July 1810, the day that is now celebrated as the nation's Independence Day. This movement followed the independence of St. Domingue (present-day Haiti) in 1804, which provided some support to an eventual leader of this rebellion: Sim\u00f3n Bol\u00edvar. Francisco de Paula Santander also would play a decisive role.\nA movement was initiated by Antonio Nari\u00f1o, who opposed Spanish centralism and led the opposition against the Viceroyalty. Cartagena became independent in November 1811. In 1811, the United Provinces of New Granada were proclaimed, headed by Camilo Torres Tenorio. The emergence of two distinct ideological currents among the patriots (federalism and centralism) gave rise to a period of instability. Shortly after the Napoleonic Wars ended, Ferdinand VII, recently restored to the throne in Spain, unexpectedly decided to send military forces to retake most of northern South America. The viceroyalty was restored under the command of Juan S\u00e1mano, whose regime punished those who participated in the patriotic movements, ignoring the political nuances of the juntas. The retribution stoked renewed rebellion, which, combined with a weakened Spain, made possible a successful rebellion led by the Venezuelan-born Sim\u00f3n Bol\u00edvar, who finally proclaimed independence in 1819. The pro-Spanish resistance was defeated in 1822 in the present territory of Colombia and in 1823 in Venezuela.\nThe territory of the Viceroyalty of New Granada became the Republic of Colombia, organized as a union of the current territories of Colombia, Panama, Ecuador, Venezuela, parts of Guyana and Brazil and north of Mara\u00f1\u00f3n River. The Congress of C\u00facuta in 1821 adopted a constitution for the new Republic. Sim\u00f3n Bol\u00edvar became the first President of Colombia, and Francisco de Paula Santander was made Vice President. However, the new republic was unstable and the Gran Colombia ultimately collapsed.\nModern Colombia comes from one of the countries that emerged after the dissolution of Gran Colombia, the other two being Ecuador and Venezuela. Colombia was the first constitutional government in South America, and the Liberal and Conservative parties, founded in 1848 and 1849, respectively, are two of the oldest surviving political parties in the Americas. Slavery was abolished in the country in 1851.\nInternal political and territorial divisions led to the dissolution of Gran Colombia in 1830. The so-called \"Department of Cundinamarca\" adopted the name \"New Granada\", which it kept until 1858 when it became the \"Confederaci\u00f3n Granadina\" (Granadine Confederation). After a two-year civil war in 1863, the \"United States of Colombia\" was created, lasting until 1886, when the country finally became known as the Republic of Colombia. Internal divisions remained between the bipartisan political forces, occasionally igniting very bloody civil wars, the most significant being the Thousand Days' War (1899\u20131902).\n20th century.\nThe United States of America's intentions to influence the area (especially the Panama Canal construction and control) led to the separation of the Department of Panama in 1903 and the establishment of it as a nation. The United States paid Colombia $25,000,000 in 1921, seven years after completion of the canal, for redress of President Roosevelt's role in the creation of Panama, and Colombia recognized Panama under the terms of the Thomson\u2013Urrutia Treaty. Colombia and Peru went to war because of territory disputes far in the Amazon basin. The war ended with a peace deal brokered by the League of Nations. The League finally awarded the disputed area to Colombia in June 1934.\nSoon after, Colombia achieved some degree of political stability, which was interrupted by a bloody conflict that took place between the late 1940s and the early 1950s, a period known as \"La Violencia\" (\"The Violence\"). Its cause was mainly mounting tensions between the two leading political parties, which subsequently ignited after the assassination of the Liberal presidential candidate Jorge Eli\u00e9cer Gait\u00e1n on 9 April 1948. The ensuing riots in Bogot\u00e1, known as El Bogotazo, spread throughout the country and claimed the lives of at least 180,000 Colombians.\nColombia entered the Korean War when Laureano G\u00f3mez was elected president. It was the only Latin American country to join the war in a direct military role as an ally of the United States. Particularly important was the resistance of the Colombian troops at Old Baldy.\nThe violence between the two political parties decreased first when Gustavo Rojas deposed the President of Colombia in a coup d'\u00e9tat and negotiated with the guerrillas, and then under the military junta of General Gabriel Par\u00eds.\nAfter Rojas' deposition, the Colombian Conservative Party and Colombian Liberal Party agreed to create the National Front, a coalition that would jointly govern the country. Under the deal, the presidency would alternate between conservatives and liberals every 4 years for 16 years; the two parties would have parity in all other elective offices. The National Front ended \"La Violencia\", and National Front administrations attempted to institute far-reaching social and economic reforms in cooperation with the Alliance for Progress. Despite the progress in certain sectors, many social and political problems continued, and guerrilla groups were formally created such as the FARC, the ELN and the M-19 to fight the government and political apparatus.\nSince the 1960s, the country has suffered from an asymmetric low-intensity armed conflict between government forces, leftist guerrilla groups and right wing paramilitaries. The conflict escalated in the 1990s, mainly in remote rural areas. Since the beginning of the armed conflict, human rights defenders have fought for the respect for human rights, despite staggering opposition. Several guerrillas' organizations decided to demobilize after peace negotiations in 1989\u20131994.\nThe United States has been heavily involved in the conflict since its beginnings, when in the early 1960s the U.S. government encouraged the Colombian military to attack leftist militias in rural Colombia. This was part of the U.S. fight against communism. Mercenaries and multinational corporations such as Chiquita Brands International are some of the international actors that have contributed to the violence of the conflict.\nBeginning in the mid-1970s Colombian drug cartels became major producers, processors and exporters of illegal drugs, primarily marijuana and cocaine.\nOn 4 July 1991, a new Constitution was promulgated. The changes generated by the new constitution are viewed as positive by Colombian society.\n21st century.\nThe administration of President \u00c1lvaro Uribe (2002\u20132010) adopted the democratic security policy which included an integrated counter-terrorism and counter-insurgency campaign. The government economic plan also promoted confidence in investors. As part of a controversial peace process, the AUC (right-wing paramilitaries) had ceased to function formally as an organization . In February 2008, millions of Colombians demonstrated against FARC and other outlawed groups.\nAfter peace negotiations in Cuba, the Colombian government of President Juan Manuel Santos and the guerrillas of the FARC-EP announced a final agreement to end the conflict. However, a referendum to ratify the deal was unsuccessful. Afterward, the Colombian government and the FARC signed a revised peace deal in November 2016, which the Colombian congress approved. In 2016, President Santos was awarded the Nobel Peace Prize. The Government began a process of attention and comprehensive reparation for victims of conflict. Colombia shows modest progress in the struggle to defend human rights, as expressed by HRW. A Special Jurisdiction of Peace has been created to investigate, clarify, prosecute and punish serious human rights violations and grave breaches of international humanitarian law which occurred during the armed conflict and to satisfy victims' right to justice. During his visit to Colombia, Pope Francis paid tribute to the victims of the conflict. \nIn June 2018, Ivan Duque, the candidate of the right-wing Democratic Center party, won the presidential election. On 7 August 2018, he was sworn in as the new President of Colombia to succeed Juan Manuel Santos. Colombia's relations with Venezuela have fluctuated due to ideological differences between the two governments. Colombia has offered humanitarian support with food and medicines to mitigate the shortage of supplies in Venezuela. Colombia's Foreign Ministry said that all efforts to resolve Venezuela's crisis should be peaceful. Colombia proposed the idea of the Sustainable Development Goals and a final document was adopted by the United Nations. In February 2019, Venezuelan president Nicol\u00e1s Maduro cut off diplomatic relations with Colombia after Colombian President Ivan Duque had helped Venezuelan opposition politicians deliver humanitarian aid to their country. Colombia recognized Venezuelan opposition leader Juan Guaid\u00f3 as the country's legitimate president. In January 2020, Colombia rejected Maduro's proposal that the two countries restore diplomatic relations.\nProtests started on 28 April 2021 when the government proposed a tax bill which would greatly expand the range of the 19 percent value-added tax. The 19 June 2022 election run-off vote ended in a win for former guerrilla, Gustavo Petro, taking 50.47% of the vote compared to 47.27% for independent candidate Rodolfo Hern\u00e1ndez. The single-term limit for the country's presidency prevented president Iv\u00e1n Duque from seeking re-election. On 7 August 2022, Petro was sworn in, becoming the country's first leftist president.\nGeography.\nThe geography of Colombia is characterized by its six main natural regions that present their own unique characteristics, from the Andes mountain range region shared with Ecuador and Venezuela; the Pacific Coastal region shared with Panama and Ecuador; the Caribbean coastal region shared with Venezuela and Panama; the \"Llanos\" (plains) shared with Venezuela; the Amazon rainforest region shared with Venezuela, Brazil, Peru and Ecuador; to the insular area, comprising islands in both the Atlantic and Pacific oceans. It shares its maritime limits with Costa Rica, Nicaragua, Honduras, Jamaica, Haiti, and the Dominican Republic.\nColombia is bordered to the northwest by Panama, to the east by Venezuela and Brazil, and to the south by Ecuador and Peru; it established its maritime boundaries with neighboring countries through seven agreements on the Caribbean Sea and three on the Pacific Ocean. It lies between latitudes 12\u00b0N and 4\u00b0S and between longitudes 67\u00b0 and 79\u00b0W.\nEast of the Andes lies the savanna of the \"Llanos\", part of the Orinoco River basin, and in the far southeast, the jungle of the Amazon rainforest. Together these lowlands make up over half Colombia's territory, but they contain less than 6% of the population. To the north the Caribbean coast, home to 21.9% of the population and the location of the major port cities of Barranquilla and Cartagena, generally consists of low-lying plains, but it also contains the Sierra Nevada de Santa Marta mountain range, which includes the country's tallest peaks (Pico Crist\u00f3bal Col\u00f3n and Pico Sim\u00f3n Bol\u00edvar), and the La Guajira Desert. By contrast the narrow and discontinuous Pacific coastal lowlands, backed by the Serran\u00eda de Baud\u00f3 mountains, are sparsely populated and covered in dense vegetation. The principal Pacific port is Buenaventura.\nPart of the Ring of Fire, a region of the world subject to earthquakes and volcanic eruptions, in the interior of Colombia the Andes are the prevailing geographical feature. Most of Colombia's population centers are located in these interior highlands. Beyond the Colombian Massif (in the southwestern departments of Cauca and Nari\u00f1o), these are divided into three branches known as \"cordilleras\" (mountain ranges): the Cordillera Occidental, running adjacent to the Pacific coast and including the city of Cali; the Cordillera Central, running between the Cauca and Magdalena River valleys (to the west and east, respectively) and including the cities of Medell\u00edn, Manizales, Pereira, and Armenia; and the Cordillera Oriental, extending northeast to the Guajira Peninsula and including Bogot\u00e1, Bucaramanga, and C\u00facuta. Peaks in the Cordillera Occidental exceed , and in the Cordillera Central and Cordillera Oriental they reach . At , Bogot\u00e1 is the highest city of its size in the world.\nThe main rivers of Colombia are Magdalena, Cauca, Guaviare, Atrato, Meta, Putumayo and Caquet\u00e1. Colombia has four main drainage systems: the Pacific drain, the Caribbean drain, the Orinoco Basin and the Amazon Basin. The Orinoco and Amazon Rivers mark limits with Colombia to Venezuela and Peru respectively.\nClimate.\nThe climate of Colombia is characterized for being tropical presenting variations within six natural regions and depending on the altitude, temperature, humidity, winds and rainfall. Colombia has a diverse range of climate zones, including tropical rainforests, savannas, steppes, deserts and mountain climates.\nMountain climate is one of the unique features of the Andes and other high altitude reliefs where climate is determined by elevation. Below in elevation is the warm altitudinal zone, where temperatures are above . About 82.5% of the country's total area lies in the warm altitudinal zone. The temperate climate altitudinal zone located between is characterized for presenting an average temperature ranging between . The cold climate is present between and the temperatures vary between . Beyond lies the alpine conditions of the forested zone and then the treeless grasslands of the p\u00e1ramos. Above , where temperatures are below freezing, the climate is glacial, a zone of permanent snow and ice.\nBiodiversity and conservation.\nColombia is one of the megadiverse countries in biodiversity, ranking first in bird species. Colombia is the country with the planet's highest biodiversity, having the highest rate of species by area as well as the largest number of endemisms (species that are not found naturally anywhere else) of any country. About 10% of the species of the Earth live in Colombia, including over 1,900 species of bird, more than in Europe and North America combined. Colombia has 10% of the world's mammals species, 14% of the amphibian species and 18% of the bird species of the world.\nAs for plants, the country has between 40,000 and 45,000 plant species, equivalent to 10 or 20% of total global species, which is even more remarkable given that Colombia is considered a country of intermediate size. Colombia is the second most biodiverse country in the world, lagging only after Brazil which is approximately 7 times bigger.\nColombia has about 2,000 species of marine fish and is the second most diverse country in freshwater fish. It is also the country with the most endemic species of butterflies, is first in orchid species, and has approximately 7,000 species of beetles. Colombia is second in the number of amphibian species and is the third most diverse country in reptiles and palms. There are about 1,900 species of mollusks and according to estimates there are about 300,000 species of invertebrates in the country. In Colombia there are 32 terrestrial biomes and 314 types of ecosystems.\nProtected areas and the \"National Park System\" cover an area of about and account for 12.77% of the Colombian territory. Compared to neighboring countries, rates of deforestation in Colombia are still relatively low. Colombia had a 2018 Forest Landscape Integrity Index mean score of 8.26/10, ranking it 25th globally out of 172 countries. Colombia is the sixth country in the world by magnitude of total renewable freshwater supply, and still has large reserves of freshwater.\nGovernment and politics.\nThe government of Colombia takes place within the framework of a presidential participatory democratic republic as established in the Constitution of 1991. In accordance with the principle of separation of powers, government is divided into three branches: the executive branch, the legislative branch and the judicial branch.\nAs the head of the executive branch, the President of Colombia serves as both head of state and head of government, followed by the Vice President and the Council of Ministers. The president is elected by popular vote to serve a single four-year term (In 2015, Colombia's Congress approved the repeal of a 2004 constitutional amendment that changed the one-term limit for presidents to a two-term limit). At the provincial level executive power is vested in department governors, municipal mayors and local administrators for smaller administrative subdivisions, such as \"corregimientos\" or \"comunas\". All regional elections are held one year and five months after the presidential election.\nThe legislative branch of government is represented nationally by the Congress, a bicameral institution comprising a 166-seat Chamber of Representatives and a 102-seat Senate. The Senate is elected nationally and the Chamber of Representatives is elected in electoral districts. Members of both houses are elected to serve four-year terms two months before the president, also by popular vote.\nThe judicial branch is headed by four high courts, consisting of the Supreme Court which deals with penal and civil matters, the Council of State, which has special responsibility for administrative law and also provides legal advice to the executive, the Constitutional Court, responsible for assuring the integrity of the Colombian constitution, and the Superior Council of Judicature, responsible for auditing the judicial branch. Colombia operates a system of civil law, which since 2005 has been applied through an adversarial system.\nDespite a number of controversies, the democratic security policy has ensured that former President \u00c1lvaro Uribe remained popular among Colombian people, with his approval rating peaking at 76%, according to a poll in 2009. However, having served two terms, he was constitutionally barred from seeking re-election in 2010. In the run-off elections on 20 June 2010 the former Minister of Defense Juan Manuel Santos won with 69% of the vote against the second most popular candidate, Antanas Mockus. A second round was required since no candidate received over the 50% winning threshold of votes. Santos won re-election with nearly 51% of the vote in second-round elections on 15 June 2014, beating right-wing rival \u00d3scar Iv\u00e1n Zuluaga, who won 45%. In 2018, Iv\u00e1n Duque won in the second round of the election with 54% of the vote, against 42% for his left-wing rival, Gustavo Petro. His term as Colombia's president ran for four years, beginning on 7 August 2018. In 2022, Colombia elected Gustavo Petro, who became its first leftist leader, and Francia Marquez, who was the first black person elected as vice president.\nForeign affairs.\nThe foreign affairs of Colombia are headed by the President, as head of state, and managed by the Minister of Foreign Affairs. Colombia has diplomatic missions in all continents.\nColombia was one of the four founding members of the Pacific Alliance, which is a political, economic and co-operative integration mechanism that promotes the free circulation of goods, services, capital and persons between the members, as well as a common stock exchange and joint embassies in several countries. Colombia is also a member of the United Nations, the World Trade Organization, the Organisation for Economic Co-operation and Development, the Organization of American States, the Organization of Ibero-American States, and the Andean Community of Nations. Colombia is a global partner of NATO.\nMilitary.\nThe executive branch of government is responsible for managing the defense of Colombia, with the President commander-in-chief of the armed forces. The Ministry of Defence exercises day-to-day control of the military and the Colombian National Police. Colombia has 455,461 active military personnel. In 2016, 3.4% of the country's GDP went towards military expenditure, placing it 24th in the world. Colombia's armed forces are the largest in Latin America, and it is the second largest spender on its military after Brazil. In 2018, Colombia signed the UN treaty on the Prohibition of Nuclear Weapons.\nThe Colombian military is divided into three branches: the National Army of Colombia; the Colombian Air Force; and the Colombian Navy. The National Police functions as a gendarmerie, operating independently from the military as the law enforcement agency for the entire country. Each of these operates with their own intelligence apparatus separate from the National Intelligence Directorate (DNI, in Spanish).\nThe National Army is formed by divisions, brigades, special brigades, and special units, the Colombian Navy by the Naval Infantry, the Naval Force of the Caribbean, the Naval Force of the Pacific, the Naval Force of the South, the Naval Force of the East, Colombia Coast Guards, Naval Aviation, and the Specific Command of San Andres y Providencia and the Air Force by 15 air units. The National Police has a presence in all municipalities.\nAdministrative divisions.\nColombia is divided into 32 departments and one capital district, which is treated as a department (Bogot\u00e1 also serves as the capital of the department of Cundinamarca). Departments are subdivided into municipalities, each of which is assigned a municipal seat, and municipalities are in turn subdivided into \"corregimientos\" in rural areas and into \"comunas\" in urban areas. Each department has a local government with a governor and assembly directly elected to four-year terms, and each municipality is headed by a mayor and council. There is a popularly elected local administrative board in each of the \"corregimientos\" or \"comunas\".\nIn addition to the capital, four other cities have been designated districts (in effect special municipalities), on the basis of special distinguishing features. These are Barranquilla, Cartagena, Santa Marta and Buenaventura. Some departments have local administrative subdivisions, where towns have a large concentration of population and municipalities are near each other (for example, in Antioquia and Cundinamarca). Where departments have a low population (for example Amazonas, Vaup\u00e9s and Vichada), special administrative divisions are employed, such as \"department \"corregimientos\"\", which are a hybrid of a municipality and a \"corregimiento\".\nEconomy.\nHistorically an agrarian economy, Colombia urbanized rapidly in the 20th century, by the end of which just 15.8% of the workforce were employed in agriculture, generating just 6.6% of GDP; 19.6% of the workforce were employed in industry and 64.6% in services, responsible for 33.4% and 59.9% of GDP respectively. The country's economic production is dominated by its strong domestic demand. Consumption expenditure by households is the largest component of GDP.\nColombia's market economy grew steadily in the latter part of the 20th century, with gross domestic product (GDP) increasing at an average rate of over 4% per year between 1970 and 1998. The country suffered a recession in 1999 (the first full year of negative growth since the Great Depression), and the recovery from that recession was long and painful. However, in recent years growth has been impressive, reaching 6.9% in 2007, one of the highest rates of growth in Latin America. According to International Monetary Fund estimates, in 2012, Colombia's GDP (PPP) was US$500\u00a0billion (28th in the world and third in South America).\nTotal government expenditures account for 27.9 percent of the domestic economy. External debt equals 39.9 percent of gross domestic product. A strong fiscal climate was reaffirmed by a boost in bond ratings. Annual inflation closed 2017 at 4.09% YoY (vs. 5.75% YoY in 2016). The average national unemployment rate in 2017 was 9.4%, although the informality is the biggest problem facing the labour market (the income of formal workers climbed 24.8% in 5 years while labor incomes of informal workers rose only 9%). Colombia has free-trade zones (FTZ), such as Zona Franca del Pacifico, located in the Valle del Cauca, one of the most striking areas for foreign investment.\nThe financial sector has grown favorably due to good liquidity in the economy, the growth of credit and the positive performance of the Colombian economy. The Colombian Stock Exchange through the Latin American Integrated Market (MILA) offers a regional market to trade equities. Colombia is now one of only three economies with a perfect score on the strength of legal rights index, according to the World Bank.\nColombia is rich in natural resources, and it is heavily dependent on energy and mining exports. Colombia's main exports include mineral fuels, oils, distillation products, fruit and other agricultural products, sugars and sugar confectionery, food products, plastics, precious stones, metals, forest products, chemical goods, pharmaceuticals, vehicles, electronic products, electrical equipment, perfumery and cosmetics, machinery, manufactured articles, textile and fabrics, clothing and footwear, glass and glassware, furniture, prefabricated buildings, military products, home and office material, construction equipment, software, among others. Principal trading partners are the United States, China, the European Union and some Latin American countries.\nNon-traditional exports have boosted the growth of Colombian foreign sales as well as the diversification of destinations of export thanks to new free trade agreements. Recent economic growth has led to a considerable increase of new millionaires, including the new entrepreneurs, Colombians with a net worth exceeding US$1\u00a0billion.\nIn 2017, however, the National Administrative Department of Statistics (DANE) reported that 26.9% of the population were living below the poverty line, of which 7.4% were in \"extreme poverty\". The multidimensional poverty rate stands at 17.0 percent of the population. The Government has also been developing a process of financial inclusion within the country's most vulnerable population.\nThe contribution of tourism to GDP was US$5,880.3bn (2.0% of total GDP) in 2016. Tourism generated 556,135 jobs (2.5% of total employment) in 2016. Foreign tourist visits were predicted to have risen from 0.6\u00a0million in 2007 to 4\u00a0million in 2017.\nAgriculture and natural resources.\nIn agriculture, Colombia is one of the 5 largest producers in the world of coffee, avocado and palm oil, and one of the 10 largest producers in the world of sugarcane, banana, pineapple and cocoa. The country also has considerable production of rice, potato and cassava. Although it is not the largest coffee producer in the world (since it is up to Brazil), the country has been able to carry out, for decades, a global marketing campaign to add value to the country's product. Colombian palm oil production is one of the most sustainable on the planet, compared to the largest existing producers. Colombia is also among the 20 largest producers in the world of beef and chicken meat. Colombia is also the 2nd largest flower exporter in the world, after the Netherlands.\nColombia is an important exporter of coal and petroleum \u2013 in 2020, more than 40% of the country's exports were based on these two products. In 2018 it was the 5th largest coal exporter in the world. In 2019, Colombia was the 20th largest petroleum producer in the world, with 791 thousand barrels/day, exporting a good part of its production \u2013 the country was the 19th largest oil exporter in the world in 2020. In mining, Colombia is the world's largest producer of emerald, and in the production of gold, between 2006 and 2017, the country produced 15 tons per year until 2007, when its production increased significantly, beating the record of 66.1 tons extracted in 2012. In 2017, it extracted 52.2 tons. Currently, the country is among the 25 largest gold producers in the world.\nEnergy and transportation.\nThe electricity production in Colombia comes mainly from Renewable energy sources. 69.93% is obtained from the hydroelectric generation. Colombia's commitment to renewable energy was recognized in the 2014 \"Global Green Economy Index (GGEI)\", ranking among the top 10 nations in the world in terms of greening efficiency sectors.\nTransportation in Colombia is regulated within the functions of the Ministry of Transport and entities such as the National Roads Institute (INV\u00cdAS) responsible for the Highways in Colombia, the Aerocivil, responsible for civil aviation and airports, the National Infrastructure Agency, in charge of concessions through public\u2013private partnerships, for the design, construction, maintenance, operation, and administration of the transport infrastructure, the General Maritime Directorate (Dimar) has the responsibility of coordinating maritime traffic control along with the Colombian Navy, among others and under the supervision of the Superintendency of Ports and Transport.\nIn 2021, Colombia had of roads, of which were paved. At the end of 2017, the country had around of duplicated highways. Rail transportation in Colombia is dedicated almost entirely to freight shipments and the railway network has a length of 1,700\u00a0km of potentially active rails. Colombia has 3,960 kilometers of gas pipelines, 4,900 kilometers of oil pipelines, and 2,990 kilometers of refined-products pipelines.\nThe target of Colombia's government is to build 7,000\u00a0km of roads for the 2016\u20132020 period and reduce travel times by 30 per cent and transport costs by 20 per cent. A toll road concession programme will comprise 40 projects, and is part of a larger strategic goal to invest nearly $50 bn in transport infrastructure, including: railway systems; making the Magdalena river navigable again; improving port facilities; as well as an expansion of Bogot\u00e1's airport. Colombia is a middle-income country.\nScience and technology.\nColombia has more than 3,950 research groups in science and technology. iNNpulsa, a government body that promotes entrepreneurship and innovation in the country, provides grants to startups, in addition to other services it and institutions provide. Colombia was ranked 67th in the Global Innovation Index in 2021. Co-working spaces have arisen to serve as communities for startups large and small. Organizations such as the Corporation for Biological Research (CIB) for the support of young people interested in scientific work has been successfully developed in Colombia. The International Center for Tropical Agriculture based in Colombia investigates the increasing challenge of global warming and food security.\nImportant inventions related to medicine have been made in Colombia, such as the first external artificial pacemaker with internal electrodes, invented by the electrical engineer Jorge Reynolds Pombo, an invention of great importance for those who suffer from heart failure. Also invented in Colombia were the microkeratome and keratomileusis technique, which form the fundamental basis of what now is known as LASIK (one of the most important techniques for the correction of refractive errors of vision) and the Hakim valve for the treatment of hydrocephalus. Colombia has begun to innovate in military technology for its army and other armies of the world; especially in the design and creation of personal ballistic protection products, military hardware, military robots, bombs, simulators and radar.\nSome leading Colombian scientists are Joseph M. Tohme, researcher recognized for his work on the genetic diversity of food, Manuel Elkin Patarroyo who is known for his groundbreaking work on synthetic vaccines for malaria, Francisco Lopera who discovered the \"Paisa Mutation\" or a type of early-onset Alzheimer's, Rodolfo Llin\u00e1s known for his study of the intrinsic neurons properties and the theory of a syndrome that had changed the way of understanding the functioning of the brain, Jairo Quiroga Puello recognized for his studies on the characterization of synthetic substances which can be used to fight fungus, tumors, tuberculosis and even some viruses and \u00c1ngela Restrepo who established accurate diagnoses and treatments to combat the effects of a disease caused by \"Paracoccidioides brasiliensis\".\nDemographics.\nWith an estimated 50\u00a0million people in 2020, Colombia is the third-most populous country in Latin America, after Brazil and Mexico. At the beginning of the 20th century, Colombia's population was approximately 4\u00a0million. Since the early 1970s Colombia has experienced steady declines in its fertility, mortality, and population growth rates. The population growth rate for 2016 is estimated to be 0.9%. About 26.8% of the population were 15 years old or younger, 65.7% were between 15 and 64 years old, and 7.4% were over 65 years old. The proportion of older persons in the total population has begun to increase substantially. Colombia is projected to have a population of 55.3\u00a0million by 2050.\nThe population is concentrated in the Andean highlands and along the Caribbean coast, also the population densities are generally higher in the Andean region. The nine eastern lowland departments, comprising about 54% of Colombia's area, have less than 6% of the population. Traditionally a rural society, movement to urban areas was very heavy in the mid-20th century, and Colombia is now one of the most urbanized countries in Latin America. The urban population increased from 31% of the total in 1938 to nearly 60% in 1973, and by 2014 the figure stood at 76%. The population of Bogot\u00e1 alone has increased from just over 300,000 in 1938 to approximately 8\u00a0million today. In total seventy-two cities now have populations of 100,000 or more (2015). As of 2012[ [update]] Colombia has the world's largest populations of internally displaced persons (IDPs), estimated to be up to 4.9\u00a0million people.\nThe life expectancy is 74.8 years in 2015 and infant mortality is 13.1 per thousand in 2016. In 2015, 94.58% of adults and 98.66% of youth are literate and the government spends about 4.49% of its GDP on education.\nLanguages.\nMore than 99.2% of Colombians speak Spanish, also called Castilian; 65 Amerindian languages, two Creole languages, the Romani language and Colombian Sign Language are also used in the country. English has official status in the archipelago of San Andr\u00e9s, Providencia and Santa Catalina.\nIncluding Spanish, a total of 101 languages are listed for Colombia in the Ethnologue database. The specific number of spoken languages varies slightly since some authors consider as different languages what others consider to be varieties or dialects of the same language. Best estimates recorded 71 languages that are spoken in-country today\u00a0\u2013 most of which belong to the Chibchan, Tucanoan, Bora\u2013Witoto, Guajiboan, Arawakan, Cariban, Barbacoan, and Saliban language families. There are currently about 850,000 speakers of native languages.\nEthnic groups.\n Human biological diversity and ethnicity-2018 Census \n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0 White and Mestizo\n (87.58%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Afro-Colombian (includes Mixed)\n (6.68%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Amerindian\n (4.31%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Not Stated (1.35%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Raizal\n (0.06%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Palenquero\n (0.02%)&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Romani\n (0.01%)\nColombia is ethnically diverse, its people descending from the original Amerindian inhabitants, Spanish colonists, Africans originally brought to the country as slaves, and 20th-century immigrants from Europe and the Middle East, all contributing to a diverse cultural heritage. The demographic distribution reflects a pattern that is influenced by colonial history. Whites live all throughout the country, mainly in urban centers and the burgeoning highland and coastal cities. The populations of the major cities also include mestizos. Mestizo \"campesinos\" (people living in rural areas) also live in the Andean highlands where some Spanish conquerors mixed with the women of Amerindian chiefdoms. Mestizos include artisans and small tradesmen that have played a major part in the urban expansion of recent decades.\nThe 2018 census reported that the \"non-ethnic population\", consisting of whites and mestizos (those of mixed European and Amerindian ancestry), constituted 87.6% of the national population. 6.7% is of African ancestry. Indigenous Amerindians constitute 4.3% of the population. Raizal people constitute 0.06% of the population. Palenquero people constitute 0.02% of the population. 0.01% of the population are Roma.\nThe Federal Research Division estimated that the 86% of the population that did not consider themselves part of one of the ethnic groups indicated by the 2006 census was divided into 49% Mestizo or of mixed European and Amerindian ancestry, and 37% White, mainly of Spanish lineage, but there is also a large population of Middle East descent; in some sectors of society there is a considerable input of German and Italian ancestry.\nMany of the Indigenous peoples experienced a reduction in population during the Spanish rule and many others were absorbed into the mestizo population, but the remainder currently represents over eighty distinct cultures. Reserves (\"resguardos\") established for indigenous peoples occupy (27% of the country's total) and are inhabited by more than 800,000 people. Some of the largest indigenous groups are the Wayuu, the Paez, the Pastos, the Ember\u00e1 and the Zen\u00fa. The departments of La Guajira, Cauca, Nari\u00f1o, C\u00f3rdoba and Sucre have the largest indigenous populations.\nThe Organizaci\u00f3n Nacional Ind\u00edgena de Colombia (ONIC), founded at the first National Indigenous Congress in 1982, is an organization representing the indigenous peoples of Colombia. In 1991, Colombia signed and ratified the current international law concerning indigenous peoples, Indigenous and Tribal Peoples Convention, 1989.\nSub-Saharan Africans were brought as slaves, mostly to the coastal lowlands, beginning early in the 16th century and continuing into the 19th century. Large Afro-Colombian communities are found today on the Pacific Coast. Numerous Jamaicans migrated mainly to the islands of San Andres and Providencia. A number of other Europeans and North Americans migrated to the country in the late 19th and early 20th centuries, including people from the former USSR during and after the Second World War.\nMany immigrant communities have settled on the Caribbean coast, in particular recent immigrants from the Middle East and Europe. Barranquilla (the largest city of the Colombian Caribbean) and other Caribbean cities have the largest populations of Lebanese, Palestinian, and other Levantines. There are also important communities of Romanis and Jews. There is a major migration trend of Venezuelans, due to the political and economic situation in Venezuela. In August 2019, Colombia offered citizenship to more than 24,000 children of Venezuelan refugees who were born in Colombia.\nReligion.\nThe National Administrative Department of Statistics (DANE) does not collect religious statistics, and accurate reports are difficult to obtain. However, based on various studies and a survey, about 90% of the population adheres to Christianity, the majority of which (70.9%\u201379%) are Roman Catholic, while a significant minority (16.7%) adhere to Protestantism (primarily Evangelicalism). Some 4.7% of the population is atheist or agnostic, while 3.5% claim to believe in God but do not follow a specific religion. 1.8% of Colombians adhere to Jehovah's Witnesses and Adventism and less than 1% adhere to other religions, such as the Bah\u00e1\u02bc\u00ed Faith, Islam, Judaism, Buddhism, Mormonism, Hinduism, Indigenous religions, Hare Krishna movement, Rastafari movement, Eastern Orthodox Church, and spiritual studies. The remaining people either did not respond or replied that they did not know. In addition to the above statistics, 35.9% of Colombians reported that they did not practice their faith actively.\nWhile Colombia remains a mostly Roman Catholic country by baptism numbers, the 1991 Colombian constitution guarantees freedom of religion and all religious faiths and churches are equally free before the law.\nHealth.\nThe overall life expectancy in Colombia at birth is 79.3 years (76.7 years for males and 81.9 years for females). Healthcare reforms have led to massive improvements in the healthcare systems of the country, with health standards in Colombia improving very much since the 1980s. The new system has widened population coverage by the social and health security system from 21% (pre-1993) to 96% in 2012.\nA study conducted by \"Am\u00e9rica Econom\u00eda\" magazine ranked 21 Colombian health care institutions among the top 44 in Latin America, amounting to 48 percent of the total. In 2017, the government declared a cancer research and treatment center as a Project of National Strategic Interest.\nEducation.\nThe educational experience of many Colombian children begins with attendance at a preschool academy until age five (\"Educaci\u00f3n preescolar\"). Basic education (\"Educaci\u00f3n b\u00e1sica\") is compulsory by law. It has two stages: Primary basic education (\"Educaci\u00f3n b\u00e1sica primaria\") which goes from first to fifth grade \u2013 children from six to ten years old, and Secondary basic education (\"Educaci\u00f3n b\u00e1sica secundaria\"), which goes from sixth to ninth grade. Basic education is followed by Middle vocational education (\"Educaci\u00f3n media vocacional\") that comprises the tenth and eleventh grades. It may have different vocational training modalities or specialties (academic, technical, business, and so on.) according to the curriculum adopted by each school.\nAfter the successful completion of all the basic and middle education years, a high-school diploma is awarded. The high-school graduate is known as a \"bachiller\", because secondary basic school and middle education are traditionally considered together as a unit called \"bachillerato\" (sixth to eleventh grade). Students in their final year of middle education take the ICFES test (now renamed Saber 11) to gain access to higher education (\"Educaci\u00f3n superior\"). This higher education includes undergraduate professional studies, technical, technological and intermediate professional education, and post-graduate studies. Technical professional institutions of Higher Education are also opened to students holder of a qualification in Arts and Business. This qualification is usually awarded by the SENA after a two years curriculum.\n\"Bachilleres\" (high-school graduates) may enter into a professional undergraduate career program offered by a university; these programs last up to five years (or less for technical, technological and intermediate professional education, and post-graduate studies), even as much to six to seven years for some careers, such as medicine. In Colombia, there is not an institution such as college; students go directly into a career program at a university or any other educational institution to obtain a professional, technical or technological title. Once graduated from the university, people are granted a (professional, technical or technological) diploma and licensed (if required) to practice the career they have chosen. For some professional career programs, students are required to take the Saber-Pro test, in their final year of undergraduate academic education.\nPublic spending on education as a proportion of gross domestic product in 2015 was 4.49%. This represented 15.05% of total government expenditure. The primary and secondary gross enrolment ratios stood at 113.56% and 98.09% respectively. School-life expectancy was 14.42 years. A total of 94.58% of the population aged 15 and older were recorded as literate, including 98.66% of those aged 15\u201324.\nUrbanization.\nColombia is a highly urbanized country with 77.1% of the population living in urban areas. The largest cities in the country are Bogot\u00e1, with 7,387,400 inhabitants, Medell\u00edn, with 2,382,399 inhabitants, Cali, with 2,172,527 inhabitants, and Barranquilla, with 1,205,284 inhabitants.\n&lt;templatestyles src=\"Template:Largest_cities/styles.css\" /&gt;\nCulture.\nColombia lies at the crossroads of Latin America and the broader American continent, and as such has been hit by a wide range of cultural influences. Native American, Spanish and other European, African, American, Caribbean, and Middle Eastern influences, as well as other Latin American cultural influences, are all present in Colombia's modern culture. Urban migration, industrialization, globalization, and other political, social and economic changes have also left an impression.\nMany national symbols, both objects and themes, have arisen from Colombia's diverse cultural traditions and aim to represent what Colombia, and the Colombian people, have in common. Cultural expressions in Colombia are promoted by the government through the Ministry of Culture.\nLiterature.\nColombian literature dates back to pre-Columbian era; a notable example of the period is the epic poem known as the \"Legend of Yurupary\". In Spanish colonial times, notable writers include Juan de Castellanos (\"Eleg\u00edas de varones ilustres de Indias\"), Hernando Dom\u00ednguez Camargo and his epic poem to San Ignacio de Loyola, Pedro Sim\u00f3n, Juan Rodr\u00edguez Freyle (\"El Carnero\"), Lucas Fern\u00e1ndez de Piedrahita, and the nun Francisca Josefa de Castillo, representative of mysticism.\nPost-independence literature linked to Romanticism highlighted Antonio Nari\u00f1o, Jos\u00e9 Fern\u00e1ndez Madrid, Camilo Torres Tenorio and Francisco Antonio Zea. In the second half of the nineteenth century and early twentieth century the literary genre known as \"costumbrismo\" became popular; great writers of this period were Tom\u00e1s Carrasquilla, Jorge Isaacs and Rafael Pombo (the latter of whom wrote notable works of children's literature). Within that period, authors such as Jos\u00e9 Asunci\u00f3n Silva, Jos\u00e9 Eustasio Rivera, Le\u00f3n de Greiff, Porfirio Barba-Jacob and Jos\u00e9 Mar\u00eda Vargas Vila developed the modernist movement. In 1872, Colombia established the Colombian Academy of Language, the first Spanish language academy in the Americas. Candelario Obeso wrote the groundbreaking \"Cantos Populares de mi Tierra\" (1877), the first book of poetry by an Afro-Colombian author.\nBetween 1939 and 1940 seven books of poetry were published under the name \"Stone and Sky\" in the city of Bogot\u00e1 that significantly influenced the country; they were edited by the poet Jorge Rojas. In the following decade, Gonzalo Arango founded the movement of \"nothingness\" in response to the violence of the time; he was influenced by nihilism, existentialism, and the thought of another great Colombian writer: Fernando Gonz\u00e1lez Ochoa. During the boom in Latin American literature, successful writers emerged, led by Nobel laureate Gabriel Garc\u00eda M\u00e1rquez and his magnum opus, \"One Hundred Years of Solitude\", Eduardo Caballero Calder\u00f3n, Manuel Mej\u00eda Vallejo, and \u00c1lvaro Mutis, a writer who was awarded the Cervantes Prize and the Prince of Asturias Award for Letters. Other leading contemporary authors are Fernando Vallejo, William Ospina (R\u00f3mulo Gallegos Prize) and Germ\u00e1n Castro Caycedo.\nVisual arts.\nColombian art has over 3,000 years of history. Colombian artists have captured the country's changing political and cultural backdrop using a range of styles and mediums. There is archeological evidence of ceramics being produced earlier in Colombia than anywhere else in the Americas, dating as early as 3,000\u00a0BCE.\nThe earliest examples of gold craftsmanship have been attributed to the Tumaco people of the Pacific coast and date to around 325\u00a0BCE. Roughly between 200\u00a0BCE and 800 CE, the San Agust\u00edn culture, masters of stonecutting, entered its \"classical period\". They erected raised ceremonial centers, sarcophagi, and large stone monoliths depicting anthropomorphic and zoomorphic forms out of stone.\nColombian art has followed the trends of the time, so during the 16th to 18th centuries, Spanish Catholicism had a huge influence on Colombian art, and the popular baroque style was replaced with rococo when the Bourbons ascended to the Spanish crown. More recently, Colombian artists Pedro Nel G\u00f3mez and Santiago Mart\u00ednez Delgado started the Colombian Murial Movement in the 1940s, featuring the neoclassical features of Art Deco.\nSince the 1950s, the Colombian art started to have a distinctive point of view, reinventing traditional elements under the concepts of the 20th century. Examples of this are the Greiff portraits by Ignacio G\u00f3mez Jaramillo, showing what the Colombian art could do with the new techniques applied to typical Colombian themes. Carlos Correa, with his paradigmatic \"Naturaleza muerta en silencio\" (silent dead nature), combines geometrical abstraction and cubism. Alejandro Obreg\u00f3n is often considered as the father of modern Colombian painting, and one of the most influential artist in this period, due to his originality, the painting of Colombian landscapes with symbolic and expressionist use of animals, (specially the Andean condor). Fernando Botero, Omar Rayo, Enrique Grau, \u00c9dgar Negret, David Manzur, Rodrigo Arenas Betancourt, Oscar Murillo, Doris Salcedo and Oscar Mu\u00f1oz are some of the Colombian artists featured at the international level.\nThe Colombian sculpture from the sixteenth to 18th centuries was mostly devoted to religious depictions of ecclesiastic art, strongly influenced by the Spanish schools of sacred sculpture. During the early period of the Colombian republic, the national artists were focused in the production of sculptural portraits of politicians and public figures, in a plain neoclassicist trend. During the 20th century, the Colombian sculpture began to develop a bold and innovative work with the aim of reaching a better understanding of national sensitivity.\nColombian photography was marked by the arrival of the daguerreotype. Jean-Baptiste Louis Gros was who brought the daguerreotype process to Colombia in 1841. The Piloto public library has Latin America's largest archive of negatives, containing 1.7\u00a0million antique photographs covering Colombia 1848 until 2005.\nThe Colombian press has promoted the work of the cartoonists. In recent decades, fanzines, internet and independent publishers have been fundamental to the growth of the comic in Colombia.\nArchitecture.\nThroughout the times, there have been a variety of architectural styles, from those of indigenous peoples to contemporary ones, passing through colonial (military and religious), Republican, transition and modern styles.\nAncient habitation areas, longhouses, crop terraces, roads as the Inca road system, cemeteries, hypogeums and necropolises are all part of the architectural heritage of indigenous peoples. Some prominent indigenous structures are the preceramic and ceramic archaeological site of Tequendama, Tierradentro (a park that contains the largest concentration of pre-Columbian monumental shaft tombs with side chambers), the largest collection of religious monuments and megalithic sculptures in South America, located in San Agust\u00edn, Huila, Lost city (an archaeological site with a series of terraces carved into the mountainside, a net of tiled roads, and several circular plazas), and the large villages mainly built with stone, wood, cane, and mud.\nArchitecture during the period of conquest and colonization is mainly derived of adapting European styles to local conditions, and Spanish influence, especially Andalusian and Extremaduran, can be easily seen. When Europeans founded cities two things were making simultaneously: the dimensioning of geometrical space (town square, street), and the location of a tangible point of orientation. The construction of forts was common throughout the Caribbean and in some cities of the interior, because of the dangers posed to Spanish colonial settlements from English, French and Dutch pirates and hostile indigenous groups. Churches, chapels, schools, and hospitals belonging to religious orders have a great urban influence. Baroque architecture is used in military buildings and public spaces. Marcelino Arroyo, Francisco Jos\u00e9 de Caldas and Domingo de Petr\u00e9s were great representatives of neo-classical architecture.\nThe National Capitol is a great representative of romanticism. Wood was extensively used in doors, windows, railings, and ceilings during the colonization of Antioquia. The Caribbean architecture acquires a strong Arabic influence. The Teatro Col\u00f3n in Bogot\u00e1 is a lavish example of architecture from the 19th century. The quintas houses with innovations in the volumetric conception are some of the best examples of the Republican architecture; the Republican action in the city focused on the design of three types of spaces: parks with forests, small urban parks and avenues and the Gothic style was most commonly used for the design of churches.\nDeco style, modern neoclassicism, eclecticism folklorist and art deco ornamental resources significantly influenced the architecture of Colombia, especially during the transition period. Modernism contributed with new construction technologies and new materials (steel, reinforced concrete, glass and synthetic materials) and the topology architecture and lightened slabs system also have a great influence. The most influential architects of the modern movement were Rogelio Salmona and Fernando Mart\u00ednez Sanabria.\nThe contemporary architecture of Colombia is designed to give greater importance to the materials, this architecture takes into account the specific natural and artificial geographies and is also an architecture that appeals to the senses. The conservation of the architectural and urban heritage of Colombia has been promoted in recent years.\nMusic.\nColombia has a vibrant collage of talent that touches a full spectrum of rhythms. Musicians, composers, music producers and singers from Colombia are recognized internationally such as Shakira, Juanes, Carlos Vives and others. Colombian music blends European-influenced guitar and song structure with large gaita flutes and percussion instruments from the indigenous population, while its percussion structure and dance forms come from Africa. Colombia has a diverse and dynamic musical environment.\nGuillermo Uribe Holgu\u00edn, an important cultural figure in the National Symphony Orchestra of Colombia, Luis Antonio Calvo and Blas Emilio Atehort\u00faa are some of the greatest exponents of the art music. The Bogot\u00e1 Philharmonic Orchestra is one of the most active orchestras in Colombia.\nCaribbean music has many vibrant rhythms, such as cumbia (it is played by the maracas, the drums, the gaitas and guacharaca), porro (it is a monotonous but joyful rhythm), mapal\u00e9 (with its fast rhythm and constant clapping) and the \"vallenato\", which originated in the northern part of the Caribbean coast (the rhythm is mainly played by the caja, the guacharaca, and accordion).\nThe music from the Pacific coast, such as the currulao, is characterized by its strong use of drums (instruments such as the native marimba, the conunos, the bass drum, the side drum, and the cuatro guasas or tubular rattle). An important rhythm of the south region of the Pacific coast is the contradanza (it is used in dance shows due to the striking colours of the costumes). Marimba music, traditional chants and dances from the Colombia South Pacific region are on UNESCO's Representative List of the Intangible Cultural Heritage of Humanity.\nImportant musical rhythms of the Andean Region are the danza (dance of Andean folklore arising from the transformation of the European contredance), the bambuco (it is played with guitar, tiple and mandolin, the rhythm is danced by couples), the pasillo (a rhythm inspired by the Austrian waltz and the Colombian \"danza\", the lyrics have been composed by well-known poets), the guabina (the tiple, the bandola and the requinto are the basic instruments), the sanjuanero (it originated in Tolima and Huila Departments, the rhythm is joyful and fast). Apart from these traditional rhythms, salsa music has spread throughout the country, and the city of Cali is considered by many salsa singers to be 'The New Salsa Capital of the World'.\nThe instruments that distinguish the music of the Eastern Plains are the harp, the cuatro (a type of four-stringed guitar) and maracas. Important rhythms of this region are the joropo (a fast rhythm and there is also tapping as a result of its flamenco ancestry) and the galeron (it is heard a lot while cowboys are working).\nThe music of the Amazon region is strongly influenced by the indigenous religious practices. Some of the musical instruments used are the manguar\u00e9 (a musical instrument of ceremonial type, consisting of a pair of large cylindrical drums), the quena (melodic instrument), the rondador, the congas, bells, and different types of flutes.\nThe music of the Archipelago of San Andr\u00e9s, Providencia and Santa Catalina is usually accompanied by a mandolin, a tub-bass, a jawbone, a guitar and maracas. Some popular archipelago rhythms are the Schottische, the Calypso, the Polka and the Mento.\nPopular culture.\nTheater was introduced in Colombia during the Spanish colonization in 1550 through zarzuela companies. Colombian theater is supported by the Ministry of Culture and a number of private and state owned organizations. The Ibero-American Theater Festival of Bogot\u00e1 is the cultural event of the highest importance in Colombia and one of the biggest theater festivals in the world. Other important theater events are: The Festival of Puppet The Fanfare (Medell\u00edn), The Manizales Theater Festival, The Caribbean Theatre Festival (Santa Marta) and The Art Festival of Popular Culture \"Cultural Invasion\" (Bogot\u00e1).\nAlthough the Colombian cinema is young as an industry, more recently the film industry was growing with support from the Film Act passed in 2003. Many film festivals take place in Colombia, but the two most important are the Cartagena Film Festival, which is the oldest film festival in Latin America, and the Bogot\u00e1 Film Festival.\nSome important national circulation newspapers are \"El Tiempo\" and \"El Espectador\". Television in Colombia has two privately owned TV networks and three state-owned TV networks with national coverage, as well as six regional TV networks and dozens of local TV stations. Private channels, RCN and Caracol are the highest-rated. The regional channels and regional newspapers cover a department or more and its content is made in these particular areas.\nColombia has three major national radio networks: Radiodifusora Nacional de Colombia, a state-run national radio; Caracol Radio and RCN Radio, privately owned networks with hundreds of affiliates. There are other national networks, including Cadena Super, Todelar, and Colmundo. Many hundreds of radio stations are registered with the Ministry of Information Technologies and Communications.\nCuisine.\nColombia's varied cuisine is influenced by its diverse fauna and flora as well as the cultural traditions of the ethnic groups. Colombian dishes and ingredients vary widely by region. Some of the most common ingredients are: cereals such as rice and maize; tubers such as potato and cassava; assorted legumes; meats, including beef, chicken, pork and goat; fish; and seafood. Colombia cuisine also features a variety of tropical fruits such as cape gooseberry, feijoa, araz\u00e1, dragon fruit, mangostino, granadilla, papaya, guava, mora (blackberry), lulo, soursop and passionfruit. Colombia is one of the world's largest consumers of fruit juices.\nAmong the most representative appetizers and soups are patacones (fried green plantains), sancocho de gallina (chicken soup with root vegetables) and ajiaco (potato and corn soup). Representative snacks and breads are pandebono, arepas (corn cakes), aborrajados (fried sweet plantains with cheese), torta de choclo, empanadas and almoj\u00e1banas. Representative main courses are bandeja paisa, lechona tolimense, mamona, tamales and fish dishes (such as arroz de lisa), especially in coastal regions where kibbeh, suero, coste\u00f1o cheese and carima\u00f1olas are also eaten. Representative side dishes are papas chorreadas (potatoes with cheese), remolachas rellenas con huevo duro (beets stuffed with hard-boiled egg) and arroz con coco (coconut rice). Organic food is a current trend in big cities, although in general across the country the fruits and veggies are very natural and fresh.\nRepresentative desserts are bu\u00f1uelos, natillas, Maria Luisa cake, bocadillo made of guayaba (guava jelly), cocadas (coconut balls), casquitos de guayaba (candied guava peels), torta de natas, obleas, flan de mango, rosc\u00f3n, milhoja, manjar blanco, dulce de feijoa, dulce de papayuela, torta de mojic\u00f3n, and esponjado de curuba. Typical sauces (salsas) are hogao (tomato and onion sauce) and Colombian-style aj\u00ed.\nSome representative beverages are coffee (Tinto), champ\u00fas, cholado, lulada, avena colombiana, sugarcane juice, aguapanela, aguardiente, hot chocolate and fresh fruit juices (often made with water or milk).\nSports.\nTejo is Colombia's national sport and is a team sport that involves launching projectiles to hit a target. But of all sports in Colombia, football is the most popular. Colombia was the champion of the 2001 Copa Am\u00e9rica, in which they set a new record of being undefeated, conceding no goals and winning each match. Colombia has been awarded \"mover of the year\" twice.\nColombia is a hub for roller skaters. The national team is a perennial powerhouse at the World Roller Speed Skating Championships. Colombia has traditionally been very good in cycling and a large number of Colombian cyclists have triumphed in major competitions of cycling.\nBaseball is popular in cities like Cartagena and Barranquilla. Of those cities have come good players like: Orlando Cabrera, \u00c9dgar Renter\u00eda, who was champion of the World Series in 1997 and 2010 and others who have played in Major League Baseball. Colombia was world amateur champion in 1947 and 1965.\nBoxing is one of the sports that has produced more world champions for Colombia.\nMotorsports also occupies an important place in the sporting preferences of Colombians; Juan Pablo Montoya is a race car driver known for winning 7 Formula One events. Colombia also has excelled in sports such as BMX, judo, shooting sport, taekwondo, wrestling, high diving and athletics, also has a long tradition in weightlifting and bowling.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5224", "revid": "46107994", "url": "https://en.wikipedia.org/wiki?curid=5224", "title": "Citizen Kane", "text": "1941 drama film by Orson Welles\nCitizen Kane is a 1941 American drama film directed by, produced by, and starring Orson Welles. He also co-wrote the screenplay with Herman J. Mankiewicz. The picture was Welles' first feature film. \"Citizen Kane\" is frequently cited as the greatest film ever made. For 50 consecutive years, it stood at number 1 in the British Film Institute's \"Sight &amp; Sound\" decennial poll of critics, and it topped the American Film Institute's 100 Years\u00a0... 100 Movies list in 1998, as well as its 2007 update. The film was nominated for Academy Awards in nine categories and it won for Best Writing (Original Screenplay) by Mankiewicz and Welles. \"Citizen Kane\" is praised for Gregg Toland's cinematography, Robert Wise's editing, Bernard Herrmann's music, and its narrative structure, all of which have been considered innovative and precedent-setting.\nThe quasi-biographical film examines the life and legacy of Charles Foster Kane, played by Welles, a composite character based on American media barons William Randolph Hearst and Joseph Pulitzer, Chicago tycoons Samuel Insull and Harold McCormick, as well as aspects of the screenwriters' own lives. Upon its release, Hearst prohibited the film from being mentioned in his newspapers.\nAfter the Broadway success of Welles's Mercury Theatre and the controversial 1938 radio broadcast \"The War of the Worlds\" on \"The Mercury Theatre on the Air\", Welles was courted by Hollywood. He signed a contract with RKO Pictures in 1939. Although it was unusual for an untried director, he was given freedom to develop his own story, to use his own cast and crew, and to have final cut privilege. Following two abortive attempts to get a project off the ground, he wrote the screenplay for \"Citizen Kane\", collaborating with Herman J. Mankiewicz. Principal photography took place in 1940, the same year its innovative trailer was shown, and the film was released in 1941.\nAlthough it was a critical success, \"Citizen Kane\" failed to recoup its costs at the box office. The film faded from view after its release, but it returned to public attention when it was praised by French critics such as Andr\u00e9 Bazin and re-released in 1956. In 1958, the film was voted number 9 on the prestigious Brussels 12 list at the 1958 World Expo. \"Citizen Kane\" was selected by the Library of Congress as an inductee of the 1989 inaugural group of 25 films for preservation in the United States National Film Registry for being \"culturally, historically, or aesthetically significant\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nPlot.\nIn a mansion called Xanadu, part of a vast palatial estate in Florida, the elderly Charles Foster Kane is on his deathbed. Holding a snow globe, he utters his last word, \"Rosebud\", and dies. A newsreel obituary tells the life story of Kane, an enormously wealthy newspaper publisher and industrial magnate. Kane's death becomes sensational news around the world, and the newsreel's producer tasks reporter Jerry Thompson with discovering the meaning of \"Rosebud\".\nThompson sets out to interview Kane's friends and associates. He tries to approach his second wife, Susan Alexander Kane, now an alcoholic who runs her own nightclub, but she refuses to talk to him. Thompson goes to the private archive of the late banker Walter Parks Thatcher. Through Thatcher's written memoirs, Thompson learns about Kane's rise from a Colorado boarding house and the decline of his personal fortune.\nIn 1871, gold was discovered through a mining deed belonging to Kane's mother, Mary Kane. She hired Thatcher to establish a trust that would provide for Kane's education and to assume guardianship of him. While the parents and Thatcher discussed arrangements inside the boarding house, the young Kane played happily with a sled in the snow outside. When Kane's parents introduced him to Thatcher, the boy struck Thatcher with his sled and attempted to run away.\nBy the time Kane gained control of his trust at the age of 25, the mine's productivity and Thatcher's prudent investing had made Kane one of the richest men in the world. Kane took control of the \"New York Inquirer\" newspaper and embarked on a career of yellow journalism, publishing scandalous articles that attacked Thatcher's (and his own) business interests. Kane sold his newspaper empire to Thatcher after the 1929 stock market crash left Kane short of cash.\nThompson interviews Kane's personal business manager, Mr. Bernstein. Bernstein recalls that Kane hired the best journalists available to build the \"Inquirer\"'s circulation. Kane rose to power by successfully manipulating public opinion regarding the Spanish\u2013American War and marrying Emily Norton, the niece of the President of the United States.\nThompson interviews Kane's estranged best friend, Jedediah Leland, in a retirement home. Leland says that Kane's marriage to Emily disintegrated over the years, and he began an affair with amateur singer Susan Alexander while running for Governor of New York. Both his wife and his political opponent discovered the affair and the public scandal ended his political career. Kane married Susan and forced her into a humiliating operatic career for which she had neither the talent nor the ambition, even building a large opera house for her. After Leland began to write a negative review of Susan's disastrous opera debut, Kane fired him but finished the negative review and printed it. Susan protested that she never wanted the opera career anyway, but Kane forced her to continue the season.\nSusan consents to an interview with Thompson and describes the aftermath of her opera career. She attempted suicide and so Kane finally allowed her to abandon singing. After many unhappy years and after being hit by Kane, she finally decided to leave him. Kane's butler Raymond recounts that, after Susan left him, he began violently destroying the contents of her bedroom. When he happened upon a snow globe, he grew calm and said \"Rosebud\". Thompson concludes that he cannot solve the mystery and that the meaning of Kane's last word will remain a mystery.\nBack at Xanadu, Kane's belongings are cataloged or discarded by the staff. They find the sled on which the eight-year-old Kane was playing on the day that he was taken from his home in Colorado and throw it into a furnace with other items. Behind their backs, the sled slowly burns and its trade name, printed on the back, becomes visible through the flames: \"Rosebud\".\nCast.\nThe beginning of the film's ending credits state that \"Most of the principal actors in \"Citizen Kane\" are new to motion pictures. The Mercury Theatre is proud to introduce them.\" The cast is then listed in the following order, with Orson Welles' credit for playing Charles Foster Kane appearing last:\nAdditionally, Charles Bennett appears as the entertainer at the head of the chorus line in the \"Inquirer\" party sequence, and cinematographer Gregg Toland makes a cameo appearance as an interviewer depicted in part of the \"News on the March\" newsreel. Actor Alan Ladd, still unknown at that time, makes a small appearance as a reporter smoking a pipe at the end of the film.\nPre-production.\nDevelopment.\nHollywood had shown interest in Welles as early as 1936. He turned down three scripts sent to him by Warner Bros. In 1937, he declined offers from David O. Selznick, who asked him to head his film company's story department, and William Wyler, who wanted him for a supporting role in \"Wuthering Heights\". \"Although the possibility of making huge amounts of money in Hollywood greatly attracted him,\" wrote biographer Frank Brady, \"he was still totally, hopelessly, insanely in love with the theater, and it is there that he had every intention of remaining to make his mark.\"\nFollowing \"The War of the Worlds\" broadcast of his CBS radio series \"The Mercury Theatre on the Air\", Welles was lured to Hollywood with a remarkable contract. RKO Pictures studio head George J. Schaefer wanted to work with Welles after the notorious broadcast, believing that Welles had a gift for attracting mass attention. RKO was also uncharacteristically profitable and was entering into a series of independent production contracts that would add more artistically prestigious films to its roster. Throughout the spring and early summer of 1939, Schaefer constantly tried to lure the reluctant Welles to Hollywood. Welles was in financial trouble after failure of his plays \"Five Kings\" and \"The Green Goddess\". At first he simply wanted to spend three months in Hollywood and earn enough money to pay his debts and fund his next theatrical season. Welles first arrived on July 20, 1939, and on his first tour, he called the movie studio \"the greatest electric train set a boy ever had\".\nWelles signed his contract with RKO on August 21, which stipulated that Welles would act in, direct, produce and write two films. Mercury would get $100,000 for the first film by January 1, 1940, plus 20% of profits after RKO recouped $500,000, and $125,000 for a second film by January 1, 1941, plus 20% of profits after RKO recouped $500,000. The most controversial aspect of the contract was granting Welles complete artistic control of the two films so long as RKO approved both projects' stories and so long as the budget did not exceed $500,000. RKO executives would not be allowed to see any footage until Welles chose to show it to them, and no cuts could be made to either film without Welles's approval. Welles was allowed to develop the story without interference, select his own cast and crew, and have the right of final cut. Granting final cut privilege was unprecedented for a studio since it placed artistic considerations over financial investment. The contract was deeply resented in the film industry, and the Hollywood press took every opportunity to mock RKO and Welles. Schaefer remained a great supporter and saw the unprecedented contract as good publicity. Film scholar Robert L. Carringer wrote: \"The simple fact seems to be that Schaefer believed Welles was going to pull off something really big almost as much as Welles did himself.\"\nWelles spent the first five months of his RKO contract trying to get his first project going, without success. \"They are laying bets over on the RKO lot that the Orson Welles deal will end up without Orson ever doing a picture there,\" wrote \"The Hollywood Reporter\". It was agreed that Welles would film \"Heart of Darkness\", previously adapted for \"The Mercury Theatre on the Air\", which would be presented entirely through a first-person camera. After elaborate pre-production and a day of test shooting with a hand-held camera\u2014unheard of at the time\u2014the project never reached production because Welles was unable to trim $50,000 from its budget. Schaefer told Welles that the $500,000 budget could not be exceeded; as war loomed, revenue was declining sharply in Europe by the fall of 1939.\nHe then started work on the idea that became \"Citizen Kane\". Knowing the script would take time to prepare, Welles suggested to RKO that while that was being done\u2014\"so the year wouldn't be lost\"\u2014he make a humorous political thriller. Welles proposed \"The Smiler with a Knife\", from a novel by Cecil Day-Lewis. When that project stalled in December 1939, Welles began brainstorming other story ideas with screenwriter Herman J. Mankiewicz, who had been writing Mercury radio scripts. \"Arguing, inventing, discarding, these two powerful, headstrong, dazzlingly articulate personalities thrashed toward \"Kane\"\", wrote biographer Richard Meryman.\nScreenplay.\nOne of the long-standing controversies about \"Citizen Kane\" has been the authorship of the screenplay. Welles conceived the project with screenwriter Herman J. Mankiewicz, who was writing radio plays for Welles's CBS Radio series, \"The Campbell Playhouse\". Mankiewicz based the original outline on the life of William Randolph Hearst, whom he knew socially and came to hate after being exiled from Hearst's circle.\nIn February 1940 Welles supplied Mankiewicz with 300 pages of notes and put him under contract to write the first draft screenplay under the supervision of John Houseman, Welles's former partner in the Mercury Theatre. Welles later explained, \"I left him on his own finally, because we'd started to waste too much time haggling. So, after mutual agreements on storyline and character, Mank went off with Houseman and did his version, while I stayed in Hollywood and wrote mine.\" Taking these drafts, Welles drastically condensed and rearranged them, then added scenes of his own. The industry accused Welles of underplaying Mankiewicz's contribution to the script, but Welles countered the attacks by saying, \"At the end, naturally, I was the one making the picture, after all\u2014who had to make the decisions. I used what I wanted of Mank's and, rightly or wrongly, kept what I liked of my own.\"\nThe terms of the contract stated that Mankiewicz was to receive no credit for his work, as he was hired as a script doctor. Before he signed the contract Mankiewicz was particularly advised by his agents that all credit for his work belonged to Welles and the Mercury Theatre, the \"author and creator\". As the film neared release, however, Mankiewicz began wanting a writing credit for the film and even threatened to take out full-page advertisements in trade papers and to get his friend Ben Hecht to write an expos\u00e9 for \"The Saturday Evening Post\". Mankiewicz also threatened to go to the Screen Writers Guild and claim full credit for writing the entire script by himself.\nAfter lodging a protest with the Screen Writers Guild, Mankiewicz withdrew it, then vacillated. The question was resolved in January 1941 when the studio, RKO Pictures, awarded Mankiewicz credit. The guild credit form listed Welles first, Mankiewicz second. Welles's assistant Richard Wilson said that the person who circled Mankiewicz's name in pencil, then drew an arrow that put it in first place, was Welles. The official credit reads, \"Screenplay by Herman J. Mankiewicz and Orson Welles\". Mankiewicz's rancor toward Welles grew over the remaining twelve years of his life.\nQuestions over the authorship of the \"Citizen Kane\" screenplay were revived in 1971 by influential film critic Pauline Kael, whose controversial 50,000-word essay \"Raising Kane\" was commissioned as an introduction to the shooting script in \"The Citizen Kane Book\", published in October 1971. The book-length essay first appeared in February 1971, in two consecutive issues of \"The New Yorker\" magazine.\nIn the ensuing controversy, Welles was defended by colleagues, critics, biographers and scholars, but his reputation was damaged by its charges. The essay's thesis was later questioned and some of Kael's findings were also contested in later years.\nQuestions of authorship continued to come into sharper focus with Carringer's 1978 thoroughly researched essay, \"The Scripts of \"Citizen Kane\"\". Carringer studied the collection of script records\u2014\"almost a day-to-day record of the history of the scripting\"\u2014that was then still intact at RKO. He reviewed all seven drafts and concluded that \"the full evidence reveals that Welles's contribution to the \"Citizen Kane\" script was not only substantial but definitive.\"\nSources.\nWelles never confirmed a principal source for the character of Charles Foster Kane. Houseman wrote that Kane is a synthesis of different personalities, with Hearst's life used as the main source. Some events and details were invented, and Houseman wrote that he and Mankiewicz also \"grafted anecdotes from other giants of journalism, including Pulitzer, Northcliffe and Mank's first boss, Herbert Bayard Swope.\" Welles said, \"Mr. Hearst was quite a bit like Kane, although Kane isn't really founded on Hearst in particular. Many people sat for it, so to speak\". He specifically acknowledged that aspects of Kane were drawn from the lives of two business tycoons familiar from his youth in Chicago\u2014Samuel Insull and Harold Fowler McCormick.\nThe character of Jedediah Leland was based on drama critic Ashton Stevens, George Stevens's uncle and Welles's close boyhood friend. Some detail came from Mankiewicz's own experience as a drama critic in New York.\nMany assumed that the character of Susan Alexander Kane was based on Marion Davies, Hearst's mistress whose career he managed and whom Hearst promoted as a motion picture actress. This assumption was a major reason Hearst tried to destroy \"Citizen Kane\". Welles denied that the character was based on Davies, whom he called \"an extraordinary woman\u2014nothing like the character Dorothy Comingore played in the movie.\" He cited Insull's building of the Chicago Opera House, and McCormick's lavish promotion of the opera career of his second wife, Ganna Walska, as direct influences on the screenplay.\nAs a known supporter of President Roosevelt, whom both McCormick and Hearst opposed based on his successful attempts to control the content of radio programs and his ongoing efforts to control print, Welles may have had incentive to use the film to smear both men.\nThe character of political boss Jim W. Gettys is based on Charles F. Murphy, a leader in New York City's infamous Tammany Hall political machine.\nWelles credited \"Rosebud\" to Mankiewicz. Biographer Richard Meryman wrote that the symbol of Mankiewicz's own damaged childhood was a treasured bicycle, stolen while he visited the public library and not replaced by his family as punishment. He regarded it as the prototype of Charles Foster Kane's sled. In his 2015 Welles biography, Patrick McGilligan reported that Mankiewicz himself stated that the word \"Rosebud\" was taken from the name of a famous racehorse, Old Rosebud. Mankiewicz had a bet on the horse in the 1914 Kentucky Derby, which he won, and McGilligan wrote that \"Old Rosebud symbolized his lost youth, and the break with his family\". In testimony for the Lundberg suit, Mankiewicz said, \"I had undergone psycho-analysis, and Rosebud, under circumstances slightly resembling the circumstances in [\"Citizen Kane\"], played a prominent part.\" Gore Vidal has argued in the New York Review of Books that \u201cRosebud was what Hearst called his friend Marion Davies\u2019s clitoris.\u201d\nThe \"News on the March\" sequence that begins the film satirizes the journalistic style of \"The March of Time\", the news documentary and dramatization series presented in movie theaters by Time Inc. From 1935 to 1938 Welles was a member of the uncredited company of actors that presented the original radio version.\nHouseman claimed that banker Walter P. Thatcher was loosely based on J. P. Morgan. Bernstein was named for Dr. Maurice Bernstein, appointed Welles's guardian; Sloane's portrayal was said to be based on Bernard Herrmann. Herbert Carter, editor of \"The Inquirer\", was named for actor Jack Carter.\nProduction.\nCasting.\n\"Citizen Kane\" was a rare film in that its principal roles were played by actors new to motion pictures. Ten were billed as Mercury Actors, members of the skilled repertory company assembled by Welles for the stage and radio performances of the Mercury Theatre, an independent theater company he founded with Houseman in 1937. \"He loved to use the Mercury players,\" wrote biographer Charles Higham, \"and consequently he launched several of them on movie careers.\"\nThe film represents the feature film debuts of William Alland, Ray Collins, Joseph Cotten, Agnes Moorehead, Erskine Sanford, Everett Sloane, Paul Stewart, and Welles himself. Despite never having appeared in feature films, some of the cast members were already well known to the public. Cotten had recently become a Broadway star in the hit play \"The Philadelphia Story\" with Katharine Hepburn and Sloane was well known for his role on the radio show \"The Goldbergs\". Mercury actor George Coulouris was a star of the stage in New York and London.\nNot all of the cast came from the Mercury Players. Welles cast Dorothy Comingore, an actress who played supporting parts in films since 1934 using the name \"Linda Winters\", as Susan Alexander Kane. A discovery of Charlie Chaplin, Comingore was recommended to Welles by Chaplin, who then met Comingore at a party in Los Angeles and immediately cast her.\nWelles had met stage actress Ruth Warrick while visiting New York on a break from Hollywood and remembered her as a good fit for Emily Norton Kane, later saying that she looked the part. Warrick told Carringer that she was struck by the extraordinary resemblance between herself and Welles's mother when she saw a photograph of Beatrice Ives Welles. She characterized her own personal relationship with Welles as motherly.\n\"He trained us for films at the same time that he was training himself,\" recalled Agnes Moorehead. \"Orson believed in good acting, and he realized that rehearsals were needed to get the most from his actors. That was something new in Hollywood: nobody seemed interested in bringing in a group to rehearse before scenes were shot. But Orson knew it was necessary, and we rehearsed every sequence before it was shot.\"\nWhen \"The March of Time\" narrator Westbrook Van Voorhis asked for $25,000 to narrate the \"News on the March\" sequence, Alland demonstrated his ability to imitate Van Voorhis and Welles cast him.\nWelles later said that casting character actor Gino Corrado in the small part of the waiter at the El Rancho broke his heart. Corrado had appeared in many Hollywood films, often as a waiter, and Welles wanted all of the actors to be new to films.\nOther uncredited roles went to Thomas A. Curran as Teddy Roosevelt in the faux newsreel; Richard Baer as Hillman, a man at Madison Square Garden, and a man in the \"News on the March\" screening room; and Alan Ladd, Arthur O'Connell and Louise Currie as reporters at Xanadu.\nRuth Warrick (died 2005) was the last surviving member of the principal cast. Sonny Bupp (died 2007), who played Kane's young son, was the last surviving credited cast member. Kathryn Trosper Popper (died March 6, 2016) was reported to have been the last surviving actor to have appeared in \"Citizen Kane\". Jean Forward (died September 2016), a soprano who dubbed the singing voice of Susan Alexander, was the last surviving performer from the film.\nFilming.\nProduction advisor Miriam Geiger quickly compiled a handmade film textbook for Welles, a practical reference book of film techniques that he studied carefully. He then taught himself filmmaking by matching its visual vocabulary to \"The Cabinet of Dr. Caligari\", which he ordered from the Museum of Modern Art, and films by Frank Capra, Ren\u00e9 Clair, Fritz Lang, King Vidor and Jean Renoir. The one film he genuinely studied was John Ford's \"Stagecoach\", which he watched 40 times. \"As it turned out, the first day I ever walked onto a set was my first day as a director,\" Welles said. \"I'd learned whatever I knew in the projection room\u2014from Ford. After dinner every night for about a month, I'd run \"Stagecoach\", often with some different technician or department head from the studio, and ask questions. 'How was this done?' 'Why was this done?' It was like going to school.\"\nWelles's cinematographer for the film was Gregg Toland, described by Welles as \"just then, the number-one cameraman in the world.\" To Welles's astonishment, Toland visited him at his office and said, \"I want you to use me on your picture.\" He had seen some of the Mercury stage productions (including \"Caesar\") and said he wanted to work with someone who had never made a movie. RKO hired Toland on loan from Samuel Goldwyn Productions in the first week of June 1940.\n\"And he never tried to impress us that he was doing any miracles,\" Welles recalled. \"I was calling for things only a beginner would have been ignorant enough to think anybody could ever do, and there he was, \"doing\" them.\" Toland later explained that he wanted to work with Welles because he anticipated the first-time director's inexperience and reputation for audacious experimentation in the theater would allow the cinematographer to try new and innovative camera techniques that typical Hollywood films would never have allowed him to do. Unaware of filmmaking protocol, Welles adjusted the lights on set as he was accustomed to doing in the theater; Toland quietly re-balanced them, and was angry when one of the crew informed Welles that he was infringing on Toland's responsibilities. During the first few weeks of June, Welles had lengthy discussions about the film with Toland and art director Perry Ferguson in the morning, and in the afternoon and evening he worked with actors and revised the script.\nOn June 29, 1940\u2014a Saturday morning when few inquisitive studio executives would be around\u2014Welles began filming \"Citizen Kane\". After the disappointment of having \"Heart of Darkness\" canceled, Welles followed Ferguson's suggestion and deceived RKO into believing that he was simply shooting camera tests. \"But we were shooting the \"picture\",\" Welles said, \"because we wanted to get started and be already into it before anybody knew about it.\"\nAt the time RKO executives were pressuring him to agree to direct a film called \"The Men from Mars\", to capitalize on \"The War of the Worlds\" radio broadcast. Welles said that he would consider making the project but wanted to make a different film first. At this time he did not inform them that he had already begun filming \"Citizen Kane.\"\nThe early footage was called \"Orson Welles Tests\" on all paperwork. The first \"test\" shot was the \"News on the March\" projection room scene, economically filmed in a real studio projection room in darkness that masked many actors who appeared in other roles later in the film. \"At $809 Orson did run substantially beyond the test budget of $528\u2014to create one of the most famous scenes in movie history,\" wrote Barton Whaley.\nThe next scenes were the El Rancho nightclub scenes and the scene in which Susan attempts suicide. Welles later said that the nightclub set was available after another film had wrapped and that filming took 10 to 12 days to complete. For these scenes Welles had Comingore's throat sprayed with chemicals to give her voice a harsh, raspy tone. Other scenes shot in secret included those in which Thompson interviews Leland and Bernstein, which were also shot on sets built for other films.\nDuring production, the film was referred to as \"RKO 281\". Most of the filming took place in what is now Stage 19 on the Paramount Pictures lot in Hollywood. There was some location filming at Balboa Park in San Diego and the San Diego Zoo. Photographs of German-Jewish investment banker Otto Hermann Kahn's real-life estate Oheka Castle were used to portray the fictional Xanadu.\nIn the end of July, RKO approved the film and Welles was allowed to officially begin shooting, despite having already been filming \"tests\" for several weeks. Welles leaked stories to newspaper reporters that the \"tests\" had been so good that there was no need to re-shoot them. The first \"official\" scene to be shot was the breakfast montage sequence between Kane and his first wife Emily. To strategically save money and appease the RKO executives who opposed him, Welles rehearsed scenes extensively before actually shooting and filmed very few takes of each shot set-up. Welles never shot master shots for any scene after Toland told him that Ford never shot them. To appease the increasingly curious press, Welles threw a cocktail party for selected reporters, promising that they could watch a scene being filmed. When the journalists arrived Welles told them they had \"just finished\" shooting for the day but still had the party. Welles told the press that he was ahead of schedule (without factoring in the month of \"test shooting\"), thus discrediting claims that after a year in Hollywood without making a film he was a failure in the film industry.\nWelles usually worked 16 to 18 hours a day on the film. He often began work at 4\u00a0a.m. since the special effects make-up used to age him for certain scenes took up to four hours to apply. Welles used this time to discuss the day's shooting with Toland and other crew members. The special contact lenses used to make Welles look elderly proved very painful, and a doctor was employed to place them into Welles's eyes. Welles had difficulty seeing clearly while wearing them, which caused him to badly cut his wrist when shooting the scene in which Kane breaks up the furniture in Susan's bedroom. While shooting the scene in which Kane shouts at Gettys on the stairs of Susan Alexander's apartment building, Welles fell ten feet; an X-ray revealed two bone chips in his ankle.\nThe injury required him to direct the film from a wheelchair for two weeks. He eventually wore a steel brace to resume performing on camera; it is visible in the low-angle scene between Kane and Leland after Kane loses the election. For the final scene, a stage at the Selznick studio was equipped with a working furnace, and multiple takes were required to show the sled being put into the fire and the word \"Rosebud\" consumed. Paul Stewart recalled that on the ninth take the Culver City Fire Department arrived in full gear because the furnace had grown so hot the flue caught fire. \"Orson was delighted with the commotion\", he said.\nWhen \"Rosebud\" was burned, Welles choreographed the scene while he had composer Bernard Herrmann's cue playing on the set.\nUnlike Schaefer, many members of RKO's board of governors did not like Welles or the control that his contract gave him. However such board members as Nelson Rockefeller and NBC chief David Sarnoff were sympathetic to Welles. Throughout production Welles had problems with these executives not respecting his contract's stipulation of non-interference and several spies arrived on set to report what they saw to the executives. When the executives would sometimes arrive on set unannounced the entire cast and crew would suddenly start playing softball until they left. Before official shooting began the executives intercepted all copies of the script and delayed their delivery to Welles. They had one copy sent to their office in New York, resulting in it being leaked to press.\nPrincipal shooting wrapped October 24. Welles then took several weeks away from the film for a lecture tour, during which he also scouted additional locations with Toland and Ferguson. Filming resumed November 15 with some re-shoots. Toland had to leave due to a commitment to shoot Howard Hughes' \"The Outlaw\", but Toland's camera crew continued working on the film and Toland was replaced by RKO cinematographer Harry J. Wild. The final day of shooting on November 30 was Kane's death scene. Welles boasted that he only went 21 days over his official shooting schedule, without factoring in the month of \"camera tests\". According to RKO records, the film cost $839,727. Its estimated budget had been $723,800.\nPost-production.\n\"Citizen Kane\" was edited by Robert Wise and assistant editor Mark Robson. Both would become successful film directors. Wise was hired after Welles finished shooting the \"camera tests\" and began officially making the film. Wise said that Welles \"had an older editor assigned to him for those tests and evidently he was not too happy and asked to have somebody else. I was roughly Orson's age and had several good credits.\" Wise and Robson began editing the film while it was still shooting and said that they \"could tell certainly that we were getting something very special. It was outstanding film day in and day out.\"\nWelles gave Wise detailed instructions and was usually not present during the film's editing. The film was very well planned out and intentionally shot for such post-production techniques as slow dissolves. The lack of coverage made editing easy since Welles and Toland edited the film \"in camera\" by leaving few options of how it could be put together. Wise said the breakfast table sequence took weeks to edit and get the correct \"timing\" and \"rhythm\" for the whip pans and overlapping dialogue. The \"News on the March\" sequence was edited by RKO's newsreel division to give it authenticity. They used stock footage from Path\u00e9 News and the General Film Library.\nDuring post-production Welles and special effects artist Linwood G. Dunn experimented with an optical printer to improve certain scenes that Welles found unsatisfactory from the footage. Whereas Welles was often immediately pleased with Wise's work, he would require Dunn and post-production audio engineer James G. Stewart to re-do their work several times until he was satisfied.\nWelles hired Bernard Herrmann to compose the film's score. Where most Hollywood film scores were written quickly, in as few as two or three weeks after filming was completed, Herrmann was given 12 weeks to write the music. He had sufficient time to do his own orchestrations and conducting, and worked on the film reel by reel as it was shot and cut. He wrote complete musical pieces for some of the montages, and Welles edited many of the scenes to match their length.\nTrailer.\nWritten and directed by Welles at Toland's suggestion, the theatrical trailer for \"Citizen Kane\" differs from other trailers in that it did not feature a single second of footage of the actual film itself, but acts as a wholly original, tongue-in-cheek, pseudo-documentary piece on the film's production. Filmed at the same time as \"Citizen Kane\" itself, it offers the only existing behind-the-scenes footage of the film. The trailer, shot by Wild instead of Toland, follows an unseen Welles as he provides narration for a tour around the film set, introductions to the film's core cast members, and a brief overview of Kane's character. The trailer also contains a number of trick shots, including one of Everett Sloane appearing at first to be running into the camera, which turns out to be the reflection of the camera in a mirror.\nAt the time, it was almost unprecedented for a film trailer to not actually feature anything of the film itself; and while \"Citizen Kane\" is frequently cited as a groundbreaking, influential film, Simon Callow argues its trailer was no less original in its approach. Callow writes that it has \"great playful charm\u00a0... it is a miniature documentary, almost an introduction to the cinema\u00a0... Teasing, charming, completely original, it is a sort of conjuring trick: Without his face appearing once on the screen, Welles entirely dominates its five [sic] minutes' duration.\"\nStyle.\nFilm scholars and historians view \"Citizen Kane\" as Welles's attempt to create a new style of filmmaking by studying various forms of it and combining them into one. However, Welles stated that his love for cinema began only when he started working on the film. When asked where he got the confidence as a first-time director to direct a film so radically different from contemporary cinema, he responded, \"Ignorance, ignorance, sheer ignorance\u2014you know there's no confidence to equal it. It's only when you know something about a profession, I think, that you're timid or careful.\"\nDavid Bordwell wrote that \"The best way to understand \"Citizen Kane\" is to stop worshipping it as a triumph of technique.\" Bordwell argues that the film did not invent any of its famous techniques such as deep focus cinematography, shots of the ceilings, chiaroscuro lighting and temporal jump-cuts, and that many of these stylistics had been used in German Expressionist films of the 1920s, such as \"The Cabinet of Dr. Caligari\". But Bordwell asserts that the film did put them all together for the first time and perfected the medium in one single film. In a 1948 interview, D. W. Griffith said, \"I loved \"Citizen Kane\" and particularly loved the ideas he took from me.\"\nArguments against the film's cinematic innovations were made as early as 1946 when French historian Georges Sadoul wrote, \"The film is an encyclopedia of old techniques.\" He pointed out such examples as compositions that used both the foreground and the background in the films of Auguste and Louis Lumi\u00e8re, special effects used in the films of Georges M\u00e9li\u00e8s, shots of the ceiling in Erich von Stroheim's \"Greed\" and newsreel montages in the films of Dziga Vertov.\nFrench film critic Andr\u00e9 Bazin defended the film, writing: \"In this respect, the accusation of plagiarism could very well be extended to the film's use of panchromatic film or its exploitation of the properties of gelatinous silver halide.\" Bazin disagreed with Sadoul's comparison to Lumi\u00e8re's cinematography since \"Citizen Kane\" used more sophisticated lenses, but acknowledged that it had similarities to such previous works as \"The 49th Parallel\" and \"The Power and the Glory\". Bazin stated that \"even if Welles did not invent the cinematic devices employed in \"Citizen Kane\", one should nevertheless credit him with the invention of their \"meaning\".\" Bazin championed the techniques in the film for its depiction of heightened reality, but Bordwell believed that the film's use of special effects contradicted some of Bazin's theories.\nStorytelling techniques.\n\"Citizen Kane\" rejects the traditional linear, chronological narrative and tells Kane's story entirely in flashbacks using different points of view, many of them from Kane's aged and forgetful associates, the cinematic equivalent of the unreliable narrator in literature. Welles also dispenses with the idea of a single storyteller and uses multiple narrators to recount Kane's life, a technique not used previously in Hollywood films. Each narrator recounts a different part of Kane's life, with each story overlapping another. The film depicts Kane as an enigma, a complicated man who leaves viewers with more questions than answers as to his character, such as the newsreel footage where he is attacked for being both a communist and a fascist.\nThe technique of flashbacks had been used in earlier films, notably \"The Power and the Glory\" (1933), but no film was as immersed in it as \"Citizen Kane\". Thompson the reporter acts as a surrogate for the audience, questioning Kane's associates and piecing together his life.\nFilms typically had an \"omniscient perspective\" at the time, which Marilyn Fabe says give the audience the \"illusion that we are looking with impunity into a world which is unaware of our gaze\". \"Citizen Kane\" also begins in that fashion until the \"News on the March\" sequence, after which we the audience see the film through the perspectives of others. The \"News on the March\" sequence gives an overview of Kane's entire life (and the film's entire story) at the beginning of the film, leaving the audience without the typical suspense of wondering how it will end. Instead, the film's repetitions of events compels the audience to analyze and wonder why Kane's life happened the way that it did, under the pretext of finding out what \"Rosebud\" means. The film then returns to the omniscient perspective in the final scene, when only the audience discovers what \"Rosebud\" is.\nCinematography.\nThe most innovative technical aspect of \"Citizen Kane\" is the extended use of deep focus, where the foreground, background, and everything in between are all in sharp focus. Cinematographer Toland did this through his experimentation with lenses and lighting. Toland described the achievement in an article for \"Theatre Arts\" magazine, made possible by the sensitivity of modern speed film:\nNew developments in the science of motion picture photography are not abundant at this advanced stage of the game but periodically one is perfected to make this a greater art. Of these I am in an excellent position to discuss what is termed \"Pan-focus\", as I have been active for two years in its development and used it for the first time in \"Citizen Kane\". Through its use, it is possible to photograph action from a range of eighteen inches from the camera lens to over two hundred feet away, with extreme foreground and background figures and action both recorded in sharp relief. Hitherto, the camera had to be focused either for a close or a distant shot, all efforts to encompass both at the same time resulting in one or the other being out of focus. This handicap necessitated the breaking up of a scene into long and short angles, with much consequent loss of realism. With pan-focus, the camera, like the human eye, sees an entire panorama at once, with everything clear and lifelike.\nAnother unorthodox method used in the film was the low-angle shots facing upwards, thus allowing ceilings to be shown in the background of several scenes. Every set was built with a ceiling which broke with studio convention, and many were constructed of fabric that concealed microphones. Welles felt that the camera should show what the eye sees, and that it was a bad theatrical convention to pretend that there was no ceiling\u2014\"a big lie in order to get all those terrible lights up there,\" he said. He became fascinated with the look of low angles, which made even dull interiors look interesting. One extremely low angle is used to photograph the encounter between Kane and Leland after Kane loses the election. A hole was dug for the camera, which required drilling into the concrete floor.\nWelles credited Toland on the same title card as himself. \"It's impossible to say how much I owe to Gregg,\" he said. \"He was superb.\" He called Toland \"the best director of photography that ever existed.\"\nSound.\n\"Citizen Kane\"'s sound was recorded by Bailey Fesler and re-recorded in post-production by audio engineer James G. Stewart, both of whom had worked in radio. Stewart said that Hollywood films never deviated from a basic pattern of how sound could be recorded or used, but with Welles \"deviation from the pattern was possible because he demanded it.\" Although the film is known for its complex soundtrack, much of the audio is heard as it was recorded by Fesler and without manipulation.\nWelles used techniques from radio like overlapping dialogue. The scene in which characters sing \"Oh, Mr. Kane\" was especially complicated and required mixing several soundtracks together. He also used different \"sound perspectives\" to create the illusion of distances, such as in scenes at Xanadu where characters speak to each other at far distances. Welles experimented with sound in post-production, creating audio montages, and chose to create all of the sound effects for the film instead of using RKO's library of sound effects.\nWelles used an aural technique from radio called the \"lightning-mix\". Welles used this technique to link complex montage sequences via a series of related sounds or phrases. For example, Kane grows from a child into a young man in just two shots. As Thatcher hands eight-year-old Kane a sled and wishes him a Merry Christmas, the sequence suddenly jumps to a shot of Thatcher fifteen years later, completing the sentence he began in both the previous shot and the chronological past. Other radio techniques include using a number of voices, each saying a sentence or sometimes merely a fragment of a sentence, and splicing the dialogue together in quick succession, such as the projection room scene. The film's sound cost $16,996, but was originally budgeted at $7,288.\nFilm critic and director Fran\u00e7ois Truffaut wrote that \"Before \"Kane\", nobody in Hollywood knew how to set music properly in movies. \"Kane\" was the first, in fact the only, great film that uses radio techniques.\u00a0... A lot of filmmakers know enough to follow Auguste Renoir's advice to fill the eyes with images at all costs, but only Orson Welles understood that the sound track had to be filled in the same way.\" Cedric Belfrage of \"The Clipper\" wrote \"of all of the delectable flavours that linger on the palate after seeing \"Kane\", the use of sound is the strongest.\"\nMake-up.\nThe make-up for \"Citizen Kane\" was created and applied by Maurice Seiderman (1907\u20131989), a junior member of the RKO make-up department. He had not been accepted into the union, which recognized him as only an apprentice, but RKO nevertheless used him to make up principal actors. \"Apprentices were not supposed to make up any principals, only extras, and an apprentice could not be on a set without a journeyman present,\" wrote make-up artist Dick Smith, who became friends with Seiderman in 1979. \"During his years at RKO I suspect these rules were probably overlooked often.\" \"Seiderman had gained a reputation as one of the most inventive and creatively precise up-and-coming makeup men in Hollywood,\" wrote biographer Frank Brady.\nOn an early tour of RKO, Welles met Seiderman in the small make-up lab that he created for himself in an unused dressing room. \"Welles fastened on to him at once,\" wrote biographer Charles Higham, as Seiderman had developed his own makeup methods \"that ensured complete naturalness of expression\u2014a naturalness unrivaled in Hollywood.\" Seiderman developed a thorough plan for aging the principal characters, first making a plaster cast of the face of each of the actors who aged. He made a plaster mold of Welles's body down to the hips.\n\"My sculptural techniques for the characters' aging were handled by adding pieces of white modeling clay, which matched the plaster, onto the surface of each bust,\" Seiderman told Norman Gambill. When Seiderman achieved the desired effect, he cast the clay pieces in a soft plastic material that he formulated himself. These appliances were then placed onto the plaster bust and a four-piece mold was made for each phase of aging. The castings were then fully painted and paired with the appropriate wig for evaluation.\nBefore the actors went before the cameras each day, the pliable pieces were applied directly to their faces to recreate Seiderman's sculptural image. The facial surface was underpainted in a flexible red plastic compound; The red ground resulted in a warmth of tone that was picked up by the panchromatic film. Over that was applied liquid grease paint, and finally a colorless translucent talcum. Seiderman created the effect of skin pores on Kane's face by stippling the surface with a negative cast made from an orange peel.\nWelles often arrived on the set at 2:30\u00a0am, as application of the sculptural make-up took 3\u00bd hours for the oldest incarnation of Kane. The make-up included appliances to age Welles's shoulders, breast, and stomach. \"In the film and production photographs, you can see that Kane had a belly that overhung,\" Seiderman said. \"That was not a costume, it was the rubber sculpture that created the image. You could see how Kane's silk shirt clung wetly to the character's body. It could not have been done any other way.\"\nSeiderman worked with Charles Wright on the wigs. These went over a flexible skull cover that Seiderman created and sewed into place with elastic thread. When he found the wigs too full, he untied one hair at a time to alter their shape. Kane's mustache was inserted into the makeup surface a few hairs at a time, to realistically vary the color and texture. He also made scleral lenses for Welles, Dorothy Comingore, George Coulouris, and Everett Sloane to dull the brightness of their young eyes. The lenses took a long time to fit properly, and Seiderman began work on them before devising any of the other makeup. \"I painted them to age in phases, ending with the blood vessels and the \"arcus senilis\" of old age.\" Seiderman's tour de force was the breakfast montage, shot all in one day. \"Twelve years, two years shot at each scene,\" he said.\nThe major studios gave screen credit for make-up only to the department head. When RKO make-up department head Mel Berns refused to share credit with Seiderman, who was only an apprentice, Welles told Berns that there would be no make-up credit. Welles signed a large advertisement in the Los Angeles newspaper:\nTHANKS TO EVERYBODY WHO GETS SCREEN CREDIT FOR \"CITIZEN KANE\"AND THANKS TO THOSE WHO DON'TTO ALL THE ACTORS, THE CREW, THE OFFICE, THE MUSICIANS, EVERYBODYAND PARTICULARLY TO MAURICE SEIDERMAN, THE BEST MAKE-UP MAN IN THE WORLD\nSets.\nAlthough credited as an assistant, the film's art direction was done by Perry Ferguson. Welles and Ferguson got along during their collaboration. In the weeks before production began Welles, Toland and Ferguson met regularly to discuss the film and plan every shot, set design and prop. Ferguson would take notes during these discussions and create rough designs of the sets and story boards for individual shots. After Welles approved the rough sketches, Ferguson made miniature models for Welles and Toland to experiment on with a periscope in order to rehearse and perfect each shot. Ferguson then had detailed drawings made for the set design, including the film's lighting design. The set design was an integral part of the film's overall look and Toland's cinematography.\nIn the original script the Great Hall at Xanadu was modeled after the Great Hall in Hearst Castle and its design included a mixture of Renaissance and Gothic styles. \"The Hearstian element is brought out in the almost perverse juxtaposition of incongruous architectural styles and motifs,\" wrote Carringer. Before RKO cut the film's budget, Ferguson's designs were more elaborate and resembled the production designs of early Cecil B. DeMille films and \"Intolerance\". The budget cuts reduced Ferguson's budget by 33 percent and his work cost $58,775 total, which was below average at that time.\nTo save costs Ferguson and Welles re-wrote scenes in Xanadu's living room and transported them to the Great Hall. A large staircase from another film was found and used at no additional cost. When asked about the limited budget, Ferguson said \"Very often\u2014as in that much-discussed 'Xanadu' set in \"Citizen Kane\"\u2014we can make a foreground piece, a background piece, and imaginative lighting suggests a great deal more on the screen than actually exists on the stage.\" According to the film's official budget there were 81 sets built, but Ferguson said there were between 106 and 116.\nStill photographs of Oheka Castle in Huntington, New York, were used in the opening montage, representing Kane's Xanadu estate. Ferguson also designed statues from Kane's collection with styles ranging from Greek to German Gothic. The sets were also built to accommodate Toland's camera movements. Walls were built to fold and furniture could quickly be moved. The film's famous ceilings were made out of muslin fabric and camera boxes were built into the floors for low angle shots. Welles later said that he was proud that the film production value looked much more expensive than the film's budget. Although neither worked with Welles again, Toland and Ferguson collaborated in several films in the 1940s.\nSpecial effects.\nThe film's special effects were supervised by RKO department head Vernon L. Walker. Welles pioneered several visual effects to cheaply shoot things like crowd scenes and large interior spaces. For example, the scene in which the camera in the opera house rises dramatically to the rafters, to show the workmen showing a lack of appreciation for Susan Alexander Kane's performance, was shot by a camera craning upwards over the performance scene, then a curtain wipe to a miniature of the upper regions of the house, and then another curtain wipe matching it again with the scene of the workmen. Other scenes effectively employed miniatures to make the film look much more expensive than it truly was, such as various shots of Xanadu.\nSome shots included rear screen projection in the background, such as Thompson's interview of Leland and some of the ocean backgrounds at Xanadu. Bordwell claims that the scene where Thatcher agrees to be Kane's guardian used rear screen projection to depict young Kane in the background, despite this scene being cited as a prime example of Toland's deep focus cinematography. A special effects camera crew from Walker's department was required for the extreme close-up shots such as Kane's lips when he says \"Rosebud\" and the shot of the typewriter typing Susan's bad review.\nOptical effects artist Dunn claimed that \"up to 80 percent of some reels was optically printed.\" These shots were traditionally attributed to Toland for years. The optical printer improved some of the deep focus shots. One problem with the optical printer was that it sometimes created excessive graininess, such as the optical zoom out of the snow globe. Welles decided to superimpose snow falling to mask the graininess in these shots. Toland said that he disliked the results of the optical printer, but acknowledged that \"RKO special effects expert Vernon Walker, ASC, and his staff handled their part of the production\u2014a by no means inconsiderable assignment\u2014with ability and fine understanding.\"\nAny time deep focus was impossible\u2014as in the scene in which Kane finishes a negative review of Susan's opera while at the same time firing the person who began writing the review\u2014an optical printer was used to make the whole screen appear in focus, visually layering one piece of film onto another. However, some apparently deep-focus shots were the result of in-camera effects, as in the famous scene in which Kane breaks into Susan's room after her suicide attempt. In the background, Kane and another man break into the room, while simultaneously the medicine bottle and a glass with a spoon in it are in closeup in the foreground. The shot was an in-camera matte shot. The foreground was shot first, with the background dark. Then the background was lit, the foreground darkened, the film rewound, and the scene re-shot with the background action.\nMusic.\nThe film's music was composed by Bernard Herrmann. Herrmann had composed for Welles for his Mercury Theatre radio broadcasts. Because it was Herrmann's first motion picture score, RKO wanted to pay him only a small fee, but Welles insisted he be paid at the same rate as Max Steiner.\nThe score established Herrmann as an important new composer of film soundtracks and eschewed the typical Hollywood practice of scoring a film with virtually non-stop music. Instead Herrmann used what he later described as \"radio scoring\", musical cues typically 5\u201315 seconds in length that bridge the action or suggest a different emotional response. The breakfast montage sequence begins with a graceful waltz theme and gets darker with each variation on that theme as the passage of time leads to the hardening of Kane's personality and the breakdown of his first marriage.\nHerrmann realized that musicians slated to play his music were hired for individual unique sessions; there was no need to write for existing ensembles. This meant that he was free to score for unusual combinations of instruments, even instruments that are not commonly heard. In the opening sequence, for example, the tour of Kane's estate Xanadu, Herrmann introduces a recurring leitmotif played by low woodwinds, including a quartet of alto flutes.\nFor Susan Alexander Kane's operatic sequence, Welles suggested that Herrmann compose a witty parody of a Mary Garden vehicle, an aria from \"Salammb\u00f4\". \"Our problem was to create something that would give the audience the feeling of the quicksand into which this simple little girl, having a charming but small voice, is suddenly thrown,\" Herrmann said. Writing in the style of a 19th-century French Oriental opera, Herrmann put the aria in a key that would force the singer to strain to reach the high notes, culminating in a high D, well outside the range of Susan Alexander. Soprano Jean Forward dubbed the vocal part for Comingore. Houseman claimed to have written the libretto, based on Jean Racine's \"Athalie\" and \"Phedre\", although some confusion remains since Lucille Fletcher remembered preparing the lyrics. Fletcher, then Herrmann's wife, wrote the libretto for his opera \"Wuthering Heights\".\nMusic enthusiasts consider the scene in which Susan Alexander Kane attempts to sing the famous cavatina \"Una voce poco fa\" from \"Il barbiere di Siviglia\" by Gioachino Rossini with vocal coach Signor Matiste as especially memorable for depicting the horrors of learning music through mistakes.\nIn 1972, Herrmann said, \"I was fortunate to start my career with a film like \"Citizen Kane\", it's been a downhill run ever since!\" Welles loved Herrmann's score and told director Henry Jaglom that it was 50 percent responsible for the film's artistic success.\nSome incidental music came from other sources. Welles heard the tune used for the publisher's theme, \"Oh, Mr. Kane\", in Mexico. Called \"A Poco No\", the song was written by Pepe Gu\u00edzar and special lyrics were written by Herman Ruby.\n\"In a Mizz\", a 1939 jazz song by Charlie Barnet and Haven Johnson, bookends Thompson's second interview of Susan Alexander Kane. \"I kind of based the whole scene around that song,\" Welles said. \"The music is by Nat Cole\u2014it's his trio.\" Later\u2014beginning with the lyrics, \"It can't be love\"\u2014\"In a Mizz\" is performed at the Everglades picnic, framing the fight in the tent between Susan and Kane. Musicians including bandleader Cee Pee Johnson (drums), Alton Redd (vocals), Raymond Tate (trumpet), Buddy Collette (alto sax) and Buddy Banks (tenor sax) are featured.\nAll of the music used in the newsreel came from the RKO music library, edited at Welles's request by the newsreel department to achieve what Herrmann called \"their own crazy way of cutting\". The \"News on the March\" theme that accompanies the newsreel titles is \"Belgian March\" by Anthony Collins, from the film \"Nurse Edith Cavell\". Other examples are an excerpt from Alfred Newman's score for \"Gunga Din\" (the exploration of Xanadu), Roy Webb's theme for the film \"Reno\" (the growth of Kane's empire), and bits of Webb's score for \"Five Came Back\" (introducing Walter Parks Thatcher).\nEditing.\nOne of the editing techniques used in \"Citizen Kane\" was the use of montage to collapse time and space, using an episodic sequence on the same set while the characters changed costume and make-up between cuts so that the scene following each cut would look as if it took place in the same location, but at a time long after the previous cut. In the breakfast montage, Welles chronicles the breakdown of Kane's first marriage in five vignettes that condense 16 years of story time into two minutes of screen time. Welles said that the idea for the breakfast scene \"was stolen from \"The Long Christmas Dinner\" by Thornton Wilder\u00a0... a one-act play, which is a long Christmas dinner that takes you through something like 60 years of a family's life.\" The film often uses long dissolves to signify the passage of time and its psychological effect of the characters, such as the scene in which the abandoned sled is covered with snow after the young Kane is sent away with Thatcher.\nWelles was influenced by the editing theories of Sergei Eisenstein by using jarring cuts that caused \"sudden graphic or associative contrasts\", such as the cut from Kane's deathbed to the beginning of the \"News on the March\" sequence and a sudden shot of a shrieking cockatoo at the beginning of Raymond's flashback. Although the film typically favors mise-en-sc\u00e8ne over montage, the scene in which Kane goes to Susan Alexander's apartment after first meeting her is the only one that is primarily cut as close-ups with shots and counter shots between Kane and Susan. Fabe says that \"by using a standard Hollywood technique sparingly, [Welles] revitalizes its psychological expressiveness.\"\nPolitical themes.\nLaura Mulvey explored the anti-fascist themes of \"Citizen Kane\" in her 1992 monograph for the British Film Institute. The \"News on the March\" newsreel presents Kane keeping company with Hitler and other dictators while he smugly assures the public that there will be no war. She wrote that the film reflects \"the battle between intervention and isolationism\" then being waged in the United States; the film was released six months before the attack on Pearl Harbor, while President Franklin D. Roosevelt was laboring to win public opinion for entering World War II. \"In the rhetoric of \"Citizen Kane\",\" Mulvey writes, \"the destiny of isolationism is realised in metaphor: in Kane's own fate, dying wealthy and lonely, surrounded by the detritus of European culture and history.\"\nJournalist Ignacio Ramonet has cited the film as an early example of mass media manipulation of public opinion and the power that media conglomerates have on influencing the democratic process. He believes that this early example of a media mogul influencing politics is outdated and that today \"there are media groups with the power of a thousand Citizen Kanes.\" Media mogul Rupert Murdoch is sometimes labeled as a latter-day \"Citizen Kane\".\nComparisons have also been made between the career and character of Donald Trump and Charles Foster Kane. \"Citizen Kane\" is reported to be one of Trump's favorite films, and his biographer Tim O\u2019Brien has said that Trump is fascinated by and identifies with Kane. In an interview with filmmaker Errol Morris, Trump explained his own interpretation of the film's themes, saying \"You learn in 'Kane' maybe wealth isn't everything, because he had the wealth but he didn't have the happiness. In real life I believe that wealth does in fact isolate you from other people. It's a protective mechanism \u2014 you have your guard up much more so [than] if you didn't have wealth...Perhaps I can understand that.\"\nReception.\nPre-release controversy.\nTo ensure that Hearst's life's influence on \"Citizen Kane\" was a secret, Welles limited access to dailies and managed the film's publicity. A December 1940 feature story in \"Stage\" magazine compared the film's narrative to \"Faust\" and made no mention of Hearst.\nThe film was scheduled to premiere at RKO's flagship theater Radio City Music Hall on February 14, but in early January 1941 Welles was not finished with post-production work and told RKO that it still needed its musical score. Writers for national magazines had early deadlines and so a rough cut was previewed for a select few on January 3, 1941 for such magazines as \"Life\", \"Look\" and \"Redbook\". Gossip columnist Hedda Hopper (an arch-rival of Louella Parsons, the Hollywood correspondent for Hearst papers) showed up to the screening uninvited. Most of the critics at the preview said that they liked the film and gave it good advanced reviews. Hopper wrote negatively about it, calling the film a \"vicious and irresponsible attack on a great man\" and criticizing its corny writing and old fashioned photography.\n\"Friday\" magazine ran an article drawing point-by-point comparisons between Kane and Hearst and documented how Welles had led on Parsons. Up until this Welles had been friendly with Parsons. The magazine quoted Welles as saying that he could not understand why she was so nice to him and that she should \"wait until the woman finds out that the picture's about her boss.\" Welles immediately denied making the statement and the editor of \"Friday\" admitted that it might be false. Welles apologized to Parsons and assured her that he had never made that remark.\nShortly after \"Friday\"'s article, Hearst sent Parsons an angry letter complaining that he had learned about \"Citizen Kane\" from Hopper and not her. The incident made a fool of Parsons and compelled her to start attacking Welles and the film. Parsons demanded a private screening of the film and personally threatened Schaefer on Hearst's behalf, first with a lawsuit and then with a vague threat of consequences for everyone in Hollywood. On January 10 Parsons and two lawyers working for Hearst were given a private screening of the film. James G. Stewart was present at the screening and said that she walked out of the film.\nSoon after, Parsons called Schaefer and threatened RKO with a lawsuit if they released \"Kane\". She also contacted the management of Radio City Music Hall and demanded that they should not screen it. The next day, the front page headline in \"Daily Variety\" read, \"HEARST BANS RKO FROM PAPERS.\" Hearst began this ban by suppressing promotion of RKO's \"Kitty Foyle\", but in two weeks the ban was lifted for everything except \"Kane.\"\nWhen Schaefer did not submit to Parsons she called other studio heads and made more threats on behalf of Hearst to expose the private lives of people throughout the entire film industry. Welles was threatened with an expos\u00e9 about his romance with the married actress Dolores del R\u00edo, who wanted the affair kept secret until her divorce was finalized. In a statement to journalists Welles denied that the film was about Hearst. Hearst began preparing an injunction against the film for libel and invasion of privacy, but Welles's lawyer told him that he doubted Hearst would proceed due to the negative publicity and required testimony that an injunction would bring.\n\"The Hollywood Reporter\" ran a front-page story on January 13 that Hearst papers were about to run a series of editorials attacking Hollywood's practice of hiring refugees and immigrants for jobs that could be done by Americans. The goal was to put pressure on the other studios to force RKO to shelve \"Kane\". Many of those immigrants had fled Europe after the rise of fascism and feared losing the haven of the United States. Soon afterwards, Schaefer was approached by Nicholas Schenck, head of Metro-Goldwyn-Mayer's parent company, with an offer on the behalf of Louis B. Mayer and other Hollywood executives to RKO Pictures of $805,000 to destroy all prints of the film and burn the negative.\nOnce RKO's legal team reassured Schaefer, the studio announced on January 21 that \"Kane\" would be released as scheduled, and with one of the largest promotional campaigns in the studio's history. Schaefer brought Welles to New York City for a private screening of the film with the New York corporate heads of the studios and their lawyers. There was no objection to its release provided that certain changes, including the removal or softening of specific references that might offend Hearst, were made. Welles agreed and cut the running time from 122 minutes to 119 minutes. The cuts satisfied the corporate lawyers.\nHearst's response.\nHearing about \"Citizen Kane\" enraged Hearst so much that he banned any advertising, reviewing, or mentioning of it in his papers, and had his journalists libel Welles. Welles used Hearst's opposition as a pretext for previewing the film in several opinion-making screenings in Los Angeles, lobbying for its artistic worth against the hostile campaign that Hearst was waging. A special press screening took place in early March. Henry Luce was in attendance and reportedly wanted to buy the film from RKO for $1\u00a0million to distribute it himself. The reviews for this screening were positive. A \"Hollywood Review\" headline read, \"Mr. Genius Comes Through; 'Kane' Astonishing Picture\". The \"Motion Picture Herald\" reported about the screening and Hearst's intention to sue RKO. \"Time\" magazine wrote that \"The objection of Mr. Hearst, who founded a publishing empire on sensationalism, is ironic. For to most of the several hundred people who have seen the film at private screenings, \"Citizen Kane\" is the most sensational product of the U.S. movie industry.\" A second press screening occurred in April.\nWhen Schaefer rejected Hearst's offer to suppress the film, Hearst banned every newspaper and station in his media conglomerate from reviewing\u2014or even mentioning\u2014the film. He also had many movie theaters ban it, and many did not show it through fear of being socially exposed by his massive newspaper empire. The Oscar-nominated documentary \"The Battle Over Citizen Kane\" lays the blame for the film's relative failure squarely at the feet of Hearst. The film did decent business at the box office; it went on to be the sixth highest grossing film in its year of release, a modest success its backers found acceptable. Nevertheless, the film's commercial performance fell short of its creators' expectations. Hearst's biographer David Nasaw points out that Hearst's actions were not the only reason \"Kane\" failed, however: the innovations Welles made with narrative, as well as the dark message at the heart of the film (that the pursuit of success is ultimately futile) meant that a popular audience could not appreciate its merits.\nHearst's attacks against Welles went beyond attempting to suppress the film. Welles said that while he was on his post-filming lecture tour a police detective approached him at a restaurant and advised him not to go back to his hotel. A 14-year-old girl had reportedly been hidden in the closet of his room, and two photographers were waiting for him to walk in. Knowing he would be jailed after the resulting publicity, Welles did not return to the hotel but waited until the train left town the following morning. \"But that wasn't Hearst,\" Welles said, \"that was a hatchet man from the local Hearst paper who thought he would advance himself by doing it.\"\nIn March 1941, Welles directed a Broadway version of Richard Wright's \"Native Son\" (and, for luck, used a \"Rosebud\" sled as a prop). \"Native Son\" received positive reviews, but Hearst-owned papers used the opportunity to attack Welles as a communist. The Hearst papers vociferously attacked Welles after his April 1941 radio play, \"His Honor, the Mayor\", produced for The Free Company radio series on CBS.\nWelles described his chance encounter with Hearst in an elevator at the Fairmont Hotel on the night \"Citizen Kane\" opened in San Francisco. Hearst and Welles's father were acquaintances, so Welles introduced himself and asked Hearst if he would like to come to the opening. Hearst did not respond. \"As he was getting off at his floor, I said, 'Charles Foster Kane would have accepted.' No reply\", recalled Welles. \"And Kane would have, you know. That was his style\u2014just as he finished Jed Leland's bad review of Susan as an opera singer.\"\nIn 1945, Hearst journalist Robert Shaw wrote that the film got \"a full tide of insensate fury\" from Hearst papers, \"then it ebbed suddenly. With one brain cell working, the chief realized that such hysterical barking by the trained seals would attract too much attention to the picture. But to this day the name of Orson Welles is on the official son-of-a-bitch list of every Hearst newspaper\".\nDespite Hearst's attempts to destroy the film, since 1941 references to his life and career have usually included a reference to \"Citizen Kane\", such as the headline 'Son of Citizen Kane Dies' for the obituary of Hearst's son. In 2012, the Hearst estate agreed to screen the film at Hearst Castle in San Simeon, breaking Hearst's ban on the film.\nRelease.\nRadio City Music Hall's management refused to screen \"Citizen Kane\" for its premiere. A possible factor was Parsons's threat that \"The American Weekly\" would run a defamatory story on the grandfather of major RKO stockholder Nelson Rockefeller. Other exhibitors feared being sued for libel by Hearst and refused to show the film. In March Welles threatened the RKO board of governors with a lawsuit if they did not release the film. Schaefer stood by Welles and opposed the board of governors. When RKO still delayed the film's release Welles offered to buy the film for $1\u00a0million and the studio finally agreed to release the film on May 1.\nSchaefer managed to book a few theaters willing to show the film. Hearst papers refused to accept advertising. RKO's publicity advertisements for the film erroneously promoted it as a love story.\n\"Kane\" opened at the RKO Palace Theatre on Broadway in New York on May 1, 1941, in Chicago on May 6, and in Los Angeles on May 8. Welles said that at the Chicago premiere that he attended the theater was almost empty.\nThe day after the New York release, \"The New York Times\" said \"it comes close to being the most sensational film ever made in Hollywood\". \"The Washington Post\" called it \"one of the most important films in the history\" of filmmaking. \"The Washington Evening Star\" said Welles was a genius who created \"a superbly dramatic biography of another genius\" and \"a picture that is revolutionary\". \"The Chicago Tribune\" called the film interesting and different but \"its sacrifice of simplicity to eccentricity robs it of distinction and general entertainment value\". \"The Los Angeles Times\" gave the film a mixed review, saying it was brilliant and skillful at times with an ending that \"rather fizzled\".\nThe film did well in cities and larger towns, but it fared poorly in more remote areas. RKO still had problems getting exhibitors to show the film. For example, one chain controlling more than 500 theaters got Welles's film as part of a package but refused to play it, reportedly out of fear of Hearst. Hearst's disruption of the film's release damaged its box office performance and, as a result, it lost $160,000 during its initial run. The film earned $23,878 during its first week in New York. By the ninth week it only made $7,279. Overall it lost money in New York, Boston, Chicago, Los Angeles, San Francisco and Washington, D.C., but made a profit in Seattle.\nContemporary responses.\n\"Citizen Kane\" received acclaim from several critics. \"New York Daily News\" critic Kate Cameron called it \"one of the most interesting and technically superior films that has ever come out of a Hollywood studio\". \"New York World-Telegram\" critic William Boehnel said that the film was \"staggering and belongs at once among the greatest screen achievements\". \"Time\" magazine wrote that \"it has found important new techniques in picture-making and story-telling.\" \"Life\" magazine's review said that \"few movies have ever come from Hollywood with such powerful narrative, such original technique, such exciting photography.\" John C. Mosher of \"The New Yorker\" called the film's style \"like fresh air\" and raved \"Something new has come to the movie world at last.\" Anthony Bower of \"The Nation\" called it \"brilliant\" and praised the cinematography and performances by Welles, Comingore and Cotten. John O'Hara's \"Newsweek\" review called it the best picture he'd ever seen and said Welles was \"the best actor in the history of acting.\" Welles called O'Hara's review \"the greatest review that anybody ever had.\"\nThe day following the premiere of \"Citizen Kane,\" \"The New York Times\" critic Bosley Crowther wrote that \"...\u00a0it comes close to being the most sensational film ever made in Hollywood.\"\nCount on Mr. Welles: he doesn't do things by halves.\u00a0... Upon the screen he discovered an area large enough for his expansive whims to have free play. And the consequence is that he has made a picture of tremendous and overpowering scope, not in physical extent so much as in its rapid and graphic rotation of thoughts. Mr. Welles has put upon the screen a motion picture that really moves.\nIn the UK C. A. Lejeune of \"The Observer\" called it \"The most exciting film that has come out of Hollywood in twenty-five years\" and Dilys Powell of \"The Sunday Times\" said the film's style was made \"with the ease and boldness and resource of one who controls and is not controlled by his medium.\" Edward Tangye Lean of \"Horizon\" praised the film's technical style, calling it \"perhaps a decade ahead of its contemporaries.\"\nA few reviews were mixed. Otis Ferguson of \"The New Republic\" said it was \"the boldest free-hand stroke in major screen production since Griffith and Bitzer were running wild to unshackle the camera\", but also criticized its style, calling it a \"retrogression in film technique\" and stating that \"it holds no great place\" in film history. Ferguson reacted to some of the film's celebrated visual techniques by calling them \"just willful dabbling\" and \"the old shell game.\" In a rare film review, filmmaker Erich von Stroheim criticized the film's story and non-linear structure, but praised the technical style and performances, and wrote \"Whatever the truth may be about it, \"Citizen Kane\" is a great picture and will go down in screen history. More power to Welles!\"\nSome prominent critics wrote negative reviews. In his 1941 review for \"Sur\", Jorge Luis Borges famously called the film \"a labyrinth with no center\" and predicted that its legacy would be a film \"whose historical value is undeniable but which no one cares to see again.\" \"The Argus Weekend Magazine\" critic Erle Cox called the film \"amazing\" but thought that Welles's break with Hollywood traditions was \"overdone\". \"Tatler\"'s James Agate called it \"the well-intentioned, muddled, amateurish thing one expects from high-brows\" and \"a quite good film which tries to run the psychological essay in harness with your detective thriller, and doesn't quite succeed.\" Eileen Creelman of \"The New York Sun\" called it \"a cold picture, unemotional, a puzzle rather than a drama\". Other people who disliked the film were W. H. Auden and James Agee. After watching the film on January 29, 1942 Kenneth Williams, then aged 15, writing in his first diary curtly described it as \"boshey rot\".\nModern critics have given \"Citizen Kane\" an even more positive response. Review aggregation website Rotten Tomatoes reports that 99% of 125 critics gave the film a positive review, with an average rating of 9.70/10. The site's critical consensus reads: \"Orson Welles's epic tale of a publishing tycoon's rise and fall is entertaining, poignant, and inventive in its storytelling, earning its reputation as a landmark achievement in film.\" In April 2021, it was noted that the addition of an 80-year-old negative review from the \"Chicago Tribune\" reduced the film's rating from 100% to 99% on the site; \"Citizen Kane\" held its 100% rating until early 2021. On Metacritic, however, the film still has a rare weighted average score of 100 out of 100 based on 19 critics, indicating \"universal acclaim\". On review aggregator site CherryPicks, it received a perfect score of 100.\nAccolades.\nIt was widely believed the film would win most of its Academy Award nominations, but it received only the award for Best Original Screenplay. \"Variety\" reported that block voting by screen extras deprived \"Citizen Kane\" of Best Picture and Best Actor, and similar prejudices were likely to have been responsible for the film receiving no technical awards.\nLegacy.\n\"Citizen Kane\" was the only film made under Welles's original contract with RKO Pictures, which gave him complete creative control. Welles's new business manager and attorney permitted the contract to lapse. In July 1941, Welles reluctantly signed a new and less favorable deal with RKO under which he produced and directed \"The Magnificent Ambersons\" (1942), produced \"Journey into Fear\" (1943), and began \"It's All True\", a film he agreed to do without payment. In the new contract Welles was an employee of the studio and lost the right to final cut, which later allowed RKO to modify and re-cut \"The Magnificent Ambersons\" over his objections. In June 1942, Schaefer resigned the presidency of RKO Pictures and Welles's contract was terminated by his successor.\nRelease in Europe.\nDuring World War II, \"Citizen Kane\" was not seen in most European countries. It was shown in France for the first time on July 10, 1946, at the Marbeuf theater in Paris. Initially most French film critics were influenced by the negative reviews of Jean-Paul Sartre in 1945 and Georges Sadoul in 1946. At that time many French intellectuals and filmmakers shared Sartre's negative opinion that Hollywood filmmakers were uncultured. Sartre criticized the film's flashbacks for its nostalgic and romantic preoccupation with the past instead of the realities of the present and said that \"the whole film is based on a misconception of what cinema is all about. The film is in the past tense, whereas we all know that cinema has got to be in the present tense.\"\nAndr\u00e9 Bazin, a then little-known film critic working for Sartre's \"Les Temps modernes\", was asked to give an impromptu speech about the film after a screening at the Colis\u00e9e Theatre in the autumn of 1946 and changed the opinion of much of the audience. This speech led to Bazin's 1947 article \"The Technique of Citizen Kane\", which directly influenced public opinion about the film. Carringer wrote that Bazin was \"the one who did the most to enhance the film's reputation.\" Both Bazin's critique of the film and his theories about cinema itself centered around his strong belief in mise-en-sc\u00e8ne. These theories were diametrically opposed to both the popular Soviet montage theory and the politically Marxist and anti-Hollywood beliefs of most French film critics at that time. Bazin believed that a film should depict reality without the filmmaker imposing their \"will\" on the spectator, which the Soviet theory supported. Bazin wrote that \"Citizen Kane\"'s mise-en-sc\u00e8ne created a \"new conception of filmmaking\" and that the freedom given to the audience from the deep focus shots was innovative by changing the entire concept of the cinematic image. Bazin wrote extensively about the mise-en-sc\u00e8ne in the scene where Susan Alexander attempts suicide, which was one long take while other films would have used four or five shots in the scene. Bazin wrote that the film's mise-en-sc\u00e8ne \"forces the spectator to participate in the meaning of the film\" and creates \"a psychological realism which brings the spectator back to the real conditions of perception.\"\nIn his 1950 essay \"The Evolution of the Language of Cinema\", Bazin placed \"Citizen Kane\" center stage as a work which ushered in a new period in cinema. One of the first critics to defend motion pictures as being on the same artistic level as literature or painting, Bazin often used the film as an example of cinema as an art form and wrote that \"Welles has given the cinema a theoretical restoration. He has enriched his filmic repertory with new or forgotten effects that, in today's artistic context, take on a significance we didn't know they could have.\" Bazin also compared the film to Roberto Rossellini's \"Paisan\" for having \"the same aesthetic concept of realism\" and to the films of William Wyler shot by Toland (such as \"The Little Foxes\" and \"The Best Years of Our Lives\"), all of which used deep focus cinematography that Bazin called \"a dialectical step forward in film language.\"\nBazin's praise of the film went beyond film theory and reflected his own philosophy towards life itself. His metaphysical interpretations about the film reflected humankind's place in the universe. Bazin believed that the film examined one person's identity and search for meaning. It portrayed the world as ambiguous and full of contradictions, whereas films up until then simply portrayed people's actions and motivations. Bazin's biographer Dudley Andrew wrote that:\nThe world of \"Citizen Kane\", that mysterious, dark, and infinitely deep world of space and memory where voices trail off into distant echoes and where meaning dissolves into interpretation, seemed to Bazin to mark the starting point from which all of us try to construct provisionally the sense of our lives.\nBazin went on to co-found \"Cahiers du cin\u00e9ma\", whose contributors (including future film directors Fran\u00e7ois Truffaut and Jean-Luc Godard) also praised the film. The popularity of Truffaut's auteur theory helped the film's and Welles's reputation.\nRe-evaluation.\nBy 1942 \"Citizen Kane\" had run its course theatrically and, apart from a few showings at big city arthouse cinemas, it largely vanished and both the film's and Welles's reputation fell among American critics. In 1949 critic Richard Griffith in his overview of cinema, \"The Film Till Now\", dismissed \"Citizen Kane\" as \"...\u00a0tinpot if not crackpot Freud.\"\nIn the United States, it was neglected and forgotten until its revival on television in the mid-to-late 1950s. Three key events in 1956 led to its re-evaluation in the United States: first, RKO was one of the first studios to sell its library to television, and early that year \"Citizen Kane\" started to appear on television; second, the film was re-released theatrically to coincide with Welles's return to the New York stage, where he played \"King Lear\"; and third, American film critic Andrew Sarris wrote \"Citizen Kane: The American Baroque\" for \"Film Culture\", and described it as \"the great American film\" and \"the work that influenced the cinema more profoundly than any American film since \"The Birth of a Nation\".\" Carringer considers Sarris's essay as the most important influence on the film's reputation in the US.\nDuring Expo 58, a poll of over 100 film historians named \"Kane\" one of the top ten greatest films ever made (the group gave first-place honors to \"Battleship Potemkin\"). When a group of young film directors announced their vote for the top six, they were booed for not including the film.\nIn the decades since, its critical status as one of the greatest films ever made has grown, with numerous essays and books on it including Peter Cowie's \"The Cinema of Orson Welles\", Ronald Gottesman's \"Focus on Citizen Kane\", a collection of significant reviews and background pieces, and most notably Kael's essay, \"Raising Kane\", which promoted the value of the film to a much wider audience than it had reached before. Despite its criticism of Welles, it further popularized the notion of \"Citizen Kane\" as the great American film. The rise of art house and film society circuits also aided in the film's rediscovery. David Thomson said that the film 'grows with every year as America comes to resemble it.\"\nThe British magazine \"Sight &amp; Sound\" has produced a Top Ten list surveying film critics every decade since 1952, and is regarded as one of the most respected barometers of critical taste. \"Citizen Kane\" was a runner up to the top 10 in its 1952 poll but was voted as the greatest film ever made in its 1962 poll, retaining the top spot in every subsequent poll until 2012, when \"Vertigo\" displaced it.\nThe film has also ranked number one in the following film \"best of\" lists: Julio Castedo's \"The 100 Best Films of the Century\", Cahiers du cin\u00e9ma's 100 films pour une cin\u00e9math\u00e8que id\u00e9ale, \"Kinovedcheskie Zapiski\", \"Time Out\" magazine's Top 100 Films (Centenary), \"The Village Voice\"'s 100 Greatest Films, and The Royal Belgian Film Archive's Most Important and Misappreciated American Films.\nRoger Ebert called \"Citizen Kane\" the greatest film ever made: \"But people don't always ask about the greatest film. They ask, 'What's your favorite movie?' Again, I always answer with \"Citizen Kane\".\"\nIn 1998 \"Time Out\" conducted a reader's poll and \"Citizen Kane\" was voted 3rd best film of all time. On February 18, 1999, the United States Postal Service honored \"Citizen Kane\" by including it in its Celebrate the Century series. The film was honored again February 25, 2003, in a series of U.S. postage stamps marking the 75th anniversary of the Academy of Motion Picture Arts and Sciences. Art director Perry Ferguson represents the behind-the-scenes craftsmen of filmmaking in the series; he is depicted completing a sketch for \"Citizen Kane\".\n\"Citizen Kane\" was ranked number one in the American Film Institute's polls of film industry artists and leaders in 1998 and 2007. \"Rosebud\" was chosen as the 17th most memorable movie quotation in a 2005 AFI poll. The film's score was one of 250 nominees for the top 25 film scores in American cinema in another 2005 AFI poll. In 2005 the film was included on \"Time\"'s All-Time 100 best movies list.\nIn 2012, the Motion Picture Editors Guild published a list of the 75 best-edited films of all time based on a survey of its membership. \"Citizen Kane\" was listed second. In 2015, \"Citizen Kane\" ranked 1st on BBC's \"100 Greatest American Films\" list, voted on by film critics from around the world.\nInfluence.\n\"Citizen Kane\" has been called the most influential film of all time. Richard Corliss has asserted that Jules Dassin's 1941 film \"The Tell-Tale Heart\" was the first example of its influence and the first pop culture reference to the film occurred later in 1941 when the spoof comedy \"Hellzapoppin\"' featured a \"Rosebud\" sled. The film's cinematography was almost immediately influential and in 1942 \"American Cinematographer\" wrote \"without a doubt the most immediately noticeable trend in cinematography methods during the year was the trend toward crisper definition and increased depth of field.\"\nThe cinematography influenced John Huston's \"The Maltese Falcon\". Cinematographer Arthur Edeson used a wider-angle lens than Toland and the film includes many long takes, low angles and shots of the ceiling, but it did not use deep focus shots on large sets to the extent that \"Citizen Kane\" did. Edeson and Toland are often credited together for revolutionizing cinematography in 1941. Toland's cinematography influenced his own work on \"The Best Years of Our Lives\". Other films influenced include \"Gaslight\", \"Mildred Pierce\" and \"Jane Eyre\". Cinematographer Kazuo Miyagawa said that his use of deep focus was influenced by \"the camera work of Gregg Toland in \"Citizen Kane\"\" and not by traditional Japanese art.\nIts cinematography, lighting, and flashback structure influenced such film noirs of the 1940s and 1950s as \"The Killers\", \"Keeper of the Flame\", \"Caught\", \"The Great Man\" and \"This Gun for Hire\". David Bordwell and Kristin Thompson have written that \"For over a decade thereafter American films displayed exaggerated foregrounds and somber lighting, enhanced by long takes and exaggerated camera movements.\" However, by the 1960s filmmakers such as those from the French New Wave and Cin\u00e9ma v\u00e9rit\u00e9 movements favored \"flatter, more shallow images with softer focus\" and \"Citizen Kane\"'s style became less fashionable. American filmmakers in the 1970s combined these two approaches by using long takes, rapid cutting, deep focus and telephoto shots all at once. Its use of long takes influenced films such as \"The Asphalt Jungle\", and its use of deep focus cinematography influenced \"Gun Crazy\", \"The Whip Hand\", \"The Devil's General\" and \"Justice Is Done\". The flashback structure in which different characters have conflicting versions of past events influenced \"La commare secca\" and \"Man of Marble\".\nThe film's structure influenced the biographical films \"Lawrence of Arabia\" and \"\"\u2014which begin with the subject's death and show their life in flashbacks\u2014as well as Welles's thriller \"Mr. Arkadin\". Rosenbaum sees similarities in the film's plot to \"Mr. Arkadin\", as well as the theme of nostalgia for loss of innocence throughout Welles's career, beginning with \"Citizen Kane\" and including \"The Magnificent Ambersons\", \"Mr. Arkadin\" and \"Chimes at Midnight\". Rosenbaum also points out how the film influenced Warren Beatty's \"Reds\". The film depicts the life of Jack Reed through the eyes of Louise Bryant, much as Kane's life is seen through the eyes of Thompson and the people who he interviews. Rosenbaum also compared the romantic montage between Reed and Bryant with the breakfast table montage in \"Citizen Kane\".\nAkira Kurosawa's \"Rashomon\" is often compared to the film due to both having complicated plot structures told by multiple characters in the film. Welles said his initial idea for the film was \"Basically, the idea \"Rashomon\" used later on,\" however Kurosawa had not yet seen the film before making \"Rashomon\" in 1950. Nigel Andrews has compared the film's complex plot structure to \"Rashomon\", \"Last Year at Marienbad\", \"Memento\" and \"Magnolia\". Andrews also compares Charles Foster Kane to Michael Corleone in \"The Godfather\", Jake LaMotta in \"Raging Bull\" and Daniel Plainview in \"There Will Be Blood\" for their portrayals of \"haunted megalomaniac[s], presiding over the shards of [their] own [lives].\"\nThe films of Paul Thomas Anderson have been compared to it. \"Variety\" compared \"There Will Be Blood\" to the film and called it \"one that rivals \"Giant\" and \"Citizen Kane\" in our popular lore as origin stories about how we came to be the people we are.\" \"The Master\" has been called \"movieland's only spiritual sequel to \"Citizen Kane\" that doesn't shrivel under the hefty comparison\". \"The Social Network\" has been compared to the film for its depiction of a media mogul and by the character Erica Albright being similar to \"Rosebud\". The controversy of the Sony hacking before the release of \"The Interview\" brought comparisons of Hearst's attempt to suppress the film. The film's plot structure and some specific shots influenced Todd Haynes's \"Velvet Goldmine\". Abbas Kiarostami's \"The Traveler\" has been called \"the \"Citizen Kane\" of the Iranian children's cinema.\" The film's use of overlapping dialogue has influenced the films of Robert Altman and Carol Reed. Reed's films \"Odd Man Out\", \"The Third Man\" (in which Welles and Cotten appeared) and \"Outcast of the Islands\" were also influenced by the film's cinematography.\nMany directors have listed it as one of the greatest films ever made, including Woody Allen, Michael Apted, Les Blank, Kenneth Branagh, Paul Greengrass, Satyajit Ray, Michel Hazanavicius, Michael Mann, Sam Mendes, Ji\u0159\u00ed Menzel, Paul Schrader, Martin Scorsese, Denys Arcand, Gillian Armstrong, John Boorman, Roger Corman, Alex Cox, Milo\u0161 Forman, Norman Jewison, Richard Lester, Richard Linklater, Paul Mazursky, Ronald Neame, Sydney Pollack and Stanley Kubrick. Yasujir\u014d Ozu said it was his favorite non-Japanese film and was impressed by its techniques. Fran\u00e7ois Truffaut said that the film \"has inspired more vocations to cinema throughout the world than any other\" and recognized its influence in \"The Barefoot Contessa\", \"Les Mauvaises Rencontres\", \"Lola Mont\u00e8s\", and \"8 1/2\". Truffaut's \"Day for Night\" pays tribute to the film in a dream sequence depicting a childhood memory of the character played by Truffaut stealing publicity photos from the film. Numerous film directors have cited the film as influential on their own films, including Theo Angelopoulos, Luc Besson, the Coen brothers, Francis Ford Coppola, Brian De Palma, John Frankenheimer, Stephen Frears, Sergio Leone, Michael Mann, Ridley Scott, Martin Scorsese, Bryan Singer and Steven Spielberg. Ingmar Bergman disliked the film and called it \"a total bore. Above all, the performances are worthless. The amount of respect that movie has is absolutely unbelievable!\"\nWilliam Friedkin said that the film influenced him and called it \"a veritable quarry for filmmakers, just as Joyce's \"Ulysses\" is a quarry for writers.\" The film has also influenced other art forms. Carlos Fuentes's novel \"The Death of Artemio Cruz\" was partially inspired by the film and the rock band The White Stripes paid unauthorized tribute to the film in the song \"The Union Forever\".\nFilm memorabilia.\nIn 1982, film director Steven Spielberg bought a \"Rosebud\" sled for $60,500; it was one of three balsa sleds used in the closing scenes and the only one that was not burned. Spielberg eventually donated the sled to the Academy Museum of Motion Pictures as he stated he felt it belonged in a museum. After the Spielberg purchase, it was reported that retiree Arthur Bauer claimed to own another \"Rosebud\" sled. In early 1942, when Bauer was 12, he had won an RKO publicity contest and selected the hardwood sled as his prize. In 1996, Bauer's estate offered the painted pine sled at auction through Christie's. Bauer's son told CBS News that his mother had once wanted to paint the sled and use it as a plant stand, but Bauer told her to \"just save it and put it in the closet.\" The sled was sold to an anonymous bidder for $233,500.\nWelles's Oscar for Best Original Screenplay was believed to be lost until it was rediscovered in 1994. It was withdrawn from a 2007 auction at Sotheby's when bidding failed to reach its estimate of $800,000 to $1.2\u00a0million. Owned by the charitable Dax Foundation, it was auctioned for $861,542 in 2011 to an anonymous buyer. Mankiewicz's Oscar was sold at least twice, in 1999 and again in 2012, the latest price being $588,455.\nIn 1989, Mankiewicz's personal copy of the \"Citizen Kane\" script was auctioned at Christie's. The leather-bound volume included the final shooting script and a carbon copy of \"American\" that bore handwritten annotations\u2014purportedly made by Hearst's lawyers, who were said to have obtained it in the manner described by Kael in \"Raising Kane\". Estimated to bring $70,000 to $90,000, it sold for a record $231,000.\nIn 2007, Welles's personal copy of the last revised draft of \"Citizen Kane\" before the shooting script was sold at Sotheby's for $97,000. A second draft of the script titled \"American\", marked \"Mr. Welles' working copy\", was auctioned by Sotheby's in 2014 for $164,692. A collection of 24 pages from a working script found in Welles's personal possessions by his daughter Beatrice Welles was auctioned in 2014 for $15,000.\nIn 2014, a collection of approximately 235 \"Citizen Kane\" stills and production photos that had belonged to Welles was sold at auction for $7,812.\nRights and home media.\nThe composited camera negative of \"Citizen Kane\" is believed to be lost forever. The most commonly-reported explanation is that it was destroyed in a New Jersey film laboratory fire in the 1970s. However, in 2021, Nicolas Falacci revealed that he had been told \"the real story\" by a colleague, when he was one of two employees in the film restoration lab which assembled the 1991 \"restoration\" from the best available elements. Falacci noted that throughout the process he had daily visits in 1990-1 from an unnamed \"older RKO executive showing up every day \u2013 nervous and sweating\". According to Falacci's colleague, this elderly man was keen to cover up a clerical error he had made decades earlier when in charge of the studio's inventory, which had resulted in the original camera negatives being sent to a silver reclamation plant, destroying the nitrate film to extract its valuable silver content. Falacci's account is impossible to verify, but it would have been fully in keeping with industry standard practice for many decades, which was to destroy prints and negatives of countless older films deemed non-commercially viable, to extract the silver.\nSubsequent prints were derived from a master positive (a fine-grain preservation element) made in the 1940s and originally intended for use in overseas distribution. Modern techniques were used to produce a pristine print for a 50th Anniversary theatrical reissue in 1991 which Paramount Pictures released for then-owner Turner Broadcasting System, which earned $1.6\u00a0million in North America and $ worldwide.\nIn 1955, RKO sold the American television rights to its film library, including \"Citizen Kane\", to C&amp;C Television Corp. In 1960, television rights to the pre-1959 RKO's live-action library were acquired by United Artists. RKO kept the non-broadcast television rights to its library.\nIn 1976, when home video was in its infancy, entrepreneur Snuff Garrett bought cassette rights to the RKO library for what United Press International termed \"a pittance\". In 1978 The Nostalgia Merchant released the film through Media Home Entertainment. By 1980 the 800-title library of The Nostalgia Merchant was earning $2.3\u00a0million a year. \"Nobody wanted cassettes four years ago,\" Garrett told UPI. \"It wasn't the first time people called me crazy. It was a hobby with me which became big business.\" RKO Home Video released the film on VHS and Betamax in 1985.\nOn December 3, 1984, The Criterion Collection released the film as its first LaserDisc. It was made from a fine grain master positive provided by the UCLA Film and Television Archive. When told about the then-new concept of having an audio commentary on the disc, Welles was skeptical but said \"theoretically, that's good for teaching movies, so long as they don't talk nonsense.\" In 1992 Criterion released a new 50th Anniversary Edition LaserDisc. This version had an improved transfer and additional special features, including the documentary \"The Legacy of Citizen Kane\" and Welles's early short \"The Hearts of Age\".\nTurner Broadcasting System acquired broadcast television rights to the RKO library in 1986 and the full worldwide rights to the library in 1987. The RKO Home Video unit was reorganized into Turner Home Entertainment that year. In 1991 Turner released a 50th Anniversary Edition on VHS and as a collector's edition that includes the film, the documentary \"Reflections On Citizen Kane\", Harlan Lebo's 50th anniversary album, a poster and a copy of the original script. In 1996, Time Warner acquired Turner and Warner Home Video absorbed Turner Home Entertainment. In 2011, Warner Bros. Discovery's Warner Bros. unit had distribution rights for the film.\nIn 2001, Warner Home Video released a 60th Anniversary Collectors Edition DVD. The two-disc DVD included feature-length commentaries by Roger Ebert and Peter Bogdanovich, as well as a second DVD with the feature length documentary \"The Battle Over Citizen Kane\" (1999). It was simultaneously released on VHS. The DVD was criticized for being \"too bright, too clean; the dirt and grime had been cleared away, but so had a good deal of the texture, the depth, and the sense of film grain.\"\nIn 2003, Welles's daughter Beatrice Welles sued Turner Entertainment, claiming the Welles estate is the legal copyright holder of the film. She claimed that Welles's deal to terminate his contracts with RKO meant that Turner's copyright of the film was null and void. She also claimed that the estate of Orson Welles was owed 20% of the film's profits if her copyright claim was not upheld. In 2007 she was allowed to proceed with the lawsuit, overturning the 2004 decision in favor of Turner Entertainment on the issue of video rights.\nIn 2011, it was released on Blu-ray and DVD in a 70th Anniversary Edition. The \"San Francisco Chronicle\" called it \"the Blu-ray release of the year.\" Supplements included everything available on the 2001 Warner Home Video release, including \"The Battle Over Citizen Kane\" DVD. A 70th Anniversary Ultimate Collector's Edition added a third DVD with \"RKO 281\" (1999), an award winning TV movie about the making of the film. Its packaging extras included a hardcover book and a folio containing mini reproductions of the original souvenir program, lobby cards, and production memos and correspondence. The transfer for the US releases were scanned as 4K resolution from three different 35mm prints and rectified the quality issues of the 2001 DVD. The rest of the world continued to receive home video releases based on the older transfer. This was partially rectified in 2016 with the release of the 75th Anniversary Edition in both the UK and US, which was a straight repackaging of the main disc from the 70th Anniversary Edition.\nOn August 11, 2021 Criterion announced their first 4K Ultra HD releases, a six-film slate, would include \"Citizen Kane\". Criterion indicated each title was to be available in a combo pack including a 4K UHD disc of the feature film as well as the film and special features on the companion Blu-rays. \"Citizen Kane\" was released on November 23, 2021 by the collection as a 4K and 3 Blu-ray disc package. However, the release was recalled because at the half-hour mark on the regular blu-ray, the contrast fell sharply, which resulted in a much darker image compared to what was supposed to occur. However this issue does not apply to the 4K version itself.\nColorization controversy.\nIn the 1980s, \"Citizen Kane\" became a catalyst in the controversy over the colorization of black-and-white films. One proponent of film colorization was Ted Turner, whose Turner Entertainment Company owned the RKO library. A Turner Entertainment spokesperson initially stated that \"Citizen Kane\" would not be colorized, but in July 1988 Turner said, \"\"Citizen Kane?\" I'm thinking of colorizing it.\" In early 1989 it was reported that two companies were producing color tests for Turner Entertainment. Criticism increased when filmmaker Henry Jaglom stated that shortly before his death Welles had implored him \"don't let Ted Turner deface my movie with his crayons.\"\nIn February 1989, Turner Entertainment President Roger Mayer announced that work to colorize the film had been stopped due to provisions in Welles's 1939 contract with RKO that \"could be read to prohibit colorization without permission of the Welles estate.\" Mayer added that Welles's contract was \"quite unusual\" and \"other contracts we have checked out are not like this at all.\" Turner had only colorized the final reel of the film before abandoning the project. In 1991 one minute of the colorized test footage was included in the BBC \"Arena\" documentary \"The Complete Citizen Kane\".\nThe colorization controversy was a factor in the passage of the National Film Preservation Act in 1988 which created the National Film Registry the following year. ABC News anchor Peter Jennings reported that \"one major reason for doing this is to require people like the broadcaster Ted Turner, who's been adding color to some movies and re-editing others for television, to put notices on those versions saying that the movies have been altered\".\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5225", "revid": "31844414", "url": "https://en.wikipedia.org/wiki?curid=5225", "title": "Code", "text": "System of rules to convert information into another form or representation\nIn communications and information processing, code is a system of rules to convert information\u2014such as a letter, word, sound, image, or gesture\u2014into another form, sometimes shortened or secret, for communication through a communication channel or storage in a storage medium. An early example is an invention of language, which enabled a person, through speech, to communicate what they thought, saw, heard, or felt to others. But speech limits the range of communication to the distance a voice can carry and limits the audience to those present when the speech is uttered. The invention of writing, which converted spoken language into visual symbols, extended the range of communication across space and time.\nThe process of encoding converts information from a source into symbols for communication or storage. Decoding is the reverse process, converting code symbols back into a form that the recipient understands, such as English or/and Spanish.\nOne reason for coding is to enable communication in places where ordinary plain language, spoken or written, is difficult or impossible. For example, semaphore, where the configuration of flags held by a signaler or the arms of a semaphore tower encodes parts of the message, typically individual letters, and numbers. Another person standing a great distance away can interpret the flags and reproduce the words sent.\nTheory.\nIn information theory and computer science, a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet, by \"encoded\" strings, which may be in some other target alphabet. An extension of the code for representing sequences of symbols over the source alphabet is obtained by concatenating the encoded strings.\nBefore giving a mathematically precise definition, this is a brief example. The mapping\nformula_1\nis a code, whose source alphabet is the set formula_2 and whose target alphabet is the set formula_3. Using the extension of the code, the encoded string 0011001 can be grouped into codewords as 0\u2002011\u20020\u200201, and these in turn can be decoded to the sequence of source symbols \"acab\".\nUsing terms from formal language theory, the precise mathematical definition of this concept is as follows: let S and T be two finite sets, called the source and target alphabets, respectively. A code formula_4 is a total function mapping each symbol from S to a sequence of symbols over T. The extension formula_5 of formula_6, is a homomorphism of formula_7 into formula_8, which naturally maps each sequence of source symbols to a sequence of target symbols.\nVariable-length codes.\nIn this section, we consider codes that encode each source (clear text) character by a code word from some dictionary, and concatenation of such code words give us an encoded string. Variable-length codes are especially useful when clear text characters have different probabilities; see also entropy encoding.\nA \"prefix code\" is a code with the \"prefix property\": there is no valid code word in the system that is a prefix (start) of any other valid code word in the set. Huffman coding is the most known algorithm for deriving prefix codes. Prefix codes are widely referred to as \"Huffman codes\" even when the code was not produced by a Huffman algorithm. Other examples of prefix codes are country calling codes, the country and publisher parts of ISBNs, and the Secondary Synchronization Codes used in the UMTS WCDMA 3G Wireless Standard.\nKraft's inequality characterizes the sets of codeword lengths that are possible in a prefix code. Virtually any uniquely decodable one-to-many code, not necessarily a prefix one, must satisfy Kraft's inequality.\nError-correcting codes.\nCodes may also be used to represent data in a way more resistant to errors in transmission or storage. This so-called error-correcting code works by including carefully crafted redundancy with the stored (or transmitted) data. Examples include Hamming codes, Reed\u2013Solomon, Reed\u2013Muller, Walsh\u2013Hadamard, Bose\u2013Chaudhuri\u2013Hochquenghem, Turbo, Golay, algebraic geometry codes, low-density parity-check codes, and space\u2013time codes.\nError detecting codes can be optimised to detect \"burst errors\", or \"random errors\".\nExamples.\nCodes in communication used for brevity.\nA cable code replaces words (e.g. \"ship\" or \"invoice\") with shorter words, allowing the same information to be sent with fewer characters, more quickly, and less expensively.\nCodes can be used for brevity. When telegraph messages were the state of the art in rapid long-distance communication, elaborate systems of commercial codes that encoded complete phrases into single mouths (commonly five-minute groups) were developed, so that telegraphers became conversant with such \"words\" as \"BYOXO\" (\"Are you trying to weasel out of our deal?\"), \"LIOUY\" (\"Why do you not answer my question?\"), \"BMULD\" (\"You're a skunk!\"), or \"AYYLU\" (\"Not clearly coded, repeat more clearly.\"). Code words were chosen for various reasons: length, pronounceability, etc. Meanings were chosen to fit perceived needs: commercial negotiations, military terms for military codes, diplomatic terms for diplomatic codes, any and all of the preceding for espionage codes. Codebooks and codebook publishers proliferated, including one run as a front for the American Black Chamber run by Herbert Yardley between the First and Second World Wars. The purpose of most of these codes was to save on cable costs. The use of data coding for data compression predates the computer era; an early example is the telegraph Morse code where more-frequently used characters have shorter representations. Techniques such as Huffman coding are now used by computer-based algorithms to compress large data files into a more compact form for storage or transmission.\nCharacter encodings.\nCharacter encodings are representations of textual data. A given character encoding may be associated with a specific character set (the collection of characters which it can represent), though some character sets have multiple character encodings and vice versa. Character encodings may be broadly grouped according to the number of bytes required to represent a single character: there are single-byte encodings, multibyte (also called wide) encodings, and variable-width (also called variable-length) encodings. The earliest character encodings were single-byte, the best-known example of which is ASCII. ASCII remains in use today, for example in HTTP headers. However, single-byte encodings cannot model character sets with more than 256 characters. Scripts that require large character sets such as Chinese, Japanese and Korean must be represented with multibyte encodings. Early multibyte encodings were fixed-length, meaning that although each character was represented by more than one byte, all characters used the same number of bytes (\"word length\"), making them suitable for decoding with a lookup table. The final group, variable-width encodings, is a subset of multibyte encodings. These use more complex encoding and decoding logic to efficiently represent large character sets while keeping the representations of more commonly used characters shorter or maintaining backward compatibility properties. This group includes UTF-8, an encoding of the Unicode character set; UTF-8 is the most common encoding of text media on the Internet.\nGenetic code.\nBiological organisms contain genetic material that is used to control their function and development. This is DNA, which contains units named genes from which messenger RNA is derived. This in turn produces proteins through a genetic code in which a series of triplets (codons) of four possible nucleotides can be translated into one of twenty possible amino acids. A sequence of codons results in a corresponding sequence of amino acids that form a protein molecule; a type of codon called a stop codon signals the end of the sequence.\nG\u00f6del code.\nIn mathematics, a G\u00f6del code was the basis for the proof of G\u00f6del's incompleteness theorem. Here, the idea was to map mathematical notation to a natural number (using a G\u00f6del numbering).\nOther.\nThere are codes using colors, like traffic lights, the color code employed to mark the nominal value of the electrical resistors or that of the trashcans devoted to specific types of garbage (paper, glass, organic, etc.).\nIn marketing, coupon codes can be used for a financial discount or rebate when purchasing a product from a (usual internet) retailer.\nIn military environments, specific sounds with the cornet are used for different uses: to mark some moments of the day, to command the infantry on the battlefield, etc.\nCommunication systems for sensory impairments, such as sign language for deaf people and braille for blind people, are based on movement or tactile codes.\nMusical scores are the most common way to encode music.\nSpecific games have their own code systems to record the matches, e.g. chess notation.\nCryptography.\nIn the history of cryptography, codes were once common for ensuring the confidentiality of communications, although ciphers are now used instead.\nSecret codes intended to obscure the real messages, ranging from serious (mainly espionage in military, diplomacy, business, etc.) to trivial (romance, games) can be any kind of imaginative encoding: flowers, game cards, clothes, fans, hats, melodies, birds, etc., in which the sole requirement is the pre-agreement on the meaning by both the sender and the receiver.\nOther examples.\nOther examples of encoding include:\nOther examples of decoding include:\nCodes and acronyms.\nAcronyms and abbreviations can be considered codes, and in a sense, all languages and writing systems are codes for human thought.\nInternational Air Transport Association airport codes are three-letter codes used to designate airports and used for bag tags. Station codes are similarly used on railways but are usually national, so the same code can be used for different stations if they are in different countries.\nOccasionally, a code word achieves an independent existence (and meaning) while the original equivalent phrase is forgotten or at least no longer has the precise meaning attributed to the code word. For example, '30' was widely used in journalism to mean \"end of story\", and has been used in other contexts to signify \"the end\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5227", "revid": "20836525", "url": "https://en.wikipedia.org/wiki?curid=5227", "title": "Chess Board", "text": ""}
{"id": "5228", "revid": "20319727", "url": "https://en.wikipedia.org/wiki?curid=5228", "title": "Cheirogaleidae", "text": "Family of lemurs\nThe Cheirogaleidae are the family of strepsirrhine primates containing the various dwarf and mouse lemurs. Like all other lemurs, cheirogaleids live exclusively on the island of Madagascar.\nCharacteristics.\nCheirogaleids are smaller than the other lemurs and, in fact, they are the smallest primates. They have soft, long fur, colored grey-brown to reddish on top, with a generally brighter underbelly. Typically, they have small ears, large, close-set eyes, and long hind legs. Like all strepsirrhines, they have fine claws at the second toe of the hind legs. They grow to a size of only 13 to 28\u00a0cm, with a tail that is very long, sometimes up to one and a half times as long as the body. They weigh no more than 500\u00a0grams, with some species weighing as little as 60\u00a0grams.\nDwarf and mouse lemurs are nocturnal and arboreal. They are excellent climbers and can also jump far, using their long tails for balance. When on the ground (a rare occurrence), they move by hopping on their hind legs. They spend the day in tree hollows or leaf nests. Cheirogaleids are typically solitary, but sometimes live together in pairs.\nTheir eyes possess a tapetum lucidum, a light-reflecting layer that improves their night vision. Some species, such as the lesser dwarf lemur, store fat at the hind legs and the base of the tail, and hibernate. Unlike lemurids, they have long upper incisors, although they do have the comb-like teeth typical of all strepsirhines. They have the dental formula: 2.1.3.32.1.3.3\nCheirogaleids are omnivores, eating fruits, flowers and leaves (and sometimes nectar), as well as insects, spiders, and small vertebrates.\nThe females usually have three pairs of nipples. After a meager 60-day gestation, they will bear two to four (usually two or three) young. After five to six weeks, the young are weaned and become fully mature near the end of their first year or sometime in their second year, depending on the species. In human care, they can live for up to 15 years, although their life expectancy in the wild is probably significantly shorter.\nClassification.\nThe five genera of cheirogaleids contain 42 species.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5229", "revid": "1136969803", "url": "https://en.wikipedia.org/wiki?curid=5229", "title": "Callitrichidae", "text": "Family of New World monkeys\nThe Callitrichidae (also called Arctopitheci or Hapalidae) are a family of New World monkeys, including marmosets, tamarins, and lion tamarins. At times, this group of animals has been regarded as a subfamily, called the Callitrichinae, of the family Cebidae.\nThis taxon was traditionally thought to be a primitive lineage, from which all the larger-bodied platyrrhines evolved. However, some works argue that callitrichids are actually a dwarfed lineage.\nAncestral stem-callitrichids likely were \"normal-sized\" ceboids that were dwarfed through evolutionary time. This may exemplify a rare example of insular dwarfing in a mainland context, with the \"islands\" being formed by biogeographic barriers during arid climatic periods when forest distribution became patchy, and/or by the extensive river networks in the Amazon Basin.\nAll callitrichids are arboreal. They are the smallest of the simian primates. They eat insects, fruit, and the sap or gum from trees; occasionally, they take small vertebrates. The marmosets rely quite heavily on tree exudates, with some species (e.g. \"Callithrix jacchus\" and \"Cebuella pygmaea\") considered obligate exudativores.\nCallitrichids typically live in small, territorial groups of about five or six animals. Their social organization is unique among primates, and is called a \"cooperative polyandrous group\". This communal breeding system involves groups of multiple males and females, but only one female is reproductively active. Females mate with more than one male and each shares the responsibility of carrying the offspring.\nThey are the only primate group that regularly produces twins, which constitute over 80% of births in species that have been studied. Unlike other male primates, male callitrichids generally provide as much parental care as females. Parental duties may include carrying, protecting, feeding, comforting, and even engaging in play behavior with offspring. In some cases, such as in the cotton-top tamarin (\"Saguinus oedipus\"), males, particularly those that are paternal, even show a greater involvement in caregiving than females. The typical social structure seems to constitute a breeding group, with several of their previous offspring living in the group and providing significant help in rearing the young.\nSpecies and subspecies list.\nTaxa included in the Callitrichidae are:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n Media related to at Wikimedia Commons\n Data related to at Wikispecies"}
{"id": "5230", "revid": "1136374939", "url": "https://en.wikipedia.org/wiki?curid=5230", "title": "Cebidae", "text": "Family of New World monkeys\nThe Cebidae are one of the five families of New World monkeys now recognised. Extant members are the capuchin and squirrel monkeys. These species are found throughout tropical and subtropical South and Central America.\nCharacteristics.\nCebid monkeys are arboreal animals that only rarely travel on the ground. They are generally small monkeys, ranging in size up to that of the brown capuchin, with a body length of 33 to 56\u00a0cm, and a weight of 2.5 to 3.9 kilograms. They are somewhat variable in form and coloration, but all have the wide, flat, noses typical of New World monkeys.\nThey are omnivorous, mostly eating fruit and insects, although the proportions of these foods vary greatly between species. They have the dental formula:2.1.3.2-32.1.3.2-3\nFemales give birth to one or two young after a gestation period of between 130 and 170 days, depending on species. They are social animals, living in groups of between five and forty individuals, with the smaller species typically forming larger groups. They are generally diurnal in habit.\nClassification.\nPreviously, New World monkeys were divided between Callitrichidae and this family. For a few recent years, marmosets, tamarins, and lion tamarins were placed as a subfamily (Callitrichinae) in Cebidae, while moving other genera from Cebidae into the families Aotidae, Pitheciidae and Atelidae. The most recent classification of New World monkeys again splits the callitrichids off, leaving only the capuchins and squirrel monkeys in this family.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5231", "revid": "33145", "url": "https://en.wikipedia.org/wiki?curid=5231", "title": "Cercopithecidae", "text": ""}
{"id": "5232", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5232", "title": "Chondrichthyes", "text": "Class of jawed cartilaginous fishes\nChondrichthyes (; from grc \" ' (kh\u00f3ndros)\"\u00a0'cartilage', and \" ' (ikhth\u00fas)\"\u00a0'fish') is a class that contains the cartilaginous fishes that have skeletons primarily composed of cartilage. They can be contrasted with the Osteichthyes or \"bony fishes\", which have skeletons primarily composed of bone tissue. Chondrichthyes are jawed vertebrates with paired fins, paired nares, scales, and a heart with its chambers in series. Extant chondrichthyes range in size from the 10 cm (3.9 in) finless sleeper ray to the 10 m (32 ft) whale shark. \nThe class is divided into two subclasses: Elasmobranchii (sharks, rays, skates, and sawfish) and Holocephali (chimaeras, sometimes called ghost sharks, which are sometimes separated into their own class). \nWithin the infraphylum Gnathostomata, cartilaginous fishes are distinct from all other jawed vertebrates.\nAnatomy.\nSkeleton.\nThe skeleton is cartilaginous. The notochord is gradually replaced by a vertebral column during development, except in Holocephali, where the notochord stays intact. In some deepwater sharks, the column is reduced.\nAs they do not have bone marrow, red blood cells are produced in the spleen and the epigonal organ (special tissue around the gonads, which is also thought to play a role in the immune system). They are also produced in the Leydig's organ, which is only found in certain cartilaginous fishes. The subclass Holocephali, which is a very specialized group, lacks both the Leydig's and epigonal organs.\nAppendages.\nApart from electric rays, which have a thick and flabby body, with soft, loose skin, chondrichthyans have tough skin covered with dermal teeth (again, Holocephali is an exception, as the teeth are lost in adults, only kept on the clasping organ seen on the caudal ventral surface of the male), also called placoid scales (or \"dermal denticles\"), making it feel like sandpaper. In most species, all dermal denticles are oriented in one direction, making the skin feel very smooth if rubbed in one direction and very rough if rubbed in the other.\nOriginally, the pectoral and pelvic girdles, which do not contain any dermal elements, did not connect. In later forms, each pair of fins became ventrally connected in the middle when scapulocoracoid and puboischiadic bars evolved. In rays, the pectoral fins are connected to the head and are very flexible.\nOne of the primary characteristics present in most sharks is the heterocercal tail, which aids in locomotion.\nBody covering.\nChondrichthyans have tooth-like scales called dermal denticles or placoid scales. Denticles usually provide protection, and in most cases, streamlining. Mucous glands exist in some species, as well.\nIt is assumed that their oral teeth evolved from dermal denticles that migrated into the mouth, but it could be the other way around, as the teleost bony fish \"Denticeps clupeoides\" has most of its head covered by dermal teeth (as does, probably, \"Atherion elymus\", another bony fish). This is most likely a secondary evolved characteristic, which means there is not necessarily a connection between the teeth and the original dermal scales.\nThe old placoderms did not have teeth at all, but had sharp bony plates in their mouth. Thus, it is unknown whether the dermal or oral teeth evolved first. It has even been suggested that the original bony plates of \"all\" vertebrates are now gone and that the present scales are just modified teeth, even if both the teeth and body armor had a common origin a long time ago. However, there is currently no evidence of this.\nRespiratory system.\nAll chondrichthyans breathe through five to seven pairs of gills, depending on the species. In general, pelagic species must keep swimming to keep oxygenated water moving through their gills, whilst demersal species can actively pump water in through their spiracles and out through their gills. However, this is only a general rule and many species differ.\nA spiracle is a small hole found behind each eye. These can be tiny and circular, such as found on the nurse shark (\"Ginglymostoma cirratum\"), to extended and slit-like, such as found on the wobbegongs (Orectolobidae). Many larger, pelagic species, such as the mackerel sharks (Lamnidae) and the thresher sharks (Alopiidae), no longer possess them.\nNervous system.\nIn chondrichthyans, the nervous system is composed of a small brain, 8-10 pairs of cranial nerves, and a spinal cord with spinal nerves. They have several sensory organs which provide information to be processed. Ampullae of Lorenzini are a network of small jelly filled pores called electroreceptors which help the fish sense electric fields in water. This aids in finding prey, navigation, and sensing temperature. The Lateral line system has modified epithelial cells located externally which sense motion, vibration, and pressure in the water around them. Most species have large well-developed eyes. Also, they have very powerful nostrils and olfactory organs. Their inner ears consist of 3 large semicircular canals which aid in balance and orientation. Their sound detecting apparatus has limited range and is typically more powerful at lower frequencies. Some species have electric organs which can be used for defense and predation. They have relatively simple brains with the forebrain not greatly enlarged. The structure and formation of myelin in their nervous systems are nearly identical to that of tetrapods, which has led evolutionary biologists to believe that Chondrichthyes were a cornerstone group in the evolutionary timeline of myelin development.\nImmune system.\nLike all other jawed vertebrates, members of Chondrichthyes have an adaptive immune system.\nReproduction.\nFertilization is internal. Development is usually live birth (ovoviviparous species) but can be through eggs (oviparous). Some rare species are viviparous. There is no parental care after birth; however, some chondrichthyans do guard their eggs.\nCapture-induced premature birth and abortion (collectively called capture-induced parturition) occurs frequently in sharks/rays when fished. Capture-induced parturition is often mistaken for natural birth by recreational fishers and is rarely considered in commercial fisheries management despite being shown to occur in at least 12% of live bearing sharks and rays (88 species to date).\nClassification.\nThe class Chondrichthyes has two subclasses: the subclass Elasmobranchii (sharks, rays, skates, and sawfish) and the subclass Holocephali (chimaeras). To see the full list of the species, click here.\nEvolution.\nCartilaginous fish are considered to have evolved from acanthodians. The discovery of \"Entelognathus\" and several examinations of acanthodian characteristics indicate that bony fish evolved directly from placoderm like ancestors, while acanthodians represent a paraphyletic assemblage leading to Chondrichthyes. Some characteristics previously thought to be exclusive to acanthodians are also present in basal cartilaginous fish. In particular, new phylogenetic studies find cartilaginous fish to be well nested among acanthodians, with \"Doliodus\" and \"Tamiobatis\" being the closest relatives to Chondrichthyes. Recent studies vindicate this, as \"Doliodus\" had a mosaic of chondrichthyan and acanthodian traits. Dating back to the Middle and Late Ordovician Period, many isolated scales, made of dentine and bone, have a structure and growth form that is chondrichthyan-like. They may be the remains of stem-chondrichthyans, but their classification remains uncertain.\nThe earliest unequivocal fossils of acanthodian-grade cartilaginous fishes are \"Qianodus\" and \"Fanjingshania\" from the early Silurian (Aeronian) of Guizhou, China around 439 million years ago, which are also the oldest unambiguous remains of any jawed vertebrates. \"Shenacanthus vermiformis\", which lived 436 million years ago, had thoracic armour plates resembling those of placoderms.\nBy the start of the Early Devonian, 419 million years ago, jawed fishes had divided into three distinct groups: the now extinct placoderms (a paraphyletic assemblage of ancient armoured fishes), the bony fishes, and the clade that includes spiny sharks and early cartilaginous fish. The modern bony fishes, class Osteichthyes, appeared in the late Silurian or early Devonian, about 416 million years ago. The first abundant genus of shark, \"Cladoselache\", appeared in the oceans during the Devonian Period. The first Cartilaginous fishes evolved from \"Doliodus\"-like spiny shark ancestors.\nTaxonomy.\n Subphylum Vertebrata\n \u2514\u2500Infraphylum Gnathostomata\n \u251c\u2500Placodermi \u2014 \"extinct\" (armored gnathostomes)\n \u2514Eugnathostomata (true jawed vertebrates)\n \u251c\u2500Acanthodii (stem cartilaginous fish)\n \u2514\u2500Chondrichthyes (true cartilaginous fish)\n \u251c\u2500Holocephali (chimaeras + several extinct clades)\n \u2514Elasmobranchii (shark and rays)\n \u251c\u2500Selachii (true sharks)\n \u2514\u2500Batoidea (rays and relatives)\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5233", "revid": "41588820", "url": "https://en.wikipedia.org/wiki?curid=5233", "title": "Carl Linnaeus", "text": "Swedish botanist, physician, and zoologist (1707\u20131778)\nCarl Linnaeus (; 23 May 1707 \u2013 10 January 1778), also known after his ennoblement in 1761 as Carl von Linn\u00e9 (] ()), was a Swedish botanist, zoologist, taxonomist, and physician who formalised binomial nomenclature, the modern system of naming organisms. He is known as the \"father of modern taxonomy\". Many of his writings were in Latin; his name is rendered in Latin as ' and, after his 1761 ennoblement, as '.\nLinnaeus was the son of a curate and he was born in R\u00e5shult, the countryside of Sm\u00e5land, in southern Sweden. He received most of his higher education at Uppsala University and began giving lectures in botany there in 1730. He lived abroad between 1735 and 1738, where he studied and also published the first edition of his \"\" in the Netherlands. He then returned to Sweden where he became professor of medicine and botany at Uppsala. In the 1740s, he was sent on several journeys through Sweden to find and classify plants and animals. In the 1750s and 1760s, he continued to collect and classify animals, plants, and minerals, while publishing several volumes. He was one of the most acclaimed scientists in Europe at the time of his death.\nPhilosopher Jean-Jacques Rousseau sent him the message: \"Tell him I know no greater man on Earth.\" Johann Wolfgang von Goethe wrote: \"With the exception of Shakespeare and Spinoza, I know no one among the no longer living who has influenced me more strongly.\" Swedish author August Strindberg wrote: \"Linnaeus was in reality a poet who happened to become a naturalist.\" Linnaeus has been called \"\" (Prince of Botanists) and \"The Pliny of the North\". He is also considered one of the founders of modern ecology.\nIn botany and zoology, the abbreviation L. is used to indicate Linnaeus as the authority for a species' name. In older publications, the abbreviation \"Linn.\" is found. Linnaeus's remains constitute the type specimen for the species \"Homo sapiens\" following the International Code of Zoological Nomenclature, since the sole specimen that he is known to have examined was himself.\nEarly life.\nChildhood.\nLinnaeus was born in the village of R\u00e5shult in Sm\u00e5land, Sweden, on 23 May 1707. He was the first child of Nicolaus (Nils) Ingemarsson (who later adopted the family name Linnaeus) and Christina Brodersonia. His siblings were Anna Maria Linn\u00e6a, Sofia Juliana Linn\u00e6a, Samuel Linn\u00e6us (who would eventually succeed their father as rector of Stenbrohult and write a manual on beekeeping), and Emerentia Linn\u00e6a. His father taught him Latin as a small child.\nOne of a long line of peasants and priests, Nils was an amateur botanist, a Lutheran minister, and the curate of the small village of Stenbrohult in Sm\u00e5land. Christina was the daughter of the rector of Stenbrohult, Samuel Brodersonius.\nA year after Linnaeus's birth, his grandfather Samuel Brodersonius died, and his father Nils became the rector of Stenbrohult. The family moved into the rectory from the curate's house.\nEven in his early years, Linnaeus seemed to have a liking for plants, flowers in particular. Whenever he was upset, he was given a flower, which immediately calmed him. Nils spent much time in his garden and often showed flowers to Linnaeus and told him their names. Soon Linnaeus was given his own patch of earth where he could grow plants.\nCarl's father was the first in his ancestry to adopt a permanent surname. Before that, ancestors had used the patronymic naming system of Scandinavian countries: his father was named Ingemarsson after his father Ingemar Bengtsson. When Nils was admitted to the University of Lund, he had to take on a family name. He adopted the Latinate name Linn\u00e6us after a giant linden tree (or lime tree), \"\" in Swedish, that grew on the family homestead. This name was spelled with the \u00e6 ligature. When Carl was born, he was named Carl Linn\u00e6us, with his father's family name. The son also always spelled it with the \u00e6 ligature, both in handwritten documents and in publications. Carl's patronymic would have been Nilsson, as in Carl Nilsson Linn\u00e6us.\nEarly education.\nLinnaeus's father began teaching him basic Latin, religion, and geography at an early age. When Linnaeus was seven, Nils decided to hire a tutor for him. The parents picked Johan Telander, a son of a local yeoman. Linnaeus did not like him, writing in his autobiography that Telander \"was better calculated to extinguish a child's talents than develop them\".\nTwo years after his tutoring had begun, he was sent to the Lower Grammar School at V\u00e4xj\u00f6 in 1717. Linnaeus rarely studied, often going to the countryside to look for plants. At some point, his father went to visit him and, after hearing critical assessments by his preceptors, he decided to put the youth as an apprentice to some honest cobbler. He reached the last year of the Lower School when he was fifteen, which was taught by the headmaster, Daniel Lannerus, who was interested in botany. Lannerus noticed Linnaeus's interest in botany and gave him the run of his garden.\nHe also introduced him to Johan Rothman, the state doctor of Sm\u00e5land and a teacher at Katedralskolan (a gymnasium) in V\u00e4xj\u00f6. Also a botanist, Rothman broadened Linnaeus's interest in botany and helped him develop an interest in medicine. By the age of 17, Linnaeus had become well acquainted with the existing botanical literature. He remarks in his journal that he \"read day and night, knowing like the back of my hand, Arvidh M\u00e5nsson's Rydaholm Book of Herbs, Tillandz's Flora \u00c5boensis, Palmberg's Serta Florea Suecana, Bromelii's Chloros Gothica and Rudbeckii's Hortus Upsaliensis\".\nLinnaeus entered the V\u00e4xj\u00f6 Katedralskola in 1724, where he studied mainly Greek, Hebrew, theology and mathematics, a curriculum designed for boys preparing for the priesthood. In the last year at the gymnasium, Linnaeus's father visited to ask the professors how his son's studies were progressing; to his dismay, most said that the boy would never become a scholar. Rothman believed otherwise, suggesting Linnaeus could have a future in medicine. The doctor offered to have Linnaeus live with his family in V\u00e4xj\u00f6 and to teach him physiology and botany. Nils accepted this offer.\nUniversity studies.\nLund.\nRothman showed Linnaeus that botany was a serious subject. He taught Linnaeus to classify plants according to Tournefort's system. Linnaeus was also taught about the sexual reproduction of plants, according to S\u00e9bastien Vaillant. In 1727, Linnaeus, age 21, enrolled in Lund University in Sk\u00e5ne. He was registered as \"\", the Latin form of his full name, which he also used later for his Latin publications.\nProfessor Kilian Stob\u00e6us, natural scientist, physician and historian, offered Linnaeus tutoring and lodging, as well as the use of his library, which included many books about botany. He also gave the student free admission to his lectures. In his spare time, Linnaeus explored the flora of Sk\u00e5ne, together with students sharing the same interests.\nUppsala.\nIn August 1728, Linnaeus decided to attend Uppsala University on the advice of Rothman, who believed it would be a better choice if Linnaeus wanted to study both medicine and botany. Rothman based this recommendation on the two professors who taught at the medical faculty at Uppsala: Olof Rudbeck the Younger and Lars Roberg. Although Rudbeck and Roberg had undoubtedly been good professors, by then they were older and not so interested in teaching. Rudbeck no longer gave public lectures, and had others stand in for him. The botany, zoology, pharmacology and anatomy lectures were not in their best state. In Uppsala, Linnaeus met a new benefactor, Olof Celsius, who was a professor of theology and an amateur botanist. He received Linnaeus into his home and allowed him use of his library, which was one of the richest botanical libraries in Sweden.\nIn 1729, Linnaeus wrote a thesis, ' on plant sexual reproduction. This attracted the attention of Rudbeck; in May 1730, he selected Linnaeus to give lectures at the University although the young man was only a second-year student. His lectures were popular, and Linnaeus often addressed an audience of 300 people. In June, Linnaeus moved from Celsius's house to Rudbeck's to become the tutor of the three youngest of his 24 children. His friendship with Celsius did not wane and they continued their botanical expeditions. Over that winter, Linnaeus began to doubt Tournefort's system of classification and decided to create one of his own. His plan was to divide the plants by the number of stamens and pistils. He began writing several books, which would later result in, for example, ' and '. He also produced a book on the plants grown in the Uppsala Botanical Garden, '.\nRudbeck's former assistant, Nils Ros\u00e9n, returned to the University in March 1731 with a degree in medicine. Ros\u00e9n started giving anatomy lectures and tried to take over Linnaeus's botany lectures, but Rudbeck prevented that. Until December, Ros\u00e9n gave Linnaeus private tutoring in medicine. In December, Linnaeus had a \"disagreement\" with Rudbeck's wife and had to move out of his mentor's house; his relationship with Rudbeck did not appear to suffer. That Christmas, Linnaeus returned home to Stenbrohult to visit his parents for the first time in about three years. His mother had disapproved of his failing to become a priest, but she was pleased to learn he was teaching at the University.\nExpedition to Lapland.\nDuring a visit with his parents, Linnaeus told them about his plan to travel to Lapland; Rudbeck had made the journey in 1695, but the detailed results of his exploration were lost in a fire seven years afterwards. Linnaeus's hope was to find new plants, animals and possibly valuable minerals. He was also curious about the customs of the native Sami people, reindeer-herding nomads who wandered Scandinavia's vast tundras. In April 1732, Linnaeus was awarded a grant from the Royal Society of Sciences in Uppsala for his journey.\nLinnaeus began his expedition from Uppsala on 12 May 1732, just before he turned 25. He travelled on foot and horse, bringing with him his journal, botanical and ornithological manuscripts and sheets of paper for pressing plants. Near G\u00e4vle he found great quantities of \"Campanula serpyllifolia\", later known as \"Linnaea borealis\", the twinflower that would become his favourite. He sometimes dismounted on the way to examine a flower or rock and was particularly interested in mosses and lichens, the latter a main part of the diet of the reindeer, a common and economically important animal in Lapland.\nLinnaeus travelled clockwise around the coast of the Gulf of Bothnia, making major inland incursions from Ume\u00e5, Lule\u00e5 and Tornio. He returned from his six-month-long, over expedition in October, having gathered and observed many plants, birds and rocks. Although Lapland was a region with limited biodiversity, Linnaeus described about 100 previously unidentified plants. These became the basis of his book \"\". However, on the expedition to Lapland, Linnaeus used Latin names to describe organisms because he had not yet developed the binomial system.\nIn ' Linnaeus's ideas about nomenclature and classification were first used in a practical way, making this the first proto-modern Flora. The account covered 534 species, used the Linnaean classification system and included, for the described species, geographical distribution and taxonomic notes. It was Augustin Pyramus de Candolle who attributed Linnaeus with ' as the first example in the botanical genre of Flora writing. Botanical historian E. L. Greene described \"\" as \"the most classic and delightful\" of Linnaeus's works.\nIt was also during this expedition that Linnaeus had a flash of insight regarding the classification of mammals. Upon observing the lower jawbone of a horse at the side of a road he was travelling, Linnaeus remarked: \"If I only knew how many teeth and of what kind every animal had, how many teats and where they were placed, I should perhaps be able to work out a perfectly natural system for the arrangement of all quadrupeds.\"\nIn 1734, Linnaeus led a small group of students to Dalarna. Funded by the Governor of Dalarna, the expedition was to catalogue known natural resources and discover new ones, but also to gather intelligence on Norwegian mining activities at R\u00f8ros.\nYears in the Dutch Republic (1735\u201338).\nDoctorate.\nHis relations with Nils Ros\u00e9n having worsened, Linnaeus accepted an invitation from Claes Sohlberg, son of a mining inspector, to spend the Christmas holiday in Falun, where Linnaeus was permitted to visit the mines.\nIn April 1735, at the suggestion of Sohlberg's father, Linnaeus and Sohlberg set out for the Dutch Republic, where Linnaeus intended to study medicine at the University of Harderwijk while tutoring Sohlberg in exchange for an annual salary. At the time, it was common for Swedes to pursue doctoral degrees in the Netherlands, then a highly revered place to study natural history.\nOn the way, the pair stopped in Hamburg, where they met the mayor, who proudly showed them a supposed wonder of nature in his possession: the taxidermied remains of a seven-headed hydra. Linnaeus quickly discovered the specimen was a fake, cobbled together from the jaws and paws of weasels and the skins of snakes. The provenance of the hydra suggested to Linnaeus that it had been manufactured by monks to represent the Beast of Revelation. Even at the risk of incurring the mayor's wrath, Linnaeus made his observations public, dashing the mayor's dreams of selling the hydra for an enormous sum. Linnaeus and Sohlberg were forced to flee from Hamburg.\nLinnaeus began working towards his degree as soon as he reached Harderwijk, a university known for awarding degrees in as little as a week. He submitted a dissertation, written back in Sweden, entitled \"Dissertatio medica inauguralis in qua exhibetur hypothesis nova de febrium intermittentium causa\", in which he laid out his hypothesis that malaria arose only in areas with clay-rich soils. Although he failed to identify the true source of disease transmission, (i.e., the \"Anopheles\" mosquito), he did correctly predict that \"Artemisia annua\" (wormwood) would become a source of antimalarial medications.\nWithin two weeks he had completed his oral and practical examinations and was awarded a doctoral degree.\nThat summer Linnaeus reunited with Peter Artedi, a friend from Uppsala with whom he had once made a pact that should either of the two predecease the other, the survivor would finish the decedent's work. Ten weeks later, Artedi drowned in the canals of Amsterdam, leaving behind an unfinished manuscript on the classification of fish.\nPublishing of \".\nOne of the first scientists Linnaeus met in the Netherlands was Johan Frederik Gronovius, to whom Linnaeus showed one of the several manuscripts he had brought with him from Sweden. The manuscript described a new system for classifying plants. When Gronovius saw it, he was very impressed, and offered to help pay for the printing. With an additional monetary contribution by the Scottish doctor Isaac Lawson, the manuscript was published as \" (1735).\nLinnaeus became acquainted with one of the most respected physicians and botanists in the Netherlands, Herman Boerhaave, who tried to convince Linnaeus to make a career there. Boerhaave offered him a journey to South Africa and America, but Linnaeus declined, stating he would not stand the heat. Instead, Boerhaave convinced Linnaeus that he should visit the botanist Johannes Burman. After his visit, Burman, impressed with his guest's knowledge, decided Linnaeus should stay with him during the winter. During his stay, Linnaeus helped Burman with his '. Burman also helped Linnaeus with the books on which he was working: ' and \"\".\nGeorge Clifford, Philip Miller, and Johann Jacob Dillenius.\nIn August 1735, during Linnaeus's stay with Burman, he met George Clifford III, a director of the Dutch East India Company and the owner of a rich botanical garden at the estate of Hartekamp in Heemstede. Clifford was very impressed with Linnaeus's ability to classify plants, and invited him to become his physician and superintendent of his garden. Linnaeus had already agreed to stay with Burman over the winter, and could thus not accept immediately. However, Clifford offered to compensate Burman by offering him a copy of Sir Hans Sloane's \"Natural History of Jamaica\", a rare book, if he let Linnaeus stay with him, and Burman accepted. On 24 September 1735, Linnaeus moved to Hartekamp to become personal physician to Clifford, and curator of Clifford's herbarium. He was paid 1,000 florins a year, with free board and lodging. Though the agreement was only for a winter of that year, Linnaeus practically stayed there until 1738. It was here that he wrote a book \"Hortus Cliffortianus\", in the preface of which he described his experience as \"the happiest time of my life\". (A portion of Hartekamp was declared as public garden in April 1956 by the Heemstede local authority, and was named \"Linnaeushof\". It eventually became, as it is claimed, the biggest playground in Europe.)\nIn July 1736, Linnaeus travelled to England, at Clifford's expense. He went to London to visit Sir Hans Sloane, a collector of natural history, and to see his cabinet, as well as to visit the Chelsea Physic Garden and its keeper, Philip Miller. He taught Miller about his new system of subdividing plants, as described in \"\". Miller was in fact reluctant to use the new binomial nomenclature, preferring the classifications of Joseph Pitton de Tournefort and John Ray at first. Linnaeus, nevertheless, applauded Miller's \"Gardeners Dictionary\", The conservative Scot actually retained in his dictionary a number of pre-Linnaean binomial signifiers discarded by Linnaeus but which have been retained by modern botanists. He only fully changed to the Linnaean system in the edition of \"The Gardeners Dictionary\" of 1768. Miller ultimately was impressed, and from then on started to arrange the garden according to Linnaeus's system.\nLinnaeus also travelled to Oxford University to visit the botanist Johann Jacob Dillenius. He failed to make Dillenius publicly fully accept his new classification system, though the two men remained in correspondence for many years afterwards. Linnaeus dedicated his \"Critica Botanica\" to him, as \"opus botanicum quo absolutius mundus non-vidit\". Linnaeus would later name a genus of tropical tree Dillenia in his honour. He then returned to Hartekamp, bringing with him many specimens of rare plants. The next year, 1737, he published ', in which he described 935 genera of plants, and shortly thereafter he supplemented it with ', with another sixty (\"sexaginta\") genera.\nHis work at Hartekamp led to another book, \"\", a catalogue of the botanical holdings in the herbarium and botanical garden of Hartekamp. He wrote it in nine months (completed in July 1737), but it was not published until 1738. It contains the first use of the name \"Nepenthes\", which Linnaeus used to describe a genus of pitcher plants.\nLinnaeus stayed with Clifford at Hartekamp until 18 October 1737 (new style), when he left the house to return to Sweden. Illness and the kindness of Dutch friends obliged him to stay some months longer in Holland. In May 1738, he set out for Sweden again. On the way home, he stayed in Paris for about a month, visiting botanists such as Antoine de Jussieu. After his return, Linnaeus never left Sweden again.\nReturn to Sweden.\nWhen Linnaeus returned to Sweden on 28 June 1738, he went to Falun, where he entered into an engagement to Sara Elisabeth Mor\u00e6a. Three months later, he moved to Stockholm to find employment as a physician, and thus to make it possible to support a family. Once again, Linnaeus found a patron; he became acquainted with Count Carl Gustav Tessin, who helped him get work as a physician at the Admiralty. During this time in Stockholm, Linnaeus helped found the Royal Swedish Academy of Science; he became the first Praeses of the academy by drawing of lots.\nBecause his finances had improved and were now sufficient to support a family, he received permission to marry his fianc\u00e9e, Sara Elisabeth Mor\u00e6a. Their wedding was held 26 June 1739. Seventeen months later, Sara gave birth to their first son, Carl. Two years later, a daughter, Elisabeth Christina, was born, and the subsequent year Sara gave birth to Sara Magdalena, who died when 15 days old. Sara and Linnaeus would later have four other children: Lovisa, Sara Christina, Johannes and Sophia.\nIn May 1741, Linnaeus was appointed Professor of Medicine at Uppsala University, first with responsibility for medicine-related matters. Soon, he changed place with the other Professor of Medicine, Nils Ros\u00e9n, and thus was responsible for the Botanical Garden (which he would thoroughly reconstruct and expand), botany and natural history, instead. In October that same year, his wife and nine-month-old son followed him to live in Uppsala.\n\u00d6land and Gotland.\nTen days after he was appointed Professor, he undertook an expedition to the island provinces of \u00d6land and Gotland with six students from the university to look for plants useful in medicine. First, they travelled to \u00d6land and stayed there until 21 June, when they sailed to Visby in Gotland. Linnaeus and the students stayed on Gotland for about a month, and then returned to Uppsala. During this expedition, they found 100 previously unrecorded plants. The observations from the expedition were later published in ', written in Swedish. Like ', it contained both zoological and botanical observations, as well as observations concerning the culture in \u00d6land and Gotland.\nDuring the summer of 1745, Linnaeus published two more books: ' and '. ' was a strictly botanical book, while ' was zoological. Anders Celsius had created the temperature scale named after him in 1742. Celsius's scale was inverted compared to today, the boiling point at 0\u00a0\u00b0C and freezing point at 100\u00a0\u00b0C. In 1745, Linnaeus inverted the scale to its present standard.\nV\u00e4sterg\u00f6tland.\nIn the summer of 1746, Linnaeus was once again commissioned by the Government to carry out an expedition, this time to the Swedish province of V\u00e4sterg\u00f6tland. He set out from Uppsala on 12 June and returned on 11 August. On the expedition his primary companion was Erik Gustaf Lidbeck, a student who had accompanied him on his previous journey. Linnaeus described his findings from the expedition in the book \"\", published the next year. After he returned from the journey, the Government decided Linnaeus should take on another expedition to the southernmost province Scania. This journey was postponed, as Linnaeus felt too busy.\nIn 1747, Linnaeus was given the title archiater, or chief physician, by the Swedish king Adolf Frederick\u2014a mark of great respect. The same year he was elected member of the Academy of Sciences in Berlin.\nScania.\nIn the spring of 1749, Linnaeus could finally journey to Scania, again commissioned by the Government. With him he brought his student, Olof S\u00f6derberg. On the way to Scania, he made his last visit to his brothers and sisters in Stenbrohult since his father had died the previous year. The expedition was similar to the previous journeys in most aspects, but this time he was also ordered to find the best place to grow walnut and Swedish whitebeam trees; these trees were used by the military to make rifles. While there, they also visited the Raml\u00f6sa mineral spa, where he remarked on the quality of its ferruginous water. The journey was successful, and Linnaeus's observations were published the next year in \"\".\nRector of Uppsala University.\nIn 1750, Linnaeus became rector of Uppsala University, starting a period where natural sciences were esteemed. Perhaps the most important contribution he made during his time at Uppsala was to teach; many of his students travelled to various places in the world to collect botanical samples. Linnaeus called the best of these students his \"apostles\". His lectures were normally very popular and were often held in the Botanical Garden. He tried to teach the students to think for themselves and not trust anybody, not even him. Even more popular than the lectures were the botanical excursions made every Saturday during summer, where Linnaeus and his students explored the flora and fauna in the vicinity of Uppsala.\n\"Philosophia Botanica\".\nLinnaeus published \"Philosophia Botanica\" in 1751. The book contained a complete survey of the taxonomy system he had been using in his earlier works. It also contained information of how to keep a journal on travels and how to maintain a botanical garden.\n\"Nutrix Noverca\".\nDuring Linnaeus's time it was normal for upper class women to have wet nurses for their babies. Linnaeus joined an ongoing campaign to end this practice in Sweden and promote breast-feeding by mothers. In 1752 Linnaeus published a thesis along with Frederick Lindberg, a physician student, based on their experiences. In the tradition of the period, this dissertation was essentially an idea of the presiding reviewer (\"prases\") expounded upon by the student. Linnaeus's dissertation was translated into French by J. E. Gilibert in 1770 as \"La Nourrice mar\u00e2tre, ou Dissertation sur les suites funestes du nourrisage merc\u00e9naire\". Linnaeus suggested that children might absorb the personality of their wet nurse through the milk. He admired the child care practices of the Lapps and pointed out how healthy their babies were compared to those of Europeans who employed wet nurses. He compared the behaviour of wild animals and pointed out how none of them denied their newborns their breastmilk. It is thought that his activism played a role in his choice of the term \"Mammalia\" for the class of organisms.\n\"Species Plantarum\".\nLinnaeus published \"Species Plantarum\", the work which is now internationally accepted as the starting point of modern botanical nomenclature, in 1753. The first volume was issued on 24 May, the second volume followed on 16 August of the same year. The book contained 1,200 pages and was published in two volumes; it described over 7,300 species. The same year the king dubbed him knight of the Order of the Polar Star, the first civilian in Sweden to become a knight in this order. He was then seldom seen not wearing the order's insignia.\nEnnoblement.\nLinnaeus felt Uppsala was too noisy and unhealthy, so he bought two farms in 1758: Hammarby and S\u00e4vja. The next year, he bought a neighbouring farm, Edeby. He spent the summers with his family at Hammarby; initially it only had a small one-storey house, but in 1762 a new, larger main building was added. In Hammarby, Linnaeus made a garden where he could grow plants that could not be grown in the Botanical Garden in Uppsala. He began constructing a museum on a hill behind Hammarby in 1766, where he moved his library and collection of plants. A fire that destroyed about one third of Uppsala and had threatened his residence there necessitated the move.\nSince the initial release of ' in 1735, the book had been expanded and reprinted several times; the tenth edition was released in 1758. This edition established itself as the starting point for zoological nomenclature, the equivalent of '.\nThe Swedish King Adolf Frederick granted Linnaeus nobility in 1757, but he was not ennobled until 1761. With his ennoblement, he took the name Carl von Linn\u00e9 (Latinised as \"\"), 'Linn\u00e9' being a shortened and gallicised version of 'Linn\u00e6us', and the German nobiliary particle 'von' signifying his ennoblement. The noble family's coat of arms prominently features a twinflower, one of Linnaeus's favourite plants; it was given the scientific name \"Linnaea borealis\" in his honour by Gronovius. The shield in the coat of arms is divided into thirds: red, black and green for the three kingdoms of nature (animal, mineral and vegetable) in Linnaean classification; in the centre is an egg \"to denote Nature, which is continued and perpetuated \"in ovo\".\" At the bottom is a phrase in Latin, borrowed from the Aeneid, which reads \"Famam extendere factis\": we extend our fame by our deeds. Linnaeus inscribed this personal motto in books that were given to him by friends.\nAfter his ennoblement, Linnaeus continued teaching and writing. His reputation had spread over the world, and he corresponded with many different people. For example, Catherine II of Russia sent him seeds from her country. He also corresponded with Giovanni Antonio Scopoli, \"the Linnaeus of the Austrian Empire\", who was a doctor and a botanist in Idrija, Duchy of Carniola (nowadays Slovenia). Scopoli communicated all of his research, findings, and descriptions (for example of the olm and the dormouse, two little animals hitherto unknown to Linnaeus). Linnaeus greatly respected Scopoli and showed great interest in his work. He named a solanaceous genus, \"Scopolia\", the source of scopolamine, after him, but because of the great distance between them, they never met.\nFinal years.\nLinnaeus was relieved of his duties in the Royal Swedish Academy of Science in 1763, but continued his work there as usual for more than ten years after. In 1769 he was elected to the American Philosophical Society for his work. He stepped down as rector at Uppsala University in December 1772, mostly due to his declining health.\nLinnaeus's last years were troubled by illness. He had had a disease called the Uppsala fever in 1764, but survived due to the care of Ros\u00e9n. He developed sciatica in 1773, and the next year, he had a stroke which partially paralysed him. He had a second stroke in 1776, losing the use of his right side and leaving him bereft of his memory; while still able to admire his own writings, he could not recognise himself as their author.\nIn December 1777, he had another stroke which greatly weakened him, and eventually led to his death on 10 January 1778 in Hammarby. Despite his desire to be buried in Hammarby, he was buried in Uppsala Cathedral on 22 January.\nHis library and collections were left to his widow Sara and their children. Joseph Banks, an eminent botanist, wished to purchase the collection, but his son Carl refused the offer and instead moved the collection to Uppsala. In 1783 Carl died and Sara inherited the collection, having outlived both her husband and son. She tried to sell it to Banks, but he was no longer interested; instead an acquaintance of his agreed to buy the collection. The acquaintance was a 24-year-old medical student, James Edward Smith, who bought the whole collection: 14,000 plants, 3,198 insects, 1,564 shells, about 3,000 letters and 1,600 books. Smith founded the Linnean Society of London five years later.\nThe von Linn\u00e9 name ended with his son Carl, who never married. His other son, Johannes, had died aged 3. There are over two hundred descendants of Linnaeus through two of his daughters.\nApostles.\nDuring Linnaeus's time as Professor and Rector of Uppsala University, he taught many devoted students, 17 of whom he called \"apostles\". They were the most promising, most committed students, and all of them made botanical expeditions to various places in the world, often with his help. The amount of this help varied; sometimes he used his influence as Rector to grant his apostles a scholarship or a place on an expedition. To most of the apostles he gave instructions of what to look for on their journeys. Abroad, the apostles collected and organised new plants, animals and minerals according to Linnaeus's system. Most of them also gave some of their collection to Linnaeus when their journey was finished. Thanks to these students, the Linnaean system of taxonomy spread through the world without Linnaeus ever having to travel outside Sweden after his return from Holland. The British botanist William T. Stearn notes, without Linnaeus's new system, it would not have been possible for the apostles to collect and organise so many new specimens. Many of the apostles died during their expeditions.\nEarly expeditions.\nChristopher T\u00e4rnstr\u00f6m, the first apostle and a 43-year-old pastor with a wife and children, made his journey in 1746. He boarded a Swedish East India Company ship headed for China. T\u00e4rnstr\u00f6m never reached his destination, dying of a tropical fever on C\u00f4n S\u01a1n Island the same year. T\u00e4rnstr\u00f6m's widow blamed Linnaeus for making her children fatherless, causing Linnaeus to prefer sending out younger, unmarried students after T\u00e4rnstr\u00f6m. Six other apostles later died on their expeditions, including Pehr Forssk\u00e5l and Pehr L\u00f6fling.\nTwo years after T\u00e4rnstr\u00f6m's expedition, Finnish-born Pehr Kalm set out as the second apostle to North America. There he spent two-and-a-half years studying the flora and fauna of Pennsylvania, New York, New Jersey and Canada. Linnaeus was overjoyed when Kalm returned, bringing back with him many pressed flowers and seeds. At least 90 of the 700 North American species described in \"Species Plantarum\" had been brought back by Kalm.\nCook expeditions and Japan.\nDaniel Solander was living in Linnaeus's house during his time as a student in Uppsala. Linnaeus was very fond of him, promising Solander his eldest daughter's hand in marriage. On Linnaeus's recommendation, Solander travelled to England in 1760, where he met the English botanist Joseph Banks. With Banks, Solander joined James Cook on his expedition to Oceania on the \"Endeavour\" in 1768\u201371. Solander was not the only apostle to journey with James Cook; Anders Sparrman followed on the \"Resolution\" in 1772\u201375 bound for, among other places, Oceania and South America. Sparrman made many other expeditions, one of them to South Africa.\nPerhaps the most famous and successful apostle was Carl Peter Thunberg, who embarked on a nine-year expedition in 1770. He stayed in South Africa for three years, then travelled to Japan. All foreigners in Japan were forced to stay on the island of Dejima outside Nagasaki, so it was thus hard for Thunberg to study the flora. He did, however, manage to persuade some of the translators to bring him different plants, and he also found plants in the gardens of Dejima. He returned to Sweden in 1779, one year after Linnaeus's death.\nMajor publications.\n\"Systema Naturae\".\nThe first edition of \"\" was printed in the Netherlands in 1735. It was a twelve-page work. By the time it reached its 10th edition in 1758, it classified 4,400 species of animals and 7,700 species of plants. People from all over the world sent their specimens to Linnaeus to be included. By the time he started work on the 12th edition, Linnaeus needed a new invention\u2014the index card\u2014to track classifications.\nIn \"Systema Naturae\", the unwieldy names mostly used at the time, such as \"\", were supplemented with concise and now familiar \"binomials\", composed of the generic name, followed by a specific epithet\u2014in the case given, \"Physalis angulata\". These binomials could serve as a label to refer to the species. Higher taxa were constructed and arranged in a simple and orderly manner. Although the system, now known as binomial nomenclature, was partially developed by the Bauhin brothers (see Gaspard Bauhin and Johann Bauhin) almost 200 years earlier, Linnaeus was the first to use it consistently throughout the work, including in monospecific genera, and may be said to have popularised it within the scientific community.\nAfter the decline in Linnaeus's health in the early 1770s, publication of editions of \"Systema Naturae\" went in two different directions. Another Swedish scientist, Johan Andreas Murray issued the \"Regnum Vegetabile\" section separately in 1774 as the \"Systema Vegetabilium\", rather confusingly labelled the 13th edition. Meanwhile, a 13th edition of the entire \"Systema\" appeared in parts between 1788 and 1793 under the editorship of Johann Friedrich Gmelin. It was through the \"Systema Vegetabilium\" that Linnaeus's work became widely known in England, following its translation from the Latin by the Lichfield Botanical Society as \"A System of Vegetables\" (1783\u20131785).\n\"Orbis eruditi judicium de Caroli Linnaei MD scriptis\".\n('Opinion of the learned world on the writings of Carl Linnaeus, Doctor') Published in 1740, this small octavo-sized pamphlet was presented to the State Library of New South Wales by the Linnean Society of NSW in 2018. This is considered among the rarest of all the writings of Linnaeus, and crucial to his career, securing him his appointment to a professorship of medicine at Uppsala University. From this position he laid the groundwork for his radical new theory of classifying and naming organisms for which he was considered the founder of modern taxonomy.\n' (or, more fully, ') was first published in 1753, as a two-volume work. Its prime importance is perhaps that it is the primary starting point of plant nomenclature as it exists today.\n\" was first published in 1737, delineating plant genera. Around 10 editions were published, not all of them by Linnaeus himself; the most important is the 1754 fifth edition. In it Linnaeus divided the plant Kingdom into 24 classes. One, Cryptogamia, included all the plants with concealed reproductive parts (algae, fungi, mosses and liverworts and ferns).\n' (1751) was a summary of Linnaeus's thinking on plant classification and nomenclature, and an elaboration of the work he had previously published in ' (1736) and ' (1737). Other publications forming part of his plan to reform the foundations of botany include his ' and ': all were printed in Holland (as were ' (1737) and \" (1735)), the \"Philosophia\" being simultaneously released in Stockholm.\nCollections.\nAt the end of his lifetime the Linnean collection in Uppsala was considered one of the finest collections of natural history objects in Sweden. Next to his own collection he had also built up a museum for the university of Uppsala, which was supplied by material donated by Carl Gyllenborg (in 1744\u20131745), crown-prince Adolf Fredrik (in 1745), Erik Petreus (in 1746), Claes Grill (in 1746), Magnus Lagerstr\u00f6m (in 1748 and 1750) and Jonas Alstr\u00f6mer (in 1749). The relation between the museum and the private collection was not formalised and the steady flow of material from Linnean pupils were incorporated to the private collection rather than to the museum. Linnaeus felt his work was reflecting the harmony of nature and he said in 1754 \"the earth is then nothing else but a museum of the all-wise creator's masterpieces, divided into three chambers\". He had turned his own estate into a microcosm of that 'world museum'.\nIn April 1766 parts of the town were destroyed by a fire and the Linnean private collection was subsequently moved to a barn outside the town, and shortly afterwards to a single-room stone building close to his country house at Hammarby near Uppsala. This resulted in a physical separation between the two collections; the museum collection remained in the botanical garden of the university. Some material which needed special care (alcohol specimens) or ample storage space was moved from the private collection to the museum.\nIn Hammarby the Linnean private collections suffered seriously from damp and the depredations by mice and insects. Carl von Linn\u00e9's son (Carl Linnaeus) inherited the collections in 1778 and retained them until his own death in 1783. Shortly after Carl von Linn\u00e9's death his son confirmed that mice had caused \"horrible damage\" to the plants and that also moths and mould had caused considerable damage. He tried to rescue them from the neglect they had suffered during his father's later years, and also added further specimens. This last activity however reduced rather than augmented the scientific value of the original material.\nIn 1784 the young medical student James Edward Smith purchased the entire specimen collection, library, manuscripts, and correspondence of Carl Linnaeus from his widow and daughter and transferred the collections to London. Not all material in Linn\u00e9's private collection was transported to England. Thirty-three fish specimens preserved in alcohol were not sent and were later lost.\nIn London Smith tended to neglect the zoological parts of the collection; he added some specimens and also gave some specimens away. Over the following centuries the Linnean collection in London suffered enormously at the hands of scientists who studied the collection, and in the process disturbed the original arrangement and labels, added specimens that did not belong to the original series and withdrew precious original type material.\nMuch material which had been intensively studied by Linn\u00e9 in his scientific career belonged to the collection of Queen Lovisa Ulrika (1720\u20131782) (in the Linnean publications referred to as \"Museum Ludovicae Ulricae\" or \"M. L. U.\"). This collection was donated by her grandson King Gustav IV Adolf (1778\u20131837) to the museum in Uppsala in 1804. Another important collection in this respect was that of her husband King Adolf Fredrik (1710\u20131771) (in the Linnean sources known as \"Museum Adolphi Friderici\" or \"Mus. Ad. Fr.\"), the wet parts (alcohol collection) of which were later donated to the Royal Swedish Academy of Sciences, and is today housed in the Swedish Museum of Natural History at Stockholm. The dry material was transferred to Uppsala.\nSystem of taxonomy.\nThe establishment of universally accepted conventions for the naming of organisms was Linnaeus's main contribution to taxonomy\u2014his work marks the starting point of consistent use of binomial nomenclature. During the 18th century expansion of natural history knowledge, Linnaeus also developed what became known as the \"Linnaean taxonomy\"; the system of scientific classification now widely used in the biological sciences. A previous zoologist Rumphius (1627\u20131702) had more or less approximated the Linnaean system and his material contributed to the later development of the binomial scientific classification by Linnaeus.\nThe Linnaean system classified nature within a nested hierarchy, starting with three kingdoms. Kingdoms were divided into classes and they, in turn, into orders, and thence into genera (\"singular:\" genus), which were divided into species (\"singular:\" species). Below the rank of species he sometimes recognised taxa of a lower (unnamed) rank; these have since acquired standardised names such as \"variety\" in botany and \"subspecies\" in zoology. Modern taxonomy includes a rank of family between order and genus and a rank of phylum between kingdom and class that were not present in Linnaeus's original system.\nLinnaeus's groupings were based upon shared physical characteristics, and not based upon differences. Of his higher groupings, only those for animals are still in use, and the groupings themselves have been significantly changed since their conception, as have the principles behind them. Nevertheless, Linnaeus is credited with establishing the idea of a hierarchical structure of classification which is based upon observable characteristics and intended to reflect natural relationships. While the underlying details concerning what are considered to be scientifically valid \"observable characteristics\" have changed with expanding knowledge (for example, DNA sequencing, unavailable in Linnaeus's time, has proven to be a tool of considerable utility for classifying living organisms and establishing their evolutionary relationships), the fundamental principle remains sound.\nHuman taxonomy.\nLinnaeus's system of taxonomy was especially noted as the first to include humans (\"Homo\") taxonomically grouped with apes (\"Simia\"), under the header of \"Anthropomorpha\".\nGerman biologist Ernst Haeckel speaking in 1907 noted this as the \"most important sign of Linnaeus's genius\".\nLinnaeus classified humans among the primates beginning with the first edition of \"\". During his time at Hartekamp, he had the opportunity to examine several monkeys and noted similarities between them and man. He pointed out both species basically have the same anatomy; except for speech, he found no other differences. Thus he placed man and monkeys under the same category, \"Anthropomorpha\", meaning \"manlike.\" This classification received criticism from other biologists such as Johan Gottschalk Wallerius, Jacob Theodor Klein and Johann Georg Gmelin on the ground that it is illogical to describe man as human-like. In a letter to Gmelin from 1747, Linnaeus replied:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It does not please [you] that I've placed Man among the Anthropomorpha, perhaps because of the term 'with human form', but man learns to know himself. Let's not quibble over words. It will be the same to me whatever name we apply. But I seek from you and from the whole world a generic difference between man and simian that [follows] from the principles of Natural History. I absolutely know of none. If only someone might tell me a single one! If I would have called man a simian or vice versa, I would have brought together all the theologians against me. Perhaps I ought to have by virtue of the law of the discipline.\nThe theological concerns were twofold: first, putting man at the same level as monkeys or apes would lower the spiritually higher position that man was assumed to have in the great chain of being, and second, because the Bible says man was created in the image of God (theomorphism), if monkeys/apes and humans were not distinctly and separately designed, that would mean monkeys and apes were created in the image of God as well. This was something many could not accept. The conflict between world views that was caused by asserting man was a type of animal would simmer for a century until the much greater, and still ongoing, creation\u2013evolution controversy began in earnest with the publication of \"On the Origin of Species\" by Charles Darwin in 1859.\nAfter such criticism, Linnaeus felt he needed to explain himself more clearly. The 10th edition of ' introduced new terms, including \"Mammalia\" and \"Primates\", the latter of which would replace \"Anthropomorpha\" as well as giving humans the full binomial \"Homo sapiens\". The new classification received less criticism, but many natural historians still believed he had demoted humans from their former place of ruling over nature and not being a part of it. Linnaeus believed that man biologically belongs to the animal kingdom and had to be included in it. In his book ', he said, \"One should not vent one's wrath on animals, Theology decree that man has a soul and that the animals are mere 'automata mechanica,' but I believe they would be better advised that animals have a soul and that the difference is of nobility.\"\nLinnaeus added a second species to the genus \"Homo\" in \"\" based on a figure and description by Jacobus Bontius from a 1658 publication: \"Homo troglodytes\" (\"caveman\") and published a third in 1771: \"Homo lar\". Swedish historian Gunnar Broberg states that the new human species Linnaeus described were actually simians or native people clad in skins to frighten colonial settlers, whose appearance had been exaggerated in accounts to Linnaeus. For \"Homo troglodytes\" Linnaeus asked the Swedish East India Company to search for one, but they did not find any signs of its existence. \"Homo lar\" has since been reclassified as \"Hylobates lar\", the lar gibbon.\nIn the first edition of \"\", Linnaeus subdivided the human species into four varieties: \"Europ\u00e6us albesc[ens]\" (whitish European), \"Americanus rubesc[ens]\" (reddish American), \"Asiaticus fuscus\" (tawny Asian) and \"Africanus nigr[iculus]\" (blackish African).\nIn the tenth edition of Systema Naturae he further detailed phenotypical characteristics for each variety, based on the concept of the four temperaments from classical antiquity, and changed the description of Asians' skin tone to \"luridus\" (yellow). Additionally, Linnaeus created a wastebasket taxon \"monstrosus\" for \"wild and monstrous humans, unknown groups, and more or less abnormal people\".\nIn 1959, W. T. Stearn designated Linnaeus to be the lectotype of \"H. sapiens\".\nInfluences and economic beliefs.\nLinnaeus's applied science was inspired not only by the instrumental utilitarianism general to the early Enlightenment, but also by his adherence to the older economic doctrine of Cameralism. Additionally, Linnaeus was a state interventionist. He supported tariffs, levies, export bounties, quotas, embargoes, navigation acts, subsidised investment capital, ceilings on wages, cash grants, state-licensed producer monopolies, and cartels.\nCommemoration.\nAnniversaries of Linnaeus's birth, especially in centennial years, have been marked by major celebrations. Linnaeus has appeared on numerous Swedish postage stamps and banknotes. There are numerous statues of Linnaeus in countries around the world. The Linnean Society of London has awarded the Linnean Medal for excellence in botany or zoology since 1888. Following approval by the Riksdag of Sweden, V\u00e4xj\u00f6 University and Kalmar College merged on 1 January 2010 to become Linnaeus University. Other things named after Linnaeus include the twinflower genus \"Linnaea\", \"Linnaeosicyos\" (a monotypic genus in the family Cucurbitaceae), the crater Linn\u00e9 on the Earth's moon, a street in Cambridge, Massachusetts, and the cobalt sulfide mineral Linnaeite.\nCommentary.\nAndrew Dickson White wrote in \"A History of the Warfare of Science with Theology in Christendom\" (1896):\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nLinnaeus\u00a0... was the most eminent naturalist of his time, a wide observer, a close thinker; but the atmosphere in which he lived and moved and had his being was saturated with biblical theology, and this permeated all his thinking.\u00a0... Toward the end of his life he timidly advanced the hypothesis that all the species of one genus constituted at the creation one species; and from the last edition of his \"Systema Natur\u00e6\" he quietly left out the strongly orthodox statement of the fixity of each species, which he had insisted upon in his earlier works.\u00a0... warnings came speedily both from the Catholic and Protestant sides.\nThe mathematical PageRank algorithm, applied to 24 multilingual Wikipedia editions in 2014, published in \"PLOS ONE\" in 2015, placed Carl Linnaeus at the top historical figure, above Jesus, Aristotle, Napoleon, and Adolf Hitler (in that order).\nIn the 21st century, Linn\u00e6us's taxonomy of human \"races\" has been problematised and discussed. Some critics claim that Linn\u00e6us was one of the forebears of the modern pseudoscientific notion of scientific racism, while others hold the view that while his classification was stereotyped, it did not imply that certain human \"races\" were superior to others.\nStandard author abbreviation.\nThe standard author abbreviation L. is used to indicate this person as the author when citing a botanical name.\nSelected publications by Linnaeus.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nBiographies\nResources\nOther"}
{"id": "5236", "revid": "40310821", "url": "https://en.wikipedia.org/wiki?curid=5236", "title": "Coast", "text": "Area where land meets the sea or ocean\nThe coast, also known as the coastline or seashore, is defined as the area where land meets the ocean, or as a line that forms the boundary between the land and the coastline. Shores are influenced by the topography of the surrounding landscape, as well as by water induced erosion, such as waves. The geological composition of rock and soil dictates the type of shore which is created. The Earth has around of coastline. Coasts are important zones in natural ecosystems, often home to a wide range of biodiversity. On land, they harbor important ecosystems such as freshwater or estuarine wetlands, which are important for bird populations and other terrestrial animals. In wave-protected areas they harbor saltmarshes, mangroves or seagrasses, all of which can provide nursery habitat for finfish, shellfish, and other aquatic species. Rocky shores are usually found along exposed coasts and provide habitat for a wide range of sessile animals (e.g. mussels, starfish, barnacles) and various kinds of seaweeds. In physical oceanography, a shore is the wider fringe that is geologically modified by the action of the body of water past and present, while the beach is at the edge of the shore, representing the intertidal zone where there is one. Along tropical coasts with clear, nutrient-poor water, coral reefs can often be found between depths of .\nAccording to an atlas prepared by the United Nations, 44% of all humans live within 150 km (93 mi) of the sea. Due to its importance in society and its high population concentrations, the coast is important for major parts of the global food and economic system, and they provide many ecosystem services to humankind. For example, important human activities happen in port cities. Coastal fisheries (commercial, recreational, and subsistence) and aquaculture are major economic activities and create jobs, livelihoods, and protein for the majority of coastal human populations. Other coastal spaces like beaches and seaside resorts generate large revenues through tourism. Marine coastal ecosystems can also provide protection against sea level rise and tsunamis. In many countries, mangroves are the primary source of wood for fuel (e.g. charcoal) and building material. Coastal ecosystems like mangroves and seagrasses have a much higher capacity for carbon sequestration than many terrestrial ecosystems, and as such can play a critical role in the near-future to help mitigate climate change effects by uptake of atmospheric anthropogenic carbon dioxide. \nHowever, the economic importance of coasts makes many of these communities vulnerable to climate change, which causes increases in extreme weather and sea level rise, and related issues such as coastal erosion, saltwater intrusion and coastal flooding. Other coastal issues, such as marine pollution, marine debris, coastal development, and marine ecosystem destruction, further complicate the human uses of the coast and threaten coastal ecosystems. The interactive effects of climate change, habitat destruction, overfishing and water pollution (especially eutrophication) have led to the demise of coastal ecosystem around the globe. This has resulted in population collapse of fisheries stocks, loss of biodiversity, increased invasion of alien species, and loss of healthy habitats. International attention to these issues has been captured in Sustainable Development Goal 14 \"Life Below Water\" which sets goals for international policy focused on preserving marine coastal ecosystems and supporting more sustainable economic practices for coastal communities. Likewise, the United Nations has declared 2021-2030 the UN Decade on Ecosystem Restoration, but restoration of coastal ecosystems has received insufficient attention.\nBecause coasts are constantly changing, a coastline's exact perimeter cannot be determined; this measurement challenge is called the coastline paradox. The term \"coastal zone\" is used to refer to a region where interactions of sea and land processes occur. Both the terms \"coast\" and \"coastal\" are often used to describe a geographic location or region located on a coastline (e.g., New Zealand's West Coast, or the East, West, and Gulf Coast of the United States.) Coasts with a narrow continental shelf that are close to the open ocean are called \"pelagic\" \"coast\", while other coasts are more sheltered coast in a gulf or bay. A shore, on the other hand, may refer to parts of land adjoining any large body of water, including oceans (sea shore) and lakes (lake shore). \n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nSize.\nThe Earth has approximately of coastline. Coastal habitats, which extend to the margins of the continental shelves, make up about 7 percent of the Earth's oceans, but at least 85% of commercially harvested fish depend on coastal environments during at least part of their life cycle. As of October 2010, about 2.86% of exclusive economic zones were part of marine protected areas.\nThe definition of coasts varies. Marine scientists think of the \"wet\" (aquatic or intertidal) vegetated habitats as being coastal ecosystems (including seagrass, salt marsh etc.) whilst some terrestrial scientist might only think of coastal ecosystems as purely terrestrial plants that live close to the seashore (see also estuaries and coastal ecosystems).\nWhile there is general agreement in the scientific community regarding the definition of coast, in the political sphere, the delineation of the extents of a coast differ according to jurisdiction. Government authorities in various countries may define coast differently for economic and social policy reasons. \nFormation.\nTides often determine the range over which sediment is deposited or eroded. Areas with high tidal ranges allow waves to reach farther up the shore, and areas with lower tidal ranges produce deposition at a smaller elevation interval. The tidal range is influenced by the size and shape of the coastline. Tides do not typically cause erosion by themselves; however, tidal bores can erode as the waves surge up the river estuaries from the ocean.\nGeologists classify coasts on the basis of tidal range into \"macrotidal coasts\" with a tidal range greater than ; \"mesotidal coasts\" with a tidal range of ; and \"microtidal coasts\" with a tidal range of less than . The distinction between macrotidal and mesotidal coasts is more important. Macrotidal coasts lack barrier islands and lagoons, and are characterized by funnel-shaped estuaries containing sand ridges aligned with tidal currents. Wave action is much more important for determining bedforms of sediments deposited along mesotidal and microtidal coasts than in macrotidal coasts.\nWaves erode coastline as they break on shore releasing their energy; the larger the wave the more energy it releases and the more sediment it moves. Coastlines with longer shores have more room for the waves to disperse their energy, while coasts with cliffs and short shore faces give little room for the wave energy to be dispersed. In these areas, the wave energy breaking against the cliffs is higher, and air and water are compressed into cracks in the rock, forcing the rock apart, breaking it down. Sediment deposited by waves comes from eroded cliff faces and is moved along the coastline by the waves. This forms an abrasion or cliffed coast.\nSediment deposited by rivers is the dominant influence on the amount of sediment located in the case of coastlines that have estuaries. Today, riverine deposition at the coast is often blocked by dams and other human regulatory devices, which remove the sediment from the stream by causing it to be deposited inland. Coral reefs are a provider of sediment for coastlines of tropical islands.\nLike the ocean which shapes them, coasts are a dynamic environment with constant change. The Earth's natural processes, particularly sea level rises, waves and various weather phenomena, have resulted in the erosion, accretion and reshaping of coasts as well as flooding and creation of continental shelves and drowned river valleys (rias).\nImportance for humans and ecosystems.\nHuman settlements.\nMore and more of the world's people live in coastal regions. According to a United Nations atlas, 44% of all people live within 150 km (93 mi) of the sea. Many major cities are on or near good harbors and have port facilities. Some landlocked places have achieved port status by building canals.\nNations defend their coasts against military invaders, smugglers and illegal migrants. Fixed coastal defenses have long been erected in many nations, and coastal countries typically have a navy and some form of coast guard.\nTourism.\nCoasts, especially those with beaches and warm water, attract tourists often leading to the development of seaside resort communities. In many island nations such as those of the Mediterranean, South Pacific Ocean and Caribbean, tourism is central to the economy. Coasts offer recreational activities such as swimming, fishing, surfing, boating, and sunbathing. \nGrowth management and coastal management can be a challenge for coastal local authorities who often struggle to provide the infrastructure required by new residents, and poor management practices of construction often leave these communities and infrastructure vulnerable to processes like coastal erosion and sea level rise. In many of these communities, management practices such as beach nourishment or when the coastal infrastructure is no longer financially sustainable, managed retreat to remove communities from the coast.\nTypes.\nEmergent coastline.\nAccording to one principle of classification, an emergent coastline is a coastline that has experienced a fall in sea level, because of either a global sea-level change, or local uplift. Emergent coastlines are identifiable by the coastal landforms, which are above the high tide mark, such as raised beaches. In contrast, a submergent coastline is one where the sea level has risen, due to a global sea-level change, local subsidence, or isostatic rebound. Submergent coastlines are identifiable by their submerged, or \"drowned\" landforms, such as rias (drowned valleys) and fjords\nConcordant coastline.\nAccording to the second principle of classification, a concordant coastline is a coastline where bands of different rock types run parallel to the shore. These rock types are usually of varying resistance, so the coastline forms distinctive landforms, such as coves. Discordant coastlines feature distinctive landforms because the rocks are eroded by the ocean waves. The less resistant rocks erode faster, creating inlets or bay; the more resistant rocks erode more slowly, remaining as headlands or outcroppings.\nRivieras.\n\"Riviera\" is an Italian word for \"shoreline\", ultimately derived from Latin \"ripa\" (\"riverbank\"). It came to be applied as a proper name to the coast of the Ligurian Sea, in the form \"riviera ligure\", then shortened to \"riviera\". Historically, the Ligurian Riviera extended from Capo Corvo (Punta Bianca) south of Genoa, north and west into what is now French territory past Monaco and sometimes as far as Marseilles. Today, this coast is divided into the Italian Riviera and the French Riviera, although the French use the term \"Riviera\" to refer to the Italian Riviera and call the French portion the \"C\u00f4te d'Azur\".\nAs a result of the fame of the Ligurian rivieras, the term came into English to refer to any shoreline, especially one that is sunny, topographically diverse and popular with tourists. Such places using the term include the Australian Riviera in Queensland and the Turkish Riviera along the Aegean Sea.\nLandforms.\nThe following articles describe some coastal landforms:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCoastal features formed by sediment.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nOther features on the coast.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCoastal waters.\n\"Coastal waters\" (or \"coastal seas\") is a rather general term used differently in different contexts, ranging geographically from the waters within a few kilometers of the coast, through to the entire continental shelf which may stretch for more than a hundred kilometers from land. Thus the term coastal waters is used in a slightly different way in discussions of legal and economic boundaries (see territorial waters and international waters) or when considering the geography of coastal landforms or the ecological systems operating through the continental shelf (marine coastal ecosystems). The research on coastal waters often divides into these separate areas too. \nThe dynamic fluid nature of the ocean means that all components of the whole ocean system are ultimately connected, although certain regional classifications are useful and relevant. The waters of the continental shelves represent such a region. The term \"coastal waters\" has been used in a wide variety of different ways in different contexts. In European Union environmental management it extends from the coast to just a few nautical miles while in the United States the US EPA considers this region to extend much further offshore. \n\"Coastal waters\" has specific meanings in the context of commercial coastal shipping, and somewhat different meanings in the context of naval littoral warfare. Oceanographers and marine biologists have yet other takes. Coastal waters have a wide range of marine habitats from enclosed estuaries to the open waters of the continental shelf. \nSimilarly, the term littoral zone has no single definition. It is the part of a sea, lake, or river that is close to the shore. In coastal environments, the littoral zone extends from the high water mark, which is rarely inundated, to shoreline areas that are permanently submerged.\nCoastal waters can be threatened by coastal eutrophication and harmful algal blooms.\nIn geology.\nThe identification of bodies of rock formed from sediments deposited in shoreline and nearshore environments (shoreline and nearshore \"facies\") is extremely important to geologists. These provide vital clues for reconstructing the geography of ancient continents (\"paleogeography\"). The locations of these beds show the extent of ancient seas at particular points in geological time, and provide clues to the magnitudes of tides in the distant past.\nSediments deposited in the shoreface are preserved as lenses of sandstone in which the upper part of the sandstone is coarser than the lower part (a \"coarsening upwards sequence\"). Geologists refer to these are \"parasequences\". Each records an episode of retreat of the ocean from the shoreline over a period of 10,000 to 1,000,000 years. These often show laminations reflecting various kinds of tidal cycles.\nSome of the best-studied shoreline deposits in the world are found along the former western shore of the Western Interior Seaway, a shallow sea that flooded central North America during the late Cretaceous Period (about 100 to 66 million years ago). These are beautifully exposed along the Book Cliffs of Utah and Colorado.\nGeologic processes.\nThe following articles describe the various geologic processes that affect a coastal zone:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nWildlife.\nAnimals.\nLarger animals that live in coastal areas include puffins, sea turtles and rockhopper penguins, among many others. Sea snails and various kinds of barnacles live on rocky coasts and scavenge on food deposited by the sea. Some coastal animals are used to humans in developed areas, such as dolphins and seagulls who eat food thrown for them by tourists. Since the coastal areas are all part of the littoral zone, there is a profusion of marine life found just off-coast, including sessile animals such as corals, sponges, starfish, mussels, seaweeds, fishes, and sea anemones.\nThere are many kinds of seabirds on various coasts. These include pelicans and cormorants, who join up with terns and oystercatchers to forage for fish and shellfish. There are sea lions on the coast of Wales and other countries.\nPlants.\nMany coastal areas are famous for their kelp beds. Kelp is a fast-growing seaweed that can grow up to half a meter a day in ideal conditions. Mangroves, seagrasses, macroalgal beds, and salt marsh are important coastal vegetation types in tropical and temperate environments respectively. Restinga is another type of coastal vegetation.\nThreats.\nCoasts also face many human-induced environmental impacts and coastal development hazards. The most important ones are: \nPollution.\nThe pollution of coastlines is connected to marine pollution which can occur from a number of sources: Marine debris (garbage and industrial debris); the transportation of petroleum in tankers, increasing the probability of large oil spills; small oil spills created by large and small vessels, which flush bilge water into the ocean.\nGlobal goals.\nInternational attention to address the threats of coasts has been captured in Sustainable Development Goal 14 \"Life Below Water\" which sets goals for international policy focused on preserving marine coastal ecosystems and supporting more sustainable economic practices for coastal communities. Likewise, the United Nations has declared 2021-2030 the UN Decade on Ecosystem Restoration, but restoration of coastal ecosystems has received insufficient attention.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5237", "revid": "45682988", "url": "https://en.wikipedia.org/wiki?curid=5237", "title": "Catatonia", "text": "Psychiatric behavioral syndrome\nMedical condition\nCatatonia is a complex neuropsychiatric behavioral syndrome that is characterized by abnormal movements, immobility, abnormal behaviors, and withdrawal. The onset of catatonia can be acute or subtle and symptoms can wax, wane, or change during episodes. It has historically been related to schizophrenia (catatonic schizophrenia), but catatonia is most often seen in mood disorders. It is now known that catatonic symptoms are nonspecific and may be observed in other mental, neurological, and medical conditions. Catatonia is not a stand-alone diagnosis (although some experts disagree), and the term is used to describe a feature of the underlying disorder.\nThere are several subtypes of catatonia: akinetic catatonia, excited catatonia, malignant catatonia, delirious mania, and self-injurious behaviors in autism.\nRecognizing and treating catatonia is very important as failure to do so can lead to poor outcomes and can be potentially fatal. Treatment with benzodiazepines or ECT can lead to remission of catatonia. There is growing evidence of the effectiveness of the NMDA receptor antagonists amantadine and memantine for benzodiazepine-resistant catatonia. Antipsychotics are sometimes employed, but they can worsen symptoms and have serious adverse effects.\nSigns and symptoms.\nThe presentation of a patient with catatonia varies greatly depending on the subtype and underlying cause, and can be acute or subtle.\nBecause most patients with catatonia have an underlying psychiatric illness, the majority will present with worsening depression, mania, or psychosis followed by catatonia symptoms. Catatonia presents as a motor disturbance in which patients will display marked reduction in movement, marked agitation, or a mixture of both despite having the physical capacity to move normally. These patients may be unable to start an action or stop one. Movements and mannerisms may be repetitive, or purposeless.\nThe most common signs of catatonia are immobility, mutism, withdrawal and refusal to eat, staring, negativism, posturing (rigidity), rigidity, waxy flexibility/catalepsy, stereotypy (purposeless, repetitive movements), echolalia or echopraxia, verbigeration (repeat meaningless phrases). It should not be assumed that patients presenting with catatonia are unaware of their surroundings as some patients can recall in detail their catatonic state and their actions.\nThere are several subtypes of catatonia and they are characterized by the specific movement disturbance and associated features. Although catatonia can be divided into various subtypes, the natural history of catatonia is often fluctuant and different states can exist within the same individual.\nSubtypes.\nWithdrawn Catatonia: This form of catatonia is characterized by decreased response to external stimuli, immobility or inhibited movement, mutism, staring, posturing, and negativism. Patients may sit or stand in the same position for hours, may hold odd positions, and may resist movement of their extremities.\nExcited Catatonia: Excited catatonia is characterized by odd mannerisms/gestures, performing purposeless or inappropriate actions, excessive motor activity, restlessness, stereotypy, impulsivity, agitation, and combativeness. Speech and actions may be repetitive or mimic another person's. People in this state are extremely hyperactive and may have delusions and hallucinations. Catatonic excitement is commonly cited as one of the most dangerous mental states in psychiatry.\nMalignant Catatonia: Malignant catatonia is a life-threatening condition that may progress rapidly within a few days. It is characterized by fever, abnormalities in blood pressure, heart rate, respiratory rate, diaphoresis (sweating), and delirium. Certain lab findings are common with this presentation; however, they are nonspecific, which means that they are also present in other conditions and do not diagnose catatonia. These lab findings include: leukocytosis, elevated creatine kinase, low serum iron. The signs and symptoms of malignant catatonia overlap significantly with neuroleptic malignant syndrome (NMS) and so a careful history, review of medications, and physical exam are critical to properly differentiate these conditions. For example, if the patient has waxy flexibility and holds a position against gravity when passively moved into that position, then it is likely catatonia. If the patient has a \"lead-pipe rigidity\" then NMS should be the prime suspect.\nOther forms:\nComplications.\nPatients may experience several complications from being in a catatonic state. The nature of these complications will depend on the type of catatonia being experienced by the patient. For example, patients presenting with withdrawn catatonia may have refusal to eat which will in turn lead to malnutrition and dehydration. Furthermore, if immobility is a symptom the patient is presenting with, then they may develop pressure ulcers, muscle contractions, and are at risk of developing deep vein thrombosis (DVT) and pulmonary embolus (PE). Patients with excited catatonia may be aggressive and violent, and physical trauma may result from this. Catatonia may progress to the malignant type which will present with autonomic instability and may be life-threatening. Other complications also include the development of pneumonia and neuroleptic malignant syndrome.\nCauses.\nCatatonia is almost always secondary to another underlying illness, often a psychiatric disorder. Mood disorders such as a bipolar disorder and depression are the most common etiologies to progress to catatonia. Other psychiatric associations include schizophrenia and other primary psychotic disorders. It also is related to autism spectrum disorders. Psychodynamic theorists have interpreted catatonia as a defense against the potentially destructive consequences of responsibility, and the passivity of the disorder provides relief.\nCatatonia is also seen in many medical disorders, including infections (such as encephalitis), autoimmune disorders, meningitis, focal neurological lesions (including strokes), alcohol withdrawal, abrupt or overly rapid benzodiazepine withdrawal, cerebrovascular disease, neoplasms, head injury, and some metabolic conditions (homocystinuria, diabetic ketoacidosis, hepatic encephalopathy, and hypercalcaemia).\nPathogenesis.\nThe pathophysiology that leads to catatonia is still poorly understood and a definite mechanism remains unknown. Neurologic studies have implicated several pathways; however, it remains unclear whether these findings are the cause or the consequence of the disorder.\nAbnormalities in GABA, glutamate signaling, serotonin, and dopamine transmission are believed to be implicated in catatonia.\nFurthermore, it has also been hypothesized that pathways that connect the basal ganglia with the cortex and thalamus is involved in the development of catatonia.\nDiagnosis.\nThere is not yet a definitive consensus regarding diagnostic criteria of catatonia. In the fifth edition of the American Psychiatric Association's \"Diagnostic and Statistical Manual of Mental Disorders\" (DSM-5, 2013) and the World Health Organization's eleventh edition of the \"International Classification of Diseases (\"ICD-11, 2022), the classification is more homogeneous than in earlier editions. Prominent researchers in the field have other suggestions for diagnostic criteria.\nDSM-5 classification\nThe DSM-5 does not classify catatonia as an independent disorder, but rather it classifies it as catatonia associated with another mental disorder, due to another medical condition, or as unspecified catatonia.\nCatatonia is diagnosed by the presence of three or more of the following 12 psychomotor symptoms in association with a mental disorder, medical condition, or unspecified:135\nOther disorders (additional code 293.89 [F06.1] to indicate the presence of the co-morbid catatonia):\nIf catatonic symptoms are present but do not form the catatonic syndrome, a medication- or substance-induced aetiology should first be considered.\nICD-11 classification\nIn ICD-11 catatonia is defined as a syndrome of primarily psychomotor disturbances that is characterized by the simultaneous occurrence of several symptoms such as stupor; catalepsy; waxy flexibility; mutism; negativism; posturing; mannerisms; stereotypies; psychomotor agitation; grimacing; echolalia and echopraxia. Catatonia may occur in the context of specific mental disorders, including mood disorders, schizophrenia or other primary psychotic disorders, and Neurodevelopmental disorders, and may be induced by psychoactive substances, including medications. Catatonia may also be caused by a medical condition not classified under mental, behavioral, or neurodevelopmental disorders.\nAssessment/Physical.\nCatatonia is often overlooked and under-diagnosed. Patients with catatonia most commonly have an underlying psychiatric disorder, for this reason, physicians may overlook signs of catatonia due to the severity of the psychosis the patient is presenting with. Furthermore, the patient may not be presenting with the common signs of catatonia such as mutism and posturing. Additionally, the motor abnormalities seen in catatonia are also present in psychiatric disorders. For example, a patient with mania will show increased motor activity that may progress to exciting catatonia. One way in which physicians can differentiate between the two is to observe the motor abnormality. Patients with mania present with increased goal-directed activity. On the other hand, the increased activity in catatonia is not goal-directed and often repetitive.\nCatatonia is a clinical diagnosis and there is no specific laboratory test to diagnose it. However, certain testing can help determine what is causing the catatonia. An EEG will likely show diffuse slowing. If seizure activity is driving the syndrome, then an EEG would also be helpful in detecting this. CT or MRI will not show catatonia; however, they might reveal abnormalities that might be leading to the syndrome. Metabolic screens, inflammatory markers, or autoantibodies may reveal reversible medical causes of catatonia.\nVital signs should be frequently monitored as catatonia can progress to malignant catatonia which is life-threatening. Malignant catatonia is characterized by fever, hypertension, tachycardia, and tachypnea.\nRating scale.\nVarious rating scales for catatonia have been developed, however, their utility for clinical care has not been well established. The most commonly used scale is the Bush-Francis Catatonia Rating Scale (BFCRS) (external link is provided below). The scale is composed of 23 items with the first 14 items being used as the screening tool. If 2 of the 14 are positive, this prompts for further evaluation and completion of the remaining 9 items.\nA diagnosis can be supported by the lorazepam challenge or the zolpidem challenge. While proven useful in the past, barbiturates are no longer commonly used in psychiatry; thus the option of either benzodiazepines or ECT.\nDifferential diagnosis.\nThe differential diagnosis of catatonia is extensive as signs and symptoms of catatonia may overlap significantly with those of other conditions. Therefore, a careful and detailed history, medication review, and physical exam are key to diagnosing catatonia and differentiating it from other conditions. Furthermore, some of these conditions can themselves lead to catatonia. The differential diagnosis is as follows:\nTreatment.\nThe initial treatment of catatonia is to stop medication that could be potentially leading to the syndrome. These may include steroids, stimulants, anticonvulsants, neuroleptics, dopamine blockers, etc. The next step is to provide a \"lorazepam challenge,\" in which patients are given 2\u00a0mg of IV lorazepam (or another benzodiazepine). Most patients with catatonia will respond significantly to this within the first 15\u201330 minutes. If no change is observed during the first dose, then a second dose is given and the patient is re-examined. If the patient responds to the lorazepam challenge, then lorazepam can be scheduled at interval doses until the catatonia resolves. The lorazepam must be tapered slowly, otherwise, the catatonia symptoms may return. The underlying cause of the catatonia should also be treated during this time. If within a week the catatonia is not resolved, then ECT can be used to reverse the symptoms. ECT in combination with benzodiazepines is used to treat malignant catatonia. In France, zolpidem has also been used in diagnosis, and response may occur within the same time period. Ultimately the underlying cause needs to be treated.\nElectroconvulsive therapy (ECT) is an effective treatment for catatonia that is well acknowledged. ECT has also shown favorable outcomes in patients with chronic catatonia. However, it has been pointed out that further high quality randomized controlled trials are needed to evaluate the efficacy, tolerance, and protocols of ECT in catatonia.\nAntipsychotics should be used with care as they can worsen catatonia and are the cause of neuroleptic malignant syndrome, a dangerous condition that can mimic catatonia and requires immediate discontinuation of the antipsychotic.\nThere is evidence clozapine works better than other antipsychotics to treat catatonia, following a recent systematic review.\nExcessive glutamate activity is believed to be involved in catatonia; when first-line treatment options fail, NMDA antagonists such as amantadine or memantine may be used. Amantadine may have an increased incidence of tolerance with prolonged use and can cause psychosis, due to its additional effects on the dopamine system. Memantine has a more targeted pharmacological profile for the glutamate system, reduced incidence of psychosis and may therefore be preferred for individuals who cannot tolerate amantadine. Topiramate is another treatment option for resistant catatonia; it produces its therapeutic effects by producing glutamate antagonism via modulation of AMPA receptors.\nPrognosis.\nPatients who experience an episode of catatonia are more likely to recur. Treatment response for patients with catatonia is 50\u201370% and these patients have a good prognosis. However, failure to respond to medication is a very poor prognosis. Many of these patients will require long-term and continuous mental health care. For patients with catatonia with underlying schizophrenia, the prognosis is much poorer.\nEpidemiology.\nCatatonia has been mostly studied in acutely ill psychiatric patients. Catatonia frequently goes unrecognized, leading to the belief that the syndrome is rare; however, this is not true and prevalence has been reported to be as high as 10% in patients with acute psychiatric illnesses. One large population estimate has suggested that the incidence of catatonia is 10.6 episodes per 100 000 person-years. It occurs in males and females in approximately equal numbers. 21-46% of all catatonia cases can be attributed to a general medical condition.\nHistory.\nReports of stupor-like and catatonia-like states abound in the history of psychiatry. After the middle of the 19th century there was an increase of interest in the motor disorders accompanying madness, culminating in the publication by Karl Ludwig Kahlbaum in 1874 of (Catatonia or Tension Insanity).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5238", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5238", "title": "CountriesY", "text": ""}
{"id": "5239", "revid": "37740198", "url": "https://en.wikipedia.org/wiki?curid=5239", "title": "Countably infinite", "text": ""}
{"id": "5242", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5242", "title": "Ciliates", "text": ""}
{"id": "5244", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=5244", "title": "Cipher", "text": "Algorithm for encrypting and decrypting information\nIn cryptography, a cipher (or cypher) is an algorithm for performing encryption or decryption\u2014a series of well-defined steps that can be followed as a procedure. An alternative, less common term is \"encipherment\". To encipher or encode is to convert information into cipher or code. In common parlance, \"cipher\" is synonymous with \"code\", as they are both a set of steps that encrypt a message; however, the concepts are distinct in cryptography, especially classical cryptography.\nCodes generally substitute different length strings of characters in the output, while ciphers generally substitute the same number of characters as are input. A code maps one meaning with another. Words and phrases can be coded as letters or numbers. Codes typically have direct meaning from input to key. Codes primarily function to save time. Ciphers are algorithmic. The given input must follow the cipher's process to be solved. Ciphers are commonly used to encrypt written information.\nCodes operated by substituting according to a large codebook which linked a random string of characters or numbers to a word or phrase. For example, \"UQJHSE\" could be the code for \"Proceed to the following coordinates.\" When using a cipher the original information is known as plaintext, and the encrypted form as ciphertext. The ciphertext message contains all the information of the plaintext message, but is not in a format readable by a human or computer without the proper mechanism to decrypt it.\nThe operation of a cipher usually depends on a piece of auxiliary information, called a key (or, in traditional NSA parlance, a \"cryptovariable\"). The encrypting procedure is varied depending on the key, which changes the detailed operation of the algorithm. A key must be selected before using a cipher to encrypt a message. Without knowledge of the key, it should be extremely difficult, if not impossible, to decrypt the resulting ciphertext into readable plaintext.\nMost modern ciphers can be categorized in several ways\nEtymology.\nOriginating from the Arabic word for zero \u0635\u0641\u0631 (sifr), the word \"cipher\" spread to Europe as part of the Arabic numeral system during the Middle Ages. The Roman numeral system lacked the concept of zero, and this limited advances in mathematics. In this transition, the word was adopted into Medieval Latin as cifra, and then into Middle French as cifre. This eventually led to the English word cipher (minority spelling cypher). One theory for how the term came to refer to encoding is that the concept of zero was confusing to Europeans, and so the term came to refer to a message or communication that was not easily understood. \nThe term \"cipher\" was later also used to refer to any Arabic digit, or to calculation using them, so encoding text in the form of Arabic numerals is literally converting the text to \"ciphers\".\nVersus codes.\nIn casual contexts, \"code\" and \"cipher\" can typically be used interchangeably, however, the technical usages of the words refer to different concepts. Codes contain meaning; words and phrases are assigned to numbers or symbols, creating a shorter message. \nAn example of this is the commercial telegraph code which was used to shorten long telegraph messages which resulted from entering into commercial contracts using exchanges of telegrams.\nAnother example is given by whole word ciphers, which allow the user to replace an entire word with a symbol or character, much like the way Japanese utilize Kanji (meaning Chinese characters in Japanese) characters to supplement their language. ex \"The quick brown fox jumps over the lazy dog\" becomes \"The quick brown \u72d0 jumps \u4e0a the lazy \u72ac\".\nCiphers, on the other hand, work at a lower level: the level of individual letters, small groups of letters, or, in modern schemes, individual bits and blocks of bits. Some systems used both codes and ciphers in one system, using superencipherment to increase the security. In some cases the terms \"codes\" and \"ciphers\" are used synonymously with \"substitution\" and \"transposition\", respectively.\nHistorically, cryptography was split into a dichotomy of codes and ciphers, while coding had its own terminology analogous to that of ciphers: \"\"encoding\", \"codetext\", \"decoding\"\" and so on.\nHowever, codes have a variety of drawbacks, including susceptibility to cryptanalysis and the difficulty of managing a cumbersome codebook. Because of this, codes have fallen into disuse in modern cryptography, and ciphers are the dominant technique.\nTypes.\nThere are a variety of different types of encryption. Algorithms used earlier in the history of cryptography are substantially different from modern methods, and modern ciphers can be classified according to how they operate and whether they use one or two keys.\nHistorical.\nThe Caesar Cipher is one of the earliest known cryptographic systems. Julius Caesar used a cipher that shifts the letters in the alphabet in place by three and wrapping the remaining letters to the front to write to Marcus Tullius Cicero in approximately 50 BC.[11]\nHistorical pen and paper ciphers used in the past are sometimes known as classical ciphers. They include simple substitution ciphers (such as ROT13) and transposition ciphers (such as a Rail Fence Cipher). For example, \"GOOD DOG\" can be encrypted as \"PLLX XLP\" where \"L\" substitutes for \"O\", \"P\" for \"G\", and \"X\" for \"D\" in the message. Transposition of the letters \"GOOD DOG\" can result in \"DGOGDOO\". These simple ciphers and examples are easy to crack, even without plaintext-ciphertext pairs.\nWilliam Shakespeare often used the concept of ciphers in his writing to symbolize nothingness. In Shakespeare's Henry V, he relates one of the accounting methods that brought the Arabic Numeral system and zero to Europe, to the human imagination. The actors who perform this play were not at the battles of Henry V's reign, so they represent absence. In another sense, ciphers are important to people who work with numbers, but they do not hold value. Shakespeare used this concept to outline how those who counted and identified the dead from the battles used that information as a political weapon, furthering class biases and xenophobia.\nSimple ciphers were replaced by polyalphabetic substitution ciphers (such as the Vigen\u00e8re) which changed the substitution alphabet for every letter. For example, \"GOOD DOG\" can be encrypted as \"PLSX TWF\" where \"L\", \"S\", and \"W\" substitute for \"O\". With even a small amount of known or estimated plaintext, simple polyalphabetic substitution ciphers and letter transposition ciphers designed for pen and paper encryption are easy to crack. It is possible to create a secure pen and paper cipher based on a one-time pad though, but the usual disadvantages of one-time pads apply.\nDuring the early twentieth century, electro-mechanical machines were invented to do encryption and decryption using transposition, polyalphabetic substitution, and a kind of \"additive\" substitution. In rotor machines, several rotor disks provided polyalphabetic substitution, while plug boards provided another substitution. Keys were easily changed by changing the rotor disks and the plugboard wires. Although these encryption methods were more complex than previous schemes and required machines to encrypt and decrypt, other machines such as the British Bombe were invented to crack these encryption methods.\nModern.\nModern encryption methods can be divided by two criteria: by type of key used, and by type of input data.\nBy type of key used ciphers are divided into:\nIn a symmetric key algorithm (e.g., DES and AES), the sender and receiver must have a shared key set up in advance and kept secret from all other parties; the sender uses this key for encryption, and the receiver uses the same key for decryption. The design of AES (Advanced Encryption System) was beneficial because it aimed to overcome the flaws in the design of the DES (Data encryption standard). AES's designer's claim that the common means of modern cipher cryptanalytic attacks are ineffective against AES due to its design structure.[12] \nCiphers can be distinguished into two types by the type of input data:\nKey size and vulnerability.\nIn a pure mathematical attack, (i.e., lacking any other information to help break a cipher) two factors above all count:\nSince the desired effect is computational difficulty, in theory one would choose an algorithm and desired difficulty level, thus decide the key length accordingly.\nAn example of this process can be found at Key Length which uses multiple reports to suggest that a symmetrical cipher with 128 bits, an asymmetric cipher with 3072 bit keys, and an elliptic curve cipher with 256 bits, all have similar difficulty at present.\nClaude Shannon proved, using information theory considerations, that any theoretically unbreakable cipher must have keys which are at least as long as the plaintext, and used only once: one-time pad.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5247", "revid": "40831289", "url": "https://en.wikipedia.org/wiki?curid=5247", "title": "Country music", "text": "American music genre\nCountry (also called country and western) is a music genre originating in the Southern and Southwestern United States. First produced in the 1920s, country primarily focuses on working class Americans and blue-collar American life.\nCountry music is known for its ballads and dance tunes (also known as \"honky-tonk music\") with simple form, folk lyrics, and harmonies accompanied by instruments such as banjos, fiddles, harmonicas, and many types of guitar (including acoustic, electric, steel, and resonator guitars). Though it is primarily rooted in various forms of American folk music, such as old-time music and Appalachian music, many other traditions, including African-American, Mexican, Irish, and Hawaiian music, have also had a formative influence on the genre. Blues modes have been used extensively throughout its recorded history.\nThe term \"country music\" gained popularity in the 1940s in preference to \"hillbilly music\"; it came to encompass Western music, which evolved parallel to hillbilly music from similar roots, in the mid-20th century. Contemporary styles of Western music include Texas country, red dirt, and Hispano- and Mexican American-led Tejano and New Mexico music, all extant alongside longstanding indigenous traditions.\nIn 2009, in the United States, country music was the most listened to rush hour radio genre during the evening commute, and second most popular in the morning commute.\nOrigins.\nThe main components of the modern country music style date back to music traditions throughout the Southern United States and Southwestern United States, while its place in American popular music was established in the 1920s during the early days of music recording. According to country historian Bill C. Malone, country music was \"introduced to the world as a Southern phenomenon.\"\nMigration into the southern Appalachian Mountains, of the Southeastern United States, brought the folk music and instruments of Europe, Africa, and the Mediterranean Basin along with it for nearly 300 years, which developed into Appalachian music. As the country expanded westward, the Mississippi River and Louisiana became a crossroads for country music, giving rise to Cajun music. In the Southwestern United States, it was the Rocky Mountains, American frontier, and Rio Grande that acted as a similar backdrop for Native American, Mexican, and cowboy ballads, which resulted in New Mexico music and the development of Western music, and its directly related Red Dirt, Texas country, and Tejano music styles. In the Asia-Pacific, the steel guitar sound of country music has its provenance in the music of Hawaii.\nRole of East Tennessee.\nThe U.S. Congress has formally recognized Bristol, Tennessee as the \"Birthplace of Country Music\", based on the historic Bristol recording sessions of 1927. Since 2014, the city has been home to the Birthplace of Country Music Museum. Historians have also noted the influence of the less-known Johnson City sessions of 1928 and 1929, and the Knoxville sessions of 1929 and 1930. In addition, the Mountain City Fiddlers Convention, held in 1925, helped to inspire modern country music. Before these, pioneer settlers, in the Great Smoky Mountains region, had developed a rich musical heritage.\nGenerations.\nThe first generation emerged in the 1920s, with Atlanta's music scene playing a major role in launching country's earliest recording artists. James Gideon \"Gid\" Tanner (1885\u20131960) was an American old-time fiddler and one of the earliest stars of what would come to be known as country music. His band, the Skillet Lickers, was one of the most innovative and influential string bands of the 1920s and 1930s. Its most notable members were Clayton McMichen (fiddle and vocal), Dan Hornsby (vocals), Riley Puckett (guitar and vocal) and Robert Lee Sweat (guitar). New York City record label Okeh Records began issuing hillbilly music records by Fiddlin' John Carson as early as 1923, followed by Columbia Records (series 15000D \"Old Familiar Tunes\") (Samantha Bumgarner) in 1924, and RCA Victor Records in 1927 with the first famous pioneers of the genre Jimmie Rodgers, who is widely considered the \"Father of Country Music\", and the first family of country music the Carter Family. Many \"hillbilly\" musicians recorded blues songs throughout the 1920s.\nDuring the second generation (1930s\u20131940s), radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started all over the South, as far north as Chicago, and as far west as California. The most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. During the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood, many featuring Gene Autry, who was known as king of the \"singing cowboys\", and Hank Williams. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. Country musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\".\nThe third generation (1950s\u20131960s) started at the end of World War II with \"mountaineer\" string band music known as bluegrass, which emerged when Bill Monroe, along with Lester Flatt and Earl Scruggs were introduced by Roy Acuff at the Grand Ole Opry. Gospel music remained a popular component of country music. The Native American, Hispano, and American frontier music of the Southwestern United States and Northern Mexico, became popular among poor communities in New Mexico, Oklahoma, and Texas; the basic ensemble consisted of classical guitar, bass guitar, dobro or steel guitar, though some larger ensembles featured electric guitars, trumpets, keyboards (especially the honky-tonk piano, a type of tack piano), banjos, and drums. By the early 1950s it blended with rock and roll, becoming the rockabilly sound produced by Sam Phillips, Norman Petty, and Bob Keane. Musicians like Elvis Presley, Bo Diddley, Buddy Holly, Jerry Lee Lewis, Ritchie Valens, Carl Perkins, Roy Orbison, and Johnny Cash emerged as enduring representatives of the style. Beginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee; Patsy Cline and Jim Reeves were two of the most broadly popular Nashville sound artists, and their deaths in separate plane crashes in the early 1960s were a factor in the genre's decline. Starting in the 1950s to the mid-1960s, Western singer-songwriters such as Michael Martin Murphey and Marty Robbins rose in prominence as did others, throughout Western music traditions, like New Mexico music's Al Hurricane. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock.\nFourth generation (1970s\u20131980s) music included outlaw country with roots in the Bakersfield sound, and country pop with roots in the countrypolitan, folk music and soft rock. Between 1972 and 1975 singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles. By the mid-1970s, Texas country and Tejano music gained popularity with performers like Freddie Fender. During the early 1980s country artists continued to see their records perform well on the pop charts. In 1980 a style of \"neocountry disco music\" was popularized. During the mid-1980s a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts in favor of more traditional \"back-to-basics\" production.\nDuring the fifth generation (1990s), neotraditionalists and stadium country acts prospered.\nThe sixth generation (2000s\u2013present) has seen a certain amount of diversification in regard to country music styles. It has also, however, seen a shift into patriotism and conservative politics since 9/11, though such themes are less prevalent in more modern trends. The influence of rock music in country has become more overt during the late 2000s and early 2010s. Most of the best-selling country songs of this era were those by Lady A, Florida Georgia Line, Carrie Underwood, and Taylor Swift. Hip hop also made its mark on country music with the emergence of country rap.\nHistory.\nFirst generation (1920s).\nThe first commercial recordings of what was considered instrumental music in the traditional country style were \"Arkansas Traveler\" and \"Turkey in the Straw\" by fiddlers Henry Gilliland &amp; A.C. (Eck) Robertson on June 30, 1922, for Victor Records and released in April 1923. Columbia Records began issuing records with \"hillbilly\" music (series 15000D \"Old Familiar Tunes\") as early as 1924.\nThe first commercial recording of what is widely considered to be the first country song featuring vocals and lyrics was Fiddlin' John Carson with \"Little Log Cabin in the Lane\" for Okeh Records on June 14, 1923.\nVernon Dalhart was the first country singer to have a nationwide hit in May 1924 with \"Wreck of the Old 97\". The flip side of the record was \"Lonesome Road Blues\", which also became very popular. In April 1924, \"Aunt\" Samantha Bumgarner and Eva Davis became the first female musicians to record and release country songs. Many of the early country musicians, such as the yodeler Cliff Carlisle, recorded blues songs into the 1930s. Other important early recording artists were Riley Puckett, Don Richardson, Fiddlin' John Carson, Uncle Dave Macon, Al Hopkins, Ernest V. Stoneman, Blind Alfred Reed, Charlie Poole and the North Carolina Ramblers and the Skillet Lickers. The steel guitar entered country music as early as 1922, when Jimmie Tarlton met famed Hawaiian guitarist Frank Ferera on the West Coast.\nJimmie Rodgers and the Carter Family are widely considered to be important early country musicians. From Scott County, Virginia, the Carters had learned sight reading of hymnals and sheet music using solfege. Their songs were first captured at a historic recording session in Bristol, Tennessee, on August 1, 1927, where Ralph Peer was the talent scout and sound recordist. A scene in the movie \"O Brother, Where Art Thou?\" depicts a similar occurrence in the same timeframe.\nRodgers fused hillbilly country, gospel, jazz, blues, pop, cowboy, and folk, and many of his best songs were his compositions, including \"Blue Yodel\", which sold over a million records and established Rodgers as the premier singer of early country music. Beginning in 1927, and for the next 17 years, the Carters recorded some 300 old-time ballads, traditional tunes, country songs and gospel hymns, all representative of America's southeastern folklore and heritage. Maybelle Carter went on to continue the family tradition with her daughters as The Carter Sisters; her daughter June would marry (in succession) Carl Smith, Rip Nix and Johnny Cash, having children with each who would also become country singers.\nSecond generation (1930s\u20131940s).\nRecord sales declined during the Great Depression, but radio became a popular source of entertainment, and \"barn dance\" shows featuring country music were started by radio stations all over the South, as far north as Chicago, and as far west as California.\nThe most important was the \"Grand Ole Opry\", aired starting in 1925 by WSM in Nashville and continuing to the present day. Some of the early stars on the \"Opry\" were Uncle Dave Macon, Roy Acuff and African American harmonica player DeFord Bailey. WSM's 50,000-watt signal (in 1934) could often be heard across the country. Many musicians performed and recorded songs in any number of styles. Moon Mullican, for example, played Western swing but also recorded songs that can be called rockabilly. Between 1947 and 1949, country crooner Eddy Arnold placed eight songs in the top 10. From 1945 to 1955 Jenny Lou Carson was one of the most prolific songwriters in country music.\nSinging cowboys and western swing.\nIn the 1930s and 1940s, cowboy songs, or Western music, which had been recorded since the 1920s, were popularized by films made in Hollywood. Some of the popular singing cowboys from the era were Gene Autry, the Sons of the Pioneers, and Roy Rogers. Country music and western music were frequently played together on the same radio stations, hence the term \"country and western\" music, despite Country and Western being two distinct genres.\nCowgirls contributed to the sound in various family groups. Patsy Montana opened the door for female artists with her history-making song \"I Want To Be a Cowboy's Sweetheart\". This would begin a movement toward opportunities for women to have successful solo careers. Bob Wills was another country musician from the Lower Great Plains who had become very popular as the leader of a \"hot string band,\" and who also appeared in Hollywood westerns. His mix of country and jazz, which started out as dance hall music, would become known as Western swing. Cliff Bruner, Moon Mullican, Milton Brown and Adolph Hofner were other early Western swing pioneers. Spade Cooley and Tex Williams also had very popular bands and appeared in films. At its height, Western swing rivaled the popularity of big band swing music.\nChanging instrumentation.\nDrums were scorned by early country musicians as being \"too loud\" and \"not pure\", but by 1935 Western swing big band leader Bob Wills had added drums to the Texas Playboys. In the mid-1940s, the Grand Ole Opry did not want the Playboys' drummer to appear on stage. Although drums were commonly used by rockabilly groups by 1955, the less-conservative-than-the-Grand-Ole-Opry \"Louisiana Hayride\" kept its infrequently used drummer back stage as late as 1956. By the early 1960s, however, it was rare for a country band not to have a drummer. Bob Wills was one of the first country musicians known to have added an electric guitar to his band, in 1938. A decade later (1948) Arthur Smith achieved top 10 US country chart success with his MGM Records recording of \"Guitar Boogie\", which crossed over to the US pop chart, introducing many people to the potential of the electric guitar. For several decades Nashville session players preferred the warm tones of the Gibson and Gretsch archtop electrics, but a \"hot\" Fender style, using guitars which became available beginning in the early 1950s, eventually prevailed as the signature guitar sound of country.\nHillbilly boogie.\nCountry musicians began recording boogie in 1939, shortly after it had been played at Carnegie Hall, when Johnny Barfield recorded \"Boogie Woogie\". The trickle of what was initially called hillbilly boogie, or okie boogie (later to be renamed country boogie), became a flood beginning in late 1945. One notable release from this period was the Delmore Brothers' \"Freight Train Boogie\", considered to be part of the combined evolution of country music and blues towards rockabilly. In 1948, Arthur \"Guitar Boogie\" Smith achieved top ten US country chart success with his MGM Records recordings of \"Guitar Boogie\" and \"Banjo Boogie\", with the former crossing over to the US pop charts. Other country boogie artists included Moon Mullican, Merrill Moore and Tennessee Ernie Ford. The hillbilly boogie period lasted into the 1950s and remains one of many subgenres of country into the 21st century.\nBluegrass, folk and gospel.\nBy the end of World War II, \"mountaineer\" string band music known as bluegrass had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs, introduced by Roy Acuff at the Grand Ole Opry. That was the ordination of bluegrass music and how Bill Monroe came to be known as the \"Father of Bluegrass.\" Gospel music, too, remained a popular component of bluegrass and other sorts of country music. Red Foley, the biggest country star following World War II, had one of the first million-selling gospel hits (\"Peace in the Valley\") and also sang boogie, blues and rockabilly. In the post-war period, country music was called \"folk\" in the trades, and \"hillbilly\" within the industry. In 1944, \"Billboard\" replaced the term \"hillbilly\" with \"folk songs and blues,\" and switched to \"country and Western\" in 1949.\nHonky tonk.\nAnother type of stripped down and raw music with a variety of moods and a basic ensemble of guitar, bass, dobro or steel guitar (and later) drums became popular, especially among rural residents in the three states of Texhomex, those being \"Tex\"as, Okla\"ho\"ma, and New \"Mex\"ico. It became known as honky tonk and had its roots in Western swing and the ranchera music of Mexico and the border states, particularly New Mexico and Texas, together with the blues of the American South. Bob Wills and His Texas Playboys personified this music which has been described as \"a little bit of this, and a little bit of that, a little bit of black and a little bit of white\u00a0... just loud enough to keep you from thinking too much and to go right on ordering the whiskey.\" East Texan Al Dexter had a hit with \"Honky Tonk Blues\", and seven years later \"Pistol Packin' Mama\". These \"honky tonk\" songs were associated with barrooms, and was performed by the likes of Ernest Tubb, Kitty Wells (the first major female country solo singer), Ted Daffan, Floyd Tillman, the Maddox Brothers and Rose, Lefty Frizzell and Hank Williams; the music of these artists would later be called \"traditional\" country. Williams' influence in particular would prove to be enormous, inspiring many of the pioneers of rock and roll, such as Elvis Presley, Jerry Lee Lewis, Chuck Berry and Ike Turner, while providing a framework for emerging honky tonk talents like George Jones. Webb Pierce was the top-charting country artist of the 1950s, with 13 of his singles spending 113 weeks at number one. He charted 48 singles during the decade; 31 reached the top ten and 26 reached the top four.\nThird generation (1950s\u20131960s).\nBy the early 1950s, a blend of Western swing, country boogie, and honky tonk was played by most country bands, a mixture which followed in the footsteps of Gene Autry, Lydia Mendoza, Roy Rogers, and Patsy Montana. Western music, influenced by the cowboy ballads, New Mexico, Texas country and Tejano music rhythms of the Southwestern United States and Northern Mexico, reached its peak in popularity in the late 1950s, most notably with the song \"El Paso\", first recorded by Marty Robbins in September 1959. Western music's influence would continue to grow within the country music sphere, Western musicians like Michael Martin Murphey, New Mexico music artists Al Hurricane and Antonia Apodaca, Tejano music performer Little Joe, and even folk revivalist John Denver, all first rose to prominence during this time. This Western music influence largely kept the music of the folk revival and folk rock from influencing the country music genre much, despite the similarity in instrumentation and origins (see, for instance, the Byrds' negative reception during their appearance on the \"Grand Ole Opry\"). The main concern was largely political: most folk revival was largely driven by progressive activists, a stark contrast to the culturally conservative audiences of country music. John Denver was perhaps the only musician to have major success in both the country and folk revival genres throughout his career, later only a handful of artists like Burl Ives and Canadian musician Gordon Lightfoot successfully made the crossover to country after folk revival fell out of fashion. During the mid-1950s a new style of country music became popular, eventually to be referred to as rockabilly.\nIn 1953, the first all-country radio station was established in Lubbock, Texas. The music of the 1960s and 1970s targeted the American working class, and truckers in particular. As country radio became more popular, trucking songs like the 1963 hit song \"Six Days on the Road\" by Dave Dudley began to make up their own subgenre of country. These revamped songs sought to portray American truckers as a \"new folk hero\", marking a significant shift in sound from earlier country music. The song was written by actual truckers and contained numerous references to the trucker culture of the time like \"ICC\" for Interstate Commerce Commission and \"little white pills\" as a reference to amphetamines. Starday Records in Nashville followed up on Dudley's initial success with the release of \"Give me 40 Acres\" by the Willis Brothers.\nRockabilly.\nRockabilly was most popular with country fans in the 1950s; one of the first rock and roll superstars was former Western yodeler Bill Haley, who repurposed his Four Aces of Western Swing into a rockabilly band in the early 1950s and renamed it the Comets. Bill Haley &amp; His Comets are credited with two of the first successful rock and roll records, \"Crazy Man, Crazy\" of 1953 and \"Rock Around the Clock\" in 1954.\n1956 could be called the year of rockabilly in country music. Rockabilly was an early form of rock and roll, an upbeat combination of blues and country music. The number two, three and four songs on \"Billboard's\" charts for that year were Elvis Presley, \"Heartbreak Hotel\"; Johnny Cash, \"I Walk the Line\"; and Carl Perkins, \"Blue Suede Shoes\". Reflecting this success, George Jones released a rockabilly record that year under the pseudonym \"Thumper Jones\", wanting to capitalize on the popularity of rockabilly without alienating his traditional country base. Cash and Presley placed songs in the top 5 in 1958 with No. 3 \"Guess Things Happen That Way/Come In, Stranger\" by Cash, and No. 5 by Presley \"Don't/I Beg of You.\" Presley acknowledged the influence of rhythm and blues artists and his style, saying \"The colored folk been singin' and playin' it just the way I'm doin' it now, man for more years than I know.\" Within a few years, many rockabilly musicians returned to a more mainstream style or had defined their own unique style.\nCountry music gained national television exposure through \"Ozark Jubilee\" on ABC-TV and radio from 1955 to 1960 from Springfield, Missouri. The program showcased top stars including several rockabilly artists, some from the Ozarks. As Webb Pierce put it in 1956, \"Once upon a time, it was almost impossible to sell country music in a place like New York City. Nowadays, television takes us everywhere, and country music records and sheet music sell as well in large cities as anywhere else.\"\nThe Country Music Association was founded in 1958, in part because numerous country musicians were appalled by the increased influence of rock and roll on country music.\nThe Nashville and countrypolitan sounds.\nBeginning in the mid-1950s, and reaching its peak during the early 1960s, the Nashville sound turned country music into a multimillion-dollar industry centered in Nashville, Tennessee. Under the direction of producers such as Chet Atkins, Bill Porter, Paul Cohen, Owen Bradley, Bob Ferguson, and later Billy Sherrill, the sound brought country music to a diverse audience and helped revive country as it emerged from a commercially fallow period. This subgenre was notable for borrowing from 1950s pop stylings: a prominent and smooth vocal, backed by a string section (violins and other orchestral strings) and vocal chorus. Instrumental soloing was de-emphasized in favor of trademark \"licks\". Leading artists in this genre included Jim Reeves, Skeeter Davis, Connie Smith, the Browns, Patsy Cline, and Eddy Arnold. The \"slip note\" piano style of session musician Floyd Cramer was an important component of this style. The Nashville Sound collapsed in mainstream popularity in 1964, a victim of both the British Invasion and the deaths of Reeves and Cline in separate airplane crashes. By the mid-1960s, the genre had developed into countrypolitan. Countrypolitan was aimed straight at mainstream markets, and it sold well throughout the later 1960s into the early 1970s. Top artists included Tammy Wynette, Lynn Anderson and Charlie Rich, as well as such former \"hard country\" artists as Ray Price and Marty Robbins. Despite the appeal of the Nashville sound, many traditional country artists emerged during this period and dominated the genre: Loretta Lynn, Merle Haggard, Buck Owens, Porter Wagoner, George Jones, and Sonny James among them.\nCountry-soul crossover.\nIn 1962, Ray Charles surprised the pop world by turning his attention to country and western music, topping the charts and rating number three for the year on \"Billboard's\" pop chart with the \"I Can't Stop Loving You\" single, and recording the landmark album \"Modern Sounds in Country and Western Music\".\nBakersfield sound.\nAnother subgenre of country music grew out of hardcore honky tonk with elements of Western swing and originated north-northwest of Los Angeles in Bakersfield, California, where many \"Okies\" and other Dust Bowl migrants had settled. Influenced by one-time West Coast residents Bob Wills and Lefty Frizzell, by 1966 it was known as the Bakersfield sound. It relied on electric instruments and amplification, in particular the Telecaster electric guitar, more than other subgenres of the country music of the era, and it can be described as having a sharp, hard, driving, no-frills, edgy flavor\u2014hard guitars and honky-tonk harmonies. Leading practitioners of this style were Buck Owens, Merle Haggard, Tommy Collins, Dwight Yoakam, Gary Allan, and Wynn Stewart, each of whom had his own style.\nKen Nelson, who had produced Owens and Haggard and Rose Maddox became interested in the trucking song subgenre following the success of \"Six Days on the Road\" and asked Red Simpson to record an album of trucking songs. Haggard's \"White Line Fever\" was also part of the trucking subgenre.\nWestern music merges with country.\nThe country music scene of the 1940s until the 1970s was largely dominated by Western music influences, so much so that the genre began to be called \"Country and Western\". Even today, cowboy and frontier values continue to play a role in the larger country music, with Western wear, cowboy boots, and cowboy hats continues to be in fashion for country artists.\nWest of the Mississippi river, many of these Western genres continue to flourish, including the Red Dirt of Oklahoma, New Mexico music of New Mexico, and both Texas country music and Tejano music of Texas. During the 1950s until the early 1970s, the latter part of the Western heyday in country music, many of these genres featured popular artists that continue to influence both their distinctive genres and larger country music. Red Dirt featured Bob Childers and Steve Ripley; for New Mexico music Al Hurricane, Al Hurricane Jr., and Antonia Apodaca; and within the Texas scenes Willie Nelson, Freddie Fender, Johnny Rodriguez, and Little Joe.\nAs Outlaw country music emerged as subgenre in its own right, Red Dirt, New Mexico, Texas country, and Tejano grew in popularity as a part of the Outlaw country movement. Originating in the bars, fiestas, and honky-tonks of Oklahoma, New Mexico, and Texas, their music supplemented outlaw country's singer-songwriter tradition as well as 21st-century rock-inspired alternative country and hip hop-inspired country rap artists.\nFourth generation (1970s\u20131980s).\nOutlaw movement.\nOutlaw country was derived from the traditional Western, including Red Dirt, New Mexico, Texas country, Tejano, and honky-tonk musical styles of the late 1950s and 1960s. Songs such as the 1963 Johnny Cash popularized \"Ring of Fire\" show clear influences from the likes of Al Hurricane and Little Joe, this influence just happened to culminate with artists such as Ray Price (whose band, the \"Cherokee Cowboys\", included Willie Nelson and Roger Miller) and mixed with the anger of an alienated subculture of the nation during the period, a collection of musicians that came to be known as the outlaw movement revolutionized the genre of country music in the early 1970s. \"After I left Nashville (the early 70s), I wanted to relax and play the music that I wanted to play, and just stay around Texas, maybe Oklahoma. Waylon and I had that outlaw image going, and when it caught on at colleges and we started selling records, we were O.K. The whole outlaw thing, it had nothing to do with the music, it was something that got written in an article, and the young people said, 'Well, that's pretty cool.' And started listening.\" (Willie Nelson) The term \"outlaw country\" is traditionally associated with Willie Nelson, Jerry Jeff Walker, Hank Williams, Jr., Merle Haggard, Waylon Jennings and Joe Ely. It was encapsulated in the 1976 album \"Wanted! The Outlaws\".\nThough the outlaw movement as a cultural fad had died down after the late 1970s (with Jennings noting in 1978 that it had gotten out of hand and led to real-life legal scrutiny), many Western and Outlaw country music artists maintained their popularity during the 1980s by forming supergroups, such as The Highwaymen, Texas Tornados, and Bandido.\nCountry pop.\nCountry pop or soft pop, with roots in the countrypolitan sound, folk music, and soft rock, is a subgenre that first emerged in the 1970s. Although the term first referred to country music songs and artists that crossed over to top 40 radio, country pop acts are now more likely to cross over to adult contemporary music. It started with pop music singers like Glen Campbell, Bobbie Gentry, John Denver, Olivia Newton-John, Anne Murray, B. J. Thomas, the Bellamy Brothers, and Linda Ronstadt having hits on the country charts. Between 1972 and 1975, singer/guitarist John Denver released a series of hugely successful songs blending country and folk-rock musical styles (\"Rocky Mountain High\", \"Sunshine on My Shoulders\", \"Annie's Song\", \"Thank God I'm a Country Boy\", and \"I'm Sorry\"), and was named Country Music Entertainer of the Year in 1975. The year before, Olivia Newton-John, an Australian pop singer, won the \"Best Female Country Vocal Performance\" as well as the Country Music Association's most coveted award for females, \"Female Vocalist of the Year\". In response George Jones, Tammy Wynette, Jean Shepard and other traditional Nashville country artists dissatisfied with the new trend formed the short-lived \"Association of Country Entertainers\" in 1974; the ACE soon unraveled in the wake of Jones and Wynette's bitter divorce and Shepard's realization that most others in the industry lacked her passion for the movement.\nDuring the mid-1970s, Dolly Parton, a successful mainstream country artist since the late 1960s, mounted a high-profile campaign to cross over to pop music, culminating in her 1977 hit \"Here You Come Again\", which topped the U.S. country singles chart, and also reached No. 3 on the pop singles charts. Parton's male counterpart, Kenny Rogers, came from the opposite direction, aiming his music at the country charts, after a successful career in pop, rock and folk music with the First Edition, achieving success the same year with \"Lucille\", which topped the country charts and reached No. 5 on the U.S. pop singles charts, as well as reaching Number 1 on the British all-genre chart. Parton and Rogers would both continue to have success on both country and pop charts simultaneously, well into the 1980s. Country music propelled Kenny Rogers\u2019 career, making him a three-time Grammy Award winner and six-time Country Music Association Awards winner. Having sold more than 50 million albums in the US, one of his Song \"The Gambler,\" inspired several TV films, with Rogers as the main character. Artists like Crystal Gayle, Ronnie Milsap and Barbara Mandrell would also find success on the pop charts with their records. In 1975, author Paul Hemphill stated in the \"Saturday Evening Post\", \"Country music isn't really country anymore; it is a hybrid of nearly every form of popular music in America.\"\nDuring the early 1980s, country artists continued to see their records perform well on the pop charts. Willie Nelson and Juice Newton each had two songs in the top 5 of the Billboard Hot 100 in the early eighties: Nelson charted \"Always on My Mind\" (#5, 1982) and \"To All the Girls I've Loved Before\" (#5, 1984, a duet with Julio Iglesias), and Newton achieved success with \"Queen of Hearts\" (#2, 1981) and \"Angel of the Morning\" (#4, 1981). Four country songs topped the \"Billboard\" Hot 100 in the 1980s: \"Lady\" by Kenny Rogers, from the late fall of 1980; \"9 to 5\" by Dolly Parton, \"I Love a Rainy Night\" by Eddie Rabbitt (these two back-to-back at the top in early 1981); and \"Islands in the Stream\", a duet by Dolly Parton and Kenny Rogers in 1983, a pop-country crossover hit written by Barry, Robin, and Maurice Gibb of the Bee Gees. Newton's \"Queen of Hearts\" almost reached No. 1, but was kept out of the spot by the pop ballad juggernaut \"Endless Love\" by Diana Ross and Lionel Richie. The move of country music toward neotraditional styles led to a marked decline in country/pop crossovers in the late 1980s, and only one song in that period\u2014Roy Orbison's \"You Got It\", from 1989\u2014made the top 10 of both the \"Billboard\" Hot Country Singles\" and Hot 100 charts, due largely to a revival of interest in Orbison after his sudden death. The only song with substantial country airplay to reach number one on the pop charts in the late 1980s was \"At This Moment\" by Billy Vera and the Beaters, an R&amp;B song with slide guitar embellishment that appeared at number 42 on the country charts from minor crossover airplay. The record-setting, multi-platinum group Alabama was named Artist of the Decade for the 1980s by the Academy of Country Music.\nCountry rock.\nCountry rock is a genre that started in the 1960s but became prominent in the 1970s. The late 1960s in American music produced a unique blend as a result of traditionalist backlash within separate genres. In the aftermath of the British Invasion, many desired a return to the \"old values\" of rock n' roll. At the same time there was a lack of enthusiasm in the country sector for Nashville-produced music. What resulted was a crossbred genre known as country rock. Early innovators in this new style of music in the 1960s and 1970s included Bob Dylan, who was the first to revert to country music with his 1967 album \"John Wesley Harding\" (and even more so with that album's follow-up, \"Nashville Skyline\"), followed by Gene Clark, Clark's former band the Byrds (with Gram Parsons on \"Sweetheart of the Rodeo\") and its spin-off the Flying Burrito Brothers (also featuring Gram Parsons), guitarist Clarence White, Michael Nesmith (the Monkees and the First National Band), the Grateful Dead, Neil Young, Commander Cody, the Allman Brothers Band, Charlie Daniels, the Marshall Tucker Band, Poco, Buffalo Springfield, Stephen Stills' band Manassas and Eagles, among many, even the former folk music duo Ian &amp; Sylvia, who formed Great Speckled Bird in 1969. The Eagles would become the most successful of these country rock acts, and their compilation album \"Their Greatest Hits (1971\u20131975)\" remains the second-best-selling album in the US with 29\u00a0million copies sold. The Rolling Stones also got into the act with songs like \"Dead Flowers\"; the original recording of \"Honky Tonk Women\" was performed in a country style, but it was subsequently re-recorded in a hard rock style for the single version, and the band's preferred country version was later released on the album \"Let It Bleed\", under the title \"Country Honk\".\nDescribed by AllMusic as the \"father of country-rock\", Gram Parsons' work in the early 1970s was acclaimed for its purity and for his appreciation for aspects of traditional country music. Though his career was cut tragically short by his 1973 death, his legacy was carried on by his prot\u00e9g\u00e9 and duet partner Emmylou Harris; Harris would release her debut solo in 1975, an amalgamation of country, rock and roll, folk, blues and pop. Subsequent to the initial blending of the two polar opposite genres, other offspring soon resulted, including Southern rock, heartland rock and in more recent years, alternative country. In the decades that followed, artists such as Juice Newton, Alabama, Hank Williams, Jr. (and, to an even greater extent, Hank Williams III), Gary Allan, Shania Twain, Brooks &amp; Dunn, Faith Hill, Garth Brooks, Dwight Yoakam, Steve Earle, Dolly Parton, Rosanne Cash and Linda Ronstadt moved country further towards rock influence.\nNeocountry.\nIn 1980, a style of \"neocountry disco music\" was popularized by the film \"Urban Cowboy\". It was during this time that a glut of pop-country crossover artists began appearing on the country charts: former pop stars Bill Medley (of the Righteous Brothers), \"England Dan\" Seals (of England Dan and John Ford Coley), Tom Jones, and Merrill Osmond (both alone and with some of his brothers; his younger sister Marie Osmond was already an established country star) all recorded significant country hits in the early 1980s. Sales in record stores rocketed to $250\u00a0million in 1981; by 1984, 900 radio stations began programming country or neocountry pop full-time. As with most sudden trends, however, by 1984 sales had dropped below 1979 figures.\nTruck driving country.\nTruck driving country music is a genre of country music\nand is a fusion of honky-tonk, country rock and the Bakersfield sound.\nIt has the tempo of country rock and the emotion of honky-tonk, and its lyrics focus on a truck driver's lifestyle. Truck driving country songs often deal with the profession of trucking and love. Well-known artists who sing truck driving country include Dave Dudley, Red Sovine, Dick Curless, Red Simpson, Del Reeves, the Willis Brothers and Jerry Reed, with C. W. McCall and Cledus Maggard (pseudonyms of Bill Fries and Jay Huguely, respectively) being more humorous entries in the subgenre. Dudley is known as the father of truck driving country.\nNeotraditionalist movement.\nDuring the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, \"back-to-basics\" production. Many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Reba McEntire, George Strait, Keith Whitley, Alan Jackson, John Anderson, Patty Loveless, Kathy Mattea, Randy Travis, Dwight Yoakam, Clint Black, Ricky Skaggs, and the Judds.\nBeginning in 1989, a confluence of events brought an unprecedented commercial boom to country music. New marketing strategies were used to engage fans, powered by technology that more accurately tracked the popularity of country music, and boosted by a political and economic climate that focused attention on the genre. Garth Brooks (\"Friends in Low Places\") in particular attracted fans with his fusion of neotraditionalist country and stadium rock. Other artists such as Brooks and Dunn (\"Boot Scootin' Boogie\") also combined conventional country with slick, rock elements, while Lorrie Morgan, Mary Chapin Carpenter, and Kathy Mattea updated neotraditionalist styles.\nFifth generation (1990s).\nCountry music was aided by the U.S. Federal Communications Commission's (FCC) Docket 80\u201390, which led to a significant expansion of FM radio in the 1980s by adding numerous higher-fidelity FM signals to rural and suburban areas. At this point, country music was mainly heard on rural AM radio stations; the expansion of FM was particularly helpful to country music, which migrated to FM from the AM band as AM became overcome by talk radio (the country music stations that stayed on AM developed the classic country format for the AM audience). At the same time, beautiful music stations already in rural areas began abandoning the format (leading to its effective demise) to adopt country music as well. This wider availability of country music led to producers seeking to polish their product for a wider audience. In 1990, \"Billboard\", which had published a country music chart since the 1940s, changed the methodology it used to compile the chart: singles sales were removed from the methodology, and only airplay on country radio determined a song's place on the chart.\nIn the 1990s, country music became a worldwide phenomenon thanks to Garth Brooks, who enjoyed one of the most successful careers in popular music history, breaking records for both sales and concert attendance throughout the decade. The RIAA has certified his recordings at a combined (128\u00d7 platinum), denoting roughly 113\u00a0million U.S. shipments. Other artists who experienced success during this time included Clint Black, John Michael Montgomery, Tracy Lawrence, Tim McGraw, Kenny Chesney, Travis Tritt, Alan Jackson and the newly formed duo of Brooks &amp; Dunn; George Strait, whose career began in the 1980s, also continued to have widespread success in this decade and beyond. Toby Keith began his career as a more pop-oriented country singer in the 1990s, evolving into an outlaw persona in the early 2000s with \"Pull My Chain\" and its follow-up, \"Unleashed\".\nSuccess of female artists.\nFemale artists such as Reba McEntire, Patty Loveless, Faith Hill, Martina McBride, Deana Carter, LeAnn Rimes, Mindy McCready, Pam Tillis, Lorrie Morgan, Shania Twain, and Mary Chapin Carpenter all released platinum-selling albums in the 1990s. The Dixie Chicks became one of the most popular country bands in the 1990s and early 2000s. Their 1998 debut album \"Wide Open Spaces\" went on to become certified 12\u00d7 platinum while their 1999 album \"Fly\" went on to become 10\u00d7 platinum. After their third album, \"Home\", was released in 2003, the band made political news in part because of lead singer Natalie Maines's comments disparaging then-President George W. Bush while the band was overseas (Maines stated that she and her bandmates were ashamed to be from the same state as Bush, who had just commenced the Iraq War a few days prior). The comments caused a rift between the band and the country music scene, and the band's fourth (and most recent) album, 2006's \"Taking the Long Way\", took a more rock-oriented direction; the album was commercially successful overall among non-country audiences but largely ignored among country audiences. After \"Taking the Long Way\", the band broke up for a decade (with two of its members continuing as the Court Yard Hounds) before reuniting in 2016 and releasing new material in 2020.\nCanadian artist Shania Twain became the best selling female country artist of the decade. This was primarily due to the success of her breakthrough sophomore 1995 album, \"The Woman in Me\", which was certified 12\u00d7 platinum sold over 20\u00a0million copies worldwide and its follow-up, 1997's \"Come On Over\", which was certified 20\u00d7 platinum and sold over 40\u00a0million copies. The album became a major worldwide phenomenon and became one of the world's best selling albums for three years (1998, 1999 and 2000); it also went on to become the best selling country album of all time.\nUnlike the majority of her contemporaries, Twain enjoyed large international success that had been seen by very few country artists, before or after her. Critics have noted that Twain enjoyed much of her success due to breaking free of traditional country stereotypes and for incorporating elements of rock and pop into her music. In 2002, she released her successful fourth studio album, titled \"Up!\", which was certified 11\u00d7 platinum and sold over 15\u00a0million copies worldwide. Shania Twain has been nominated eighteen times for Grammy Awards and won five Grammys. [] She was the best-paid country music star in 2016 according to Forbes, with a net worth of $27.5 million. []Twain has been credited with breaking international boundaries for country music, as well as inspiring many country artists to incorporate different genres into their music in order to attract a wider audience. She is also credited with changing the way in which many female country performers would market themselves, as unlike many before her she used fashion and her sex appeal to get rid of the stereotypical 'honky-tonk' image the majority of country singers had in order to distinguish herself from many female country artists of the time.\nLine dancing revival.\nIn the early-mid-1990s, country western music was influenced by the popularity of line dancing. This influence was so great that Chet Atkins was quoted as saying, \"The music has gotten pretty bad, I think. It's all that damn line dancing.\" By the end of the decade, however, at least one line dance choreographer complained that good country line dance music was no longer being released. In contrast, artists such as Don Williams and George Jones who had more or less had consistent chart success through the 1970s and 1980s suddenly had their fortunes fall rapidly around 1991 when the new chart rules took effect.\nAlternative country.\nCountry influences combined with Punk rock and alternative rock to forge the \"cowpunk\" scene in Southern California during the 1980s, which included bands such as the Long Ryders, Lone Justice and the Beat Farmers, as well as the established punk group X, whose music had begun to include country and rockabilly influences. Simultaneously, a generation of diverse country artists outside of California emerged that rejected the perceived cultural and musical conservatism associated with Nashville's mainstream country musicians in favor of more countercultural outlaw country and the folk singer-songwriter traditions of artists such as Woody Guthrie, Gram Parsons and Bob Dylan.\nArtists from outside California who were associated with early alternative country included singer-songwriters such as Lucinda Williams, Lyle Lovett and Steve Earle, the Nashville country rock band Jason and the Scorchers, the Providence \"cowboy pop\" band Rubber Rodeo, and the British post-punk band the Mekons. Earle, in particular, was noted for his popularity with both country and college rock audiences: He promoted his 1986 debut album \"Guitar Town\" with a tour that saw him open for both country singer Dwight Yoakam and alternative rock band the Replacements. Yoakam also cultivated a fanbase spanning multiple genres through his stripped-down honky-tonk influenced sound, association with the cowpunk scene, and performances at Los Angeles punk rock clubs.\nThese early styles had coalesced into a genre by the time the Illinois group Uncle Tupelo released their influential debut album \"No Depression\" in 1990. The album is widely credited as being the first \"alternative country\" album, and inspired the name of \"No Depression\" magazine, which exclusively covered the new genre. Following Uncle Tupelo's disbanding in 1994, its members formed two significant bands in genre: Wilco and Son Volt. Although Wilco's sound had moved away from country and towards indie rock by the time they released their critically acclaimed album \"Yankee Hotel Foxtrot\" in 2002, they have continued to be an influence on later alt-country artists.\nOther acts who became prominent in the alt-country genre during the 1990s and 2000s included the Bottle Rockets, the Handsome Family, Blue Mountain, Robbie Fulks, Blood Oranges, Bright Eyes, Drive-By Truckers, Old 97's, Old Crow Medicine Show, Nickel Creek, Neko Case, and Whiskeytown, whose lead singer Ryan Adams later had a successful solo-career. Alt-country, in various iterations overlapped with other genres, including Red Dirt country music (Cross Canadian Ragweed), jam bands (My Morning Jacket and the String Cheese Incident), and indie folk (the Avett Brothers).\nDespite the genre's growing popularity in the 1980s, 1990s and 2000s, alternative country and neo-traditionalist artists saw minimal support from country radio in those decades, despite strong sales and critical acclaim for albums such as the soundtrack to the 2000 film \"O Brother, Where Art Thou?\". In 1987, the Beat Farmers gained airplay on country music stations with their song \"Make It Last\", but the single was pulled from the format when station programmers decreed the band's music was too rock-oriented for their audience. However, some alt-country songs have been crossover hits to mainstream country radio in cover versions by established artists on the format; Lucinda Williams' \"Passionate Kisses\" was a hit for Mary Chapin Carpenter in 1993, Ryan Adams' \"When the Stars Go Blue\" was a hit for Tim McGraw in 2007, and Old Crow Medicine Show's \"Wagon Wheel\" was a hit for Darius Rucker (member of Hootie &amp; The Blowfish) in 2013.\nIn the 2010s, the alt-country genre saw an increase in its critical and commercial popularity, owing to the success of artists such as the Civil Wars, Chris Stapleton, Sturgill Simpson, Jason Isbell, Lydia Loveless and Margo Price. In 2019, Kacey Musgraves \u2013 a country artist who had gained a following with indie rock fans and music critics despite minimal airplay on country radio \u2013 won the Grammy Award for Album of the Year for her album \"Golden Hour\".\nSixth generation (2000s\u2013present).\nThe sixth generation of country music continued to be influenced by other genres such as pop, rock, and R&amp;B. Richard Marx crossed over with his \"Days in Avalon\" album, which features five country songs and several singers and musicians. Alison Krauss sang background vocals to Marx's single \"Straight from My Heart.\" Also, Bon Jovi had a hit single, \"Who Says You Can't Go Home\", with Jennifer Nettles of Sugarland. Kid Rock's collaboration with Sheryl Crow, \"Picture,\" was a major crossover hit in 2001 and began Kid Rock's transition from hard rock to a country-rock hybrid that would later produce another major crossover hit, 2008's \"All Summer Long.\" (Crow, whose music had often incorporated country elements, would also officially cross over into country with her hit \"Easy\" from her debut country album \"Feels like Home\"). Darius Rucker, frontman for the 1990s pop-rock band Hootie &amp; the Blowfish, began a country solo career in the late 2000s, one that to date has produced five albums and several hits on both the country charts and the Billboard Hot 100. Singer-songwriter Unknown Hinson became famous for his appearance in the Charlotte television show \"Wild, Wild, South\", after which Hinson started his own band and toured in southern states. Other rock stars who featured a country song on their albums were Don Henley (who released \"Cass County\" in 2015, an album which featured collaborations with numerous country artists) and Poison.\nThe back half of the 2010-2020 decade saw an increasing number of mainstream country acts collaborate with pop and R&amp;B acts; many of these songs achieved commercial success by appealing to fans across multiple genres; examples include collaborations between Kane Brown and Marshmello and Maren Morris and Zedd. There has also been interest from pop singers in country music, including Beyonc\u00e9, Lady Gaga, Alicia Keys, Gwen Stefani, Justin Timberlake, Justin Bieber and Pink. Supporting this movement is the new generation of contemporary pop-country, including Taylor Swift, Miranda Lambert, Carrie Underwood, Kacey Musgraves, Miley Cyrus, Billy Ray Cyrus, Sam Hunt, Chris Young, who introduced new themes in their works, touching on fundamental rights, feminism, and controversies about racism and religion of the older generations.\nPopular culture.\nIn 2005, country singer Carrie Underwood rose to fame as the winner of the fourth season of \"American Idol\" and has since become one of the most prominent recording artists in the genre, with worldwide sales of more than 65\u00a0million records and seven Grammy Awards. With her first single, \"Inside Your Heaven\", Underwood became the only solo country artist to have a number 1 hit on the \"Billboard\" Hot 100 chart in the 2000\u20132009 decade and also broke \"Billboard\" chart history as the first country music artist ever to debut at No. 1 on the Hot 100. Underwood's debut album, \"Some Hearts\", became the best-selling solo female debut album in country music history, the fastest-selling debut country album in the history of the SoundScan era and the best-selling country album of the last 10 years, being ranked by \"Billboard\" as the number 1 Country Album of the 2000\u20132009 decade. She has also become the female country artist with the most number one hits on the \"Billboard\" Hot Country Songs chart in the Nielsen SoundScan era (1991\u2013present), having 14 #1s and breaking her own \"Guinness Book\" record of ten. In 2007, Underwood won the Grammy Award for Best New Artist, becoming only the second Country artist in history (and the first in a decade) to win it. She also made history by becoming the seventh woman to win Entertainer of the Year at the Academy of Country Music Awards, and the first woman in history to win the award twice, as well as twice consecutively. \"Time\" has listed Underwood as one of the 100 most influential people in the world.\nIn 2016, Underwood topped the Country Airplay chart for the 15th time, becoming the female artist with most number ones on that chart.\nCarrie Underwood was only one of several country stars produced by a television series in the 2000s. In addition to Underwood, \"American Idol\" launched the careers of Kellie Pickler, Josh Gracin, Bucky Covington, Kristy Lee Cook, Danny Gokey, Lauren Alaina and Scotty McCreery (as well as that of occasional country singer Kelly Clarkson) in the decade, and would continue to launch country careers in the 2010s. The series \"Nashville Star\", while not nearly as successful as \"Idol\", did manage to bring Miranda Lambert, Kacey Musgraves and Chris Young to mainstream success, also launching the careers of lower-profile musicians such as Buddy Jewell, Sean Patrick McGraw, and Canadian musician George Canyon. \"Can You Duet?\" produced the duos Steel Magnolia and Joey + Rory. Teen sitcoms also have influenced modern country music; in 2008, actress Jennette McCurdy (best known as the sidekick Sam on the teen sitcom \"iCarly\") released her first single, \"So Close\", following that with the single \"Generation Love\" in 2011. Another teen sitcom star, Miley Cyrus (of Disney Channel's \"Hannah Montana\"), also had a crossover hit in the late 2000s with \"The Climb\" and another with a duet with her father, Billy Ray Cyrus, with \"Ready, Set, Don't Go.\" Jana Kramer, an actress in the teen drama \"One Tree Hill\", released a country album in 2012 that has produced two hit singles as of 2013. Actresses Hayden Panettiere and Connie Britton began recording country songs as part of their roles in the TV shows \"Nashville\" and \"Pretty Little Liars\" star Lucy Hale released her debut album \"Road Between\" in 2014.\nIn 2010, the group Lady Antebellum won five Grammys, including the coveted Song of the Year and Record of the Year for \"Need You Now\". A large number of duos and vocal groups emerged on the charts in the 2010s, many of which feature close harmony in the lead vocals. In addition to Lady A, groups such as Little Big Town, the Band Perry, Gloriana, Thompson Square, Eli Young Band, Zac Brown Band and British duo the Shires have emerged to occupy a large share of mainstream success alongside solo singers such as Kacey Musgraves and Miranda Lambert.\nOne of the most commercially successful country artists of the late 2000s and early 2010s has been singer-songwriter Taylor Swift. Swift first became widely known in 2006 when her debut single, \"Tim McGraw,\" was released when Swift was only 16 years old. In 2006, Swift released her self-titled debut studio album, which spent 275 weeks on \"Billboard\" 200, one of the longest runs of any album on that chart. In 2008, Taylor Swift released her second studio album, \"Fearless\", which made her the second longest number-one charted on \"Billboard\" 200 and the second best-selling album (just behind Adele's \"21\") within the past 5 years. At the 2010 Grammys, Taylor Swift was 20 and won Album of the Year for \"Fearless\", which made her the youngest artist to win this award. Swift has received twelve Grammys already.\nBuoyed by her teen idol status among girls and a change in the methodology of compiling the \"Billboard\" charts to favor pop-crossover songs, Swift's 2012 single \"We Are Never Ever Getting Back Together\" spent the most weeks at the top of Billboard's Hot 100 chart and Hot Country Songs chart of any song in nearly five decades. The song's long run at the top of the chart was somewhat controversial, as the song is largely a pop song without much country influence and its success on the charts driven by a change to the chart's criteria to include airplay on non-country radio stations, prompting disputes over what constitutes a country song; many of Swift's later releases, such as album \"1989\" (2014), \"Reputation\" (2017), and \"Lover\" (2019) were released solely to pop audiences. Swift returned to country music in her recent folk-inspired releases, \"Folklore\" (2020) and \"Evermore\" (2020), with songs like \"Betty\" and \"No Body, No Crime\".\nModern variations.\nInfluence of rock, pop and hip-hop.\nIn the mid to late 2010s, country music began to increasingly sound more like the style of modern-day Pop music, with more simple and repetitive lyrics, more electronic-based instrumentation, and experimentation with \"talk-singing\" and rap, pop-country pulled farther away from the traditional sounds of country music and received criticisms from country music purists while gaining in popularity with mainstream audiences. The topics addressed have also changed, turning controversial such as acceptance of the LGBT community, safe sex, recreational marijuana use, and questioning religious sentiment. Influences also come from some pop artists' interest in the country genre, including Justin Timberlake with the album \"Man of the Woods,\" Beyonc\u00e9's single \"Daddy Lessons\" from \"Lemonade\", Gwen Stefani with \"Nobody but You\", Bruno Mars, Lady Gaga, Alicia Keys, Kelly Clarkson, and Pink.\nThe influence of rock music in country has become more overt during the late 2000s and early 2010s as artists like Eric Church, Jason Aldean, and Brantley Gilbert have had success; Aaron Lewis, former frontman for the rock group Staind, had a moderately successful entry into country music in 2011 and 2012, as did Dallas Smith, former frontman of the band Default.\nMaren Morris success collaboration \"The Middle\" with EDM producer Zedd is considered one of the representations of the fusion of electro-pop with country music.\nLil Nas X song \"Old Town Road\" spent 19 weeks atop the US \"Billboard\" Hot 100 chart, becoming the longest-running number-one song since the chart debuted in 1958, winning Billboard Music Awards, MTV Video Music Awards and Grammy Award. Sam Hunt \"Leave the Night On\" peaked concurrently on the Hot Country Songs and Country Airplay charts, making Hunt the first country artist in 22 years, since Billy Ray Cyrus, to reach the top of three country charts simultaneously in the Nielsen SoundScan-era. With the fusion genre of \"country trap\"\u2014a fusion of country/western themes to a hip hop beat, but usually with fully sung lyrics\u2014emerging in the late 2010s, line dancing country had a minor revival, examples of the phenomenon include \"The Git Up\" by Blanco Brown. Blanco Brown has gone of to make more traditional country soul songs such as \"I Need Love\" and a rendition of \"Don't Take the Girl\" with Tim McGraw, and collaborations like \"Just the Way\" with Parmalee. Another country trap artist known as Breland has seen success with \"My Truck, \"Throw It Back\" with Keith Urban, and \"Praise the Lord\" featuring Thomas Rhett.\nEmo rap musician Sueco, released a cowpunk song in collaboration is country musician Warren Zeiders titled \"Ride It Hard\". Alex Melton, known for his music covers, blends pop punk with country music.\nBro country.\nIn the early 2010s, \"bro-country\", a genre noted primarily for its themes on drinking and partying, girls, and pickup trucks became particularly popular. Notable artists associated with this genre are Luke Bryan, Jason Aldean, Blake Shelton, Jake Owen and Florida Georgia Line whose song \"Cruise\" became the best-selling country song of all time. Research in the mid-2010s suggested that about 45 percent of country's best-selling songs could be considered bro-country, with the top two artists being Luke Bryan and Florida Georgia Line. Albums by bro-country singers also sold very well\u2014in 2013, Luke Bryan's \"Crash My Party\" was the third best-selling of all albums in the United States, with Florida Georgia Line's \"Here's to the Good Times\" at sixth, and Blake Shelton's \"Based on a True Story\" at ninth. It is also thought that the popularity of bro-country helped country music to surpass classic rock as the most popular genre in the American country in 2012. The genre however is controversial as it has been criticized by other country musicians and commentators over its themes and depiction of women, opening up a divide between the older generation of country singers and the younger bro country singers that was described as \"civil war\" by musicians, critics, and journalists.\" In 2014, Maddie &amp; Tae's \"Girl in a Country Song\", addressing many of the controversial bro-country themes, peaked at number one on the \"Billboard\" Country Airplay chart.\nBluegrass and Americana.\nis a genre that contain songs about going through hard times, country loving, and telling stories. Newer artists like Billy Strings, the Grascals, Molly Tuttle, Tyler Childers and the Infamous Stringdusters have been increasing the popularity of this genre, alongside some of the genres more established stars who still remain popular including Rhonda Vincent, Alison Krauss and Union Station, Ricky Skaggs and Del McCoury. The genre has developed in the Northern Kentucky and Cincinnati area. Other artists include New South (band), Doc Watson, Osborne Brothers, and many others.\nIn an effort to combat the over-reliance of mainstream country music on pop-infused artists, the sister genre of Americana began to gain popularity and increase in prominence, receiving eight Grammy categories of its own in 2009. Americana music incorporates elements of country music, bluegrass, folk, blues, gospel, rhythm and blues, roots rock and southern soul and is overseen by the Americana Music Association and the Americana Music Honors &amp; Awards. As a result of an increasingly pop-leaning mainstream, many more traditional-sounding artists such as Tyler Childers, Zach Bryan and Old Crow Medicine Show began to associate themselves more with Americana and the alternative country scene where their sound was more celebrated. Similarly, many established country acts who no longer received commercial airplay, including Emmylou Harris and Lyle Lovett, began to flourish again.\nContemporary country and Western revival.\nDuring the mid-1980s, a group of new artists began to emerge who rejected the more polished country-pop sound that had been prominent on radio and the charts, in favor of more, traditional, \"back-to-basics\" production. Many of the artists during the latter half of the 1980s drew on traditional honky-tonk, bluegrass, folk and western swing. Artists who typified this sound included Travis Tritt, Reba McEntire, George Strait, Keith Whitley, Alan Jackson, John Anderson, Patty Loveless, Kathy Mattea, Randy Travis, Dwight Yoakam, Clint Black, Ricky Skaggs, and the Judds.\nBeginning in 1989, a confluence of events brought an unprecedented commercial boom to country music. New marketing strategies were used to engage fans, powered by technology that more accurately tracked the popularity of country music, and boosted by a political and economic climate that focused attention on the genre. Garth Brooks (\"Friends in Low Places\") in particular attracted fans with his fusion of neotraditionalist country and stadium rock. Other artists such as Brooks and Dunn (\"Boot Scootin' Boogie\") also combined conventional country with slick, rock elements, while Lorrie Morgan, Mary Chapin Carpenter, and Kathy Mattea updated neotraditionalist styles.\nRoots of conservative country was Lee Greenwood's \"God Bless the USA\". The September 11 attacks of 2001 and the economic recession helped move country music back into the spotlight. Many country artists, such as Alan Jackson with his ballad on terrorist attacks, \"Where Were You (When the World Stopped Turning)\", wrote songs that celebrated the military, highlighted the gospel, and emphasized home and family values over wealth. Alt-Country singer Ryan Adams song \"New York, New York\" pays tribute to New York City, and its popular music video (which was shot 4 days before the attacks) shows Adams playing in front of the Manhattan skyline, Along with several shots of the city. In contrast, more rock-oriented country singers took more direct aim at the attacks' perpetrators; Toby Keith's \"Courtesy of the Red, White and Blue (The Angry American)\" threatened to \"a boot in\" the posterior of the enemy, while Charlie Daniels's \"This Ain't No Rag, It's a Flag\" promised to \"hunt\" the perpetrators \"down like a mad dog hound.\" These songs gained such recognition that it put country music back into popular culture. Darryl Worley recorded \"Have You Forgotten\" also. There have been numerous patriotic country songs throughout the years.\nSome modern artists that primarily or entirely produce country pop music include Kacey Musgraves, Maren Morris, Kelsea Ballerini, Sam Hunt, Kane Brown, Chris Lane, and Dan + Shay. The singers who are part of this country movement are also defined as \"Nashville's new generation of country\".\nAlthough the changes made by the new generation, it has been recognized by major music awards associations and successes in Billboard and international charts. \"Golden Hour\" by Kacey Musgraves won album of the year at 61st Annual Grammy Awards, Academy of Country Music Awards, Country Music Association Awards, although it has received widespread criticism from the more traditionalist public.\nInternational.\nAustralia.\nAustralian country music has a long tradition. Influenced by US country music, it has developed a distinct style, shaped by British and Irish folk ballads and Australian bush balladeers like Henry Lawson and Banjo Paterson. Country instruments, including the guitar, banjo, fiddle and harmonica, create the distinctive sound of country music in Australia and accompany songs with strong storyline and memorable chorus.\nFolk songs sung in Australia between the 1780s and 1920s, based around such themes as the struggle against government tyranny, or the lives of bushrangers, swagmen, drovers, stockmen and shearers, continue to influence the genre. This strain of Australian country, with lyrics focusing on Australian subjects, is generally known as \"bush music\" or \"bush band music\". \"Waltzing Matilda\", often regarded as Australia's unofficial national anthem, is a quintessential Australian country song, influenced more by British and Irish folk ballads than by US country and western music. The lyrics were composed by the poet Banjo Paterson in 1895. Other popular songs from this tradition include \"The Wild Colonial Boy\", \"Click Go the Shears\", \"The Queensland Drover\" and \"The Dying Stockman\". Later themes which endure to the present include the experiences of war, of droughts and flooding rains, of Aboriginality and of the railways and trucking routes which link Australia's vast distances.\nPioneers of a more Americanised popular country music in Australia included Tex Morton (known as \"The Father of Australian Country Music\") in the 1930s. Author Andrew Smith delivers a through research and engaged view of Tex Morton's life and his impact on the country music scene in Australia in the 1930s and 1940s. Other early stars included Buddy Williams, Shirley Thoms and Smoky Dawson. Buddy Williams (1918\u20131986) was the first Australian-born to record country music in Australia in the late 1930s and was the pioneer of a distinctly Australian style of country music called the bush ballad that others such as Slim Dusty would make popular in later years. During the Second World War, many of Buddy Williams recording sessions were done whilst on leave from the Army. At the end of the war, Williams would go on to operate some of the largest travelling tent rodeo shows Australia has ever seen.\nIn 1952, Dawson began a radio show and went on to national stardom as a singing cowboy of radio, TV and film. Slim Dusty (1927\u20132003) was known as the \"King of Australian Country Music\" and helped to popularise the Australian bush ballad. His successful career spanned almost six decades, and his 1957 hit \"A Pub with No Beer\" was the biggest-selling record by an Australian to that time, and with over seven million record sales in Australia he is the most successful artist in Australian musical history. Dusty recorded and released his one-hundredth album in the year 2000 and was given the honour of singing \"Waltzing Matilda\" in the closing ceremony of the Sydney 2000 Olympic Games. Dusty's wife Joy McKean penned several of his most popular songs.\nChad Morgan, who began recording in the 1950s, has represented a vaudeville style of comic Australian country; Frank Ifield achieved considerable success in the early 1960s, especially in the UK Singles Charts and Reg Lindsay was one of the first Australians to perform at Nashville's Grand Ole Opry in 1974. Eric Bogle's 1972 folk lament to the Gallipoli Campaign \"And the Band Played Waltzing Matilda\" recalled the British and Irish origins of Australian folk-country. Singer-songwriter Paul Kelly, whose music style straddles folk, rock and country, is often described as the poet laureate of Australian music. \nBy the 1990s, country music had attained crossover success in the pop charts, with artists like James Blundell and James Reyne singing \"Way Out West\", and country star Kasey Chambers winning the ARIA Award for Best Female Artist in three years (2000, 2002 and 2004), tying with pop stars Wendy Matthews and Sia for the most wins in that category. Furthermore, Chambers has gone on to win nine ARIA Awards for Best Country Album and, in 2018, became the youngest artist to ever be inducted into the ARIA Hall of Fame. The crossover influence of Australian country is also evident in the music of successful contemporary bands the Waifs and the John Butler Trio. Nick Cave has been heavily influenced by the country artist Johnny Cash. In 2000, Cash, covered Cave's \"The Mercy Seat\" on the album ', seemingly repaying Cave for the compliment he paid by covering Cash's \"The Singer\" (originally \"The Folk Singer\") on his \"Kicking Against the Pricks\" album. Subsequently, Cave cut a duet with Cash on a version of Hank Williams' \"I'm So Lonesome I Could Cry\" for Cash's ' album (2002).\nPopular contemporary performers of Australian country music include John Williamson (who wrote the iconic \"True Blue\"), Lee Kernaghan (whose hits include \"Boys from the Bush\" and \"The Outback Club\"), Gina Jeffreys, Forever Road and Sara Storer. In the U.S., Olivia Newton-John, Sherri\u00e9 Austin and Keith Urban have attained great success. During her time as a country singer in the 1970s, Newton-John became the first (and to date only) non-US winner of the Country Music Association Award for Female Vocalist of the Year which many considered a controversial decision by the CMA; after starring in the rock-and-roll musical film \"Grease\" in 1978, Newton-John (mirroring the character she played in the film) shifted to pop music in the 1980s. Urban is arguably considered the most successful international Australian country star, winning nine CMA Awards, including three Male Vocalist of the Year wins and two wins of the CMA's top honour Entertainer of the Year. Pop star Kylie Minogue found success with her 2018 country pop album \"Golden\" which she recorded in Nashville reaching number one in Scotland, the UK and her native Australia.\nCountry music has been a particularly popular form of musical expression among Indigenous Australians. Troy Cassar-Daley is among Australia's successful contemporary indigenous performers, and Kev Carmody and Archie Roach employ a combination of folk-rock and country music to sing about Aboriginal rights issues.\nThe Tamworth Country Music Festival began in 1973 and now attracts up to 100,000 visitors annually. Held in Tamworth, New South Wales (country music capital of Australia), it celebrates the culture and heritage of Australian country music. During the festival the CMAA holds the Country Music Awards of Australia ceremony awarding the Golden Guitar trophies. Other significant country music festivals include the Whittlesea Country Music Festival (near Melbourne) and the Mildura Country Music Festival for \"independent\" performers during October, and the Canberra Country Music Festival held in the national capital during November.\n\"Country HQ\" showcases new talent on the rise in the country music scene down under. CMC (the Country Music Channel), a 24\u2011hour music channel dedicated to non-stop country music, can be viewed on pay TV and features once a year the Golden Guitar Awards, CMAs and CCMAs alongside international shows such as \"The Wilkinsons\", \"The Road Hammers\", and \"Country Music Across America\".\nCanada.\nOutside of the United States, Canada has the largest country music fan and artist base, something that is to be expected given the two countries' proximity and cultural parallels. Mainstream country music is culturally ingrained in the prairie provinces, the British Columbia Interior, Ontario, and in Atlantic Canada. Celtic traditional music developed in Atlantic Canada in the form of Scottish, Acadian and Irish folk music popular amongst Irish, French and Scottish immigrants to Canada's Atlantic Provinces (Newfoundland, Nova Scotia, New Brunswick, and Prince Edward Island). Like the southern United States and Appalachia, all four regions are of heavy British Isles stock and rural; as such, the development of traditional music in the Maritimes somewhat mirrored the development of country music in the US South and Appalachia. Country and Western music never really developed separately in Canada; however, after its introduction to Canada, following the spread of radio, it developed quite quickly out of the Atlantic Canadian traditional scene. While true Atlantic Canadian traditional music is very Celtic or \"sea shanty\" in nature, even today, the lines have often been blurred. Certain areas often are viewed as embracing one strain or the other more openly. For example, in Newfoundland the traditional music remains unique and Irish in nature, whereas traditional musicians in other parts of the region may play both genres interchangeably.\n\"Don Messer's Jubilee\" was a Halifax, Nova Scotia-based country/folk variety television show that was broadcast nationally from 1957 to 1969. In Canada it out-performed \"The Ed Sullivan Show\" broadcast from the United States and became the top-rated television show throughout much of the 1960s. \"Don Messer's Jubilee\" followed a consistent format throughout its years, beginning with a tune named \"Goin' to the Barndance Tonight\", followed by fiddle tunes by Messer, songs from some of his \"Islanders\" including singers Marg Osburne and Charlie Chamberlain, the featured guest performance, and a closing hymn. It ended with \"Till We Meet Again\". The guest performance slot gave national exposure to numerous Canadian folk musicians, including Stompin' Tom Connors and Catherine McKinnon. Some Maritime country performers went on to further fame beyond Canada. Hank Snow, Wilf Carter (also known as Montana Slim), and Anne Murray are the three most notable. The cancellation of the show by the public broadcaster in 1969 caused a nationwide protest, including the raising of questions in the Parliament of Canada.\nThe Prairie provinces, due to their western cowboy and agrarian nature, are the true heartland of Canadian country music. While the Prairies never developed a traditional music culture anything like the Maritimes, the folk music of the Prairies often reflected the cultural origins of the settlers, who were a mix of Scottish, Ukrainian, German and others. For these reasons polkas and Western music were always popular in the region, and with the introduction of the radio, mainstream country music flourished. As the culture of the region is western and frontier in nature, the specific genre of country and western is more popular today in the Prairies than in any other part of the country. No other area of the country embraces all aspects of the culture, from two-step dancing, to the cowboy dress, to rodeos, to the music itself, like the Prairies do. The Atlantic Provinces, on the other hand, produce far more traditional musicians, but they are not usually specifically country in nature, usually bordering more on the folk or Celtic genres.\nCanadian country pop star Shania Twain is the best-selling female country artist of all time and one of the best-selling artists of all time in any genre. Furthermore, she is the only woman to have three consecutive albums be certified Diamond.\nMexico and Latin America.\nCountry music artists from the U.S. have seen crossover with Latin American audiences, particularly in Mexico. Country music artists from throughout the U.S. have recorded renditions of Mexican folk songs, including \"El Rey\" which was performed on George Strait's \"Twang\" album and during Al Hurricane's tribute concert. American Latin pop crossover musicians, like Lorenzo Antonio's \"Ranchera Jam\" have also combined Mexican songs with country songs in a New Mexico music style.\nWhile Tejano and New Mexico music is typically thought of as being Spanish language, the genres have also had charting musicians focused on English language music. During the 1970s, singer-songwriter Freddy Fender had two #1 country music singles, that were popular throughout North America, with \"Before the Next Teardrop Falls\" and \"Wasted Days and Wasted Nights\". Notable songs which have been influenced by Hispanic and Latin culture as performed by US country music artists include Marty Robbins' \"El Paso\" trilogy, Willie Nelson and Merle Haggard covering the Townes Van Zandt song \"Pancho and Lefty\", \"Toes\" by Zac Brown Band, and \"Sangria\" by Blake Shelton.\nRegional Mexican is a radio format featuring many of Mexico's versions of country music. It includes a number of different styles, usually named after their region of origin. One specific song style, the Canci\u00f3n Ranchera, or simply Ranchera, literally meaning \"ranch song\", found its origins in the Mexican countryside and was first popularized with Mariachi. It has since also become popular with Grupero, Banda, Norte\u00f1o, Tierra Caliente, Duranguense and other regional Mexican styles. The Corrido, a different song style with a similar history, is also performed in many other regional styles, and is most related to the Western style of the United States and Canada. Other song styles performed in regional Mexican music include Ballads, Cumbias, Boleros, among others. Country en Espa\u00f1ol (Country in Spanish) is also popular in Mexico. Some Mexican artists began performing country songs in Spanish during the 1970s, and the genre became prominent mainly in the northern regions of the country during the 1980s. A Country en Espa\u00f1ol popularity boom also reached the central regions of Mexico during the 1990s. For most of its history, Country en Espa\u00f1ol mainly resembled Neotraditional country. However, in more modern times, some artists have incorporated influences from other country music subgenres.\nIn Brazil, there is M\u00fasica Sertaneja, the most popular music genre in that country. It originated in the countryside of S\u00e3o Paulo state in the 1910s, before the development of US country music.\nIn Argentina, on the last weekend of September, the yearly San Pedro Country Music Festival takes place in the town of San Pedro, Buenos Aires. The festival features bands from different places in Argentina, as well as international artists from Brazil, Uruguay, Chile, Peru and the U.S.\nUnited Kingdom.\nCountry music is popular in the United Kingdom, although somewhat less so than in other English-speaking countries. There are some British country music acts and publications. Although radio stations devoted to country are among the most popular in other Anglophone nations, none of the top ten most-listened-to stations in the UK are country stations, and national broadcaster BBC Radio does not offer a full-time country station (BBC Radio 2 Country, a \"pop-up\" station, operated four days each year between 2015 and 2017). The BBC does offer a country show on BBC Radio 2 each week hosted by Bob Harris.\nThe most successful British country music act of the 21st century are Ward Thomas and the Shires. In 2015, the Shires' album \"Brave\", became the first UK country act ever to chart in the Top 10 of the UK Albums Chart and they became the first UK country act to receive an award from the American Country Music Association. In 2016, Ward Thomas then became the first UK country act to hit number 1 in the UK Albums Chart with their album \"Cartwheels\".\nThere is the festival held every year, and for many years there was a festival at Wembley Arena, which was broadcast on the BBC, the International Festivals of Country Music, promoted by Mervyn Conn, held at the venue between 1969 and 1991. The shows were later taken into Europe, and featured such stars as Johnny Cash, Dolly Parton, Tammy Wynette, David Allan Coe, Emmylou Harris, Boxcar Willie, Johnny Russell and Jerry Lee Lewis. A handful of country musicians had even greater success in mainstream British music than they did in the U.S., despite a certain amount of disdain from the music press. Britain's largest music festival Glastonbury has featured major US country acts in recent years, such as Kenny Rogers in 2013 and Dolly Parton in 2014.\nFrom within the UK, few country musicians achieved widespread mainstream success. Many British singers who performed the occasional country songs are of other genres. Tom Jones, by this point near the end of his peak success as a pop singer, had a string of country hits in the late 1970s and early 1980s. The Bee Gees had some fleeting success in the genre, with one country hit as artists (\"Rest Your Love on Me\") and a major hit as songwriters (\"Islands in the Stream\"); Barry Gibb, the band's usual lead singer and last surviving member, acknowledged that country music was a major influence on the band's style. Singer Engelbert Humperdinck, while charting only once in the U.S. country top 40 with \"After the Lovin',\" achieved widespread success on both the U.S. and British pop charts with his covers of Nashville country ballads such as \"Release Me,\" \"Am I That Easy to Forget\" and \"There Goes My Everything.\" Welsh singer Bonnie Tyler initially started her career making country records, and in 1978 her single \"It's a Heartache\" reached number four on the UK Singles Chart. In 2013, Tyler returned to her roots, blending the country elements of her early work with the rock of her successful material on her album \"Rocks and Honey\" which featured a duet with Vince Gill. The songwriting tandem of Roger Cook and Roger Greenaway wrote a number of country hits, in addition to their widespread success in pop songwriting; Cook is notable for being the only Briton to be inducted into the Nashville Songwriters Hall of Fame. \nA niche country subgenre popular in the West Country is Scrumpy and Western, which consists mostly of novelty songs and comedy music recorded there (its name comes from scrumpy, an alcoholic beverage). A primarily local interest, the largest Scrumpy and Western hit in the UK and Ireland was \"The Combine Harvester,\" which pioneered the genre and reached number one in both the UK and Ireland; Fred Wedlock had a number-six hit in 1981 with \"The Oldest Swinger in Town.\" In 1975, comedian Billy Connolly topped the UK Singles Chart with \"D.I.V.O.R.C.E.\", a parody of the Tammy Wynette song \"D-I-V-O-R-C-E\".\nThe British Country Music Festival is an annual three-day festival held in the seaside resort of Blackpool. It uniquely promotes artists from the United Kingdom and Ireland to celebrate the impact that Celtic and British settlers to America had on the origins of country music. Past headline artists have included Amy Wadge, Ward Thomas, Tom Odell, Nathan Carter, Lisa McHugh, Catherine McGrath, Wildwood Kin, The Wandering Hearts and Henry Priestman.\nIreland.\nIn Ireland, Country and Irish is a music genre that combines traditional Irish folk music with US country music. Television channel TG4 began a quest for Ireland's next country star called \"Gl\u00f3r T\u00edre\", translated as \"Country Voice\". It is now in its sixth season and is one of TG4's most-watched TV shows. Over the past ten years, country and gospel recording artist James Kilbane has reached multi-platinum success with his mix of Christian and traditional country influenced albums. James Kilbane like many other Irish artists is today working closer with Nashville. Daniel O'Donnell achieved international success with his brand of music crossing country, Irish folk and European easy listening, earning a strong following among older women both in the British Isles and in North America. A recent success in the Irish arena has been Crystal Swing.\nJapan and Asia.\nIn Japan, there are forms of J-country and J-Western similar to other J-pop movements, J-hip hop and J-rock. One of the first J-Western musicians was Biji Kuroda &amp; The Chuck Wagon Boys, other vintage artists included Jimmie Tokita and His Mountain Playboys, The Blue Rangers, Wagon Aces, and Tomi Fujiyama. J-country continues to have a dedicated following in Japan, thanks to Charlie Nagatani, Katsuoshi Suga, J.T. Kanehira, Dicky Kitano, and Manami Sekiya. Country and Western venues in Japan include the former annual Country Gold which were put together by Charlie Nagatani, and the modern honky tonks at Little Texas in Tokyo and Armadillo in Nagoya.\nIn India, there is an annual concert festival called \"Blazing Guitars\" held in Chennai brings together Anglo-Indian musicians from all over the country (including some who have emigrated to places like Australia). The year 2003 brought home-grown Indian, Bobby Cash to the forefront of the country music culture in India when he became India's first international country music artist to chart singles in Australia.\nIn the Philippines, country music has found their way into Cordilleran way of life, which often compares the Igorot lifestyle to that of US cowboys. Baguio City has an FM station that caters to country music, DZWR 99.9 Country, which is part of the Catholic Media Network. Bombo Radyo Baguio has a segment on its Sunday slot for Igorot, Ilocano and country music. And as of recently, DWUB occasionally plays country music. Many country music musicians tour the Philippines. Original Pinoy Music has influences from country.\nOther international country music.\nTom Roland, from the Country Music Association International, explains country music's global popularity: \"In this respect, at least, Country Music listeners around the globe have something in common with those in the United States. In Germany, for instance, Rohrbach identifies three general groups that gravitate to the genre: people intrigued with the US cowboy icon, middle-aged fans who seek an alternative to harder rock music and younger listeners drawn to the pop-influenced sound that underscores many current Country hits.\" One of the first US people to perform country music abroad was George Hamilton IV. He was the first country musician to perform in the Soviet Union; he also toured in Australia and the Middle East. He was deemed the \"International Ambassador of Country Music\" for his contributions to the globalization of country music. Johnny Cash, Emmylou Harris, Keith Urban, and Dwight Yoakam have also made numerous international tours. The Country Music Association undertakes various initiatives to promote country music internationally.\nMiddle East.\nIn Iran, country music has appeared in recent years. According to \"Melody Music Magazine\", the pioneer of country music in Iran is the English-speaking country music band Dream Rovers, whose founder, singer and songwriter is Erfan Rezayatbakhsh (elf). The band was formed in 2007 in Tehran, and during this time they have been trying to introduce and popularize country music in Iran by releasing two studio albums and performing live at concerts, despite the difficulties that the Islamic regime in Iran makes for bands that are active in the western music field.\nMusician Toby Keith performed alongside Saudi Arabian folk musician Rabeh Sager in 2017. This concert was similar to the performances of Jazz ambassadors that performed distinctively American style music internationally.\nContinental Europe.\nIn Sweden, Rednex rose to stardom combining country music with electro-pop in the 1990s. In 1994, the group had a worldwide hit with their version of the traditional Southern tune \"Cotton-Eyed Joe\". Artists popularizing more traditional country music in Sweden have been Ann-Louise Hanson, Hasse Andersson, Kikki Danielsson, Elisabeth Andreassen and Jill Johnson. In Poland an international country music festival, known as Piknik Country, has been organised in Mr\u0105gowo in Masuria since 1983. The number of country music artists in France has increased. Some of the most important are Liane Edwards, Annabel, Rockie Mountains, Tahiana, and Lili West. French rock and roll singer Eddy Mitchell is also inspired by Americana and country music.\nIn the Netherlands there are many artists producing popular country and Americana music, which is mostly in the English language, as well as Dutch country and country-like music in the Dutch language. The latter is mainly popular on the countrysides in the northern and eastern parts of the Netherlands and is less associated with its US brethren, although it sounds sometimes very similar. Well-known popular artists mainly performing in English are Waylon, Danny Vera, Ilse DeLange, Douwe Bob and Henk Wijngaard.\nPerformers and shows.\nUS cable television.\nSeveral US television networks are at least partly devoted to the genre: Country Music Television (the first channel devoted to country music) and CMT Music (both owned by Paramount Global), RFD-TV and The Cowboy Channel (both owned by Rural Media Group), Heartland (owned by Get After It Media), Circle (a joint venture of the \"Grand Ole Opry\" and Gray Television), and The Country Network (owned by TCN Country, LLC).\nThe Nashville Network (TNN) was launched in 1983 as a channel devoted to country music, and later added sports and outdoor lifestyle programming. It actually launched just two days after CMT. In 2000, after TNN and CMT fell under the same corporate ownership, TNN was stripped of its country format and rebranded as \"The National Network\", then \"Spike TV\" in 2003, \"Spike\" in 2006, and finally Paramount Network in 2018. TNN was later revived from 2012 to 2013 after Jim Owens Entertainment (the company responsible for prominent TNN hosts Crook &amp; Chase) acquired the trademark and licensed it to Luken Communications; that channel renamed itself Heartland after Luken was embroiled in an unrelated dispute that left the company bankrupt.\nGreat American Country (GAC) was launched in 1995, also as a country music-oriented channel that would later add lifestyle programming pertaining to the American Heartland and South. In Spring 2021, GAC's then-owner, Discovery, Inc. divested the network to GAC Media, which also acquired the equestrian network Ride TV. Later, in the summer of that year, GAC Media relaunched Great American Country as GAC Family, a family-oriented general entertainment network, while Ride TV was relaunched as GAC Living, a network devoted to programming pertaining to lifestyles of the American South. The GAC acronym which once stood for \"Great American Country\" now stands for \"Great American Channels\".\nCanadian television.\nOnly one television channel was dedicated to country music in Canada: CMT owned by Corus Entertainment (90%) and Viacom (10%). However, the lifting of strict genre licensing restrictions saw the network remove the last of its music programming at the end of August 2017 for a schedule of generic off-network family sitcoms, Cancom-compliant lifestyle programming, and reality programming. In the past, the current-day Cottage Life network saw some country focus as Country Canada and later, CBC Country Canada before that network drifted into an alternate network for overflow CBC content as Bold. Stingray Music continues to maintain several country music audio-only channels on cable radio.\nIn the past, country music had an extensive presence, especially on the Canadian national broadcaster, CBC Television. The show \"Don Messer's Jubilee\" significantly affected country music in Canada; for instance, it was the program that launched Anne Murray's career. Gordie Tapp's \"Country Hoedown\" and its successor, \"The Tommy Hunter Show\", ran for a combined 36 years on the CBC, from 1956 to 1992; in its last nine years on air, the U.S. cable network TNN carried Hunter's show.\nAustralian cable television.\nThe only network dedicated to country music in Australia was the Country Music Channel owned by Foxtel. It ceased operations in June 2020 and was replaced by CMT (owned by Network 10 parent company Paramount Networks UK &amp; Australia).\nBritish digital television.\nOne music video channel is now dedicated to country music in the United Kingdom: Spotlight TV, owned by Canis Media.\nCriticism.\nSubgenres misrepresented on streaming services.\nComputer science and music experts identified issues with algorithms on streaming services such as Spotify and Apple Music, specifically the categorical homogenization of music curation and metadata within larger genres such as country music. Musicians and songs from minority heritage styles, such as Appalachian, Cajun, New Mexico, and Tejano music, underperform on these platforms due to underrepresentation and miscategorization of these subgenres.\nRace issue in modern country music.\nThe Country Music Association has awarded the New Artist award to a black American only twice in 63 years, and never to a Hispanic musician. The broader modern Nashville-based Country music industry has underrepresented significant black and Latino contributions within Country music, including popular subgenres such as Cajun, Creole, Tejano, and New Mexico music. A 2021 CNN article states, \"Some in country music have signaled that they are no longer content to be associated with a painful history of racism.\u2009\"\nBlack country-music artist Mickey Guyton has been included among the nominees for the 2021 award, effectively creating a litmus-test for the genre. Guyton has expressed bewilderment that, despite substantial coverage by online platforms like Spotify and Apple Music, her music, like that of Valerie June, another black musician who embraces aspects of country in her Appalachian- and Gospel-tinged work and who has been embraced by international music audiences, is still effectively ignored by American broadcast country-music radio. Guyton's 2021 album \"Remember Her Name\" in part references the case of black health-care professional Breonna Taylor, who was killed in her home by police.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5248", "revid": "22872292", "url": "https://en.wikipedia.org/wiki?curid=5248", "title": "Cold War (1948\u20131953)", "text": "Phase of the Cold War\nThe Cold War (1948\u20131953) is the period within the Cold War from the incapacitation of the Allied Control Council in 1948 to the conclusion of the Korean War in 1953.\nThe list of world leaders in these years is as follows:\nEurope.\nBerlin Blockade.\nAfter the Marshall Plan, the introduction of a new currency to Western Germany to replace the debased Reichsmark and massive electoral losses for communist parties in 1946, in June 1948, the Soviet Union cut off surface road access to Berlin.\nOn the day of the Berlin Blockade, a Soviet representative told the other occupying powers \"We are warning both you and the population of Berlin that we shall apply economic and administrative sanctions that will lead to circulation in Berlin exclusively of the currency of the Soviet occupation zone.\"\nThereafter, street and water communications were severed, rail and barge traffic was stopped and the Soviets initially stopped supplying food to the civilian population in the non-Soviet sectors of Berlin. Because Berlin was located within the Soviet-occupied zone of Germany and the other occupying powers had previously relied on Soviet good will for access to Berlin, the only available methods of supplying the city were three limited air corridors.\nBy February 1948, because of massive post-war military cuts, the entire United States army had been reduced to 552,000 men. Military forces in non-Soviet Berlin sectors totaled only 8,973 Americans, 7,606 British and 6,100 French. Soviet military forces in the Soviet sector that surrounded Berlin totaled one and a half million men. The two United States regiments in Berlin would have provided little resistance against a Soviet attack. Believing that Britain, France and the United States had little option other than to acquiesce, the Soviet Military Administration in Germany celebrated the beginning of the blockade. Thereafter, a massive aerial supply campaign of food, water and other goods was initiated by the United States, Britain, France and other countries. The Soviets derided \"the futile attempts of the Americans to save face and to maintain their untenable position in Berlin.\" The success of the airlift eventually caused the Soviets to lift their blockade in May 1949.\nHowever, the Soviet Army was still capable of conquering Western Europe without much difficulty. In September 1948, US military intelligence experts estimated that the Soviets had about 485,000 troops in their German occupation zone and in Poland, and some 1.785 million troops in Europe in total. At the same time, the number of US troops in 1948 was about 140,000.\nTito\u2013Stalin Split.\nAfter disagreements between Yugoslavian leader Josip Broz Tito and the Soviet Union regarding Greece and the People's Republic of Albania, a Tito\u2013Stalin Split occurred, followed by Yugoslavia being expelled from the Cominform in June 1948 and a brief failed Soviet putsch in Belgrade. The split created two separate communist forces in Europe. A vehement campaign against \"Titoism\" was immediately started in the Eastern Bloc, describing agents of both the West and Tito in all places engaging in subversive activity. This resulted in the persecution of many major party cadres, including those in East Germany.\nwas split up and dissolved in 1954 and 1975, also because of the d\u00e9tente between the West and Tito.\nNATO.\nThe United States joined Britain, France, Canada, Denmark, Portugal, Norway, Belgium, Iceland, Luxembourg, Italy, and the Netherlands in 1949 to form the North Atlantic Treaty Organization (NATO), the United States' first \"entangling\" European alliance in 170 years. West Germany, Spain, Greece, and Turkey would later join this alliance. The Eastern leaders retaliated against these steps by integrating the economies of their nations in Comecon, their version of the Marshall Plan; exploding the first Soviet atomic device in 1949; signing an alliance with People's Republic of China in February 1950; and forming the Warsaw Pact, Eastern Europe's counterpart to NATO, in 1955. The Soviet Union, Albania, Czechoslovakia, Hungary, East Germany, Bulgaria, Romania, and Poland founded this military alliance.\nNSC 68.\nU.S. officials quickly moved to escalate and expand \"containment.\" In a secret 1950 document, NSC 68, they proposed to strengthen their alliance systems, quadruple defense spending, and embark on an elaborate propaganda campaign to convince the U.S. public to fight this costly cold war. Truman ordered the development of a hydrogen bomb. In early 1950, the U.S. took its first efforts to oppose communist forces in Vietnam; planned to form a West German army, and prepared proposals for a peace treaty with Japan that would guarantee long-term U.S. military bases there.\nOutside Europe.\nThe Cold War took place worldwide, but it had a partially different timing and trajectory outside Europe.\nIn Africa, decolonization took place first; it was largely accomplished in the 1950s. The main rivals then sought bases of support in the new national political alignments. In Latin America, the first major confrontation took place in Guatemala in 1954. When the new Castro government of Cuba turned to Soviets support in 1960, Cuba became the center of the anti-American Cold War forces, supported by the Soviet Union.\nChinese Civil War.\nAs Japan's empire collapsed in 1945 the civil war resumed in China between the Kuomintang (KMT) led by Generalissimo Chiang Kai-shek and the Chinese Communist Party led by Mao Zedong. The USSR had signed a Treaty of Friendship with the Kuomintang in 1945 and disavowed support for the Chinese Communists. The outcome was closely fought, with the Communists finally prevailing with superior military tactics. Although the Nationalists had an advantage in numbers of men and weapons, initially controlled a much larger territory and population than their adversaries, and enjoyed considerable international support, they were exhausted by the long war with Japan and the attendant internal responsibilities. In addition, the Chinese Communists were able to fill the political vacuum left in Manchuria after Soviet forces withdrew from the area and thus gained China's prime industrial base. The Chinese Communists were able to fight their way from the north and northeast, and virtually all of mainland China was taken by the end of 1949. On October 1, 1949, Mao Zedong proclaimed the People's Republic of China (PRC). Chiang Kai-shek and 600,000 Nationalist troops and 2 million refugees, predominantly from the government and business community, fled from the mainland to the island of Taiwan. In December 1949, Chiang proclaimed Taipei the temporary capital of the Republic of China (ROC) and continued to assert his government as the sole legitimate authority in China.\nThe continued hostility between the Communists on the mainland and the Nationalists on Taiwan continued throughout the Cold War. Though the United States refused to aide Chiang Kai-shek in his hope to \"recover the mainland,\" it continued supporting the Republic of China with military supplies and expertise to prevent Taiwan from falling into PRC hands. Through the support of the Western bloc (most Western countries continued to recognize the ROC as the sole legitimate government of China), the Republic of China on Taiwan retained China's seat in the United Nations until 1971.\nMadiun Affair.\nMadiun Affair took place on September 18, 1948 in the city of Madiun, East Java. This rebellion was carried out by the Front Demokrasi Rakyat (FDR, People's Democratic Front) which united all socialist and communist groups in Indonesia. This rebellion ended 3 months later after its leaders were arrested and executed by the TNI.\nThis revolt began with the fall of the Amir Syarifuddin Cabinet due to the signing of the Renville Agreement which benefited the Dutch and was eventually replaced by the Hatta Cabinet which did not belong to the left wing. This led Amir Syarifuddin to declare opposition to the Hatta Cabinet government and to declare the formation of the People's Democratic Front.\nBefore it, In the PKI Politburo session on August 13\u201314, 1948, Musso, an Indonesian communist figure, introduced a political concept called \"Jalan Baru\". He also wanted a single Marxism party called the PKI (Communist Party of Indonesia) consisting of illegal communists, the Labour Party of Indonesia, and Partai Sosialis(Socialist Party).\nOn September 18, 1948, the FDR declared the formation of the Republic of Soviet-Indonesia. In addition, the communists also carried out a rebellion in the Pati Residency and the kidnapping of groups who were considered to be against communists. Even this rebellion resulted in the murder of the Governor of East Java at the time, Raden Mas Tumenggung Ario Soerjo.\nThe crackdown operation against this movement began. This operation was led by A.H. Nasution. The Indonesian government also applied Commander General Sudirman to the Military Operations Movement I where General Sudirman ordered Colonel Gatot Soebroto and Colonel Sungkono to mobilize the TNI and police to crush the rebellion.\nOn September 30, 1948, Madiun was captured again by the Republic of Indonesia. Musso was shot dead on his escape in Sumoroto and Amir Syarifuddin was executed after being captured in Central Java. In early December 1948, the Madiun Affair crackdown was declared complete.\nKorean War.\nIn early 1950, the United States made its first commitment to form a peace treaty with Japan that would guarantee long-term U.S. military bases. Some observers (including George Kennan) believed that the Japanese treaty led Stalin to approve a plan to invade U.S.-supported South Korea on June 25, 1950. Korea had been divided at the end of World War II along the 38th parallel into Soviet and U.S. occupation zones, in which a communist government was installed in the North by the Soviets, and an elected government in the South came to power after UN-supervised elections in 1948.\nIn June 1950, Kim Il Sung's North Korean People's Army invaded South Korea. Fearing that communist Korea under a Kim Il Sung dictatorship could threaten Japan and foster other communist movements in Asia, Truman committed U.S. forces and obtained help from the United Nations to counter the North Korean invasion. The Soviets boycotted UN Security Council meetings while protesting the Council's failure to seat the People's Republic of China and, thus, did not veto the Council's approval of UN action to oppose the North Korean invasion. A joint UN force of personnel from South Korea, the United States, Britain, Turkey, Canada, Australia, France, the Philippines, the Netherlands, Belgium, New Zealand and other countries joined to stop the invasion. After a Chinese invasion to assist the North Koreans, fighting stabilized along the 38th parallel, which had separated the Koreas. Truman faced a hostile China, a Sino-Soviet partnership, and a defense budget that had quadrupled in eighteen months.\nThe Korean Armistice Agreement was signed in July 1953 after the death of Stalin, who had been insisting that the North Koreans continue fighting. In North Korea, Kim Il Sung created a highly centralized and brutal dictatorship, according himself unlimited power and generating a formidable cult of personality.\nHydrogen bomb.\nA hydrogen bomb\u2014which produced nuclear fusion instead of nuclear fission\u2014was first tested by the United States in November 1952 and the Soviet Union in August 1953. Such bombs were first deployed in the 1960s.\nCulture and media.\nFear of a nuclear war spurred the production of public safety films by the United States federal government's Civil Defense branch that demonstrated ways on protecting oneself from a Soviet nuclear attack. The 1951 children's film \"Duck and Cover\" is a prime example.\nGeorge Orwell's classic dystopia Nineteen Eighty-Four was published in 1949. The novel explores life in an imagined future world where a totalitarian government has achieved terrifying levels of power and control. With Nineteen Eighty-Four, Orwell taps into the anti-communist fears that would continue to haunt so many in the West for decades to come. In a Cold War setting his descriptions could hardly fail to evoke comparison to Soviet communism and the seeming willingness of Stalin and his successors to control those within the Soviet bloc by whatever means necessary. Orwell's famous allegory of totalitarian rule, Animal Farm, published in 1945, provoked similar anti-communist sentiments.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5249", "revid": "45969799", "url": "https://en.wikipedia.org/wiki?curid=5249", "title": "Crony capitalism", "text": "Capitalism featuring undue alliances between business interests and politicians\nCrony capitalism, sometimes called cronyism, is an economic system in which businesses thrive not as a result of free enterprise, but rather as a return on money amassed through collusion between a business class and the political class. This is often achieved by the manipulation of relationships with state power by business interests rather than unfettered competition in obtaining permits, government grants, tax breaks, or other forms of state intervention over resources where business interests exercise undue influence over the state's deployment of public goods, for example, mining concessions for primary commodities or contracts for public works. Money is then made not merely by making a profit in the market, but through profiteering by rent seeking using this monopoly or oligopoly. Entrepreneurship and innovative practices which seek to reward risk are stifled since the value-added is little by crony businesses, as hardly anything of significant value is created by them, with transactions taking the form of trading. Crony capitalism spills over into the government, the politics, and the media, when this nexus distorts the economy and affects society to an extent it corrupts public-serving economic, political, and social ideals.\nHistorical usage.\nThe first extensive use of the term \"crony capitalism\" came about in the 1980s, to characterize the Philippine economy under the dictatorship of Ferdinand Marcos. Early uses of this term to describe the economic practices of the Marcos regime included that of Ricardo Manapat, who introduced it in his 1979 pamphlet \"Some are Smarter than Others\", which was later published in 1991; former \"Time\" magazine business editor George M. Taber, who used the term in a \"Time\" magazine article in 1980, and activist (and later Finance Minister) Jaime Ongpin, who used the term extensively in his writing and is sometimes credited for having coined it.\nThe term crony capitalism made a significant impact in the public as an explanation of the Asian financial crisis. \nIt is also used to describe governmental decisions favoring cronies of governmental officials. In this context, the term is often used comparatively with corporate welfare, a technical term often used to assess government bailouts and favoritistic monetary policy as opposed to the economic theory described by crony capitalism. The extent of difference between these terms is whether a government action can be said to benefit the individual (crony capitalism) rather than the industry (corporate welfare).\nIn practice.\nCrony capitalism exists along a continuum. In its lightest form, crony capitalism consists of collusion among market players which is officially tolerated or encouraged by the government. While perhaps lightly competing against each other, they will present a unified front (sometimes called a trade association or industry trade group) to the government in requesting subsidies or aid or regulation. For instance, newcomers to a market then need to surmount significant barriers to entry in seeking loans, acquiring shelf space, or receiving official sanction. Some such systems are very formalized, such as sports leagues and the Medallion System of the taxicabs of New York City, but often the process is more subtle, such as expanding training and certification exams to make it more expensive for new entrants to enter a market and thereby limiting potential competition. In technological fields, there may evolve a system whereby new entrants may be accused of infringing on patents that the established competitors never assert against each other. In spite of this, some competitors may succeed when the legal barriers are light. The term crony capitalism is generally used when these practices either come to dominate the economy as a whole, or come to dominate the most valuable industries in an economy. Intentionally ambiguous laws and regulations are common in such systems. Taken strictly, such laws would greatly impede practically all business activity, but in practice they are only erratically enforced. The specter of having such laws suddenly brought down upon a business provides an incentive to stay in the good graces of political officials. Troublesome rivals who have overstepped their bounds can have these laws suddenly enforced against them, leading to fines or even jail time. Even in high-income democracies with well-established legal systems and freedom of the press in place, a larger state is generally associated with increased political corruption.\nThe term crony capitalism was initially applied to states involved in the 1997 Asian financial crisis such as Indonesia, South Korea and Thailand. In these cases, the term was used to point out how family members of the ruling leaders become extremely wealthy with no non-political justification. Southeast Asian nations, such as Hong Kong and Malaysia, still score very poorly in rankings measuring this. The term has also been applied to the system of oligarchs in Russia. Other states to which the term has been applied include India, in particular the system after the 1990s liberalization, whereby land and other resources were given at throwaway prices in the name of public private partnerships, the more recent coal-gate scam and cheap allocation of land and resources to Adani SEZ under the Congress and BJP governments.\nSimilar references to crony capitalism have been made to other countries such as Argentina and Greece. Wu Jinglian, one of China's leading economists and a longtime advocate of its transition to free markets, says that it faces two starkly contrasting futures, namely a market economy under the rule of law or crony capitalism. A dozen years later, prominent political scientist Pei Minxin had concluded that the latter course had become deeply embedded in China. The anti-corruption campaign under Xi Jinping (2012\u2013) has seen more than 100,000 high- and low-ranking Chinese officials indicted and jailed.\nMany prosperous nations have also had varying amounts of cronyism throughout their history, including the United Kingdom especially in the 1600s and 1700s, the United States and Japan.\nCrony capitalism index.\n\"The Economist\" benchmarks countries based on a crony-capitalism index calculated via how much economic activity occurs in industries prone to cronyism. Its 2014 Crony Capitalism Index ranking listed Hong Kong, Russia and Malaysia in the top three spots.\nIn finance.\nCrony capitalism in finance was found in the Second Bank of the United States. It was a private company, but its largest stockholder was the federal government which owned 20%. It was an early bank regulator and grew to be one being the most powerful organizations in the country due largely to being the depository of the government's revenue.\nThe Gramm\u2013Leach\u2013Bliley Act in 1999 completely removed Glass\u2013Steagall\u2019s separation between commercial banks and investment banks. After this repeal, commercial banks, investment banks and insurance companies combined their lobbying efforts. Critics claim this was instrumental in the passage of the Bankruptcy Abuse Prevention and Consumer Protection Act of 2005.\nIn sections of an economy.\nMore direct government involvement in a specific sector can also lead to specific areas of crony capitalism, even if the economy as a whole may be competitive. This is most common in natural resource sectors through the granting of mining or drilling concessions, but it is also possible through a process known as regulatory capture where the government agencies in charge of regulating an industry come to be controlled by that industry. Governments will often establish in good faith government agencies to regulate an industry. However, the members of an industry have a very strong interest in the actions of that regulatory body while the rest of the citizenry are only lightly affected. As a result, it is not uncommon for current industry players to gain control of the watchdog and to use it against competitors. This typically takes the form of making it very expensive for a new entrant to enter the market. An 1824 landmark United States Supreme Court ruling overturned a New York State-granted monopoly (\"a veritable model of state munificence\" facilitated by Robert R. Livingston, one of the Founding Fathers) for the then-revolutionary technology of steamboats. Leveraging the Supreme Court's establishment of Congressional supremacy over commerce, the Interstate Commerce Commission was established in 1887 with the intent of regulating railroad robber barons. President Grover Cleveland appointed Thomas M. Cooley, a railroad ally, as its first chairman and a permit system was used to deny access to new entrants and legalize price fixing.\nThe defense industry in the United States is often described as an example of crony capitalism in an industry. Connections with the Pentagon and lobbyists in Washington are described by critics as more important than actual competition due to the political and secretive nature of defense contracts. In the Airbus-Boeing WTO dispute, Airbus (which receives outright subsidies from European governments) has stated Boeing receives similar subsidies which are hidden as inefficient defense contracts. Other American defense companies were put under scrutiny for no-bid contracts for Iraq War and Hurricane Katrina related contracts purportedly due to having cronies in the Bush administration.\nGerald P. O'Driscoll, former vice president at the Federal Reserve Bank of Dallas, stated that Fannie Mae and Freddie Mac became examples of crony capitalism as government backing let Fannie and Freddie dominate mortgage underwriting, saying. \"The politicians created the mortgage giants, which then returned some of the profits to the pols\u2014sometimes directly, as campaign funds; sometimes as \"contributions\" to favored constituents\".\nIn developing economies.\nIn its worst form, crony capitalism can devolve into simple corruption where any pretense of a free market is dispensed with, bribes to government officials are considered \"de rigueur\" and tax evasion is common. This is seen in many parts of Africa and is sometimes called plutocracy (rule by wealth) or kleptocracy (rule by theft). Kenyan economist David Ndii has repeatedly brought to light how this system has manifested over time, occasioned by the reign of Uhuru Kenyatta as president.\nCorrupt governments may favor one set of business owners who have close ties to the government over others. This may also be done with, religious, or ethnic favoritism. For instance, Alawites in Syria have a disproportionate share of power in the government and business there (President Assad himself is an Alawite). This can be explained by considering personal relationships as a social network. As government and business leaders try to accomplish various things, they naturally turn to other powerful people for support in their endeavors. These people form hubs in the network. In a developing country those hubs may be very few, thus concentrating economic and political power in a small interlocking group.\nNormally, this will be untenable to maintain in business as new entrants will affect the market. However, if business and government are entwined, then the government can maintain the small-hub network.\nRaymond Vernon, specialist in economics and international affairs, wrote that the Industrial Revolution began in Great Britain because they were the first to successfully limit the power of veto groups (typically cronies of those with power in government) to block innovations, writing: \"Unlike most other national environments, the British environment of the early 19th century contained relatively few threats to those who improved and applied existing inventions, whether from business competitors, labor, or the government itself. In other European countries, by contrast, the merchant guilds ... were a pervasive source of veto for many centuries. This power was typically bestowed upon them by government.\" For example, a Russian inventor produced a steam engine in 1766 and disappeared without a trace. Vermon further stated that \"a steam powered horseless carriage produced in France in 1769 was officially suppressed.\" James Watt began experimenting with steam in 1763, got a patent in 1769 and began commercial production in 1775.\nRaghuram Rajan, former governor of the Reserve Bank of India, has said: \"One of the greatest dangers to the growth of developing countries is the middle income trap, where crony capitalism creates oligarchies that slow down growth. If the debate during the elections is any pointer, this is a very real concern of the public in India today\". Tavleen Singh, columnist for \"The Indian Express\", has disagreed. According to Singh, India's corporate success is not a product of crony capitalism, but because India is no longer under the influence of crony socialism.\nPolitical viewpoints.\nWhile the problem is generally accepted across the political spectrum, ideology shades the view of the problem's causes and therefore its solutions. Political views mostly fall into two camps which might be called the socialist and capitalist critique. The socialist position is that crony capitalism is the inevitable result of any strictly capitalist system and thus broadly democratic government must regulate economic, or wealthy, interests to restrict monopoly. The capitalist position is that natural monopolies are rare, therefore governmental regulations generally abet established wealthy interests by restricting competition.\nSocialist critique.\nCritics of crony capitalism including socialists and anti-capitalists often assert that crony capitalism is the inevitable result of any strictly capitalist system. Jane Jacobs described it as a natural consequence of collusion between those managing power and trade while Noam Chomsky has argued that the word crony is superfluous when describing capitalism. Since businesses make money and money leads to political power, business will inevitably use their power to influence governments. Much of the impetus behind campaign finance reform in the United States and in other countries is an attempt to prevent economic power being used to take political power.\nRavi Batra argues that \"all official economic measures adopted since 1981 ... have devastated the middle class\" and that the Occupy Wall Street movement should push for their repeal and thus end the influence of the super wealthy in the political process which he considers a manifestation of crony capitalism.\nSocialist economists, such as Robin Hahnel, have criticized the term as an ideologically motivated attempt to cast what is in their view the fundamental problems of capitalism as avoidable irregularities. Socialist economists dismiss the term as an apologetic for failures of neoliberal policy and more fundamentally their perception of the weaknesses of market allocation.\nCapitalist critique.\nSupporters of capitalism also generally oppose crony capitalism. Further, supporters such as classical liberals, neoliberals and right-libertarians consider it an aberration brought on by governmental favors incompatible with free market. Such proponents of capitalism tend to regard the term as an oxymoron, arguing that crony capitalism is not capitalism at all. In the capitalist view, cronyism is the result of an excess of interference in the market which inevitably will result in a toxic combination of corporations and government officials running sectors of the economy. For instance, the \"Financial Times\" observed that, in Vietnam during the 2010s, the primary beneficiaries of cronyism were Communist party officials, noting also the \"common practice of employing only party members and their family members and associates to government jobs or to jobs in state-owned enterprises.\"\nConservative commentator Ben Shapiro prefers to equate this problem with terms such as corporatocracy or corporatism, considered \"a modern form of mercantilism\", to emphasize that the only way to run a profitable business in such a system is to have help from corrupt government officials. Likewise, Hernando de Soto said that mercantilism \"is also known as 'crony' or 'noninclusive' capitalism\".\nEven if the initial regulation was well-intentioned (to curb actual abuses) and even if the initial lobbying by corporations was well-intentioned (to reduce illogical regulations), the mixture of business and government stifles competition, a collusive result called regulatory capture. Burton W. Folsom Jr. distinguishes those that engage in crony capitalism\u2014designated by him political entrepreneurs\u2014from those who compete in the marketplace without special aid from government, whom he calls market entrepreneurs. The market entrepreneurs such as James J. Hill, Cornelius Vanderbilt and John D. Rockefeller succeeded by producing a quality product at a competitive price. For example, the political entrepreneurs such as Edward Collins in steamships and the leaders of the Union Pacific Railroad in railroads were men who used the power of government to succeed. They tried to gain subsidies or in some way use government to stop competitors.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5252", "revid": "45628959", "url": "https://en.wikipedia.org/wiki?curid=5252", "title": "Lists of universities and colleges", "text": "This is a list of lists of universities and colleges.\n* Ecclesiastical universities\n* Benedictine colleges and universities\n* Jesuit institutions\n* Opus Dei universities\n* Pontifical universities\n* International Council of Universities of Saint Thomas Aquinas\n* International Federation of Catholic Universities\nSee also.\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\nThis article includes . &lt;br&gt; If an [ internal link] incorrectly led you here, you may wish to change the link to point directly to the intended article."}
{"id": "5253", "revid": "31010335", "url": "https://en.wikipedia.org/wiki?curid=5253", "title": "Constitution", "text": "Fundamental principles that govern a state\nA constitution is the aggregate of fundamental principles or established precedents that constitute the legal basis of a polity, organisation or other type of entity and commonly determine how that entity is to be governed.\nWhen these principles are written down into a single document or set of legal documents, those documents may be said to embody a \"written constitution\"; if they are encompassed in a single comprehensive document, it is said to embody a \"codified constitution\". The Constitution of the United Kingdom is a notable example of an \"uncodified constitution\"; it is instead written in numerous fundamental Acts of a legislature, court cases, or treaties.\nConstitutions concern different levels of organizations, from sovereign countries to companies and unincorporated associations. A treaty that establishes an international organization is also its constitution, in that it would define how that organization is constituted. Within states, a constitution defines the principles upon which the state is based, the procedure in which laws are made and by whom. Some constitutions, especially codified constitutions, also act as limiters of state power, by establishing lines which a state's rulers cannot cross, such as fundamental rights.\nThe Constitution of India is the longest written constitution of any country in the world, with 146,385 words in its English-language version, while the Constitution of Monaco is the shortest written constitution with 3,814 words. The Constitution of San Marino might be the world's oldest active written constitution, since some of its core documents have been in operation since 1600, while the Constitution of the United States is the oldest active codified constitution. The historical life expectancy of a constitution since 1789 is approximately 19 years.\nEtymology.\nThe term \"constitution\" comes through French from the Latin word \"constitutio\", used for regulations and orders, such as the imperial enactments (\"constitutiones principis\": edicta, mandata, decreta, rescripta). Later, the term was widely used in canon law for an important determination, especially a decree issued by the Pope, now referred to as an \"apostolic constitution\".\nWilliam Blackstone used the term for significant and egregious violations of public trust, of a nature and extent that the transgression would justify a revolutionary response. The term as used by Blackstone was not for a legal text, nor did he intend to include the later American concept of judicial review: \"for that were to set the judicial power above that of the legislature, which would be subversive of all government\".\nGeneral features.\nGenerally, every modern written constitution confers specific powers on an organization or institutional entity, established upon the primary condition that it abides by the constitution's limitations. According to Scott Gordon, a political organization is constitutional to the extent that it \"contain[s] institutionalized mechanisms of power control for the protection of the interests and liberties of the citizenry, including those that may be in the minority\".\nActivities of officials within an organization or polity that fall within the constitutional or statutory authority of those officials are termed \"within power\" (or, in Latin, \"intra vires\"); if they do not, they are termed \"beyond power\" (or, in Latin, \"ultra vires\"). For example, a students' union may be prohibited as an organization from engaging in activities not concerning students; if the union becomes involved in non-student activities, these activities are considered to be \"ultra vires\" of the union's charter, and nobody would be compelled by the charter to follow them. An example from the constitutional law of sovereign states would be a provincial parliament in a federal state trying to legislate in an area that the constitution allocates exclusively to the federal parliament, such as ratifying a treaty. Action that appears to be beyond power may be judicially reviewed and, if found to be beyond power, must cease. Legislation that is found to be beyond power will be \"invalid\" and of no force; this applies to primary legislation, requiring constitutional authorization, and secondary legislation, ordinarily requiring statutory authorization. In this context, \"within power\", \"intra vires\", \"authorized\" and \"valid\" have the same meaning; as do \"beyond power\", \"ultra vires\", \"not authorized\" and \"invalid\".\nIn most but not all modern states the constitution has supremacy over ordinary statutory law (see Uncodified constitution below); in such states when an official act is unconstitutional, i.e. it is not a power granted to the government by the constitution, that act is \"null and void\", and the nullification is \"ab initio\", that is, from inception, not from the date of the finding. It was never \"law\", even though, if it had been a statute or statutory provision, it might have been adopted according to the procedures for adopting legislation. Sometimes the problem is not that a statute is unconstitutional, but that the application of it is, on a particular occasion, and a court may decide that while there are ways it could be applied that are constitutional, that instance was not allowed or legitimate. In such a case, only that application may be ruled unconstitutional. Historically, the remedies for such violations have been petitions for common law writs, such as \"quo warranto\".\nScholars debate whether a constitution must necessarily be autochthonous, resulting from the nations \"spirit\". Hegel said \"A constitution...is the work of centuries; it is the idea, the consciousness of rationality so far as that consciousness is developed in a particular nation.\"\nHistory and development.\nSince 1789, along with the Constitution of the United States of America (U.S. Constitution), which is the oldest and shortest written constitution still in force, close to 800 constitutions have been adopted and subsequently amended around the world by independent states.\nIn the late 18th century, Thomas Jefferson predicted that a period of 20 years would be the optimal time for any constitution to be still in force, since \"the earth belongs to the living, and not to the dead\". Indeed, according to recent studies, the average life of any new written constitution is around 19 years. However, a great number of constitutions do not last more than 10 years, and around 10% do not last more than one year, as was the case of the French Constitution of 1791. By contrast, some constitutions, notably that of the United States, have remained in force for several centuries, often without major revision for long periods of time.\nThe most common reasons for these frequent changes are the political desire for an immediate outcome and the short time devoted to the constitutional drafting process. A study in 2009 showed that the average time taken to draft a constitution is around 16 months, however there were also some extreme cases registered. For example, the Myanmar 2008 Constitution was being secretly drafted for more than 17 years, whereas at the other extreme, during the drafting of Japan's 1946 Constitution, the bureaucrats drafted everything in no more than a week. Japan has the oldest unamended constitution in the world. The record for the shortest overall process of drafting, adoption, and ratification of a national constitution belongs to the Romania's 1938 constitution, which installed a royal dictatorship in less than a month. Studies showed that typically extreme cases where the constitution-making process either takes too long or is extremely short were non-democracies. Constitutional rights are not a specific characteristic of democratic countries. Non-democratic countries have constitutions, such as that of North Korea, which officially grants every citizen, among other rights, the freedom of expression.\nPre-modern constitutions.\nAncient.\nExcavations in modern-day Iraq by Ernest de Sarzec in 1877 found evidence of the earliest known code of justice, issued by the Sumerian king Urukagina of Lagash c. 2300 BC. Perhaps the earliest prototype for a law of government, this document itself has not yet been discovered; however it is known that it allowed some rights to his citizens. For example, it is known that it relieved tax for widows and orphans, and protected the poor from the usury of the rich.\nAfter that, many governments ruled by special codes of written laws. The oldest such document still known to exist seems to be the Code of Ur-Nammu of Ur (c. 2050 BC). Some of the better-known ancient law codes are the code of Lipit-Ishtar of Isin, the code of Hammurabi of Babylonia, the Hittite code, the Assyrian code, and Mosaic law.\nIn 621 BC, a scribe named Draco codified the oral laws of the city-state of Athens; this code prescribed the death penalty for many offenses (thus creating the modern term \"draconian\" for very strict rules). In 594 BC, Solon, the ruler of Athens, created the new \"Solonian Constitution\". It eased the burden of the workers, and determined that membership of the ruling class was to be based on wealth (plutocracy), rather than on birth (aristocracy). Cleisthenes again reformed the Athenian constitution and set it on a democratic footing in 508 BC.\nAristotle (c. 350 BC) was the first to make a formal distinction between ordinary law and constitutional law, establishing ideas of constitution and constitutionalism, and attempting to classify different forms of constitutional government. The most basic definition he used to describe a constitution in general terms was \"the arrangement of the offices in a state\". In his works \"Constitution of Athens\", \"Politics\", and \"Nicomachean Ethics\", he explores different constitutions of his day, including those of Athens, Sparta, and Carthage. He classified both what he regarded as good and what he regarded as bad constitutions, and came to the conclusion that the best constitution was a mixed system including monarchic, aristocratic, and democratic elements. He also distinguished between citizens, who had the right to participate in the state, and non-citizens and slaves, who did not.\nThe Romans initially codified their constitution in 450 BC as the \"Twelve Tables\". They operated under a series of laws that were added from time to time, but Roman law was not reorganised into a single code until the \"Codex Theodosianus\" (438 AD); later, in the Eastern Empire, the \"Codex repetit\u00e6 pr\u00e6lectionis\" (534) was highly influential throughout Europe. This was followed in the east by the \"Ecloga\" of Leo III the Isaurian (740) and the \"Basilica\" of Basil I (878).\nThe \"Edicts of Ashoka\" established constitutional principles for the 3rd century BC Maurya king's rule in India. For constitutional principles almost lost to antiquity, see the code of Manu.\nEarly Middle Ages.\nMany of the Germanic peoples that filled the power vacuum left by the Western Roman Empire in the Early Middle Ages codified their laws. One of the first of these Germanic law codes to be written was the Visigothic \"Code of Euric\" (471 AD). This was followed by the \"Lex Burgundionum\", applying separate codes for Germans and for Romans; the \"Pactus Alamannorum\"; and the Salic Law of the Franks, all written soon after 500. In 506, the \"Breviarum\" or \"Lex Romana\" of Alaric II, king of the Visigoths, adopted and consolidated the \"Codex Theodosianus\" together with assorted earlier Roman laws. Systems that appeared somewhat later include the \"Edictum Rothari\" of the Lombards (643), the \"Lex Visigothorum\" (654), the \"Lex Alamannorum\" (730), and the \"Lex Frisionum\" (c. 785). These continental codes were all composed in Latin, while Anglo-Saxon was used for those of England, beginning with the Code of \u00c6thelberht of Kent (602). Around 893, Alfred the Great combined this and two other earlier Saxon codes, with various Mosaic and Christian precepts, to produce the \"Doom book\" code of laws for England.\nJapan's \"Seventeen-article constitution\" written in 604, reportedly by Prince Sh\u014dtoku, is an early example of a constitution in Asian political history. Influenced by Buddhist teachings, the document focuses more on social morality than on institutions of government, and remains a notable early attempt at a government constitution.\nThe Constitution of Medina (, \u1e62a\u1e25\u012bfat al-Mad\u012bna), also known as the Charter of Medina, was drafted by the Islamic prophet Muhammad after his flight (hijra) to Yathrib where he became political leader. It constituted a formal agreement between Muhammad and all of the significant tribes and families of Yathrib (later known as Medina), including Muslims, Jews, and pagans. The document was drawn up with the explicit concern of bringing to an end the bitter intertribal fighting between the clans of the Aws (Aus) and Khazraj within Medina. To this effect it instituted a number of rights and responsibilities for the Muslim, Jewish, and pagan communities of Medina bringing them within the fold of one community\u00a0\u2013 the Ummah. The precise dating of the Constitution of Medina remains debated, but generally scholars agree it was written shortly after the Hijra (622).\nIn Wales, the \"Cyfraith Hywel\" (Law of Hywel) was codified by Hywel Dda c. 942\u2013950.\nMiddle Ages after 1000.\nThe \"Pravda Yaroslava\", originally combined by Yaroslav the Wise the Grand Prince of Kiev, was granted to Great Novgorod around 1017, and in 1054 was incorporated into the \"Russkaya Pravda\"; it became the law for all of Kievan Rus'. It survived only in later editions of the 15th century.\nIn England, Henry I's proclamation of the Charter of Liberties in 1100 bound the king for the first time in his treatment of the clergy and the nobility. This idea was extended and refined by the English barony when they forced King John to sign \"Magna Carta\" in 1215. The most important single article of the \"Magna Carta\", related to \"habeas corpus\", provided that the king was not permitted to imprison, outlaw, exile or kill anyone at a whim\u00a0\u2013 there must be due process of law first. This article, Article 39, of the \"Magna Carta\" read:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;No free man shall be arrested, or imprisoned, or deprived of his property, or outlawed, or exiled, or in any way destroyed, nor shall we go against him or send against him, unless by legal judgement of his peers, or by the law of the land.\nThis provision became the cornerstone of English liberty after that point. The social contract in the original case was between the king and the nobility, but was gradually extended to all of the people. It led to the system of Constitutional Monarchy, with further reforms shifting the balance of power from the monarchy and nobility to the House of Commons.\nThe Nomocanon of Saint Sava () was the first Serbian constitution from 1219. St. Sava's Nomocanon was the compilation of civil law, based on Roman Law, and canon law, based on Ecumenical Councils. Its basic purpose was to organize the functioning of the young Serbian kingdom and the Serbian church. Saint Sava began the work on the Serbian Nomocanon in 1208 while he was at Mount Athos, using \"The Nomocanon in Fourteen Titles\", \"Synopsis of Stefan the Efesian\", \"Nomocanon of John Scholasticus\", and Ecumenical Council documents, which he modified with the canonical commentaries of Aristinos and Joannes Zonaras, local church meetings, rules of the Holy Fathers, the law of Moses, the translation of Prohiron, and the Byzantine emperors' Novellae (most were taken from Justinian's Novellae). The Nomocanon was a completely new compilation of civil and canonical regulations, taken from Byzantine sources but completed and reformed by St. Sava to function properly in Serbia. Besides decrees that organized the life of church, there are various norms regarding civil life; most of these were taken from Prohiron. Legal transplants of Roman-Byzantine law became the basis of the Serbian medieval law. The essence of Zakonopravilo was based on Corpus Iuris Civilis.\nStefan Du\u0161an, emperor of Serbs and Greeks, enacted Du\u0161an's Code () in Serbia, in two state congresses: in 1349 in Skopje and in 1354 in Serres. It regulated all social spheres, so it was the second Serbian constitution, after St. Sava's Nomocanon (Zakonopravilo). The Code was based on Roman-Byzantine law. The legal transplanting within articles 171 and 172 of Du\u0161an's Code, which regulated the juridical independence, is notable. They were taken from the Byzantine code Basilika (book VII, 1, 16\u201317).\nIn 1222, Hungarian King Andrew II issued the Golden Bull of 1222.\nBetween 1220 and 1230, a Saxon administrator, Eike von Repgow, composed the \"Sachsenspiegel\", which became the supreme law used in parts of Germany as late as 1900.\nAround 1240, the Coptic Egyptian Christian writer, 'Abul Fada'il Ibn al-'Assal, wrote the \"Fetha Negest\" in Arabic. 'Ibn al-Assal took his laws partly from apostolic writings and Mosaic law and partly from the former Byzantine codes. There are a few historical records claiming that this law code was translated into Ge'ez and entered Ethiopia around 1450 in the reign of Zara Yaqob. Even so, its first recorded use in the function of a constitution (supreme law of the land) is with Sarsa Dengel beginning in 1563. The \"Fetha Negest\" remained the supreme law in Ethiopia until 1931, when a modern-style Constitution was first granted by Emperor Haile Selassie I.\nIn the Principality of Catalonia, the Catalan constitutions were promulgated by the Court from 1283 (or even two centuries before, if Usatges of Barcelona is considered part of the compilation of Constitutions) until 1716, when Philip V of Spain gave the Nueva Planta decrees, finishing with the historical laws of Catalonia. These Constitutions were usually made formally as a royal initiative, but required for its approval or repeal the favorable vote of the Catalan Courts, the medieval antecedent of the modern Parliaments. These laws, like other modern constitutions, had preeminence over other laws, and they could not be contradicted by mere decrees or edicts of the king.\nThe Kouroukan Founga was a 13th-century charter of the Mali Empire, reconstructed from oral tradition in 1988 by Siriman Kouyat\u00e9.\nThe Golden Bull of 1356 was a decree issued by a \"Reichstag\" in Nuremberg headed by Emperor Charles IV that fixed, for a period of more than four hundred years, an important aspect of the constitutional structure of the Holy Roman Empire.\nIn China, the Hongwu Emperor created and refined a document he called \"Ancestral Injunctions\" (first published in 1375, revised twice more before his death in 1398). These rules served as a constitution for the Ming Dynasty for the next 250 years.\nThe oldest written document still governing a sovereign nation today is that of San Marino. The \"Leges Statutae Republicae Sancti Marini\" was written in Latin and consists of six books. The first book, with 62 articles, establishes councils, courts, various executive officers, and the powers assigned to them. The remaining books cover criminal and civil law and judicial procedures and remedies. Written in 1600, the document was based upon the \"Statuti Comunali\" (Town Statute) of 1300, itself influenced by the \"Codex Justinianus\", and it remains in force today.\nIn 1392 the \"Carta de Logu\" was legal code of the Giudicato of Arborea promulgated by the \"giudicessa\" Eleanor. It was in force in Sardinia until it was superseded by the code of Charles Felix in April 1827. The Carta was a work of great importance in Sardinian history. It was an organic, coherent, and systematic work of legislation encompassing the civil and penal law.\nThe \"Gayanashagowa\", the oral constitution of the Haudenosaunee nation also known as the Great Law of Peace, established a system of governance as far back as 1190 AD (though perhaps more recently at 1451) in which the Sachems, or tribal chiefs, of the Iroquois League's member nations made decisions on the basis of universal consensus of all chiefs following discussions that were initiated by a single nation. The position of Sachem descends through families and are allocated by the senior female clan heads, though, prior to the filling of the position, candidacy is ultimately democratically decided by the community itself.\nModern constitutions.\nIn 1634 the Kingdom of Sweden adopted the 1634 Instrument of Government, drawn up under the Lord High Chancellor of Sweden Axel Oxenstierna after the death of king Gustavus Adolphus, it can be seen as the first written constitution adopted by a modern state.\nIn 1639, the Colony of Connecticut adopted the Fundamental Orders, which was the first North American constitution, and is the basis for every new Connecticut constitution since, and is also the reason for Connecticut's nickname, \"the Constitution State\".\nThe English Protectorate that was set up by Oliver Cromwell after the English Civil War promulgated the first detailed written constitution adopted by a modern state; it was called the Instrument of Government. This formed the basis of government for the short-lived republic from 1653 to 1657 by providing a legal rationale for the increasing power of Cromwell after Parliament consistently failed to govern effectively. Most of the concepts and ideas embedded into modern constitutional theory, especially bicameralism, separation of powers, the written constitution, and judicial review, can be traced back to the experiments of that period.\nDrafted by Major-General John Lambert in 1653, the \"Instrument of Government\" included elements incorporated from an earlier document \"Heads of Proposals\", which had been agreed to by the Army Council in 1647, as a set of propositions intended to be a basis for a constitutional settlement after King Charles I was defeated in the First English Civil War. Charles had rejected the propositions, but before the start of the Second Civil War, the Grandees of the New Model Army had presented the \"Heads of Proposals\" as their alternative to the more radical Agreement of the People presented by the Agitators and their civilian supporters at the Putney Debates.\nOn January 4, 1649, the Rump Parliament declared \"that the people are, under God, the original of all just power; that the Commons of England, being chosen by and representing the people, have the supreme power in this nation\".\nThe \"Instrument of Government\" was adopted by Parliament on December 15, 1653, and Oliver Cromwell was installed as Lord Protector on the following day. The constitution set up a state council consisting of 21 members while executive authority was vested in the office of \"Lord Protector of the Commonwealth.\" This position was designated as a non-hereditary life appointment. The \"Instrument\" also required the calling of triennial Parliaments, with each sitting for at least five months.\nThe \"Instrument of Government\" was replaced in May 1657 by England's second, and last, codified constitution, the Humble Petition and Advice, proposed by Sir Christopher Packe. The Petition offered hereditary monarchy to Oliver Cromwell, asserted Parliament's control over issuing new taxation, provided an independent council to advise the king and safeguarded \"Triennial\" meetings of Parliament. A modified version of the Humble Petition with the clause on kingship removed was ratified on 25 May. This finally met its demise in conjunction with the death of Cromwell and the Restoration of the monarchy.\nOther examples of European constitutions of this era were the Corsican Constitution of 1755 and the Swedish Constitution of 1772.\nAll of the British colonies in North America that were to become the 13 original United States, adopted their own constitutions in 1776 and 1777, during the American Revolution (and before the later Articles of Confederation and United States Constitution), with the exceptions of Massachusetts, Connecticut and Rhode Island. The Commonwealth of Massachusetts adopted its Constitution in 1780, the oldest still-functioning constitution of any U.S. state; while Connecticut and Rhode Island officially continued to operate under their old colonial charters, until they adopted their first state constitutions in 1818 and 1843, respectively.\nDemocratic constitutions.\nWhat is sometimes called the \"enlightened constitution\" model was developed by philosophers of the Age of Enlightenment such as Thomas Hobbes, Jean-Jacques Rousseau, and John Locke. The model proposed that constitutional governments should be stable, adaptable, accountable, open and should represent the people (i.e., support democracy).\n\"Agreements and Constitutions of Laws and Freedoms of the Zaporizian Host\" was written in 1710 by Pylyp Orlyk, \"hetman\" of the Zaporozhian Host. It was written to establish a free Zaporozhian-Ukrainian Republic, with the support of Charles XII of Sweden. It is notable in that it established a democratic standard for the separation of powers in government between the legislative, executive, and judiciary branches, well before the publication of Montesquieu's \"Spirit of the Laws\". This Constitution also limited the executive authority of the \"hetman\", and established a democratically elected Cossack parliament called the General Council. However, Orlyk's project for an independent Ukrainian State never materialized, and his constitution, written in exile, never went into effect.\nCorsican Constitutions of 1755 and 1794 were inspired by Jean-Jacques Rousseau. The latter introduced universal suffrage for property owners.\nThe Swedish constitution of 1772 was enacted under King Gustavus III and was inspired by the separation of powers by Montesquieu. The king also cherished other enlightenment ideas (as an enlighted despot) and repealed torture, liberated agricultural trade, diminished the use of the death penalty and instituted a form of religious freedom. The constitution was commended by Voltaire.\nThe United States Constitution, ratified June 21, 1788, was influenced by the writings of Polybius, Locke, Montesquieu, and others. The document became a benchmark for republicanism and codified constitutions written thereafter.\nThe Polish\u2013Lithuanian Commonwealth Constitution was passed on May 3, 1791. Its draft was developed by the leading minds of the Enlightenment in Poland such as King Stanislaw August Poniatowski, Stanis\u0142aw Staszic, Scipione Piattoli, Julian Ursyn Niemcewicz, Ignacy Potocki and Hugo Ko\u0142\u0142\u0105taj. It was adopted by the Great Sejm and is considered the first constitution of its kind in Europe and the world's second oldest one after the American Constitution.\nAnother landmark document was the French Constitution of 1791.\nThe 1811 Constitution of Venezuela was the first Constitution of Venezuela and Latin America, promulgated and drafted by Crist\u00f3bal Mendoza and Juan Germ\u00e1n Roscio and in Caracas. It established a federal government but was repealed one year later.\nOn March 19, the Spanish Constitution of 1812 was ratified by a parliament gathered in Cadiz, the only Spanish continental city which was safe from French occupation. The Spanish Constitution served as a model for other liberal constitutions of several South European and Latin American nations, for example, the Portuguese Constitution of 1822, constitutions of various Italian states during Carbonari revolts (i.e., in the Kingdom of the Two Sicilies), the Norwegian constitution of 1814, or the Mexican Constitution of 1824.\nIn Brazil, the Constitution of 1824 expressed the option for the monarchy as political system after Brazilian Independence. The leader of the national emancipation process was the Portuguese prince Pedro I, elder son of the king of Portugal. Pedro was crowned in 1822 as first emperor of Brazil. The country was ruled by Constitutional monarchy until 1889, when it adopted the Republican model.\nIn Denmark, as a result of the Napoleonic Wars, the absolute monarchy lost its personal possession of Norway to Sweden. Sweden had already enacted its 1809 Instrument of Government, which saw the division of power between the Riksdag, the king and the judiciary. However the Norwegians managed to infuse a radically democratic and liberal constitution in 1814, adopting many facets from the American constitution and the revolutionary French ones, but maintaining a hereditary monarch limited by the constitution, like the Spanish one.\nThe first Swiss Federal Constitution was put in force in September 1848 (with official revisions in 1878, 1891, 1949, 1971, 1982 and 1999).\nThe Serbian revolution initially led to a proclamation of a proto-constitution in 1811; the full-fledged Constitution of Serbia followed few decades later, in 1835. The first Serbian constitution (Sretenjski ustav) was adopted at the national assembly in Kragujevac on February 15, 1835.\nThe Constitution of Canada came into force on July 1, 1867, as the British North America Act, an act of the British Parliament. Over a century later, the BNA Act was patriated to the Canadian Parliament and augmented with the Canadian Charter of Rights and Freedoms. Apart from the \"Constitution Acts, 1867 to 1982\", Canada's constitution also has unwritten elements based in common law and convention.\nPrinciples of constitutional design.\nAfter tribal people first began to live in cities and establish nations, many of these functioned according to unwritten customs, while some developed autocratic, even tyrannical monarchs, who ruled by decree, or mere personal whim. Such rule led some thinkers to take the position that what mattered was not the design of governmental institutions and operations, as much as the character of the rulers. This view can be seen in Plato, who called for rule by \"philosopher-kings\". Later writers, such as Aristotle, Cicero and Plutarch, would examine designs for government from a legal and historical standpoint.\nThe Renaissance brought a series of political philosophers who wrote implied criticisms of the practices of monarchs and sought to identify principles of constitutional design that would be likely to yield more effective and just governance from their viewpoints. This began with revival of the Roman law of nations concept and its application to the relations among nations, and they sought to establish customary \"laws of war and peace\" to ameliorate wars and make them less likely. This led to considerations of what authority monarchs or other officials have and don't have, from where that authority derives, and the remedies for the abuse of such authority.\nA seminal juncture in this line of discourse arose in England from the Civil War, the Cromwellian Protectorate, the writings of Thomas Hobbes, Samuel Rutherford, the Levellers, John Milton, and James Harrington, leading to the debate between Robert Filmer, arguing for the divine right of monarchs, on the one side, and on the other, Henry Neville, James Tyrrell, Algernon Sidney, and John Locke. What arose from the latter was a concept of government being erected on the foundations of first, a state of nature governed by natural laws, then a state of society, established by a social contract or compact, which bring underlying natural or social laws, before governments are formally established on them as foundations.\nAlong the way several writers examined how the design of government was important, even if the government were headed by a monarch. They also classified various historical examples of governmental designs, typically into democracies, aristocracies, or monarchies, and considered how just and effective each tended to be and why, and how the advantages of each might be obtained by combining elements of each into a more complex design that balanced competing tendencies. Some, such as Montesquieu, also examined how the functions of government, such as legislative, executive, and judicial, might appropriately be separated into branches. The prevailing theme among these writers was that the design of constitutions is not completely arbitrary or a matter of taste. They generally held that there are underlying principles of design that constrain all constitutions for every polity or organization. Each built on the ideas of those before concerning what those principles might be.\nThe later writings of Orestes Brownson would try to explain what constitutional designers were trying to do. According to Brownson there are, in a sense, three \"constitutions\" involved: The first the \"constitution of nature\" that includes all of what was called \"natural law\". The second is the \"constitution of society\", an unwritten and commonly understood set of rules for the society formed by a social contract before it establishes a government, by which it establishes the third, a \"constitution of government\". The second would include such elements as the making of decisions by public conventions called by public notice and conducted by established rules of procedure. Each constitution must be consistent with, and derive its authority from, the ones before it, as well as from a historical act of society formation or constitutional ratification. Brownson argued that a state is a society with effective dominion over a well-defined territory, that consent to a well-designed constitution of government arises from presence on that territory, and that it is possible for provisions of a written constitution of government to be \"unconstitutional\" if they are inconsistent with the constitutions of nature or society. Brownson argued that it is not ratification alone that makes a written constitution of government legitimate, but that it must also be competently designed and applied.\nOther writers have argued that such considerations apply not only to all national constitutions of government, but also to the constitutions of private organizations, that it is not an accident that the constitutions that tend to satisfy their members contain certain elements, as a minimum, or that their provisions tend to become very similar as they are amended after experience with their use. Provisions that give rise to certain kinds of questions are seen to need additional provisions for how to resolve those questions, and provisions that offer no course of action may best be omitted and left to policy decisions. Provisions that conflict with what Brownson and others can discern are the underlying \"constitutions\" of nature and society tend to be difficult or impossible to execute, or to lead to unresolvable disputes.\nConstitutional design has been treated as a kind of metagame in which play consists of finding the best design and provisions for a written constitution that will be the rules for the game of government, and that will be most likely to optimize a balance of the utilities of justice, liberty, and security. An example is the metagame Nomic.\nPolitical economy theory regards constitutions as coordination devices that help citizens to prevent rulers from abusing power. If the citizenry can coordinate a response to police government officials in the face of a constitutional fault, then the government have the incentives to honor the rights that the constitution guarantees. An alternative view considers that constitutions are not enforced by the citizens at-large, but rather by the administrative powers of the state. Because rulers cannot themselves implement their policies, they need to rely on a set of organizations (armies, courts, police agencies, tax collectors) to implement it. In this position, they can directly sanction the government by refusing to cooperate, disabling the authority of the rulers. Therefore, constitutions could be characterized by a self-enforcing equilibria between the rulers and powerful administrators.\nKey features.\nMost commonly, the term \"constitution\" refers to a set of rules and principles that define the nature and extent of government. Most constitutions seek to regulate the relationship between institutions of the state, in a basic sense the relationship between the executive, legislature and the judiciary, but also the relationship of institutions within those branches. For example, executive branches can be divided into a head of government, government departments/ministries, executive agencies and a civil service/administration. Most constitutions also attempt to define the relationship between individuals and the state, and to establish the broad rights of individual citizens. It is thus the most basic law of a territory from which all the other laws and rules are hierarchically derived; in some territories it is in fact called \"Basic Law\".\nClassification.\nCodification.\nA fundamental classification is codification or lack of codification. A codified constitution is one that is contained in a single document, which is the single source of constitutional law in a state. An uncodified constitution is one that is not contained in a single document, consisting of several different sources, which may be written or unwritten; see constitutional convention.\nCodified constitution.\nMost states in the world have codified constitutions.\nCodified constitutions are often the product of some dramatic political change, such as a revolution. The process by which a country adopts a constitution is closely tied to the historical and political context driving this fundamental change. The legitimacy (and often the longevity) of codified constitutions has often been tied to the process by which they are initially adopted and some scholars have pointed out that high constitutional turnover within a given country may itself be detrimental to separation of powers and the rule of law.\nStates that have codified constitutions normally give the constitution supremacy over ordinary statute law. That is, if there is any conflict between a legal statute and the codified constitution, all or part of the statute can be declared \"ultra vires\" by a court, and struck down as unconstitutional. In addition, exceptional procedures are often required to amend a constitution. These procedures may include: convocation of a special constituent assembly or constitutional convention, requiring a supermajority of legislators' votes, approval in two terms of parliament, the consent of regional legislatures, a referendum process, and/or other procedures that make amending a constitution more difficult than passing a simple law.\nConstitutions may also provide that their most basic principles can never be abolished, even by amendment. In case a formally valid amendment of a constitution infringes these principles protected against any amendment, it may constitute a so-called \"unconstitutional constitutional law\".\nCodified constitutions normally consist of a ceremonial preamble, which sets forth the goals of the state and the motivation for the constitution, and several articles containing the substantive provisions. The preamble, which is omitted in some constitutions, may contain a reference to God and/or to fundamental values of the state such as liberty, democracy or human rights. In ethnic nation-states such as Estonia, the mission of the state can be defined as preserving a specific nation, language and culture.\nUncodified constitution.\nAs of 2017[ [update]] only two sovereign states, New Zealand and the United Kingdom, have wholly uncodified constitutions. The Basic Laws of Israel have since 1950 been intended to be the basis for a constitution, but as of 2017 it had not been drafted. The various Laws are considered to have precedence over other laws, and give the procedure by which they can be amended, typically by a simple majority of members of the Knesset (parliament).\nUncodified constitutions are the product of an \"evolution\" of laws and conventions over centuries (such as in the Westminster System that developed in Britain). By contrast to codified constitutions, uncodified constitutions include both written sources \u2013 e.g. constitutional statutes enacted by the Parliament \u2013 and unwritten sources \u2013 constitutional conventions, observation of precedents, royal prerogatives, customs and traditions, such as holding general elections on Thursdays; together these constitute British constitutional law.\nMixed constitutions.\nSome constitutions are largely, but not wholly, codified. For example, in the Constitution of Australia, most of its fundamental political principles and regulations concerning the relationship between branches of government, and concerning the government and the individual are codified in a single document, the Constitution of the Commonwealth of Australia. However, the presence of statutes with constitutional significance, namely the Statute of Westminster, as adopted by the Commonwealth in the Statute of Westminster Adoption Act 1942, and the Australia Act 1986 means that Australia's constitution is not contained in a single constitutional document. It means the Constitution of Australia is uncodified, it also contains constitutional conventions, thus is partially unwritten.\nThe Constitution of Canada resulted from the passage of several British North America Acts from 1867 to the Canada Act 1982, the act that formally severed British Parliament's ability to amend the Canadian constitution. The Canadian constitution includes specific legislative acts as mentioned in section 52(2) of the Constitution Act, 1982. However, some documents not explicitly listed in section 52(2) are also considered constitutional documents in Canada, entrenched via reference; such as the Proclamation of 1763. Although Canada's constitution includes a number of different statutes, amendments, and references, some constitutional rules that exist in Canada is derived from unwritten sources and constitutional conventions.\nThe terms \"written constitution\" and \"codified constitution\" are often used interchangeably, as are \"unwritten constitution\" and \"uncodified constitution\", although this usage is technically inaccurate. A codified constitution is a single document; states that do not have such a document have uncodified, but not entirely unwritten, constitutions, since much of an uncodified constitution is usually written in laws such as the Basic Laws of Israel and the Parliament Acts of the United Kingdom. Uncodified constitutions largely lack protection against amendment by the government of the time. For example, the U.K. Fixed-term Parliaments Act 2011 legislated by simple majority for strictly fixed-term parliaments; until then the ruling party could call a general election at any convenient time up to the maximum term of five years. This change would require a constitutional amendment in most nations.\nAmendments.\nA constitutional amendment is a modification of the constitution of a polity, organization or other type of entity. Amendments are often interwoven into the relevant sections of an existing constitution, directly altering the text. Conversely, they can be appended to the constitution as supplemental additions (codicils), thus changing the frame of government without altering the existing text of the document.\nMost constitutions require that amendments cannot be enacted unless they have passed a special procedure that is more stringent than that required of ordinary legislation.\nMethods of amending.\n\"Some countries are listed under more than one method because alternative procedures may be used.\"\nEntrenched clauses.\nAn entrenched clause or entrenchment clause of a basic law or constitution is a provision that makes certain amendments either more difficult or impossible to pass, making such amendments inadmissible. Overriding an entrenched clause may require a supermajority, a referendum, or the consent of the minority party. For example, the U.S. Constitution has an entrenched clause that prohibits abolishing equal suffrage of the States within the Senate without their consent. The term eternity clause is used in a similar manner in the constitutions of the Czech Republic, Germany, Turkey, Greece, Italy, Morocco, the Islamic Republic of Iran, Brazil and Norway. India's constitution does not contain specific provisions on entrenched clauses but the basic structure doctrine makes it impossible for certain basic features of the Constitution to be altered or destroyed by the Parliament of India through an amendment. The Constitution of Colombia also lacks explicit entrenched clauses, but has a similar substantive limit on amending its fundamental principles through judicial interpretations.\nConstitutional rights and duties.\nConstitutions include various rights and duties. These include the following:\nSeparation of powers.\nConstitutions usually explicitly divide power between various branches of government. The standard model, described by the Baron de Montesquieu, involves three branches of government: executive, legislative and judicial. Some constitutions include additional branches, such as an auditory branch. Constitutions vary extensively as to the degree of separation of powers between these branches.\nAccountability.\nIn presidential and semi-presidential systems of government, department secretaries/ministers are accountable to the president, who has patronage powers to appoint and dismiss ministers. The president is accountable to the people in an election.\nIn parliamentary systems, Cabinet Ministers are accountable to Parliament, but it is the prime minister who appoints and dismisses them. In the case of the United Kingdom and other countries with a monarchy, it is the monarch who appoints and dismisses ministers, on the advice of the prime minister. In turn the prime minister will resign if the government loses the confidence of the parliament (or a part of it). Confidence can be lost if the government loses a vote of no confidence or, depending on the country, loses a particularly important vote in parliament, such as vote on the budget. When a government loses confidence, it stays in office until a new government is formed; something which normally but not necessarily required the holding of a general election.\nOther independent institutions.\nOther independent institutions which some constitutions have set out include a central bank, an anti-corruption commission, an electoral commission, a judicial oversight body, a human rights commission, a media commission, an ombudsman, and a truth and reconciliation commission.\nPower structure.\nConstitutions also establish where sovereignty is located in the state. There are three basic types of distribution of sovereignty according to the degree of centralisation of power: unitary, federal, and confederal. The distinction is not absolute.\nIn a unitary state, sovereignty resides in the state itself, and the constitution determines this. The territory of the state may be divided into regions, but they are not sovereign and are subordinate to the state. In the UK, the constitutional doctrine of Parliamentary sovereignty dictates that sovereignty is ultimately contained at the centre. Some powers have been devolved to Northern Ireland, Scotland, and Wales (but not England). Some unitary states (Spain is an example) devolve more and more power to sub-national governments until the state functions in practice much like a federal state.\nA federal state has a central structure with at most a small amount of territory mainly containing the institutions of the federal government, and several regions (called \"states\", \"provinces\", etc.) which compose the territory of the whole state. Sovereignty is divided between the centre and the constituent regions. The constitutions of Canada and the United States establish federal states, with power divided between the federal government and the provinces or states. Each of the regions may in turn have its own constitution (of unitary nature).\nA confederal state comprises again several regions, but the central structure has only limited coordinating power, and sovereignty is located in the regions. Confederal constitutions are rare, and there is often dispute to whether so-called \"confederal\" states are actually federal.\nTo some extent a group of states which do not constitute a federation as such may by treaties and accords give up parts of their sovereignty to a supranational entity. For example, the countries constituting the European Union have agreed to abide by some Union-wide measures which restrict their absolute sovereignty in some ways, e.g., the use of the metric system of measurement instead of national units previously used.\nState of emergency.\nMany constitutions allow the declaration under exceptional circumstances of some form of state of emergency during which some rights and guarantees are suspended. This provision can be and has been abused to allow a government to suppress dissent without regard for human rights\u00a0\u2013 see the article on state of emergency.\nFacade constitutions.\nItalian political theorist Giovanni Sartori noted the existence of national constitutions which are a facade for authoritarian sources of power. While such documents may express respect for human rights or establish an independent judiciary, they may be ignored when the government feels threatened, or never put into practice. An extreme example was the Constitution of the Soviet Union that on paper supported freedom of assembly and freedom of speech; however, citizens who transgressed unwritten limits were summarily imprisoned. The example demonstrates that the protections and benefits of a constitution are ultimately provided not through its written terms but through deference by government and society to its principles. A constitution may change from being real to a facade and back again as democratic and autocratic governments succeed each other.\nConstitutional courts.\nConstitutions are often, but by no means always, protected by a legal body whose job it is to interpret those constitutions and, where applicable, declare void executive and legislative acts which infringe the constitution. In some countries, such as Germany, this function is carried out by a dedicated constitutional court which performs this (and only this) function. In other countries, such as Ireland, the ordinary courts may perform this function in addition to their other responsibilities. While elsewhere, like in the United Kingdom, the concept of declaring an act to be unconstitutional does not exist.\nA constitutional violation is an action or legislative act that is judged by a constitutional court to be contrary to the constitution, that is, unconstitutional. An example of constitutional violation by the executive could be a public office holder who acts outside the powers granted to that office by a constitution. An example of constitutional violation by the legislature is an attempt to pass a law that would contradict the constitution, without first going through the proper constitutional amendment process.\nSome countries, mainly those with uncodified constitutions, have no such courts at all. For example, the United Kingdom has traditionally operated under the principle of parliamentary sovereignty under which the laws passed by United Kingdom Parliament could not be questioned by the courts.\nSee also.\n\"Judicial philosophies of constitutional interpretation (note: generally specific to United States constitutional law)\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5254", "revid": "42244930", "url": "https://en.wikipedia.org/wiki?curid=5254", "title": "Common law", "text": "Law created by judicial precedent\nIn law, common law (also known as judicial precedent, judge-made law, or case law) is the body of law created by judges and similar quasi-judicial tribunals by virtue of being stated in written opinions.\nThe defining characteristic of common law is that it arises as precedent. Common law courts look to the past decisions of courts to synthesize the legal principles of past cases. \"Stare decisis\", the principle that cases should be decided according to consistent principled rules so that similar facts will yield similar results, lies at the heart of all common law systems. If a court finds that a similar dispute as the present one has been resolved in the past, the court is generally bound to follow the reasoning used in the prior decision. If, however, the court finds that the current dispute is fundamentally distinct from all previous cases (a \"matter of first impression\"), and legislative statutes are either silent or ambiguous on the question, judges have the authority and duty to resolve the issue. The opinion that a common law judge gives agglomerates with past decisions as precedent to bind future judges and litigants.\nThe common law, so named because it was \"common\" to all the king's courts across England, originated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. The British Empire later spread the English legal system to its colonies, many of which retain the common law system today. These common law systems are legal systems that give great weight to judicial precedent, and to the style of reasoning inherited from the English legal system.\nThe term \"common law\", referring to the body of law made by the judiciary, is often distinguished from statutory law and regulations, which are laws adopted by the legislature and executive respectively. In legal systems that recognise the common law, judicial precedent stands in contrast to and on equal footing with statutes. The other major legal system used by countries is the civil law, which codifies its legal principles into legal codes and does not recognise judicial opinions as binding.\nToday, one-third of the world's population lives in common law jurisdictions or in mixed legal systems that combine the common law with the civil law, including Antigua and Barbuda, Australia, Bahamas, Bangladesh, Barbados, Belize, Botswana, Burma, Cameroon, Canada (both the federal system and all its provinces except Quebec), Cyprus, Dominica, Fiji, Ghana, Grenada, Guyana, Hong Kong, India, Ireland, Israel, Jamaica, Kenya, Liberia, Malaysia, Malta, Marshall Islands, Micronesia, Namibia, Nauru, New Zealand, Nigeria, Pakistan, Palau, Papua New Guinea, Philippines, Sierra Leone, Singapore, South Africa, Sri Lanka, Trinidad and Tobago, the United Kingdom (including its overseas territories such as Gibraltar), the United States (both the federal system and 49 of its 50 states), and Zimbabwe.\nDefinitions.\nThe term \"common law\" has many connotations. The first three set out here are the most-common usages within the legal community. Other connotations from past centuries are sometimes seen and are sometimes heard in everyday speech.\nCommon law as opposed to statutory law and regulatory law.\nThe first definition of \"common law\" given in \"Black's Law Dictionary\", 10th edition, 2014, is \"The body of law derived from judicial decisions, rather than from statutes or constitutions; [synonym] CASELAW, [contrast] STATUTORY LAW\". This usage is given as the first definition in modern legal dictionaries, is characterized as the \"most common\" usage among legal professionals, and is the usage frequently seen in decisions of courts. In this connotation, \"common law\" distinguishes the authority that promulgated a law. For example, the law in most Anglo-American jurisdictions includes \"statutory law\" enacted by a legislature, \"regulatory law\" (in the U.S.) or \"delegated legislation\" (in the U.K.) promulgated by executive branch agencies pursuant to delegation of rule-making authority from the legislature, and common law or \"case law\", i.e., decisions issued by courts (or quasi-judicial tribunals within agencies). This first connotation can be further differentiated into:\nPublication of decisions, and indexing, is essential to the development of common law, and thus governments and private publishers publish law reports. While all decisions in common law jurisdictions are precedent (at varying levels and scope as discussed throughout the article on precedent), some become \"leading cases\" or \"landmark decisions\" that are cited especially often.\nCommon law legal systems as opposed to civil law legal systems.\n\"Black's Law Dictionary\", 10th ed., definition 2, differentiates \"common law\" jurisdictions and legal systems from \"civil law\" or \"code\" jurisdictions. Common law systems place great weight on court decisions, which are considered \"law\" with the same force of law as statutes\u2014for nearly a millennium, common law courts have had the authority to make law where no legislative statute exists, and statutes mean what courts interpret them to mean.\nBy contrast, in civil law jurisdictions (the legal tradition that prevails, or is combined with common law, in Europe and most non-Islamic, non-common law countries), courts lack authority to act if there is no statute. Civil law judges tend to give less weight to judicial precedent, which means that a civil law judge deciding a given case has more freedom to interpret the text of a statute independently (compared to a common law judge in the same circumstances), and therefore less predictably. For example, the Napoleonic code expressly forbade French judges to pronounce general principles of law. The role of providing overarching principles, which in common law jurisdictions is provided in judicial opinions, in civil law jurisdictions is filled by giving greater weight to scholarly literature, as explained below.\nCommon law systems trace their history to England, while civil law systems trace their history through the Napoleonic Code back to the of Roman law.\nLaw as opposed to equity.\n\"Black's Law Dictionary\", 10th ed., definition 4, differentiates \"common law\" (or just \"law\") from \"equity\". Before 1873, England had two complementary court systems: courts of \"law\" which could only award money damages and recognized only the legal owner of property, and courts of \"equity\" (courts of chancery) that could issue injunctive relief (that is, a court order to a party to do something, give something to someone, or stop doing something) and recognized trusts of property. This split propagated to many of the colonies, including the United States. The states of Delaware, Mississippi, South Carolina, and Tennessee continue to have divided Courts of Law and Courts of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.\nFor most purposes, most jurisdictions, including the U.S. federal system and most states, have merged the two courts. Additionally, even before the separate courts were merged, most courts were permitted to apply both law and equity, though under potentially different procedural law. Nonetheless, the historical distinction between \"law\" and \"equity\" remains important today when the case involves issues such as the following:\nCourts of equity rely on common law (in the sense of this first connotation) principles of binding precedent.\nArchaic meanings and historical uses.\nIn addition, there are several historical (but now archaic) uses of the term that, while no longer current, provide background context that assists in understanding the meaning of \"common law\" today.\nIn one usage that is now archaic, but that gives insight into the history of the common law, \"common law\" referred to the pre-Christian system of law, imported by the Saxons to England, and dating to before the Norman conquest, and before there was any consistent law to be applied.\n\"Common law\" as the term is used today in common law countries contrasts with . While historically the became a secure point of reference in continental European legal systems, in England it was not a point of reference at all.\nThe English Court of Common Pleas dealt with lawsuits in which the Monarch had no interest, i.e., between commoners.\n\"Black's Law Dictionary\", 10th ed., definition 3 is \"General law common to a country as a whole, as opposed to special law that has only local application.\" From at least the 11th century and continuing for several centuries after that, there were several different circuits in the royal court system, served by itinerant judges who would travel from town to town dispensing the King's justice in \"assizes\". The term \"common law\" was used to describe the law held in common between the circuits and the different stops in each circuit. The more widely a particular law was recognized, the more weight it held, whereas purely local customs were generally subordinate to law recognized in a plurality of jurisdictions.\nMisconceptions and imprecise nonlawyer usages.\nAs used by non-lawyers in popular culture, the term \"common law\" connotes law based on ancient and unwritten universal custom of the people. The \"ancient unwritten universal custom\" view was the foundation of the first treatises by Blackstone and Coke, and was universal among lawyers and judges from the earliest times to the mid-19th century. However, for 100 years, lawyers and judges have recognized that the \"ancient unwritten universal custom\" view does not accord with the facts of the origin and growth of the law, and it is not held within the legal profession today.\nUnder the modern view, \"common law\" is not grounded in \"custom\" or \"ancient usage\", but rather acquires force of law instantly (without the delay implied by the term \"custom\" or \"ancient\") when pronounced by a higher court, because and to the extent the proposition is stated in judicial opinion. From the earliest times through the late 19th century, the dominant theory was that the common law was a pre-existent law or system of rules, a social standard of justice that existed in the habits, customs, and thoughts of the people. Under this older view, the legal profession considered it no part of a judge's duty to make new or change existing law, but only to expound and apply the old. By the early 20th century, largely at the urging of Oliver Wendell Holmes (as discussed throughout this article), this view had fallen into the minority view: Holmes pointed out that the older view worked undesirable and unjust results, and hampered a proper development of the law. In the century since Holmes, the dominant understanding has been that common law \"decisions are themselves law, or rather the rules which the courts lay down in making the decisions constitute law\". Holmes wrote in a 1917 opinion, \"The common law is not a brooding omnipresence in the sky, but the articulate voice of some sovereign or quasi-sovereign that can be identified.\" Among legal professionals (lawyers and judges), the change in understanding occurred in the late 19th and early 20th centuries (as explained later in this article), though lay (non-legal) dictionaries were decades behind in recognizing the change.\nThe reality of the modern view, and implausibility of the old \"ancient unwritten universal custom\" view, can be seen in practical operation: under the pre-1870 view, (a) the \"common law\" should have been absolutely static over centuries (but it evolved), (b) jurisdictions could not logically diverge from each other (but nonetheless did and do today), (c) a new decision logically needed to operate retroactively (but did not), and (d) there was no standard to decide which English medieval customs should be \"law\" and which should not. All five tensions resolve under the modern view: (a) the common law evolved to meet the needs of the times (e.g., trial by combat passed out of the law by the 15th century), (b) the common law in different jurisdictions may diverge, (c) new decisions may (but need not) have retroactive operation, and (d) court decisions are effective immediately as they are issued, not years later, or after they become \"custom\", and questions of what \"custom\" might have been at some \"ancient\" time are simply irrelevant.\nPeople using pseudolegal tactics and arguments have frequently claimed to base those arguments on common law; notably, the radical anti-government sovereign citizens and freemen on the land movements, who deny the legitimacy of their countries' legal systems, base their beliefs on idiosyncratic interpretations of common law. \"Common law\" has also been used as an alibi by groups such as the far-right American Patriot movement for setting up kangaroo courts in order to conduct vigilante actions or intimidate their opponents.\nBasic principles of common law.\nCommon law adjudication.\nIn a common law jurisdiction several stages of research and analysis are required to determine \"what the law is\" in a given situation. First, one must ascertain the facts. Then, one must locate any relevant statutes and cases. Then one must extract the principles, analogies and statements by various courts of what they consider important to determine how the next court is likely to rule on the facts of the present case. Later decisions, and decisions of higher courts or legislatures carry more weight than earlier cases and those of lower courts. Finally, one integrates all the lines drawn and reasons given, and determines \"what the law is\". Then, one applies that law to the facts.\nIn practice, common law systems are considerably more complicated than the simplified system described above. The decisions of a court are binding only in a particular jurisdiction, and even within a given jurisdiction, some courts have more power than others. For example, in most jurisdictions, decisions by appellate courts are binding on lower courts in the same jurisdiction, and on future decisions of the same appellate court, but decisions of lower courts are only non-binding persuasive authority. Interactions between common law, constitutional law, statutory law and regulatory law also give rise to considerable complexity.\nCommon law evolves to meet changing social needs and improved understanding.\nOliver Wendell Holmes Jr. cautioned that \"the proper derivation of general principles in both common and constitutional law ... arise gradually, in the emergence of a consensus from a multitude of particularized prior decisions\". Justice Cardozo noted the \"common law does not work from pre-established truths of universal and inflexible validity to conclusions derived from them deductively\", but \"[i]ts method is inductive, and it draws its generalizations from particulars\".\nThe common law is more malleable than statutory law. First, common law courts are not absolutely bound by precedent, but can (when extraordinarily good reason is shown) reinterpret and revise the law, without legislative intervention, to adapt to new trends in political, legal and social philosophy. Second, the common law evolves through a series of gradual steps, that gradually works out all the details, so that over a decade or more, the law can change substantially but without a sharp break, thereby reducing disruptive effects. In contrast to common law incrementalism, the legislative process is very difficult to get started, as legislatures tend to delay action until a situation is intolerable. For these reasons, legislative changes tend to be large, jarring and disruptive (sometimes positively, sometimes negatively, and sometimes with unintended consequences).\nOne example of the gradual change that typifies evolution of the common law is the gradual change in liability for negligence. The traditional common law rule through most of the 19th century was that a plaintiff could not recover for a defendant's negligent production or distribution of a harmful instrumentality unless the two were in privity of contract. Thus, only the immediate purchaser could recover for a product defect, and if a part was built up out of parts from parts manufacturers, the ultimate buyer could not recover for injury caused by a defect in the part. In an 1842 English case, \"Winterbottom v. Wright\", the postal service had contracted with Wright to maintain its coaches. Winterbottom was a driver for the post. When the coach failed and injured Winterbottom, he sued Wright. The \"Winterbottom\" court recognized that there would be \"absurd and outrageous consequences\" if an injured person could sue any person peripherally involved, and knew it had to draw a line somewhere, a limit on the causal connection between the negligent conduct and the injury. The court looked to the contractual relationships, and held that liability would only flow as far as the person in immediate contract (\"privity\") with the negligent party.\nA first exception to this rule arose in 1852, in the case of \"Thomas v. Winchester\", when New York's highest court held that mislabeling a poison as an innocuous herb, and then selling the mislabeled poison through a dealer who would be expected to resell it, put \"human life in imminent danger\". \"Thomas\" relied on this reason to create an exception to the \"privity\" rule. In 1909, New York held in \"Statler v. Ray Mfg. Co.\" that a coffee urn manufacturer was liable to a person injured when the urn exploded, because the urn \"was of such a character inherently that, when applied to the purposes for which it was designed, it was liable to become a source of great danger to many people if not carefully and properly constructed\".\nYet the privity rule survived. In \"Cadillac Motor Car Co. v. Johnson\" (decided in 1915 by the federal appeals court for New York and several neighboring states), the court held that a car owner could not recover for injuries from a defective wheel, when the automobile owner had a contract only with the automobile dealer and not with the manufacturer, even though there was \"no question that the wheel was made of dead and 'dozy' wood, quite insufficient for its purposes\". The \"Cadillac\" court was willing to acknowledge that the case law supported exceptions for \"an article dangerous in its nature or likely to become so in the course of the ordinary usage to be contemplated by the vendor\". However, held the \"Cadillac\" court, \"one who manufactures articles dangerous only if defectively made, or installed, e.g., tables, chairs, pictures or mirrors hung on the walls, carriages, automobiles, and so on, is not liable to third parties for injuries caused by them, except in case of willful injury or fraud\".\nFinally, in the famous case of \"MacPherson v. Buick Motor Co.\", in 1916, Judge Benjamin Cardozo for New York's highest court pulled a broader principle out of these predecessor cases. The facts were almost identical to \"Cadillac\" a year earlier: a wheel from a wheel manufacturer was sold to Buick, to a dealer, to MacPherson, and the wheel failed, injuring MacPherson. Judge Cardozo held:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It may be that \"Statler v. Ray Mfg. Co.\" have extended the rule of \"Thomas v. Winchester\". If so, this court is committed to the extension. The defendant argues that things imminently dangerous to life are poisons, explosives, deadly weapons\u2014things whose normal function it is to injure or destroy. But whatever the rule in \"Thomas v. Winchester\" may once have been, it has no longer that restricted meaning. A scaffold (\"Devlin v. Smith\", supra) is not inherently a destructive instrument. It becomes destructive only if imperfectly constructed. A large coffee urn (\"Statler v. Ray Mfg. Co.\", supra) may have within itself, if negligently made, the potency of danger, yet no one thinks of it as an implement whose normal function is destruction. What is true of the coffee urn is equally true of bottles of aerated water (\"Torgesen v. Schultz\", 192 N. Y. 156). We have mentioned only cases in this court. But the rule has received a like extension in our courts of intermediate appeal. In \"Burke v. Ireland\" (26 App. Div. 487), in an opinion by CULLEN, J., it was applied to a builder who constructed a defective building; in \"Kahner v. Otis Elevator Co.\" (96 App. Div. 169) to the manufacturer of an elevator; in \"Davies v. Pelham Hod Elevating Co.\" (65 Hun, 573; affirmed in this court without opinion, 146 N. Y. 363) to a contractor who furnished a defective rope with knowledge of the purpose for which the rope was to be used. We are not required at this time either to approve or to disapprove the application of the rule that was made in these cases. It is enough that they help to characterize the trend of judicial thought.\nWe hold, then, that the principle of \"Thomas v. Winchester\" is not limited to poisons, explosives, and things of like nature, to things which in their normal operation are implements of destruction. If the nature of a thing is such that it is reasonably certain to place life and limb in peril when negligently made, it is then a thing of danger. Its nature gives warning of the consequences to be expected. If to the element of danger there is added knowledge that the thing will be used by persons other than the purchaser, and used without new tests then, irrespective of contract, the manufacturer of this thing of danger is under a duty to make it carefully. ... There must be knowledge of a danger, not merely possible, but probable.\nCardozo's new \"rule\" exists in no prior case, but is inferrable as a synthesis of the \"thing of danger\" principle stated in them, merely extending it to \"foreseeable danger\" even if \"the purposes for which it was designed\" were not themselves \"a source of great danger\". \"MacPherson\" takes some care to present itself as foreseeable progression, not a wild departure. Cardozo continues to adhere to the original principle of \"Winterbottom\", that \"absurd and outrageous consequences\" must be avoided, and he does so by drawing a new line in the last sentence quoted above: \"There must be knowledge of a danger, not merely possible, but probable.\" But while adhering to the underlying principle that \"some\" boundary is necessary, \"MacPherson\" overruled the prior common law by rendering the formerly dominant factor in the boundary, that is, the privity formality arising out of a contractual relationship between persons, totally irrelevant. Rather, the most important factor in the boundary would be the nature of the thing sold and the foreseeable uses that downstream purchasers would make of the thing.\nThe example of the evolution of the law of negligence in the preceding paragraphs illustrates two crucial principles: (a) The common law evolves, this evolution is in the hands of judges, and judges have \"made law\" for hundreds of years. (b) The reasons given for a decision are often more important in the long run than the outcome in a particular case. This is the reason that judicial opinions are usually quite long, and give rationales and policies that can be balanced with judgment in future cases, rather than the bright-line rules usually embodied in statutes.\nPublication of decisions.\nAll law systems rely on written publication of the law, so that it is accessible to all. Common law decisions are published in law reports for use by lawyers, courts and the general public.\nAfter the American Revolution, Massachusetts became the first state to establish an official Reporter of Decisions. As newer states needed law, they often looked first to the Massachusetts Reports for authoritative precedents as a basis for their own common law. The United States federal courts relied on private publishers until after the Civil War, and only began publishing as a government function in 1874. West Publishing in Minnesota is the largest private-sector publisher of law reports in the United States. Government publishers typically issue only decisions \"in the raw\", while private sector publishers often add indexing, including references to the key principles of the common law involved, editorial analysis, and similar finding aids.\nInteraction of constitution, statute, and executive branch regulation with common law.\nIn common law legal systems, the common law is crucial to understanding almost all important areas of law. For example, in England and Wales, in English Canada, and in most states of the United States, the basic law of contracts, torts and property do not exist in statute, but only in common law (though there may be isolated modifications enacted by statute). As another example, the Supreme Court of the United States in 1877, held that a Michigan statute that established rules for solemnization of marriages did not abolish pre-existing common-law marriage, because the statute did not affirmatively require statutory solemnization and was silent as to preexisting common law.\nIn almost all areas of the law (even those where there is a statutory framework, such as contracts for the sale of goods, or the criminal law), legislature-enacted statutes or agency-promulgated regulations generally give only terse statements of general principle, and the fine boundaries and definitions exist only in the interstitial common law. To find out what the precise law is that applies to a particular set of facts, one has to locate precedential decisions on the topic, and reason from those decisions by analogy.\nIn common law jurisdictions (in the sense opposed to \"civil law\"), legislatures operate under the assumption that statutes will be interpreted against the backdrop of the pre-existing common law. As the United States Supreme Court explained in \"United States v Texas\", 507 U.S. 529 (1993):\nFor example, in most U.S. states, the criminal statutes are primarily codification of pre-existing common law. (Codification is the process of enacting a statute that collects and restates pre-existing law in a single document\u2014when that pre-existing law is common law, the common law remains relevant to the interpretation of these statutes.) In reliance on this assumption, modern statutes often leave a number of terms and fine distinctions unstated\u2014for example, a statute might be very brief, leaving the precise definition of terms unstated, under the assumption that these fine distinctions would be resolved in the future by the courts based upon what they then understand to be the pre-existing common law. (For this reason, many modern American law schools teach the common law of crime as it stood in England in 1789, because that centuries-old English common law is a necessary foundation to interpreting modern criminal statutes.)\nWith the transition from English law, which had common law crimes, to the new legal system under the U.S. Constitution, which prohibited \"ex post facto\" laws at both the federal and state level, the question was raised whether there could be common law crimes in the United States. It was settled in the case of \"United States v. Hudson\", which decided that federal courts had no jurisdiction to define new common law crimes, and that there must always be a (constitutionally valid) statute defining the offense and the penalty for it.\nStill, many states retain selected common law crimes. For example, in Virginia, the definition of the conduct that constitutes the crime of robbery exists only in the common law, and the robbery statute only sets the punishment. Virginia Code section 1-200 establishes the continued existence and vitality of common law principles and provides that \"The common law of England, insofar as it is not repugnant to the principles of the Bill of Rights and Constitution of this Commonwealth, shall continue in full force within the same, and be the rule of decision, except as altered by the General Assembly.\"\nBy contrast to statutory codification of common law, some statutes displace common law, for example to create a new cause of action that did not exist in the common law, or to legislatively overrule the common law. An example is the tort of wrongful death, which allows certain persons, usually a spouse, child or estate, to sue for damages on behalf of the deceased. There is no such tort in English common law; thus, any jurisdiction that lacks a wrongful death statute will not allow a lawsuit for the wrongful death of a loved one. Where a wrongful death statute exists, the compensation or other remedy available is limited to the remedy specified in the statute (typically, an upper limit on the amount of damages). Courts generally interpret statutes that create new causes of action narrowly\u2014that is, limited to their precise terms\u2014because the courts generally recognize the legislature as being supreme in deciding the reach of judge-made law unless such statute should violate some \"second order\" constitutional law provision (\"cf\". judicial activism). This principle is applied more strongly in fields of commercial law (contracts and the like) where predictability is of relatively higher value, and less in torts, where courts recognize a greater responsibility to \"do justice\".\nWhere a tort is rooted in common law, all traditionally recognized damages for that tort may be sued for, whether or not there is mention of those damages in the current statutory law. For instance, a person who sustains bodily injury through the negligence of another may sue for medical costs, pain, suffering, loss of earnings or earning capacity, mental and/or emotional distress, loss of quality of life, disfigurement and more. These damages need not be set forth in statute as they already exist in the tradition of common law. However, without a wrongful death statute, most of them are extinguished upon death.\nIn the United States, the power of the federal judiciary to review and invalidate unconstitutional acts of the federal executive branch is stated in the constitution, Article III sections 1 and 2: \"The judicial Power of the United States, shall be vested in one supreme Court, and in such inferior Courts as the Congress may from time to time ordain and establish. ... The judicial Power shall extend to all Cases, in Law and Equity, arising under this Constitution, the Laws of the United States, and Treaties made, or which shall be made, under their Authority\". The first landmark decision on \"the judicial power\" was \"Marbury v. Madison\", 5 U.S. (1 Cranch) 137 (1803). Later cases interpreted the \"judicial power\" of Article III to establish the power of federal courts to consider or overturn any action of Congress or of any state that conflicts with the Constitution.\nThe interactions between decisions of different courts is discussed further in the article on precedent. Further interactions between common law and either statute or regulation are discussed further in the articles on \"Skidmore\" deference, \"Chevron\" deference, and \"Auer\" deference.\nOverruling precedent\u2014the limits of \"stare decisis\".\nThe United States federal courts are divided into twelve regional circuits, each with a circuit court of appeals (plus a thirteenth, the Court of Appeals for the Federal Circuit, which hears appeals in patent cases and cases against the federal government, without geographic limitation). Decisions of one circuit court are binding on the district courts within the circuit and on the circuit court itself, but are only persuasive authority on sister circuits. District court decisions are not binding precedent at all, only persuasive.\nMost of the U.S. federal courts of appeal have adopted a rule under which, in the event of any conflict in decisions of panels (most of the courts of appeal almost always sit in panels of three), the earlier panel decision is controlling, and a panel decision may only be overruled by the court of appeals sitting \"en banc\" (that is, all active judges of the court) or by a higher court. In these courts, the older decision remains controlling when an issue comes up the third time.\nOther courts, for example, the Court of Customs and Patent Appeals and the Supreme Court, always sit \"en banc\", and thus the \"later\" decision controls. These courts essentially overrule all previous cases in each new case, and older cases survive only to the extent they do not conflict with newer cases. The interpretations of these courts\u2014for example, Supreme Court interpretations of the constitution or federal statutes\u2014are stable only so long as the older interpretation maintains the support of a majority of the court. Older decisions persist through some combination of belief that the old decision is right, and that it is not sufficiently wrong to be overruled.\nIn the jurisdictions of England and Wales and of Northern Ireland, since 2009, the Supreme Court of the United Kingdom has the authority to overrule and unify criminal law decisions of lower courts; it is the final court of appeal for civil law cases in all three of the UK jurisdictions, but not for criminal law cases in Scotland, where the High Court of Justiciary has this power instead (except on questions of law relating to reserved matters such as devolution and human rights). From 1966 to 2009, this power lay with the House of Lords, granted by the Practice Statement of 1966.\nCanada's federal system, described below, avoids regional variability of federal law by giving national jurisdiction to both layers of appellate courts.\nCommon law as a foundation for commercial economies.\nThe reliance on judicial opinion is a strength of common law systems, and is a significant contributor to the robust commercial systems in the United Kingdom and United States. Because there is reasonably precise guidance on almost every issue, parties (especially commercial parties) can predict whether a proposed course of action is likely to be lawful or unlawful, and have some assurance of consistency. As Justice Brandeis famously expressed it, \"in most matters it is more important that the applicable rule of law be settled than that it be settled right.\" This ability to predict gives more freedom to come close to the boundaries of the law. For example, many commercial contracts are more economically efficient, and create greater wealth, because the parties know ahead of time that the proposed arrangement, though perhaps close to the line, is almost certainly legal. Newspapers, taxpayer-funded entities with some religious affiliation, and political parties can obtain fairly clear guidance on the boundaries within which their freedom of expression rights apply.\nIn contrast, in jurisdictions with very weak respect for precedent, fine questions of law are redetermined anew each time they arise, making consistency and prediction more difficult, and procedures far more protracted than necessary because parties cannot rely on written statements of law as reliable guides. In jurisdictions that do not have a strong allegiance to a large body of precedent, parties have less \"a priori\" guidance (unless the written law is very clear and kept updated) and must often leave a bigger \"safety margin\" of unexploited opportunities, and final determinations are reached only after far larger expenditures on legal fees by the parties.\nThis is the reason for the frequent choice of the law of the State of New York in commercial contracts, even when neither entity has extensive contacts with New York\u2014and remarkably often even when neither party has contacts with the United States. Commercial contracts almost always include a \"choice of law clause\" to reduce uncertainty. Somewhat surprisingly, contracts throughout the world (for example, contracts involving parties in Japan, France and Germany, and from most of the other states of the United States) often choose the law of New York, even where the relationship of the parties and transaction to New York is quite attenuated. Because of its history as the United States' commercial center, New York common law has a depth and predictability not (yet) available in any other jurisdictions of the United States. Similarly, American corporations are often formed under Delaware corporate law, and American contracts relating to corporate law issues (merger and acquisitions of companies, rights of shareholders, and so on) include a Delaware choice of law clause, because of the deep body of law in Delaware on these issues. On the other hand, some other jurisdictions have sufficiently developed bodies of law so that parties have no real motivation to choose the law of a foreign jurisdiction (for example, England and Wales, and the state of California), but not yet so fully developed that parties with no relationship to the jurisdiction choose that law. Outside the United States, parties that are in different jurisdictions from each other often choose the law of England and Wales, particularly when the parties are each in former British colonies and members of the Commonwealth. The common theme in all cases is that commercial parties seek predictability and simplicity in their contractual relations, and frequently choose the law of a common law jurisdiction with a well-developed body of common law to achieve that result.\nLikewise, for litigation of commercial disputes arising out of unpredictable torts (as opposed to the prospective choice of law clauses in contracts discussed in the previous paragraph), certain jurisdictions attract an unusually high fraction of cases, because of the predictability afforded by the depth of decided cases. For example, London is considered the pre-eminent centre for litigation of admiralty cases.\nThis is not to say that common law is better in every situation. For example, civil law can be clearer than case law when the legislature has had the foresight and diligence to address the precise set of facts applicable to a particular situation. For that reason, civil law statutes tend to be somewhat more detailed than statutes written by common law legislatures\u2014but, conversely, that tends to make the statute more difficult to read (the United States tax code is an example).\nHistory.\nOrigins.\nThe common law\u2014so named because it was \"common\" to all the king's courts across England\u2014originated in the practices of the courts of the English kings in the centuries following the Norman Conquest in 1066. Prior to the Norman Conquest, much of England's legal business took place in the local folk courts of its various shires and hundreds. A variety of other individual courts also existed across the land: urban boroughs and merchant fairs held their own courts, and large landholders also held their own manorial and seigniorial courts as needed. The degree to which common law drew from earlier Anglo-Saxon traditions such as the jury, ordeals, the penalty of outlawry, and writs \u2013 all of which were incorporated into the Norman common law \u2013 is still a subject of much discussion. Additionally, the Catholic Church operated its own court system that adjudicated issues of canon law.\nThe main sources for the history of the common law in the Middle Ages are the plea rolls and the Year Books. The plea rolls, which were the official court records for the Courts of Common Pleas and King's Bench, were written in Latin. The rolls were made up in bundles by law term: Hilary, Easter, Trinity, and Michaelmas, or winter, spring, summer, and autumn. They are currently deposited in the UK National Archives, by whose permission images of the rolls for the Courts of Common Pleas, King's Bench, and Exchequer of Pleas, from the 13th century to the 17th, can be viewed online at the Anglo-American Legal Tradition site (The O'Quinn Law Library of the University of Houston Law Center).\nThe doctrine of precedent developed during the 12th and 13th centuries, as the collective judicial decisions that were based in tradition, custom and precedent.\nThe form of reasoning used in common law is known as casuistry or case-based reasoning. The common law, as applied in civil cases (as distinct from criminal cases), was devised as a means of compensating someone for wrongful acts known as torts, including both intentional torts and torts caused by negligence, and as developing the body of law recognizing and regulating contracts. The type of procedure practiced in common law courts is known as the adversarial system; this is also a development of the common law.\nMedieval English common law.\nIn 1154, Henry II became the first Plantagenet king. Among many achievements, Henry institutionalized common law by creating a unified system of law \"common\" to the country through incorporating and elevating local custom to the national, ending local control and peculiarities, eliminating arbitrary remedies and reinstating a jury system\u2014citizens sworn on oath to investigate reliable criminal accusations and civil claims. The jury reached its verdict through evaluating common local knowledge, not necessarily through the presentation of evidence, a distinguishing factor from today's civil and criminal court systems.\nAt the time, royal government centered on the \"Curia Regis\" (king's court), the body of aristocrats and prelates who assisted in the administration of the realm and the ancestor of Parliament, the Star Chamber, and Privy Council. Henry II developed the practice of sending judges (numbering around 20 to 30 in the 1180s) from his Curia Regis to hear the various disputes throughout the country, and return to the court thereafter. The king's itinerant justices would generally receive a writ or commission under the great seal. They would then resolve disputes on an ad hoc basis according to what they interpreted the customs to be. The king's judges would then return to London and often discuss their cases and the decisions they made with the other judges. These decisions would be recorded and filed. In time, a rule, known as \"stare decisis\" (also commonly known as precedent) developed, whereby a judge would be bound to follow the decision of an earlier judge; he was required to adopt the earlier judge's interpretation of the law and apply the same principles promulgated by that earlier judge if the two cases had similar facts to one another. Once judges began to regard each other's decisions to be binding precedent, the pre-Norman system of local customs and law varying in each locality was replaced by a system that was (at least in theory, though not always in practice) common throughout the whole country, hence the name \"common law\".\nThe king's object was to preserve public order, but providing law and order was also extremely profitable\u2013cases on forest use as well as fines and forfeitures can generate \"great treasure\" for the government. Eyres (a Norman French word for judicial circuit, originating from Latin \"iter\") are more than just courts; they would supervise local government, raise revenue, investigate crimes, and enforce feudal rights of the king. There were complaints that the \"eyre\" of 1198 reducing the kingdom to poverty and Cornishmen fleeing to escape the eyre of 1233.\nHenry II's creation of a powerful and unified court system, which curbed somewhat the power of canonical (church) courts, brought him (and England) into conflict with the church, most famously with Thomas Becket, the Archbishop of Canterbury. The murder of the Archbishop gave rise to a wave of popular outrage against the King. International pressure on Henry grew, and in May 1172 he negotiated a settlement with the papacy in which the King swore to go on crusade as well as effectively overturned the more controversial clauses of the Constitutions of Clarendon. Henry nevertheless continued to exert influence in any ecclesiastical case which interested him and royal power was exercised more subtly with considerable success.\nThe English Court of Common Pleas was established after Magna Carta to try lawsuits between commoners in which the monarch had no interest. Its judges sat in open court in the Great Hall of the king's Palace of Westminster, permanently except in the vacations between the four terms of the Legal year.\nJudge-made common law operated as the primary source of law for several hundred years, before Parliament acquired legislative powers to create statutory law. It is important to understand that common law is the older and more traditional source of law, and legislative power is simply a layer applied on top of the older common law foundation. Since the 12th century, courts have had parallel and co-equal authority to make law\u2014\"legislating from the bench\" is a traditional and essential function of courts, which was carried over into the U.S. system as an essential component of the \"judicial power\" specified by Article III of the U.S. Constitution. Justice Oliver Wendell Holmes Jr. summarized centuries of history in 1917, \"judges do and must legislate.\" In the United States, state courts continue to exercise full common law powers, and create both general common law and interstitial common law. In U.S. federal courts, after \"Erie R. Co. v. Tompkins\", 304 U.S. 64, 78 (1938), the general dividing line is that federal courts can only \"interpret\" to create interstitial common law not exercise general common law powers. However, that authority to \"interpret\" can be an expansive power to \"make law,\" especially on Constitutional issues where the Constitutional text is so terse. There are legitimate debates on how the powers of courts and legislatures should be balanced around \"interpretation.\" However, the view that courts lack law-making power is historically inaccurate and constitutionally unsupportable.\nIn England, judges have devised a number of rules as to how to deal with precedent decisions. The early development of case-law in the thirteenth century has been traced to Bracton's \"On the Laws and Customs of England\" and led to the yearly compilations of court cases known as Year Books, of which the first extant was published in 1268, the same year that Bracton died. The Year Books are known as the law reports of medieval England, and are a principal source for knowledge of the developing legal doctrines, concepts, and methods in the period from the 13th to the 16th centuries, when the common law developed into recognizable form.\nInfluence of Roman law.\nThe term \"common law\" is often used as a contrast to Roman-derived \"civil law\", and the fundamental processes and forms of reasoning in the two are quite different. Nonetheless, there has been considerable cross-fertilization of ideas, while the two traditions and sets of foundational principles remain distinct.\nBy the time of the rediscovery of the Roman law in Europe in the 12th and 13th centuries, the common law had already developed far enough to prevent a Roman law reception as it occurred on the continent. However, the first common law scholars, most notably Glanvill and Bracton, as well as the early royal common law judges, had been well accustomed with Roman law. Often, they were clerics trained in the Roman canon law. One of the first and throughout its history one of the most significant treatises of the common law, Bracton's \"De Legibus et Consuetudinibus Angliae\" (On the Laws and Customs of England), was heavily influenced by the division of the law in Justinian's \"Institutes\". The impact of Roman law had decreased sharply after the age of Bracton, but the Roman divisions of actions into \"in rem\" (typically, actions against a \"thing\" or property for the purpose of gaining title to that property; must be filed in a court where the property is located) and \"in personam\" (typically, actions directed against a person; these can affect a person's rights and, since a person often owns things, his property too) used by Bracton had a lasting effect and laid the groundwork for a return of Roman law structural concepts in the 18th and 19th centuries. Signs of this can be found in Blackstone's \"Commentaries on the Laws of England\", and Roman law ideas regained importance with the revival of academic law schools in the 19th century. As a result, today, the main systematic divisions of the law into property, contract, and tort (and to some extent unjust enrichment) can be found in the civil law as well as in the common law.\nCoke and Blackstone.\nThe first attempt at a comprehensive compilation of centuries of common law was by Lord Chief Justice Edward Coke, in his treatise, \"Institutes of the Lawes of England\" in the 17th century.\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765\u20131769.\nPropagation of the common law to the colonies and Commonwealth by reception statutes.\nA reception statute is a statutory law adopted as a former British colony becomes independent, by which the new nation adopts (i.e. receives) pre-independence common law, to the extent not explicitly rejected by the legislative body or constitution of the new nation. Reception statutes generally consider the English common law dating prior to independence, and the precedent originating from it, as the default law, because of the importance of using an extensive and predictable body of law to govern the conduct of citizens and businesses in a new state. All U.S. states, with the partial exception of Louisiana, have either implemented reception statutes or adopted the common law by judicial opinion.\nOther examples of reception statutes in the United States, the states of the U.S., Canada and its provinces, and Hong Kong, are discussed in the reception statute article.\nYet, adoption of the common law in the newly independent nation was not a foregone conclusion, and was controversial. Immediately after the American Revolution, there was widespread distrust and hostility to anything British, and the common law was no exception. Jeffersonians decried lawyers and their common law tradition as threats to the new republic. The Jeffersonians preferred a legislatively enacted civil law under the control of the political process, rather than the common law developed by judges that\u2014by design\u2014were insulated from the political process. The Federalists believed that the common law was the birthright of Independence: after all, the natural rights to \"life, liberty, and the pursuit of happiness\" were the rights protected by common law. Even advocates for the common law approach noted that it was not an ideal fit for the newly independent colonies: judges and lawyers alike were severely hindered by a lack of printed legal materials. Before Independence, the most comprehensive law libraries had been maintained by Tory lawyers, and those libraries vanished with the loyalist expatriation, and the ability to print books was limited. Lawyer (later President) John Adams complained that he \"suffered very much for the want of books\". To bootstrap this most basic need of a common law system\u2014knowable, written law\u2014in 1803, lawyers in Massachusetts donated their books to found a law library. A Jeffersonian newspaper criticized the library, as it would carry forward \"all the old authorities practiced in England for centuries back ... whereby a new system of jurisprudence [will be founded] on the high monarchical system [to] become the Common Law of this Commonwealth... [The library] may hereafter have a very unsocial purpose.\"\nFor several decades after independence, English law still exerted influence over American common law\u2014for example, with \"Byrne v Boadle\" (1863), which first applied the res ipsa loquitur doctrine.\nDecline of Latin maxims and \"blind imitation of the past\", and adding flexibility to \"stare decisis\".\nWell into the 19th century, ancient maxims played a large role in common law adjudication. Many of these maxims had originated in Roman Law, migrated to England before the introduction of Christianity to the British Isles, and were typically stated in Latin even in English decisions. Many examples are familiar in everyday speech even today, \"One cannot be a judge in one's own cause\" (see Dr. Bonham's Case), rights are reciprocal to obligations, and the like. Judicial decisions and treatises of the 17th and 18th centuries, such at those of Lord Chief Justice Edward Coke, presented the common law as a collection of such maxims.\nReliance on old maxims and rigid adherence to precedent, no matter how old or ill-considered, came under critical discussion in the late 19th century, starting in the United States. Oliver Wendell Holmes Jr. in his famous article, \"The Path of the Law\", commented, \"It is revolting to have no better reason for a rule of law than that so it was laid down in the time of Henry IV. It is still more revolting if the grounds upon which it was laid down have vanished long since, and the rule simply persists from blind imitation of the past.\" Justice Holmes noted that study of maxims might be sufficient for \"the man of the present\", but \"the man of the future is the man of statistics and the master of economics\". In an 1880 lecture at Harvard, he wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The life of the law has not been logic; it has been experience. The felt necessities of the time, the prevalent moral and political theories, intuitions of public policy, avowed or unconscious, even the prejudices which judges share with their fellow men, have had a good deal more to do than the syllogism in determining the rules by which men should be governed. The law embodies the story of a nation's development through many centuries, and it cannot be dealt with as if it contained only the axioms and corollaries of a book of mathematics.\nIn the early 20th century, Louis Brandeis, later appointed to the United States Supreme Court, became noted for his use of policy-driving facts and economics in his briefs, and extensive appendices presenting facts that lead a judge to the advocate's conclusion. By this time, briefs relied more on facts than on Latin maxims.\nReliance on old maxims is now deprecated. Common law decisions today reflect both precedent and policy judgment drawn from economics, the social sciences, business, decisions of foreign courts, and the like. The degree to which these external factors \"should\" influence adjudication is the subject of active debate, but it is indisputable that judges \"do\" draw on experience and learning from everyday life, from other fields, and from other jurisdictions.\n1870 through 20th century, and the procedural merger of law and equity.\nAs early as the 15th century, it became the practice that litigants who felt they had been cheated by the common law system would petition the King in person. For example, they might argue that an award of damages (at common law (as opposed to equity)) was not sufficient redress for a trespasser occupying their land, and instead request that the trespasser be evicted. From this developed the system of equity, administered by the Lord Chancellor, in the courts of chancery. By their nature, equity and law were frequently in conflict and litigation would frequently continue for years as one court countermanded the other, even though it was established by the 17th century that equity should prevail.\nIn England, courts of law (as opposed to equity) were combined with courts of equity by the Judicature Acts of 1873 and 1875, with equity prevailing in case of conflict.\nIn the United States, parallel systems of law (providing money damages, with cases heard by a jury upon either party's request) and equity (fashioning a remedy to fit the situation, including injunctive relief, heard by a judge) survived well into the 20th century. The United States federal courts procedurally separated law and equity: the same judges could hear either kind of case, but a given case could only pursue causes in law or in equity, and the two kinds of cases proceeded under different procedural rules. This became problematic when a given case required both money damages and injunctive relief. In 1937, the new Federal Rules of Civil Procedure combined law and equity into one form of action, the \"civil action\". Fed.R.Civ.P. 2. The distinction survives to the extent that issues that were \"common law (as opposed to equity)\" as of 1791 (the date of adoption of the Seventh Amendment) are still subject to the right of either party to request a jury, and \"equity\" issues are decided by a judge.\nThe states of Delaware, Illinois, Mississippi, South Carolina, and Tennessee continue to have divided courts of law and courts of chancery, for example, the Delaware Court of Chancery. In New Jersey, the appellate courts are unified, but the trial courts are organized into a Chancery Division and a Law Division.\nCommon law pleading and its abolition in the early 20th century.\nFor centuries, through to the 19th century, the common law recognized only specific forms of action, and required very careful drafting of the opening pleading (called a writ) to slot into exactly one of them: debt, detinue, covenant, special assumpsit, general assumpsit, trespass, trover, replevin, case (or trespass on the case), and ejectment. To initiate a lawsuit, a pleading had to be drafted to meet myriad technical requirements: correctly categorizing the case into the correct legal pigeonhole (pleading in the alternative was not permitted), and using specific \"magic words\" encrusted over the centuries. Under the old common law pleading standards, a suit by a \"pro se\" (\"for oneself\", without a lawyer) party was all but impossible, and there was often considerable procedural jousting at the outset of a case over minor wording issues.\nOne of the major reforms of the late 19th century and early 20th century was the abolition of common law pleading requirements. A plaintiff can initiate a case by giving the defendant \"a short and plain statement\" of facts that constitute an alleged wrong. This reform moved the attention of courts from technical scrutiny of words to a more rational consideration of the facts, and opened access to justice far more broadly.\nAlternatives to common law systems.\nCivil law systems\u2014comparisons and contrasts to common law.\nThe main alternative to the common law system is the civil law system, which is used in Continental Europe, and most of Central and South America.\nJudicial decisions play only a minor role in shaping civil law.\nThe primary contrast between the two systems is the role of written decisions and precedent.\nIn common law jurisdictions, nearly every case that presents a \"bona fide\" disagreement on the law is resolved in a written opinion. The legal reasoning for the decision, known as \"ratio decidendi\", not only determines the court's judgment between the parties, but also stands as precedent for resolving future disputes. In contrast, civil law decisions typically do not include explanatory opinions, and thus no precedent flows from one decision to the next.\nIn common law systems, a single decided case is binding common law (connotation 1) to the same extent as statute or regulation, under the principle of \"stare decisis\". In contrast, in civil law systems, individual decisions have only advisory, not binding effect. In civil law systems, case law only acquires weight when a long series of cases use consistent reasoning, called \"jurisprudence constante\". Civil law lawyers consult case law to obtain their best prediction of how a court will rule, but comparatively, civil law judges are less bound to follow it.\nFor that reason, statutes in civil law systems are more comprehensive, detailed, and continuously updated, covering all matters capable of being brought before a court.\nAdversarial system vs. inquisitorial system.\nCommon law systems tend to give more weight to separation of powers between the judicial branch and the executive branch. In contrast, civil law systems are typically more tolerant of allowing individual officials to exercise both powers. One example of this contrast is the difference between the two systems in allocation of responsibility between prosecutor and adjudicator.\nCommon law courts usually use an adversarial system, in which two sides present their cases to a neutral judge. In contrast, in civil law systems, criminal proceedings proceed under an inquisitorial system in which an examining magistrate serves two roles by developing the evidence and arguments for one side and then the other during the investigation phase.\nThe examining magistrate then presents the dossier detailing his or her findings to the president of the bench that will adjudicate on the case where it has been decided that a trial shall be conducted. Therefore, the president of the bench's view of the case is not neutral and may be biased while conducting the trial after the reading of the dossier. Unlike the common law proceedings, the president of the bench in the inquisitorial system is not merely an umpire and is entitled to directly interview the witnesses or express comments during the trial, as long as he or she does not express his or her view on the guilt of the accused.\nThe proceeding in the inquisitorial system is essentially by writing. Most of the witnesses would have given evidence in the investigation phase and such evidence will be contained in the dossier under the form of police reports. In the same way, the accused would have already put his or her case at the investigation phase but he or she will be free to change his or her evidence at trial. Whether the accused pleads guilty or not, a trial will be conducted. Unlike the adversarial system, the conviction and sentence to be served (if any) will be released by the trial jury together with the president of the trial bench, following their common deliberation.\nIn contrast, in an adversarial system, the onus of framing the case rests on the parties, and judges generally decide the case presented to them, rather than acting as active investigators, or actively reframing the issues presented. \"In our adversary system, in both civil and criminal cases, in the first instance and on appeal, we follow the principle of party presentation. That is, we rely on the parties to frame the issues for decision and assign to courts the role of neutral arbiter of matters the parties present.\" This principle applies with force in all issues in criminal matters, and to factual issues: courts seldom engage in fact gathering on their own initiative, but decide facts on the evidence presented (even here, there are exceptions, for \"legislative facts\" as opposed to \"adjudicative facts\"). On the other hand, on issues of law, courts regularly raise new issues (such as matters of jurisdiction or standing), perform independent research, and reformulate the legal grounds on which to analyze the facts presented to them. The United States Supreme Court regularly decides based on issues raised only in amicus briefs from non-parties. One of the most notable such cases was \"Erie Railroad v. Tompkins\", a 1938 case in which neither party questioned the ruling from the 1842 case \"Swift v. Tyson\" that served as the foundation for their arguments, but which led the Supreme Court to overturn \"Swift\" during their deliberations. To avoid lack of notice, courts may invite briefing on an issue to ensure adequate notice. However, there are limits\u2014an appeals court may not introduce a theory that contradicts the party's own contentions.\nThere are many exceptions in both directions. For example, most proceedings before U.S. federal and state agencies are inquisitorial in nature, at least the initial stages (\"e.g.\", a patent examiner, a social security hearing officer, and so on), even though the law to be applied is developed through common law processes.\nContrasting role of treatises and academic writings in common law and civil law systems.\nThe role of the legal academy presents a significant \"cultural\" difference between common law (connotation 2) and civil law jurisdictions. In both systems, treatises compile decisions and state overarching principles that (in the author's opinion) explain the results of the cases. In neither system are treatises considered \"law,\" but the weight given them is nonetheless quite different.\nIn common law jurisdictions, lawyers and judges tend to use these treatises as only \"finding aids\" to locate the relevant cases. In common law jurisdictions, scholarly work is seldom cited as authority for what the law is. Chief Justice Roberts noted the \"great disconnect between the academy and the profession.\" When common law courts rely on scholarly work, it is almost always only for factual findings, policy justification, or the history and evolution of the law, but the court's legal conclusion is reached through analysis of relevant statutes and common law, seldom scholarly commentary.\nIn contrast, in civil law jurisdictions, courts give the writings of law professors significant weight, partly because civil law decisions traditionally were very brief, sometimes no more than a paragraph stating who wins and who loses. The rationale had to come from somewhere else: the academy often filled that role.\nNarrowing of differences between common law and civil law.\nThe contrast between civil law and common law legal systems has become increasingly blurred, with the growing importance of jurisprudence (similar to case law but not binding) in civil law countries, and the growing importance of statute law and codes in common law countries.\nExamples of common law being replaced by statute or codified rule in the United States include criminal law (since 1812, U.S. federal courts and most but not all of the states have held that criminal law must be embodied in statute if the public is to have fair notice), commercial law (the Uniform Commercial Code in the early 1960s) and procedure (the Federal Rules of Civil Procedure in the 1930s and the Federal Rules of Evidence in the 1970s). But in each case, the statute sets the general principles, but the interstitial common law process determines the scope and application of the statute.\nAn example of convergence from the other direction is shown in the 1982 decision \"Srl CILFIT and Lanificio di Gavardo SpA v Ministry of Health\" (ECLI:EU:C:1982:335), in which the European Court of Justice held that questions it has already answered need not be resubmitted. This showed how a historically distinctly common law principle is used by a court composed of judges (at that time) of essentially civil law jurisdiction.\nOther alternatives.\nThe former Soviet Bloc and other socialist countries used a socialist law system, although there is controversy as to whether socialist law ever constituted a separate legal system or not.\nMuch of the Muslim world uses legal systems based on Sharia (also called Islamic law).\nMany churches use a system of canon law. The canon law of the Catholic Church influenced the common law during the medieval period through its preservation of Roman law doctrine such as the presumption of innocence.\nCommon law legal systems in the present day.\nIn jurisdictions around the world.\nThe common law constitutes the basis of the legal systems of:\nand many other generally English-speaking countries or Commonwealth countries (except Scotland, which is bijuridicial, and Malta). Essentially, every country that was colonised at some time by England, Great Britain, or the United Kingdom uses common law except those that were formerly colonised by other nations, such as Quebec (which follows the bijuridicial law or civil code of France in part), South Africa and Sri Lanka (which follow Roman Dutch law), where the prior civil law system was retained to respect the civil rights of the local colonists. Guyana and Saint Lucia have mixed common law and civil law systems.\nThe remainder of this section discusses jurisdiction-specific variants, arranged chronologically.\nScotland.\nScotland is often said to use the civil law system, but it has a unique system that combines elements of an uncodified civil law dating back to the with an element of its own common law long predating the Treaty of Union with England in 1707 (see Legal institutions of Scotland in the High Middle Ages), founded on the customary laws of the tribes residing there. Historically, Scottish common law differed in that the use of \"precedent\" was subject to the courts' seeking to discover the principle that justifies a law rather than searching for an example as a \"precedent\", and principles of natural justice and fairness have always played a role in Scots Law. From the 19th century, the Scottish approach to precedent developed into a \"stare decisis\" akin to that already established in England thereby reflecting a narrower, more modern approach to the application of case law in subsequent instances. This is not to say that the substantive rules of the common laws of both countries are the same, but in many matters (particularly those of UK-wide interest), they are similar.\nScotland shares the Supreme Court with England, Wales and Northern Ireland for civil cases; the court's decisions are binding on the jurisdiction from which a case arises but only influential on similar cases arising in Scotland. This has had the effect of converging the law in certain areas. For instance, the modern UK law of negligence is based on \"Donoghue v Stevenson\", a case originating in Paisley, Scotland.\nScotland maintains a separate criminal law system from the rest of the UK, with the High Court of Justiciary being the final court for criminal appeals. The highest court of appeal in civil cases brought in Scotland is now the Supreme Court of the United Kingdom (before October 2009, final appellate jurisdiction lay with the House of Lords).\nThe United States \u2013 its states, federal courts, and executive branch agencies (17th century on).\nThe centuries-old authority of the common law courts in England to develop law case by case and to apply statute law\u2014\"legislating from the bench\"\u2014is a traditional function of courts, which was carried over into the U.S. system as an essential component of the judicial power for states. Justice Oliver Wendell Holmes Jr. summarized centuries of history in 1917, \"judges do and must legislate\" (in the federal courts, only interstitially, in state courts, to the full limits of common law adjudicatory authority).\nNew York (17th century).\nThe original colony of New Netherland was settled by the Dutch and the law was also Dutch. When the English captured pre-existing colonies they continued to allow the local settlers to keep their civil law. However, the Dutch settlers revolted against the English and the colony was recaptured by the Dutch. In 1664, the colony of New York had two distinct legal systems: on Manhattan Island and along the Hudson River, sophisticated courts modeled on those of the Netherlands were resolving disputes learnedly in accordance with Dutch customary law. On Long Island, Staten Island, and in Westchester, on the other hand, English courts were administering a crude, untechnical variant of the common law carried from Puritan New England and practiced without the intercession of lawyers. When the English finally regained control of New Netherland they imposed common law upon all the colonists, including the Dutch. This was problematic, as the patroon system of land holding, based on the feudal system and civil law, continued to operate in the colony until it was abolished in the mid-19th century. New York began a codification of its law in the 19th century. The only part of this codification process that was considered complete is known as the Field Code applying to civil procedure. The influence of Roman-Dutch law continued in the colony well into the late 19th century. The codification of a law of general obligations shows how remnants of the civil law tradition in New York continued on from the Dutch days.\nLouisiana (1700s).\nUnder Louisiana's codified system, the Louisiana Civil Code, private law\u2014that is, substantive law between private sector parties\u2014is based on principles of law from continental Europe, with some common law influences. These principles derive ultimately from Roman law, transmitted through French law and Spanish law, as the state's current territory intersects the area of North America colonized by Spain and by France. Contrary to popular belief, the Louisiana code does not directly derive from the Napoleonic Code, as the latter was enacted in 1804, one year after the Louisiana Purchase. However, the two codes are similar in many respects due to common roots.\nLouisiana's criminal law largely rests on English common law. Louisiana's administrative law is generally similar to the administrative law of the U.S. federal government and other U.S. states. Louisiana's procedural law is generally in line with that of other U.S. states, which in turn is generally based on the U.S. Federal Rules of Civil Procedure.\nHistorically notable among the Louisiana code's differences from common law is the role of property rights among women, particularly in inheritance gained by widows.\nCalifornia (1850s).\nThe U.S. state of California has a system based on common law, but it has codified the law in the manner of civil law jurisdictions. The reason for the enactment of the California Codes in the 19th century was to replace a pre-existing system based on Spanish civil law with a system based on common law, similar to that in most other states. California and a number of other Western states, however, have retained the concept of community property derived from civil law. The California courts have treated portions of the codes as an extension of the common-law tradition, subject to judicial development in the same manner as judge-made common law. (Most notably, in the case \"Li v. Yellow Cab Co.\", 13 Cal.3d 804 (1975), the California Supreme Court adopted the principle of comparative negligence in the face of a California Civil Code provision codifying the traditional common-law doctrine of contributory negligence.)\nUnited States federal courts (1789 and 1938).\nThe United States federal government (as opposed to the states) has a variant on a common law system. United States federal courts only act as interpreters of statutes and the constitution by elaborating and precisely defining broad statutory language (connotation 1(b) above), but, unlike state courts, do not generally act as an independent source of common law.\nBefore 1938, the federal courts, like almost all other common law courts, decided the law on any issue where the relevant legislature (either the U.S. Congress or state legislature, depending on the issue), had not acted, by looking to courts in the same system, that is, other federal courts, even on issues of state law, and even where there was no express grant of authority from Congress or the Constitution.\nIn 1938, the U.S. Supreme Court in \"Erie Railroad Co. v. Tompkins\" 304 U.S. 64, 78 (1938), overruled earlier precedent, and held \"There is no federal general common law,\" thus confining the federal courts to act only as interstitial interpreters of law originating elsewhere. \"E.g.\", \"Texas Industries v. Radcliff\", 451 U.S. 630 (1981) (without an express grant of statutory authority, federal courts cannot create rules of intuitive justice, for example, a right to contribution from co-conspirators). Post-1938, federal courts deciding issues that arise under state law are required to defer to state court interpretations of state statutes, or reason what a state's highest court would rule if presented with the issue, or to certify the question to the state's highest court for resolution.\nLater courts have limited \"Erie\" slightly, to create a few situations where United States federal courts are permitted to create federal common law rules without express statutory authority, for example, where a federal rule of decision is necessary to protect uniquely federal interests, such as foreign affairs, or financial instruments issued by the federal government. \"See, e.g.\", \"Clearfield Trust Co. v. United States\", 318 U.S. 363 (1943) (giving federal courts the authority to fashion common law rules with respect to issues of federal power, in this case negotiable instruments backed by the federal government); \"see also\" \"International News Service v. Associated Press\", 248 U.S. 215 (1918) (creating a cause of action for misappropriation of \"hot news\" that lacks any statutory grounding); \"but see National Basketball Association v. Motorola, Inc.\", 105 F.3d 841, 843\u201344, 853 (2d Cir. 1997) (noting continued vitality of \"INS\" \"hot news\" tort under New York state law, but leaving open the question of whether it survives under federal law). Except on Constitutional issues, Congress is free to legislatively overrule federal courts' common law.\nUnited States executive branch agencies (1946).\nMost executive branch agencies in the United States federal government have some adjudicatory authority. To greater or lesser extent, agencies honor their own precedent to ensure consistent results. Agency decision making is governed by the Administrative Procedure Act of 1946.\nFor example, the National Labor Relations Board issues relatively few regulations, but instead promulgates most of its substantive rules through common law (connotation 1).\nIndia, Pakistan, and Bangladesh (19th century and 1948).\nThe law of India, Pakistan, and Bangladesh are largely based on English common law because of the long period of British colonial influence during the period of the British Raj.\nAncient India represented a distinct tradition of law, and had a historically independent school of legal theory and practice. The \"Arthashastra\", dating from 400 BCE and the \"Manusmriti\", from 100 CE, were influential treatises in India, texts that were considered authoritative legal guidance. Manu's central philosophy was tolerance and pluralism, and was cited across Southeast Asia. Early in this period, which finally culminated in the creation of the Gupta Empire, relations with ancient Greece and Rome were not infrequent. The appearance of similar fundamental institutions of international law in various parts of the world show that they are inherent in international society, irrespective of culture and tradition. Inter-State relations in the pre-Islamic period resulted in clear-cut rules of warfare of a high humanitarian standard, in rules of neutrality, of treaty law, of customary law embodied in religious charters, in exchange of embassies of a temporary or semi-permanent character.\nWhen India became part of the British Empire, there was a break in tradition, and Hindu and Islamic law were supplanted by the common law. After the failed rebellion against the British in 1857, the British Parliament took over control of India from the British East India Company, and British India came under the direct rule of the Crown. The British Parliament passed the Government of India Act 1858 to this effect, which set up the structure of British government in India. It established in Britain the office of the Secretary of State for India through whom the Parliament would exercise its rule, along with a Council of India to aid him. It also established the office of the Governor-General of India along with an Executive Council in India, which consisted of high officials of the British Government. As a result, the present judicial system of the country derives largely from the British system and has little correlation to the institutions of the pre-British era.\nPost-partition India (1948).\nPost-partition, India retained its common law system. Much of contemporary Indian law shows substantial European and American influence. Legislation first introduced by the British is still in effect in modified form today. During the drafting of the Indian Constitution, laws from Ireland, the United States, Britain, and France were all synthesized to produce a refined set of Indian laws. Indian laws also adhere to the United Nations guidelines on human rights law and environmental law. Certain international trade laws, such as those on intellectual property, are also enforced in India.\nPost-partition Pakistan (1948).\nPost-partition, Pakistan retained its common law system.\nPost-partition Bangladesh (1968).\nPost-partition, Bangladesh retained its common law system.\nCanada (1867).\nCanada has separate federal and provincial legal systems.\nCanadian provincial legal systems.\nEach province and territory is considered a separate jurisdiction with respect to case law. Each has its own procedural law in civil matters, statutorily created provincial courts and superior trial courts with inherent jurisdiction culminating in the Court of Appeal of the province. These Courts of Appeal are then subject to the Supreme Court of Canada in terms of appeal of their decisions.\nAll but one of the provinces of Canada use a common law system for civil matters (the exception being Quebec, which uses a French-heritage civil law system for issues arising within provincial jurisdiction, such as property ownership and contracts).\nCanadian federal legal system.\nCanadian Federal Courts operate under a separate system throughout Canada and deal with narrower range of subject matter than superior courts in each province and territory. They only hear cases on subjects assigned to them by federal statutes, such as immigration, intellectual property, judicial review of federal government decisions, and admiralty. The Federal Court of Appeal is the appellate court for federal courts and hears cases in multiple cities; unlike the United States, the Canadian Federal Court of Appeal is not divided into appellate circuits.\nCanadian federal statutes must use the terminology of both the common law and civil law for civil matters; this is referred to as legislative bijuralism.\nCanadian criminal law.\nCriminal law is uniform throughout Canada. It is based on the federal statutory Criminal Code, which in addition to substance also details procedural law. The administration of justice are the responsibilities of the provinces. Canadian criminal law uses a common law system no matter which province a case proceeds.\nNicaragua.\nNicaragua's legal system is also a mixture of the English Common Law and Civil Law. This situation was brought through the influence of British administration of the Eastern half of the Mosquito Coast from the mid-17th century until about 1894, the William Walker period from about 1855 through 1857, US interventions/occupations during the period from 1909 to 1933, the influence of US institutions during the Somoza family administrations (1933 through 1979) and the considerable importation between 1979 and the present of US culture and institutions.\nIsrael (1948).\nIsrael has no formal written constitution. Its basic principles are inherited from the law of the British Mandate of Palestine and thus resemble those of British and American law, namely: the role of courts in creating the body of law and the authority of the supreme court in reviewing and if necessary overturning legislative and executive decisions, as well as employing the adversarial system. However, because Israel has no written constitution, basic laws can be changed by a vote of 61 out of 120 votes in the parliament. One of the primary reasons that the Israeli constitution remains unwritten is the fear by whatever party holds power that creating a written constitution, combined with the common-law elements, would severely limit the powers of the Knesset (which, following the doctrine of parliamentary sovereignty, holds near-unlimited power).\nRoman Dutch common law.\nRoman Dutch common law is a bijuridical or mixed system of law similar to the common law system in Scotland and Louisiana. Roman Dutch common law jurisdictions include South Africa, Botswana, Lesotho, Namibia, Swaziland, Sri Lanka and Zimbabwe. Many of these jurisdictions recognise customary law, and in some, such as South Africa the Constitution requires that the common law be developed in accordance with the Bill of Rights. Roman Dutch common law is a development of Roman Dutch law by courts in the Roman Dutch common law jurisdictions. During the Napoleonic wars the Kingdom of the Netherlands adopted the French \"code civil\" in 1809, however the Dutch colonies in the Cape of Good Hope and Sri Lanka, at the time called Ceylon, were seized by the British to prevent them being used as bases by the French Navy. The system was developed by the courts and spread with the expansion of British colonies in Southern Africa. Roman Dutch common law relies on legal principles set out in Roman law sources such as Justinian's Institutes and Digest, and also on the writing of Dutch jurists of the 17th century such as Grotius and Voet. In practice, the majority of decisions rely on recent precedent.\nGhana.\nGhana follows the English common-law tradition which was inherited from the British during her colonisation. Consequently, the laws of Ghana are, for the most part, a modified version of imported law that is continuously adapting to changing socio-economic and political realities of the country. The Bond of 1844 marked the period when the people of Ghana (then Gold Coast) ceded their independence to the British and gave the British judicial authority. Later, the Supreme Court Ordinance of 1876 formally introduced British law, be it the common law or statutory law, in the Gold Coast. Section 14 of the Ordinance formalised the application of the common-law tradition in the country.\nGhana, after independence, did not do away with the common law system inherited from the British, and today it has been enshrined in the 1992 Constitution of the country. Chapter four of Ghana's Constitution, entitled \"The Laws of Ghana\", has in Article 11(1) the list of laws applicable in the state. This comprises (a) the Constitution; (b) enactments made by or under the authority of the Parliament established by the Constitution; (c) any Orders, Rules and Regulations made by any person or authority under a power conferred by the Constitution; (d) the existing law; and (e) the common law. Thus, the modern-day Constitution of Ghana, like those before it, embraced the English common law by entrenching it in its provisions. The doctrine of judicial precedence which is based on the principle of \"stare decisis\" as applied in England and other pure common law countries also applies in Ghana.\nScholarly works.\nEdward Coke, a 17th-century Lord Chief Justice of the English Court of Common Pleas and a Member of Parliament (MP), wrote several legal texts that collected and integrated centuries of case law. Lawyers in both England and America learned the law from his \"Institutes\" and \"Reports\" until the end of the 18th century. His works are still cited by common law courts around the world.\nThe next definitive historical treatise on the common law is \"Commentaries on the Laws of England\", written by Sir William Blackstone and first published in 1765\u20131769. Since 1979, a facsimile edition of that first edition has been available in four paper-bound volumes. Today it has been superseded in the English part of the United Kingdom by Halsbury's Laws of England that covers both common and statutory English law.\nWhile he was still on the Massachusetts Supreme Judicial Court, and before being named to the U.S. Supreme Court, Justice Oliver Wendell Holmes Jr. published a short volume called \"The Common Law\", which remains a classic in the field. Unlike Blackstone and the Restatements, Holmes' book only briefly discusses what the law \"is\"; rather, Holmes describes the common law \"process\". Law professor John Chipman Gray's \"The Nature and Sources of the Law\", an examination and survey of the common law, is also still commonly read in U.S. law schools.\nIn the United States, Restatements of various subject matter areas (Contracts, Torts, Judgments, and so on.), edited by the American Law Institute, collect the common law for the area. The ALI Restatements are often cited by American courts and lawyers for propositions of uncodified common law, and are considered highly persuasive authority, just below binding precedential decisions. The Corpus Juris Secundum is an encyclopedia whose main content is a compendium of the common law and its variations throughout the various state jurisdictions.\nScots \"common law\" covers matters including murder and theft, and has sources in custom, in legal writings and previous court decisions. The legal writings used are called \"Institutional Texts\" and come mostly from the 17th, 18th and 19th centuries. Examples include Craig, \"Jus Feudale\" (1655) and Stair, \"The Institutions of the Law of Scotland\" (1681).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5255", "revid": "359256", "url": "https://en.wikipedia.org/wiki?curid=5255", "title": "Civil law", "text": "Civil law may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5257", "revid": "36064032", "url": "https://en.wikipedia.org/wiki?curid=5257", "title": "Court of appeals (disambiguation)", "text": "A court of appeals is generally an appellate court.\nCourt of Appeals may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5258", "revid": "2135234", "url": "https://en.wikipedia.org/wiki?curid=5258", "title": "Computer Storage", "text": ""}
{"id": "5259", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5259", "title": "Common descent", "text": "Characteristic of a group of organisms with a common ancestor\nCommon descent is a concept in evolutionary biology applicable when one species is the ancestor of two or more species later in time. According to modern evolutionary biology, all living beings could be descendants of a unique ancestor commonly referred to as the last universal common ancestor (LUCA) of all life on Earth.\nCommon descent is an effect of speciation, in which multiple species derive from a single ancestral population. The more recent the ancestral population two species have in common, the more closely are they related. The most recent common ancestor of all currently living organisms is the last universal ancestor, which lived about 3.9 billion years ago. The two earliest pieces of evidence for life on Earth are graphite found to be biogenic in 3.7 billion-year-old metasedimentary rocks discovered in western Greenland and microbial mat fossils found in 3.48 billion-year-old sandstone discovered in Western Australia. All currently living organisms on Earth share a common genetic heritage, though the suggestion of substantial horizontal gene transfer during early evolution has led to questions about the monophyly (single ancestry) of life. 6,331 groups of genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.\nUniversal common descent through an evolutionary process was first proposed by the British naturalist Charles Darwin in the concluding sentence of his 1859 book \"On the Origin of Species\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nHistory.\nThe idea that all living things (including things considered non-living by science) are related is a recurring theme in many indigenous worldviews across the world. Later on, in the 1740s, the French mathematician Pierre Louis Maupertuis arrived at the idea that all organisms had a common ancestor, and had diverged through random variation and natural selection. In \"Essai de cosmologie\" (1750), Maupertuis noted:\nMay we not say that, in the fortuitous combination of the productions of Nature, since only those creatures \"could\" survive in whose organizations a certain degree of adaptation was present, there is nothing extraordinary in the fact that such adaptation is actually found in all these species which now exist? Chance, one might say, turned out a vast number of individuals; a small proportion of these were organized in such a manner that the animals' organs could satisfy their needs. A much greater number showed neither adaptation nor order; these last have all perished... Thus the species which we see today are but a small part of all those that a blind destiny has produced.\nIn 1790, the philosopher Immanuel Kant wrote in \"Kritik der Urteilskraft\" (\"Critique of Judgment\") that the similarity of animal forms implies a common original type, and thus a common parent.\nIn 1794, Charles Darwin's grandfather, Erasmus Darwin asked:\n[W]ould it be too bold to imagine, that in the great length of time, since the earth began to exist, perhaps millions of ages before the commencement of the history of mankind, would it be too bold to imagine, that all warm-blooded animals have arisen from one living filament, which the great First Cause endued with animality, with the power of acquiring new parts attended with new propensities, directed by irritations, sensations, volitions, and associations; and thus possessing the faculty of continuing to improve by its own inherent activity, and of delivering down those improvements by generation to its posterity, world without end?\nCharles Darwin's views about common descent, as expressed in \"On the Origin of Species\", were that it was probable that there was only one progenitor for all life forms:\nTherefore I should infer from analogy that probably all the organic beings which have ever lived on this earth have descended from some one primordial form, into which life was first breathed.\nBut he precedes that remark by, \"Analogy would lead me one step further, namely, to the belief that all animals and plants have descended from some one prototype. But analogy may be a deceitful guide.\" And in the subsequent edition, he asserts rather, \"We do not know all the possible transitional gradations between the simplest and the most perfect organs; it cannot be pretended that we know all the varied means of Distribution during the long lapse of years, or that we know how imperfect the Geological Record is. Grave as these several difficulties are, in my judgment they do not overthrow the theory of descent from a few created forms with subsequent modification\". \nCommon descent was widely accepted amongst the scientific community after Darwin's publication. In 1907, Vernon Kellogg commented that \"practically no naturalists of position and recognized attainment doubt the theory of descent.\"\nIn 2008, biologist T. Ryan Gregory noted that:\nNo reliable observation has ever been found to contradict the general notion of common descent. It should come as no surprise, then, that the scientific community at large has accepted evolutionary descent as a historical reality since Darwin\u2019s time and considers it among the most reliably established and fundamentally important facts in all of science.\nEvidence.\nCommon biochemistry.\nAll known forms of life are based on the same fundamental biochemical organization: genetic information encoded in DNA, transcribed into RNA, through the effect of protein- and RNA-enzymes, then translated into proteins by (highly similar) ribosomes, with ATP, NADPH and others as energy sources. Analysis of small sequence differences in widely shared substances such as cytochrome c further supports universal common descent. Some 23 proteins are found in all organisms, serving as enzymes carrying out core functions like DNA replication. The fact that only one such set of enzymes exists is convincing evidence of a single ancestry. 6,331 genes common to all living animals have been identified; these may have arisen from a single common ancestor that lived 650 million years ago in the Precambrian.\nCommon genetic code.\nThe genetic code (the \"translation table\" according to which DNA information is translated into amino acids, and hence proteins) is nearly identical for all known lifeforms, from bacteria and archaea to animals and plants. The universality of this code is generally regarded by biologists as definitive evidence in favor of universal common descent.\nThe way that codons (DNA triplets) are mapped to amino acids seems to be strongly optimised. Richard Egel argues that in particular the hydrophobic (non-polar) side-chains are well organised, suggesting that these enabled the earliest organisms to create peptides with water-repelling regions able to support the essential electron exchange (redox) reactions for energy transfer.\nSelectively neutral similarities.\nSimilarities which have no adaptive relevance cannot be explained by convergent evolution, and therefore they provide compelling support for universal common descent. Such evidence has come from two areas: amino acid sequences and DNA sequences. Proteins with the same three-dimensional structure need not have identical amino acid sequences; any irrelevant similarity between the sequences is evidence for common descent. In certain cases, there are several codons (DNA triplets) that code redundantly for the same amino acid. Since many species use the same codon at the same place to specify an amino acid that can be represented by more than one codon, that is evidence for their sharing a recent common ancestor. Had the amino acid sequences come from different ancestors, they would have been coded for by any of the redundant codons, and since the correct amino acids would already have been in place, natural selection would not have driven any change in the codons, however much time was available. Genetic drift could change the codons, but it would be extremely unlikely to make all the redundant codons in a whole sequence match exactly across multiple lineages. Similarly, shared nucleotide sequences, especially where these are apparently neutral such as the positioning of introns and pseudogenes, provide strong evidence of common ancestry.\nOther similarities.\nBiologists often point to the universality of many aspects of cellular life as supportive evidence to the more compelling evidence listed above. These similarities include the energy carrier adenosine triphosphate (ATP), and the fact that all amino acids found in proteins are left-handed. It is, however, possible that these similarities resulted because of the laws of physics and chemistry - rather than through universal common descent - and therefore resulted in convergent evolution. In contrast, there is evidence for homology of the central subunits of Transmembrane ATPases throughout all living organisms, especially how the rotating elements are bound to the membrane. This supports the assumption of a LUCA as a cellular organism, although primordial membranes may have been semipermeable and evolved later to the membranes of modern bacteria, and on a second path to those of modern archaea also.\nPhylogenetic trees.\nAnother important piece of evidence is from detailed phylogenetic trees (i.e., \"genealogic trees\" of species) mapping out the proposed divisions and common ancestors of all living species. In 2010, Douglas L. Theobald published a statistical analysis of available genetic data, mapping them to phylogenetic trees, that gave \"strong quantitative support, by a formal test, for the unity of life.\"\nTraditionally, these trees have been built using morphological methods, such as appearance, embryology, etc. Recently, it has been possible to construct these trees using molecular data, based on similarities and differences between genetic and protein sequences. All these methods produce essentially similar results, even though most genetic variation has no influence over external morphology. That phylogenetic trees based on different types of information agree with each other is strong evidence of a real underlying common descent.\nObjections.\nGene exchange clouds phylogenetic analysis.\nTheobald noted that substantial horizontal gene transfer could have occurred during early evolution. Bacteria today remain capable of gene exchange between distantly-related lineages. This weakens the basic assumption of phylogenetic analysis, that similarity of genomes implies common ancestry, because sufficient gene exchange would allow lineages to share much of their genome whether or not they shared an ancestor (monophyly). This has led to questions about the single ancestry of life. However, biologists consider it very unlikely that completely unrelated proto-organisms could have exchanged genes, as their different coding mechanisms would have resulted only in garble rather than functioning systems. Later, however, many organisms all derived from a single ancestor could readily have shared genes that all worked in the same way, and it appears that they have.\nConvergent evolution.\nIf early organisms had been driven by the same environmental conditions to evolve similar biochemistry convergently, they might independently have acquired similar genetic sequences. Theobald's \"formal test\" was accordingly criticised by Takahiro Yonezawa and colleagues for not including consideration of convergence. They argued that Theobald's test was insufficient to distinguish between the competing hypotheses. Theobald has defended his method against this claim, arguing that his tests distinguish between phylogenetic structure and mere sequence similarity. Therefore, Theobald argued, his results show that \"real universally conserved proteins are homologous.\"\nRNA world.\nThe possibility is mentioned, above, that all living organisms may be descended from an original single-celled organism with a DNA genome, and that this implies a single origin for life. Although such a universal common ancestor may have existed, such a complex entity is unlikely to have arisen spontaneously from non-life and thus a cell with a DNA genome cannot reasonably be regarded as the \u201corigin\u201d of life. To understand the \u201corigin\u201d of life, it has been proposed that DNA based cellular life descended from relatively simple pre-cellular self-replicating RNA molecules able to undergo natural selection (see RNA world). During the course of evolution, this RNA world was replaced by the evolutionary emergence of the DNA world. A world of independently self-replicating RNA genomes apparently no longer exists (RNA viruses are dependent on host cells with DNA genomes). Because the RNA world is apparently gone, it is not clear how scientific evidence could be brought to bear on the question of whether there was a single \u201corigin\u201d of life event from which all life descended.\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5261", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=5261", "title": "Celtic music", "text": "Grouping of folk music genres\nCeltic music is a broad grouping of music genres that evolved out of the folk music traditions of the Celtic people of Northwestern Europe. It refers to both orally-transmitted traditional music and recorded music and the styles vary considerably to include everything from traditional music to a wide range of hybrids.\nDescription and definition.\n\"Celtic music\" means two things mainly. First, it is the music of the people that identify themselves as Celts. Secondly, it refers to whatever qualities may be unique to the music of the Celtic nations. Many notable Celtic musicians such as Alan Stivell and Paddy Moloney claim that the different Celtic music genres have a lot in common.\nThese following melodic practices may be used widely across the different variants of Celtic Music:\nThese two latter usage patterns may simply be remnants of formerly widespread melodic practices.\nOften, the term \"Celtic music\" is applied to the music of Ireland and Scotland because both lands have produced well-known distinctive styles which actually have genuine commonality and clear mutual influences. The definition is further complicated by the fact that Irish independence has allowed Ireland to promote 'Celtic' music as a specifically Irish product. However, these are modern geographical references to a people who share a common Celtic ancestry and consequently, a common musical heritage.\nThese styles are known because of the importance of Irish and Scottish people in the English speaking world, especially in the United States, where they had a profound impact on American music, particularly bluegrass and country music. The music of Wales, Cornwall, the Isle of Man, Brittany, Galician traditional music (Spain) and music of Portugal are also considered Celtic music, the tradition being particularly strong in Brittany, where Celtic festivals large and small take place throughout the year, and in Wales, where the ancient eisteddfod tradition has been revived and flourishes. Additionally, the musics of ethnically Celtic peoples abroad are vibrant, especially in Canada and the United States. In Canada the provinces of Atlantic Canada are known for being a home of Celtic music, most notably on the islands of Newfoundland, Cape Breton and Prince Edward Island. The traditional music of Atlantic Canada is heavily influenced by the Irish, Scottish and Acadian ethnic makeup of much of the region's communities. In some parts of Atlantic Canada, such as Newfoundland, Celtic music is as or more popular than in the old country. Further, some older forms of Celtic music that are rare in Scotland and Ireland today, such as the practice of accompanying a fiddle with a piano, or the Gaelic spinning songs of Cape Breton remain common in the Maritimes. Much of the music of this region is Celtic in nature, but originates in the local area and celebrates the sea, seafaring, fishing and other primary industries.\nDivisions.\nIn \"Celtic Music: A Complete Guide\", June Skinner Sawyers acknowledges six Celtic nationalities divided into two groups according to their linguistic heritage. The Q-Celtic nationalities are the Irish, Scottish and Manx peoples, while the P-Celtic groups are the Cornish, Bretons and Welsh peoples. Musician Alan Stivell uses a similar dichotomy, between the Gaelic (Irish/Scottish/Manx) and the Brythonic (Breton/Welsh/Cornish) branches, which differentiate \"mostly by the extended range (sometimes more than two octaves) of Irish and Scottish melodies and the closed range of Breton and Welsh melodies (often reduced to a half-octave), and by the frequent use of the pure pentatonic scale in Gaelic music.\"\nThere is also tremendous variation between \"Celtic\" regions. Ireland, Scotland, Wales, Cornwall, and Brittany have living traditions of language and music, and there has been a recent major revival of interest in Celtic heritage in the Isle of Man. Galicia has a Celtic language revival movement to revive the Q-Celtic \"Gallaic language\" used into Roman times., which is not an attested language unlike Celtiberian. A Brythonic language may have been spoken in parts of Galicia and Asturias into early Medieval times brought by Britons fleeing the Anglo-Saxon invasions via Brittany., but here again there are several hypothesis and very little traces of it : lack of archeological, linguistic evidence and documents. The Romance language currently spoken in Galicia, Galician (\"Galego\") is closely related to the Portuguese language used mainly in Brazil and Portugal and in many ways closer to Latin than other Romance languages. Galician music is claimed to be \"Celtic\". The same is true of the music of Asturias, Cantabria, and that of Northern Portugal (some say even traditional music from Central Portugal can be labeled Celtic).\nBreton artist Alan Stivell was one of the earliest musicians to use the word \"Celtic\" and \"Keltia\" in his marketing materials, starting in the early 1960s as part of the worldwide folk music revival of that era with the term quickly catching on with other artists worldwide. Today, the genre is well established and incredibly diverse.\nForms.\nThere are musical genres and styles specific to each Celtic country, due in part to the influence of individual song traditions and the characteristics of specific languages:\n\"See list of Celtic festivals for a more complete list of Celtic festivals by country, including music festivals. Festivals focused largely or partly on Celtic music can be found at .\"\nFestivals.\nThe modern Celtic music scene involves a large number of music festivals, as it has traditionally. Some of the most prominent festivals focused solely on music include:\nCeltic fusion.\nThe oldest musical tradition which fits under the label of Celtic fusion originated in the rural American south in the early colonial period and incorporated English, Scottish, Irish, Welsh, German, and African influences. Variously referred to as roots music, American folk music, or old-time music, this tradition has exerted a strong influence on all forms of American music, including country, blues, and rock and roll. In addition to its lasting effects on other genres, it marked the first modern large-scale mixing of musical traditions from multiple ethnic and religious communities within the Celtic diaspora.\nIn the 1960s several bands put forward modern adaptations of Celtic music pulling influences from several of the Celtic nations at once to create a modern pan-celtic sound. A few of those include bagado\u00f9 (Breton pipe bands), Fairport Convention, Pentangle, Steeleye Span and Horslips.\nIn the 1970s Clannad made their mark initially in the folk and traditional scene, and then subsequently went on to bridge the gap between traditional Celtic and pop music in the 1980s and 1990s, incorporating elements from new-age, smooth jazz, and folk rock. Traces of Clannad's legacy can be heard in the music of many artists, including Enya, Donna Taggart, Altan, Capercaillie, The Corrs, Loreena McKennitt, An\u00fana, Riverdance and U2. The solo music of Clannad's lead singer, Moya Brennan (often referred to as the First Lady of Celtic Music) has further enhanced this influence.\nLater, beginning in 1982 with The Pogues' invention of Celtic folk-punk and Stockton's Wing blend of Irish traditional and Pop, Rock and Reggae, there has been a movement to incorporate Celtic influences into other genres of music. Bands like Flogging Molly, Black 47, Dropkick Murphys, The Young Dubliners, The Tossers introduced a hybrid of Celtic rock, punk, reggae, hardcore and other elements in the 1990s that has become popular with Irish-American youth.\nToday there are Celtic-influenced subgenres of virtually every type of popular music including electronica, rock, metal, punk, hip hop, reggae, new-age, Latin, Andean and pop. Collectively these modern interpretations of Celtic music are sometimes referred to as Celtic fusion.\nOther modern adaptations.\nOutside of America, the first deliberate attempts to create a \"Pan-Celtic music\" were made by the Breton Taldir Jaffrennou, having translated songs from Ireland, Scotland, and Wales into Breton between the two world wars. One of his major works was to bring \"Hen Wlad Fy Nhadau\" (the Welsh national anthem) back in Brittany and create lyrics in Breton. Eventually this song became \"Bro goz va zado\u00f9\" (\"Old land of my fathers\") and is the most widely accepted Breton anthem. In the 70s, the Breton Alan Cochevelou (future Alan Stivell) began playing a mixed repertoire from the main Celtic countries on the Celtic harp his father created. \nProbably the most successful all-inclusive Celtic music composition in recent years is Shaun Daveys composition 'The Pilgrim'. This suite depicts the journey of St. Colum Cille through the Celtic nations of Ireland, Scotland, the Isle of Man, Wales, Cornwall, Brittany and Galicia. The suite which includes a Scottish pipe band, Irish and Welsh harpists, Galician gaitas, Irish uilleann pipes, the bombardes of Brittany, two vocal soloists and a narrator is set against a background of a classical orchestra and a large choir.\nModern music may also be termed \"Celtic\" because it is written and recorded in a Celtic language, regardless of musical style. Many of the Celtic languages have experienced resurgences in modern years, spurred on partly by the action of artists and musicians who have embraced them as hallmarks of identity and distinctness. In 1971, the Irish band \"Skara Brae\" recorded its only LP (simply called \"Skara Brae\"), all songs in Irish. In 1978 Runrig recorded an album in Scottish Gaelic. In 1992 Capercaillie recorded \"A Prince Among Islands\", the first Scottish Gaelic language record to reach the UK top 40. In 1996, a song in Breton represented France in the 41st Eurovision Song Contest, the first time in history that France had a song without a word in French. Since about 2005, Oi Polloi (from Scotland) have recorded in Scottish Gaelic. Mill a h-Uile Rud (a Scottish Gaelic punk band from Seattle) recorded in the language in 2004.\nSeveral contemporary bands have Welsh language songs, such as Ceredwen, which fuses traditional instruments with trip hop beats, the Super Furry Animals, Fernhill, and so on (see the Music of Wales article for more Welsh and Welsh-language bands). The same phenomenon occurs in Brittany, where many singers record songs in Breton, traditional or modern (hip hop, rap, and so on.).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5264", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5264", "title": "Computer Monitor", "text": ""}
{"id": "5267", "revid": "1159550815", "url": "https://en.wikipedia.org/wiki?curid=5267", "title": "Constellation", "text": "Group of stars forming a pattern on the celestial sphere\nA constellation is an area on the celestial sphere in which a group of visible stars forms a perceived pattern or outline, typically representing an animal, mythological subject, or inanimate object.\nThe origins of the earliest constellations likely go back to prehistory. People used them to relate stories of their beliefs, experiences, creation, or mythology. Different cultures and countries adopted their own constellations, some of which lasted into the early 20th century before today's constellations were internationally recognized. The recognition of constellations has changed significantly over time. Many changed in size or shape. Some became popular, only to drop into obscurity. Some were limited to a single culture or nation. Naming constellations also helped astronomers and navigators identify stars more easily.\nTwelve (or thirteen) ancient constellations belong to the zodiac (straddling the ecliptic, which the Sun, Moon, and planets all traverse). The origins of the zodiac remain historically uncertain; its astrological divisions became prominent c. 400 BC in Babylonian or Chaldean astronomy. Constellations appear in Western culture via Greece and are mentioned in the works of Hesiodus, Eudoxus and Aratus. The traditional 48 constellations, consisting of the Zodiac and 36 more (now 38, following the division of Argo Navis in three constellations) are listed by Ptolemy, a Greco-Roman astronomer from Alexandria, Egypt, in his \"Almagest\". The formation of constellations was the subject of extensive mythology, most notably in the \"Metamorphoses\" of Latin poet Ovid. Constellations in the far southern sky were added from the 15th century until the mid-18th century when European explorers began traveling to the Southern Hemisphere. Due to Roman and European transmission each constellation has a Latin name.\nIn 1922, the International Astronomical Union (IAU) formally accepted the modern list of 88 constellations, and in 1928 adopted official constellation boundaries that together cover the entire celestial sphere. Any given point in a celestial coordinate system lies in one of the modern constellations. Some astronomical naming systems include the constellation where a given celestial object is found to convey its approximate location in the sky. The Flamsteed designation of a star, for example, consists of a number and the genitive form of the constellation's name.\nOther star patterns or groups called asterisms are not constellations under the formal definition, but are also used by observers to navigate the night sky. Asterisms may be several stars within a constellation, or they may share stars with more than one constellation. Examples of asterisms include the teapot within the constellation Sagittarius, or the big dipper in the constellation of Ursa Major.\nTerminology.\nThe word \"constellation\" comes from the Late Latin term , which can be translated as \"set of stars\"; it came into use in Middle English during the 14th century. The Ancient Greek word for constellation is \u1f04\u03c3\u03c4\u03c1\u03bf\u03bd (\"astron\"). These terms historically referred to any recognisable pattern of stars whose appearance was associated with mythological characters or creatures, earthbound animals, or objects. Over time, among European astronomers, the constellations became clearly defined and widely recognised. Today, there are 88 IAU designated constellations.\nA constellation or star that never sets below the horizon when viewed from a particular latitude on Earth is termed circumpolar. From the North Pole or South Pole, all constellations south or north of the celestial equator are circumpolar. Depending on the definition, equatorial constellations may include those that lie between declinations 45\u00b0 north and 45\u00b0 south, or those that pass through the declination range of the ecliptic or zodiac ranging between 23\u00bd\u00b0 north, the celestial equator, and 23\u00bd\u00b0 south.\nStars in constellations can appear near each other in the sky, but they usually lie at a variety of distances away from the Earth. Since each star has its own independent motion, all constellations will change slowly over time. After tens to hundreds of thousands of years, familiar outlines will become unrecognizable. Astronomers can predict the past or future constellation outlines by measuring individual stars' common proper motions or cpm by accurate astrometry and their radial velocities by astronomical spectroscopy.\nIdentification.\nThe 88 constellations recognized by the International Astronomical Union as well as those that cultures have recognized throughout history are imagined figures and shapes derived from the patterns of stars in the observable sky. Many officially recognized constellations are based in the imaginations of ancient, Near Eastern and Mediterranean mythologies. H.A. Rey, who wrote popular books on astronomy, pointed out the imaginative nature of the constellations and their mythological, artistic basis, and the practical use of identifying them through definite images, according to the classical names they were given.\nHistory of the early constellations.\nLascaux Caves, Southern France.\nIt has been suggested that the 17,000-year-old cave paintings in Lascaux Southern France depict star constellations such as Taurus, Orion's Belt, and the Pleiades. However, this view is not generally accepted among scientists.\nMesopotamia.\nInscribed stones and clay writing tablets from Mesopotamia (in modern Iraq) dating to 3000 BC provide the earliest generally accepted evidence for humankind's identification of constellations. It seems that the bulk of the Mesopotamian constellations were created within a relatively short interval from around 1300 to 1000 BC. Mesopotamian constellations appeared later in many of the classical Greek constellations.\nAncient Near East.\nThe oldest Babylonian catalogues of stars and constellations date back to the beginning of the Middle Bronze Age, most notably the \"Three Stars Each\" texts and the \"MUL.APIN\", an expanded and revised version based on more accurate observation from around 1000 BC. However, the numerous Sumerian names in these catalogues suggest that they built on older, but otherwise unattested, Sumerian traditions of the Early Bronze Age.\nThe classical Zodiac is a revision of Neo-Babylonian constellations from the 6th century BC. The Greeks adopted the Babylonian constellations in the 4th century BC. Twenty Ptolemaic constellations are from the Ancient Near East. Another ten have the same stars but different names.\nBiblical scholar E. W. Bullinger interpreted some of the creatures mentioned in the books of Ezekiel and Revelation as the middle signs of the four-quarters of the Zodiac, with the Lion as Leo, the Bull as Taurus, the Man representing Aquarius, and the Eagle standing in for Scorpio. The biblical Book of Job also makes reference to a number of constellations, including \"bier\", \"fool\" and \"heap\" (Job 9:9, 38:31\u201332), rendered as \"Arcturus, Orion and Pleiades\" by the KJV, but \"\u2018Ayish\" \"the bier\" actually corresponding to Ursa Major. The term \"Mazzaroth\" , translated as \"a garland of crowns\", is a \"hapax legomenon\" in Job 38:32, and it might refer to the zodiacal constellations.\nClassical antiquity.\nThere is only limited information on ancient Greek constellations, with some fragmentary evidence being found in the \"Works and Days\" of the Greek poet Hesiod, who mentioned the \"heavenly bodies\". Greek astronomy essentially adopted the older Babylonian system in the Hellenistic era, first introduced to Greece by Eudoxus of Cnidus in the 4th century BC. The original work of Eudoxus is lost, but it survives as a versification by Aratus, dating to the 3rd century BC. The most complete existing works dealing with the mythical origins of the constellations are by the Hellenistic writer termed pseudo-Eratosthenes and an early Roman writer styled pseudo-Hyginus. The basis of Western astronomy as taught during Late Antiquity and until the Early Modern period is the \"Almagest\" by Ptolemy, written in the 2nd century.\nIn the Ptolemaic Kingdom, native Egyptian tradition of anthropomorphic figures represented the planets, stars, and various constellations. Some of these were combined with Greek and Babylonian astronomical systems culminating in the Zodiac of Dendera; it remains unclear when this occurred, but most were placed during the Roman period between 2nd to 4th centuries AD. The oldest known depiction of the zodiac showing all the now familiar constellations, along with some original Egyptian constellations, decans, and planets. Ptolemy's \"Almagest\" remained the standard definition of constellations in the medieval period both in Europe and in Islamic astronomy.\nAncient China.\nAncient China had a long tradition of observing celestial phenomena. Nonspecific Chinese star names, later categorized in the twenty-eight mansions, have been found on oracle bones from Anyang, dating back to the middle Shang dynasty. These constellations are some of the most important observations of Chinese sky, attested from the 5th century BC. Parallels to the earliest Babylonian (Sumerian) star catalogues suggest that the ancient Chinese system did not arise independently.\nThree schools of classical Chinese astronomy in the Han period are attributed to astronomers of the earlier Warring States period. The constellations of the three schools were conflated into a single system by Chen Zhuo, an astronomer of the 3rd century (Three Kingdoms period). Chen Zhuo's work has been lost, but information on his system of constellations survives in Tang period records, notably by Qutan Xida. The oldest extant Chinese star chart dates to that period and was preserved as part of the Dunhuang Manuscripts. Native Chinese astronomy flourished during the Song dynasty, and during the Yuan dynasty became increasingly influenced by medieval Islamic astronomy (see Treatise on Astrology of the Kaiyuan Era). As maps were prepared during this period on more scientific lines, they were considered as more reliable.\nA well-known map from the Song period is the Suzhou Astronomical Chart, which was prepared with carvings of stars on the planisphere of the Chinese sky on a stone plate; it is done accurately based on observations, and it shows the supernova of the year of 1054 in Taurus.\nInfluenced by European astronomy during the late Ming dynasty, charts depicted more stars but retained the traditional constellations. Newly observed stars were incorporated as supplementary to old constellations in the southern sky, which did not depict the traditional stars recorded by ancient Chinese astronomers. Further improvements were made during the later part of the Ming dynasty by Xu Guangqi and Johann Adam Schall von Bell, the German Jesuit and was recorded in Chongzhen Lishu (Calendrical Treatise of Chongzhen period, 1628). Traditional Chinese star maps incorporated 23 new constellations with 125 stars of the southern hemisphere of the sky based on the knowledge of Western star charts; with this improvement, the Chinese Sky was integrated with the World astronomy.\nAncient Greece\nA lot of well-known constellations also have histories that connect to ancient Greece.\nEarly modern astronomy.\nHistorically, the origins of the constellations of the northern and southern skies are distinctly different. Most northern constellations date to antiquity, with names based mostly on Classical Greek legends. Evidence of these constellations has survived in the form of star charts, whose oldest representation appears on the statue known as the Farnese Atlas, based perhaps on the star catalogue of the Greek astronomer Hipparchus. Southern constellations are more modern inventions, sometimes as substitutes for ancient constellations (e.g. Argo Navis). Some southern constellations had long names that were shortened to more usable forms; e.g. Musca Australis became simply Musca.\nSome of the early constellations were never universally adopted. Stars were often grouped into constellations differently by different observers, and the arbitrary constellation boundaries often led to confusion as to which constellation a celestial object belonged. Before astronomers delineated precise boundaries (starting in the 19th century), constellations generally appeared as ill-defined regions of the sky. Today they now follow officially accepted designated lines of right ascension and declination based on those defined by Benjamin Gould in epoch 1875.0 in his star catalogue \"Uranometria Argentina\".\nThe 1603 star atlas \"Uranometria\" of Johann Bayer assigned stars to individual constellations and formalized the division by assigning a series of Greek and Latin letters to the stars within each constellation. These are known today as Bayer designations. Subsequent star atlases led to the development of today's accepted modern constellations.\nOrigin of the southern constellations.\nThe southern sky, below about \u221265\u00b0 declination, was only partially catalogued by ancient Babylonians, Egyptians, Greeks, Chinese, and Persian astronomers of the north. The knowledge that northern and southern star patterns differed goes back to Classical writers, who describe, for example, the African circumnavigation expedition commissioned by Egyptian Pharaoh Necho II in c. 600 BC and those of Hanno the Navigator in c. 500 BC.\nThe history of southern constellations is not straightforward. Different groupings and different names were proposed by various observers, some reflecting national traditions or designed to promote various sponsors. Southern constellations were important from the 14th to 16th centuries, when sailors used the stars for celestial navigation. Italian explorers who recorded new southern constellations include Andrea Corsali, Antonio Pigafetta, and Amerigo Vespucci.\nMany of the 88 IAU-recognized constellations in this region first appeared on celestial globes developed in the late 16th century by Petrus Plancius, based mainly on observations of the Dutch navigators Pieter Dirkszoon Keyser and Frederick de Houtman. These became widely known through Johann Bayer's star atlas \"Uranometria\" of 1603. more were created in 1763 by the French astronomer Nicolas Louis de Lacaille, who also split the ancient constellation Argo Navis into three; these new figures appeared in his star catalogue, published in 1756.\nSeveral modern proposals have not survived. The French astronomers Pierre Lemonnier and Joseph Lalande, for example, proposed constellations that were once popular but have since been dropped. The northern constellation Quadrans Muralis survived into the 19th century (when its name was attached to the Quadrantid meteor shower), but is now divided between Bo\u00f6tes and Draco.\n88 modern constellations.\nA list of 88 constellations was produced for the International Astronomical Union in 1922. It is roughly based on the traditional Greek constellations listed by Ptolemy in his \"Almagest\" in the 2nd century and Aratus' work \"Phenomena\", with early modern modifications and additions (most importantly introducing constellations covering the parts of the southern sky unknown to Ptolemy) by Petrus Plancius (1592, 1597/98 and 1613), Johannes Hevelius (1690) and Nicolas Louis de Lacaille (1763), who introduced fourteen new constellations. Lacaille studied the stars of the southern hemisphere from 1751 until 1752 from the Cape of Good Hope, when he was said to have observed more than 10,000 stars using a refracting telescope.\nIn 1922, Henry Norris Russell produced a list of 88 constellations with three-letter abbreviations for them. However, these constellations did not have clear borders between them. In 1928, the International Astronomical Union (IAU) formally accepted 88 modern constellations, with contiguous boundaries along vertical and horizontal lines of right ascension and declination developed by Eugene Delporte that, together, cover the entire celestial sphere; this list was finally published in 1930. Where possible, these modern constellations usually share the names of their Graeco-Roman predecessors, such as Orion, Leo or Scorpius. The aim of this system is area-mapping, i.e. the division of the celestial sphere into contiguous fields. Out of the 88 modern constellations, 36 lie predominantly in the northern sky, and the other 52 predominantly in the southern.\nThe boundaries developed by Delporte used data that originated back to epoch B1875.0, which was when Benjamin A. Gould first made his proposal to designate boundaries for the celestial sphere, a suggestion on which Delporte based his work. The consequence of this early date is that because of the precession of the equinoxes, the borders on a modern star map, such as epoch J2000, are already somewhat skewed and no longer perfectly vertical or horizontal. This effect will increase over the years and centuries to come.\nSymbols.\nThe constellations have no official symbols, though those of the ecliptic may take the signs of the zodiac. Symbols for the other modern constellations, as well as older ones that still occur in modern nomenclature, have occasionally been published.\nDark cloud constellations.\nThe Great Rift, a series of dark patches in the Milky Way, is more visible and striking in the southern hemisphere than in the northern. It vividly stands out when conditions are otherwise so dark that the Milky Way's central region casts shadows on the ground. Some cultures have discerned shapes in these patches and have given names to these \"dark cloud constellations\". Members of the Inca civilization identified various dark areas or dark nebulae in the Milky Way as animals and associated their appearance with the seasonal rains. Australian Aboriginal astronomy also describes dark cloud constellations, the most famous being the \"emu in the sky\" whose head is formed by the Coalsack, a dark nebula, instead of the stars.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5269", "revid": "44151335", "url": "https://en.wikipedia.org/wiki?curid=5269", "title": "Character", "text": "Character or Characters may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5270", "revid": "30243347", "url": "https://en.wikipedia.org/wiki?curid=5270", "title": "Car (disambiguation)", "text": "A car is a wheeled motor vehicle used for transporting passengers.\nCar(s), CAR(s), or The Car(s) may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5272", "revid": "1490139", "url": "https://en.wikipedia.org/wiki?curid=5272", "title": "Printer (computing)", "text": "Computer peripheral that prints text or graphics\n \nIn computing, a printer is a peripheral machine which makes a persistent representation of graphics or text, usually on paper. While most output is human-readable, bar code printers are an example of an expanded use for printers. Different types of printers include 3D printers, inkjet printers, laser printers, and thermal printers.\nHistory.\nThe first computer printer designed was a mechanically driven apparatus by Charles Babbage for his difference engine in the 19th century; however, his mechanical printer design was not built until 2000.\nThe first patented printing mechanism for applying a marking medium to a recording medium or more particularly an electrostatic inking apparatus and a method for electrostatically depositing ink on controlled areas of a receiving medium, was in 1962 by C. R. Winston, Teletype Corporation, using continuous inkjet printing. The ink was a red stamp-pad ink manufactured by Phillips Process Company of Rochester, NY under the name Clear Print. This patent (US3060429) led to the Teletype Inktronic Printer product delivered to customers in late 1966.\nThe first compact, lightweight digital printer was the EP-101, invented by Japanese company Epson and released in 1968, according to Epson.\nThe first commercial printers generally used mechanisms from electric typewriters and Teletype machines. The demand for higher speed led to the development of new systems specifically for computer use. In the 1980s there were daisy wheel systems similar to typewriters, line printers that produced similar output but at much higher speed, and dot-matrix systems that could mix text and graphics but produced relatively low-quality output. The plotter was used for those requiring high-quality line art like blueprints.\nThe introduction of the low-cost laser printer in 1984, with the first HP LaserJet, and the addition of PostScript in next year's Apple LaserWriter set off a revolution in printing known as desktop publishing. Laser printers using PostScript mixed text and graphics, like dot-matrix printers, but at quality levels formerly available only from commercial typesetting systems. By 1990, most simple printing tasks like fliers and brochures were now created on personal computers and then laser printed; expensive offset printing systems were being dumped as scrap. The HP Deskjet of 1988 offered the same advantages as a laser printer in terms of flexibility, but produced somewhat lower-quality output (depending on the paper) from much less-expensive mechanisms. Inkjet systems rapidly displaced dot-matrix and daisy-wheel printers from the market. By the 2000s, high-quality printers of this sort had fallen under the $100 price point and became commonplace.\nThe rapid improvement of internet email through the 1990s and into the 2000s has largely displaced the need for printing as a means of moving documents, and a wide variety of reliable storage systems means that a \"physical backup\" is of little benefit today.\nStarting around 2010, 3D printing became an area of intense interest, allowing the creation of physical objects with the same sort of effort as an early laser printer required to produce a brochure. As of the 2020s, 3D printing has become a widespread hobby due to the abundance of cheap 3D printer kits, with the most common process being Fused deposition modeling.\nTypes.\n\"Personal\" printers are mainly designed to support individual users, and may be connected to only a single computer. These printers are designed for low-volume, short-turnaround print jobs, requiring minimal setup time to produce a hard copy of a given document. However, they are generally slow devices ranging from 6 to around 25 pages per minute (ppm), \nand the cost per page is relatively high. However, this is offset by the on-demand convenience. Some printers can print documents stored on memory cards or from digital cameras and scanners.\n\"Networked\" or \"shared\" printers are \"designed for high-volume, high-speed printing\". They are usually shared by many users on a network and can print at speeds of 45 to around 100 ppm. The Xerox 9700 could achieve 120 ppm.\nA \"virtual printer\" is a piece of computer software whose user interface and API resembles that of a printer driver, but which is not connected with a physical computer printer. A virtual printer can be used to create a file which is an image of the data which would be printed, for archival purposes or as input to another program, for example to create a PDF or to transmit to another system or user.\nA \"barcode printer\" is a computer peripheral for printing barcode labels or tags that can be attached to, or printed directly on, physical objects. Barcode printers are commonly used to label cartons before shipment, or to label retail items with UPCs or EANs.\nA \"3D printer\" is a device for making a three-dimensional object from a 3D model or other electronic data source through additive processes in which successive layers of material (including plastics, metals, food, cement, wood, and other materials) are laid down under computer control. It is called a printer by analogy with an inkjet printer which produces a two-dimensional document by a similar process of depositing a layer of ink on paper.\nID Card printers.\nA card printer is an electronic desktop printer with single card feeders which print and personalize plastic cards. In this respect they differ from, for example, label printers which have a continuous supply feed. Card dimensions are usually 85.60 \u00d7 53.98\u00a0mm, standardized under ISO/IEC 7810 as ID-1. This format is also used in EC-cards, telephone cards, credit cards, driver's licenses and health insurance cards. This is commonly known as the bank card format. Card printers are controlled by corresponding printer drivers or by means of a specific programming language. Generally card printers are designed with laminating, striping, and punching functions, and use desktop or web-based software. The hardware features of a card printer differentiate a card printer from the more traditional printers, as ID cards are usually made of PVC plastic and require laminating and punching. Different card printers can accept different card thickness and dimensions.\nThe principle is the same for practically all card printers: the plastic card is passed through a thermal print head at the same time as a color ribbon. The color from the ribbon is transferred onto the card through the heat given out from the print head. The standard performance for card printing is 300 dpi (300 dots per inch, equivalent to 11.8 dots per mm). There are different printing processes, which vary in their detail:\nSoftware.\nThere are basically two categories of card printer software: desktop-based, and web-based (online). The biggest difference between the two is whether or not a customer has a printer on their network that is capable of printing identification cards. If a business already owns an ID card printer, then a desktop-based badge maker is probably suitable for their needs. Typically, large organizations who have high employee turnover will have their own printer. A desktop-based badge maker is also required if a company needs their IDs make instantly. An example of this is the private construction site that has restricted access. However, if a company does not already have a local (or network) printer that has the features they need, then the web-based option is a perhaps a more affordable solution. The web-based solution is good for small businesses that don't anticipate a lot of rapid growth, or organizations who either can't afford a card printer, or don't have the resources to learn how to set up and use one. Generally speaking, desktop-based solutions involve software, a database (or spreadsheet) and can be installed on a single computer or network.\nOther options.\nAlongside the basic function of printing cards, card printers can also read and encode magnetic stripes as well as contact and contact free RFID chip cards (smart cards). Thus card printers enable the encoding of plastic cards both visually and logically. Plastic cards can also be laminated after printing. Plastic cards are laminated after printing to achieve a considerable increase in durability and a greater degree of counterfeit prevention. Some card printers come with an option to print both sides at the same time, which cuts down the time taken to print and less margin of error. In such printers one side of id card is printed and then the card is flipped in the flip station and other side is printed.\nApplications.\nAlongside the traditional uses in time attendance and access control (in particular with photo personalization), countless other applications have been found for plastic cards, e.g. for personalized customer and members\u2019 cards, for sports ticketing and in local public transport systems for the production of season tickets, for the production of school and college identity cards as well as for the production of national ID cards.\nTechnology.\nThe choice of print technology has a great effect on the cost of the printer and cost of operation, speed, quality and permanence of documents, and noise. Some printer technologies do not work with certain types of physical media, such as carbon paper or transparencies.\nA second aspect of printer technology that is often forgotten is resistance to alteration: liquid ink, such as from an inkjet head or fabric ribbon, becomes absorbed by the paper fibers, so documents printed with liquid ink are more difficult to alter than documents printed with toner or solid inks, which do not penetrate below the paper surface.\nCheques can be printed with liquid ink or on special cheque paper with toner anchorage so that alterations may be detected. The machine-readable lower portion of a cheque must be printed using MICR toner or ink. Banks and other clearing houses employ automation equipment that relies on the magnetic flux from these specially printed characters to function properly.\nModern print technology.\nThe following printing technologies are routinely found in modern printers:\nToner-based printers.\nA laser printer rapidly produces high quality text and graphics. As with digital photocopiers and multifunction printers (MFPs), laser printers employ a xerographic printing process but differ from analog photocopiers in that the image is produced by the direct scanning of a laser beam across the printer's photoreceptor.\nAnother toner-based printer is the LED printer which uses an array of LEDs instead of a laser to cause toner adhesion to the print drum.\nLiquid inkjet printers.\nInkjet printers operate by propelling variably sized droplets of liquid ink onto almost any sized page. They are the most common type of computer printer used by consumers.\nSolid ink printers.\nSolid ink printers, also known as phase-change ink or hot-melt ink printers, are a type of thermal transfer printer, graphics sheet printer or 3D printer . They use solid sticks, crayons, pearls or granular ink materials. Common inks are CMYK-colored ink, similar in consistency to candle wax, which are melted and fed into a piezo crystal operated print-head. A Thermal transfer printhead jets the liquid ink on a rotating, oil coated drum. The paper then passes over the print drum, at which time the image is immediately transferred, or transfixed, to the page. Solid ink printers are most commonly used as color office printers and are excellent at printing on transparencies and other non-porous media. Solid ink is also called phase-change or hot-melt ink was first used by Data Products and Howtek, Inc., in 1984. Solid ink printers can produce excellent results with text and images. Some solid ink printers have evolved to print 3D models, for example, Visual Impact Corporation of Windham, NH was started by retired Howtek employee, Richard Helinski whose 3D patents US4721635 and then US5136515 was licensed to Sanders Prototype, Inc., later named Solidscape, Inc. Acquisition and operating costs are similar to laser printers. Drawbacks of the technology include high energy consumption and long warm-up times from a cold state. Also, some users complain that the resulting prints are difficult to write on, as the wax tends to repel inks from pens, and are difficult to feed through automatic document feeders, but these traits have been significantly reduced in later models. This type of thermal transfer printer is only available from one manufacturer, Xerox, manufactured as part of their Xerox Phaser office printer line. Previously, solid ink printers were manufactured by Tektronix, but Tektronix sold the printing business to Xerox in 2001.\nDye-sublimation printers.\nA dye-sublimation printer (or dye-sub printer) is a printer that employs a printing process that uses heat to transfer dye to a medium such as a plastic card, paper, or canvas. The process is usually to lay one color at a time using a ribbon that has color panels. Dye-sub printers are intended primarily for high-quality color applications, including color photography; and are less well-suited for text. While once the province of high-end print shops, dye-sublimation printers are now increasingly used as dedicated consumer photo printers.\nThermal printers.\nThermal printers work by selectively heating regions of special heat-sensitive paper. Monochrome thermal printers are used in cash registers, ATMs, gasoline dispensers and some older inexpensive fax machines. Colors can be achieved with special papers and different temperatures and heating rates for different colors; these colored sheets are not required in black-and-white output. One example is Zink (a portmanteau of \"zero ink\").\nObsolete and special-purpose printing technologies.\nThe following technologies are either obsolete, or limited to special applications though most were, at one time, in widespread use.\nImpact printers.\n Impact printers rely on a forcible impact to transfer ink to the media. The impact printer uses a print head that either hits the surface of the ink ribbon, pressing the ink ribbon against the paper (similar to the action of a typewriter), or, less commonly, hits the back of the paper, pressing the paper against the ink ribbon (the IBM 1403 for example). All but the dot matrix printer rely on the use of \"fully formed characters\", letterforms that represent each of the characters that the printer was capable of printing. In addition, most of these printers were limited to monochrome, or sometimes two-color, printing in a single typeface at one time, although bolding and underlining of text could be done by \"overstriking\", that is, printing two or more impressions either in the same character position or slightly offset. Impact printers varieties include typewriter-derived printers, teletypewriter-derived printers, daisywheel printers, dot matrix printers, and line printers. Dot-matrix printers remain in common use in businesses where multi-part forms are printed. \"An overview of impact printing\" contains a detailed description of many of the technologies used.\nTypewriter-derived printers.\nSeveral different computer printers were simply computer-controllable versions of existing electric typewriters. The Friden Flexowriter and IBM Selectric-based printers were the most-common examples. The Flexowriter printed with a conventional typebar mechanism while the Selectric used IBM's well-known \"golf ball\" printing mechanism. In either case, the letter form then struck a ribbon which was pressed against the paper, printing one character at a time. The maximum speed of the Selectric printer (the faster of the two) was 15.5 characters per second.\nTeletypewriter-derived printers.\nThe common teleprinter could easily be interfaced with the computer and became very popular except for those computers manufactured by IBM. Some models used a \"typebox\" that was positioned, in the X- and Y-axes, by a mechanism, and the selected letter form was struck by a hammer. Others used a type cylinder in a similar way as the Selectric typewriters used their type ball. In either case, the letter form then struck a ribbon to print the letterform. Most teleprinters operated at ten characters per second although a few achieved 15 CPS.\nDaisy wheel printers.\nDaisy wheel printers operate in much the same fashion as a typewriter. A hammer strikes a wheel with petals, the \"daisy wheel\", each petal containing a letter form at its tip. The letter form strikes a ribbon of ink, depositing the ink on the page and thus printing a character. By rotating the daisy wheel, different characters are selected for printing. These printers were also referred to as \"letter-quality printers\" because they could produce text which was as clear and crisp as a typewriter. The fastest letter-quality printers printed at 30 characters per second.\nDot-matrix printers.\nThe term dot matrix printer is used for impact printers that use a matrix of small pins to transfer ink to the page. The advantage of dot matrix over other impact printers is that they can produce graphical images in addition to text; however the text is generally of poorer quality than impact printers that use letterforms (\"type\").\nDot-matrix printers can be broadly divided into two major classes:\nDot matrix printers can either be character-based or line-based (that is, a single horizontal series of pixels across the page), referring to the configuration of the print head.\nIn the 1970s and '80s, dot matrix printers were one of the more common types of printers used for general use, such as for home and small office use. Such printers normally had either 9 or 24 pins on the print head (early 7 pin printers also existed, which did not print descenders). There was a period during the early home computer era when a range of printers were manufactured under many brands such as the Commodore VIC-1525 using the Seikosha Uni-Hammer system. This used a single solenoid with an oblique striker that would be actuated 7 times for each column of 7 vertical pixels while the head was moving at a constant speed. The angle of the striker would align the dots vertically even though the head had moved one dot spacing in the time. The vertical dot position was controlled by a synchronized longitudinally ribbed platen behind the paper that rotated rapidly with a rib moving vertically seven dot spacings in the time it took to print one pixel column. 24-pin print heads were able to print at a higher quality and started to offer additional type styles and were marketed as Near Letter Quality by some vendors. Once the price of inkjet printers dropped to the point where they were competitive with dot matrix printers, dot matrix printers began to fall out of favour for general use.\nSome dot matrix printers, such as the NEC P6300, can be upgraded to print in color. This is achieved through the use of a four-color ribbon mounted on a mechanism (provided in an upgrade kit that replaces the standard black ribbon mechanism after installation) that raises and lowers the ribbons as needed. Color graphics are generally printed in four passes at standard resolution, thus slowing down printing considerably. As a result, color graphics can take up to four times longer to print than standard monochrome graphics, or up to 8-16 times as long at high resolution mode.\nDot matrix printers are still commonly used in low-cost, low-quality applications such as cash registers, or in demanding, very high volume applications like invoice printing. Impact printing, unlike laser printing, allows the pressure of the print head to be applied to a stack of two or more forms to print multi-part documents such as sales invoices and credit card receipts using continuous stationery with carbonless copy paper. It also has security advantages as ink impressed into a paper matrix by force is harder to erase invisibly. Dot-matrix printers were being superseded even as receipt printers after the end of the twentieth century.\nLine printers.\nLine printers print an entire line of text at a time. Four principal designs exist.\nIn each case, to print a line, precisely timed hammers strike against the back of the paper at the exact moment that the correct character to be printed is passing in front of the paper. The paper presses forward against a ribbon which then presses against the character form and the impression of the character form is printed onto the paper. Each system could have slight timing issues, which could cause minor misalignment of the resulting printed characters. For drum or typebar printers, this appeared as vertical misalignment, with characters being printed slightly above or below the rest of the line. In chain or bar printers, the misalignment was horizontal, with printed characters being crowded closer together or farther apart. This was much less noticeable to human vision than vertical misalignment, where characters seemed to bounce up and down in the line, so they were considered as higher quality print.\nLine printers are the fastest of all impact printers and are used for bulk printing in large computer centres. A line printer can print at 1100 lines per minute or faster, frequently printing pages more rapidly than many current laser printers. On the other hand, the mechanical components of line printers operate with tight tolerances and require regular preventive maintenance (PM) to produce a top quality print. They are virtually never used with personal computers and have now been replaced by high-speed laser printers. The legacy of line printers lives on in many operating systems, which use the abbreviations \"lp\", \"lpr\", or \"LPT\" to refer to printers.\nLiquid ink electrostatic printers.\nLiquid ink electrostatic printers use a chemical coated paper, which is charged by the print head according to the image of the document. The paper is passed near a pool of liquid ink with the opposite charge. The charged areas of the paper attract the ink and thus form the image. This process was developed from the process of electrostatic copying. Color reproduction is very accurate, and because there is no heating the scale distortion is less than \u00b10.1%. (All laser printers have an accuracy of \u00b11%.)\nWorldwide, most survey offices used this printer before color inkjet plotters become popular. Liquid ink electrostatic printers were mostly available in width and also 6 color printing. These were also used to print large billboards. It was first introduced by Versatec, which was later bought by Xerox. 3M also used to make these printers.\nPlotters.\nPen-based plotters were an alternate printing technology once common in engineering and architectural firms. Pen-based plotters rely on contact with the paper (but not impact, per se) and special purpose pens that are mechanically run over the paper to create text and images. Since the pens output continuous lines, they were able to produce technical drawings of higher resolution than was achievable with dot-matrix technology. Some plotters used roll-fed paper, and therefore had a minimal restriction on the size of the output in one dimension. These plotters were capable of producing quite sizable drawings.\nOther printers.\nA number of other sorts of printers are important for historical reasons, or for special purpose uses.\nAttributes.\nConnectivity.\nPrinters can be connected to computers in many ways: directly by a dedicated data cable such as the USB, through a short-range radio like Bluetooth, a local area network using cables (such as the Ethernet) or radio (such as WiFi), or on a standalone basis without a computer, using a memory card or other portable data storage device.\nMore than half of all printers sold at U.S. retail in 2010 were wireless-capable, but nearly three-quarters of consumers who have access to those printers weren't taking advantage of the increased access to print from multiple devices according to the new Wireless Printing Study.\nPrinter control languages.\nMost printers other than line printers accept control characters or unique character sequences to control various printer functions. These may range from shifting from lower to upper case or from black to red ribbon on typewriter printers to switching fonts and changing character sizes and colors on raster printers. Early printer controls were not standardized, with each manufacturer's equipment having its own set. The IBM Personal Printer Data Stream (PPDS) became a commonly used command set for dot-matrix printers.\nToday, most printers accept one or more page description languages (PDLs). Laser printers with greater processing power frequently offer support for variants of Hewlett-Packard's Printer Command Language (PCL), PostScript or XML Paper Specification. Most inkjet devices support manufacturer proprietary PDLs such as ESC/P. The diversity in mobile platforms have led to various standardization efforts around device PDLs such as the Printer Working Group (PWG's) PWG Raster.\nPrinting speed.\nThe speed of early printers was measured in units of \"characters per minute\" (cpm) for character printers, or \"lines per minute\" (lpm) for line printers. Modern printers are measured in \"pages per minute\" (ppm). These measures are used primarily as a marketing tool, and are not as well standardised as toner yields. Usually pages per minute refers to sparse monochrome office documents, rather than dense pictures which usually print much more slowly, especially color images. Speeds in ppm usually apply to A4 paper in most countries in the world, and letter paper size, about 6% shorter, in North America.\nPrinting mode.\nThe data received by a printer may be:\nSome printers can process all four types of data, others not.\nToday it is possible to print everything (even plain text) by sending ready bitmapped images to the printer. This allows better control over formatting, especially among machines from different vendors. Many printer drivers do not use the text mode at all, even if the printer is capable of it.\nMonochrome, color and photo printers.\nA monochrome printer can only produce monochrome images, with only shades of a single color. Most printers can produce only two colors, black (ink) and white (no ink). With half-tonning techniques, however, such a printer can produce acceptable grey-scale images too\nA color printer can produce images of multiple colors. A photo printer is a color printer that can produce images that mimic the color range (gamut) and resolution of prints made from photographic film.\nPage yield.\nThe page yield is number of pages that can be printed from a toner cartridge or ink cartridge\u2014before the cartridge needs to be refilled or replaced.\nThe actual number of pages yielded by a specific cartridge depends on a number of factors.\nFor a fair comparison, many laser printer manufacturers use the ISO/IEC 19752 process to measure the toner cartridge yield.\nEconomics.\nIn order to fairly compare operating expenses of printers with a relatively small ink cartridge to printers with a larger, more expensive toner cartridge that typically holds more toner and so prints more pages before the cartridge needs to be replaced, many people prefer to estimate operating expenses in terms of cost per page (CPP).\nRetailers often apply the \"razor and blades\" model: a company may sell a printer at cost and make profits on the ink cartridge, paper, or some other replacement part. This has caused legal disputes regarding the right of companies other than the printer manufacturer to sell compatible ink cartridges. To protect their business model, several manufacturers invest heavily in developing new cartridge technology and patenting it.\nOther manufacturers, in reaction to the challenges from using this business model, choose to make more money on printers and less on ink, promoting the latter through their advertising campaigns. Finally, this generates two clearly different proposals: \"cheap printer\u00a0\u2013 expensive ink\" or \"expensive printer\u00a0\u2013 cheap ink\". Ultimately, the consumer decision depends on their reference interest rate or their time preference. From an economics viewpoint, there is a clear trade-off between cost per copy and cost of the printer.\nPrinter steganography.\nPrinter steganography is a type of steganography \u2013 \"hiding data within data\" \u2013 produced by color printers, including Brother, Canon, Dell, Epson, HP, IBM, Konica Minolta, Kyocera, Lanier, Lexmark, Ricoh, Toshiba and Xerox brand color laser printers, where tiny yellow dots are added to each page. The dots are barely visible and contain encoded printer serial numbers, as well as date and time stamps.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5278", "revid": "4033135", "url": "https://en.wikipedia.org/wiki?curid=5278", "title": "Copyright", "text": "Legal concept regulating rights of a creative work\nA copyright is a type of intellectual property that gives its owner the exclusive right to copy, distribute, adapt, display, and perform a creative work, usually for a limited time. The creative work may be in a literary, artistic, educational, or musical form. Copyright is intended to protect the original expression of an idea in the form of a creative work, but not the idea itself. A copyright is subject to limitations based on public interest considerations, such as the fair use doctrine in the United States.\nSome jurisdictions require \"fixing\" copyrighted works in a tangible form. It is often shared among multiple authors, each of whom holds a set of rights to use or license the work, and who are commonly referred to as rights holders. These rights frequently include reproduction, control over derivative works, distribution, public performance, and moral rights such as attribution.\nCopyrights can be granted by public law and are in that case considered \"territorial rights\". This means that copyrights granted by the law of a certain state do not extend beyond the territory of that specific jurisdiction. Copyrights of this type vary by country; many countries, and sometimes a large group of countries, have made agreements with other countries on procedures applicable when works \"cross\" national borders or national rights are inconsistent.\nTypically, the public law duration of a copyright expires 50 to 100 years after the creator dies, depending on the jurisdiction. Some countries require certain copyright formalities to establishing copyright, others recognize copyright in any completed work, without a formal registration. When the copyright of a work expires, it enters the public domain.\nHistory.\nBackground.\nThe concept of copyright developed after the printing press came into use in Europe in the 15th and 16th centuries. The printing press made it much cheaper to produce works, but as there was initially no copyright law, anyone could buy or rent a press and print any text. Popular new works were immediately re-set and re-published by competitors, so printers needed a constant stream of new material. Fees paid to authors for new works were high, and significantly supplemented the incomes of many academics.\nPrinting brought profound social changes. The rise in literacy across Europe led to a dramatic increase in the demand for reading matter. Prices of reprints were low, so publications could be bought by poorer people, creating a mass audience. In German language markets before the advent of copyright, technical materials, like popular fiction, were inexpensive and widely available; it has been suggested this contributed to Germany's industrial and economic success. After copyright law became established (in 1710 in England and Scotland, and in the 1840s in German-speaking areas) the low-price mass market vanished, and fewer, more expensive editions were published; distribution of scientific and technical information was greatly reduced.\nConception.\nThe concept of copyright first developed in England. In reaction to the printing of \"scandalous books and pamphlets\", the English Parliament passed the Licensing of the Press Act 1662, which required all intended publications to be registered with the government-approved Stationers' Company, giving the Stationers the right to regulate what material could be printed.\nThe Statute of Anne, enacted in 1710 in England and Scotland provided the first legislation to protect copyrights (but not authors' rights). The Copyright Act of 1814 extended more rights for authors but did not protect British from reprinting in the US. The Berne International Copyright Convention of 1886 finally provided protection for authors among the countries who signed the agreement, although the US did not join the Berne Convention until 1989.\nIn the US, the Constitution grants Congress the right to establish copyright and patent laws. Shortly after the Constitution was passed, Congress enacted the Copyright Act of 1790, modeling it after the Statute of Anne. While the national law protected authors\u2019 published works, authority was granted to the states to protect authors\u2019 unpublished works. The most recent major overhaul of copyright in the US, the 1976 Copyright Act, extended federal copyright to works as soon as they are created and \"fixed\", without requiring publication or registration. State law continues to apply to unpublished works that are not otherwise copyrighted by federal law. This act also changed the calculation of copyright term from a fixed term (then a maximum of fifty-six years) to \"life of the author plus 50 years\". These changes brought the US closer to conformity with the Berne Convention, and in 1989 the United States further revised its copyright law and joined the Berne Convention officially.\nCopyright laws allow products of creative human activities, such as literary and artistic production, to be preferentially exploited and thus incentivized. Different cultural attitudes, social organizations, economic models and legal frameworks are seen to account for why copyright emerged in Europe and not, for example, in Asia. In the Middle Ages in Europe, there was generally a lack of any concept of literary property due to the general relations of production, the specific organization of literary production and the role of culture in society. The latter refers to the tendency of oral societies, such as that of Europe in the medieval period, to view knowledge as the product and expression of the collective, rather than to see it as individual property. However, with copyright laws, intellectual production comes to be seen as a product of an individual, with attendant rights. The most significant point is that patent and copyright laws support the expansion of the range of creative human activities that can be commodified. This parallels the ways in which capitalism led to the commodification of many aspects of social life that earlier had no monetary or economic value per\u00a0se.\nCopyright has developed into a concept that has a significant effect on nearly every modern industry, including not just literary work, but also forms of creative work such as sound recordings, films, photographs, software, and architecture.\nNational copyrights.\nOften seen as the first real copyright law, the 1709 British Statute of Anne gave the publishers rights for a fixed period, after which the copyright expired.\nThe act also alluded to individual rights of the artist. It began, \"Whereas Printers, Booksellers, and other Persons, have of late frequently taken the Liberty of Printing\u00a0... Books, and other Writings, without the Consent of the Authors\u00a0... to their very great Detriment, and too often to the Ruin of them and their Families:\". A right to benefit financially from the work is articulated, and court rulings and legislation have recognized a right to control the work, such as ensuring that the integrity of it is preserved. An irrevocable right to be recognized as the work's creator appears in some countries' copyright laws.\nThe Copyright Clause of the United States, Constitution (1787) authorized copyright legislation: \"To promote the Progress of Science and useful Arts, by securing for limited Times to Authors and Inventors the exclusive Right to their respective Writings and Discoveries.\" That is, by guaranteeing them a period of time in which they alone could profit from their works, they would be enabled and encouraged to invest the time required to create them, and this would be good for society as a whole. A right to profit from the work has been the philosophical underpinning for much legislation extending the duration of copyright, to the life of the creator and beyond, to their heirs.\nThe original length of copyright in the United States was 14\u00a0years, and it had to be explicitly applied for. If the author wished, they could apply for a second 14\u2011year monopoly grant, but after that the work entered the public domain, so it could be used and built upon by others.\nCopyright law was enacted rather late in German states, and the historian Eckhard H\u00f6ffner argues that the absence of copyright laws in the early 19th century encouraged publishing, was profitable for authors, led to a proliferation of books, enhanced knowledge, and was ultimately an important factor in the ascendency of Germany as a power during that century. However, empirical evidence derived from the exogenous differential introduction of copyright in Napoleonic Italy shows that \"basic copyrights increased both the number and the quality of operas, measured by their popularity and durability\".\nInternational copyright treaties.\nThe 1886 Berne Convention first established recognition of copyrights among sovereign nations, rather than merely bilaterally. Under the Berne Convention, copyrights for creative works do not have to be asserted or declared, as they are automatically in force at creation: an author need not \"register\" or \"apply for\" a copyright in countries adhering to the Berne Convention. As soon as a work is \"fixed\", that is, written or recorded on some physical medium, its author is automatically entitled to all copyrights in the work, and to any derivative works unless and until the author explicitly disclaims them, or until the copyright expires. The Berne Convention also resulted in foreign authors being treated equivalently to domestic authors, in any country signed onto the Convention. The UK signed the Berne Convention in 1887 but did not implement large parts of it until 100\u00a0years later with the passage of the Copyright, Designs and Patents Act 1988. Specially, for educational and scientific research purposes, the Berne Convention provides the developing countries issue compulsory licenses for the translation or reproduction of copyrighted works within the limits prescribed by the Convention. This was a special provision that had been added at the time of 1971 revision of the Convention, because of the strong demands of the developing countries. The United States did not sign the Berne Convention until 1989.\nThe United States and most Latin American countries instead entered into the Buenos Aires Convention in 1910, which required a copyright notice on the work (such as \"all rights reserved\"), and permitted signatory nations to limit the duration of copyrights to shorter and renewable terms. The Universal Copyright Convention was drafted in 1952 as another less demanding alternative to the Berne Convention, and ratified by nations such as the Soviet Union and developing nations.\nThe regulations of the Berne Convention are incorporated into the World Trade Organization's TRIPS agreement (1995), thus giving the Berne Convention effectively near-global application.\nIn 1961, the United International Bureaux for the Protection of Intellectual Property signed the Rome Convention for the Protection of Performers, Producers of Phonograms and Broadcasting Organizations. In 1996, this organization was succeeded by the founding of the World Intellectual Property Organization, which launched the 1996 WIPO Performances and Phonograms Treaty and the 2002 WIPO Copyright Treaty, which enacted greater restrictions on the use of technology to copy works in the nations that ratified it. The Trans-Pacific Partnership includes intellectual Property Provisions relating to copyright.\nCopyright laws are standardized somewhat through these international conventions such as the Berne Convention and Universal Copyright Convention. These multilateral treaties have been ratified by nearly all countries, and international organizations such as the European Union or World Trade Organization require their member states to comply with them.\nObtaining protection.\nOwnership.\nThe original holder of the copyright may be the employer of the author rather than the author themself if the work is a \"work for hire\". For example, in English law the Copyright, Designs and Patents Act 1988 provides that if a copyrighted work is made by an employee in the course of that employment, the copyright is automatically owned by the employer which would be a \"Work for Hire\". Typically, the first owner of a copyright is the person who created the work i.e. the author. But when more than one person creates the work, then a case of joint authorship can be made provided some criteria are met.\nEligible works.\nCopyright may apply to a wide range of creative, intellectual, or artistic forms, or \"works\". Specifics vary by jurisdiction, but these can include poems, theses, fictional characters, plays and other literary works, motion pictures, choreography, musical compositions, sound recordings, paintings, drawings, sculptures, photographs, computer software, radio and television broadcasts, and industrial designs. Graphic designs and industrial designs may have separate or overlapping laws applied to them in some jurisdictions.\nCopyright does not cover ideas and information themselves, only the form or manner in which they are expressed. For example, the copyright to a Mickey Mouse cartoon restricts others from making copies of the cartoon or creating derivative works based on Disney's particular anthropomorphic mouse, but does not prohibit the creation of other works about anthropomorphic mice in general, so long as they are different enough to not be judged copies of Disney's. Note additionally that Mickey Mouse is not copyrighted because characters cannot be copyrighted; rather, \"Steamboat Willie\" is copyrighted and Mickey Mouse, as a character in that copyrighted work, is afforded protection.\nOriginality.\nTypically, a work must meet minimal standards of originality in order to qualify for copyright, and the copyright expires after a set period of time (some jurisdictions may allow this to be extended). Different countries impose different tests, although generally the requirements are low; in the United Kingdom there has to be some \"skill, labour, and judgment\" that has gone into it. In Australia and the United Kingdom it has been held that a single word is insufficient to comprise a copyright work. However, single words or a short string of words can sometimes be registered as a trademark instead.\nCopyright law recognizes the right of an author based on whether the work actually is an original creation, rather than based on whether it is unique; two authors may own copyright on two substantially identical works, if it is determined that the duplication was coincidental, and neither was copied from the other.\nRegistration.\nIn all countries where the Berne Convention standards apply, copyright is automatic, and need not be obtained through official registration with any government office. Once an idea has been reduced to tangible form, for example by securing it in a fixed medium (such as a drawing, sheet music, photograph, a videotape, or a computer file), the copyright holder is entitled to enforce their exclusive rights. However, while registration is not needed to exercise copyright, in jurisdictions where the laws provide for registration, it serves as \"prima facie\" evidence of a valid copyright and enables the copyright holder to seek statutory damages and attorney's fees. (In the US, registering after an infringement only enables one to receive actual damages and lost profits.)\nA widely circulated strategy to avoid the cost of copyright registration is referred to as the poor man's copyright. It proposes that the creator send the work to themself in a sealed envelope by registered mail, using the postmark to establish the date. This technique has not been recognized in any published opinions of the United States courts. The United States Copyright Office says the technique is not a substitute for actual registration. The United Kingdom Intellectual Property Office discusses the technique and notes that the technique (as well as commercial registries) does not constitute dispositive proof that the work is original or establish who created the work. \nFixing.\nThe Berne Convention allows member countries to decide whether creative works must be \"fixed\" to enjoy copyright. Article 2, Section 2 of the Berne Convention states: \"It shall be a matter for legislation in the countries of the Union to prescribe that works in general or any specified categories of works shall not be protected unless they have been fixed in some material form.\" Some countries do not require that a work be produced in a particular form to obtain copyright protection. For instance, Spain, France, and Australia do not require fixation for copyright protection. The United States and Canada, on the other hand, require that most works must be \"fixed in a tangible medium of expression\" to obtain copyright protection. US law requires that the fixation be stable and permanent enough to be \"perceived, reproduced or communicated for a period of more than transitory duration\". Similarly, Canadian courts consider fixation to require that the work be \"expressed to some extent at least in some material form, capable of identification and having a more or less permanent endurance\".\nNote this provision of US law: \"c) Effect of Berne Convention.\u2014No right or interest in a work eligible for protection under this title may be claimed by virtue of, or in reliance upon, the provisions of the Berne Convention, or the adherence of the United States thereto. Any rights in a work eligible for protection under this title that derive from this title, other Federal or State statutes, or the common law, shall not be expanded or reduced by virtue of, or in reliance upon, the provisions of the Berne Convention, or the adherence of the United States thereto.\"\nCopyright notice.\nBefore 1989, United States law required the use of a copyright notice, consisting of the copyright symbol (\u00a9, the letter C inside a circle), the abbreviation \"Copr.\", or the word \"Copyright\", followed by the year of the first publication of the work and the name of the copyright holder. Several years may be noted if the work has gone through substantial revisions. The proper copyright notice for sound recordings of musical or other audio works is a sound recording copyright symbol (\u2117, the letter\u00a0P inside a circle), which indicates a sound recording copyright, with the letter\u00a0P indicating a \"phonorecord\". In addition, the phrase \"All rights reserved\" which indicates that the copyright holder reserves, or holds for their own use was once required to assert copyright, but that phrase is now legally obsolete. Almost everything on the Internet has some sort of copyright attached to it. Whether these things are watermarked, signed, or have any other sort of indication of the copyright is a different story however.\nIn 1989 the United States enacted the Berne Convention Implementation Act, amending the 1976\u00a0Copyright Act to conform to most of the provisions of the Berne Convention. As a result, the use of copyright notices has become optional to claim copyright, because the Berne Convention makes copyright automatic. However, the lack of notice of copyright using these marks may have consequences in terms of reduced damages in an infringement lawsuit\u00a0\u2013 using notices of this form may reduce the likelihood of a defense of \"innocent infringement\" being successful.\nEnforcement.\nCopyrights are generally enforced by the holder in a civil law court, but there are also criminal infringement statutes in some jurisdictions. While central registries are kept in some countries which aid in proving claims of ownership, registering does not necessarily prove ownership, nor does the fact of copying (even without permission) necessarily prove that copyright was infringed. Criminal sanctions are generally aimed at serious counterfeiting activity, but are now becoming more commonplace as copyright collectives such as the RIAA are increasingly targeting the file sharing home Internet user. Thus far, however, most such cases against file sharers have been settled out of court. (\"See Legal aspects of file sharing\")\nIn most jurisdictions the copyright holder must bear the cost of enforcing copyright. This will usually involve engaging legal representation, administrative or court costs. In light of this, many copyright disputes are settled by a direct approach to the infringing party in order to settle the dispute out of court.\n\"...by 1978, the scope was expanded to apply to any 'expression' that has been 'fixed' in any medium, this protection granted automatically whether the maker wants it or not, no registration required.\"\nCopyright infringement.\nFor a work to be considered to infringe upon copyright, its use must have occurred in a nation that has domestic copyright laws or adheres to a bilateral treaty or established international convention such as the Berne Convention or WIPO Copyright Treaty. Improper use of materials outside of legislation is deemed \"unauthorized edition\", not copyright infringement.\nStatistics regarding the effects of copyright infringement are difficult to determine. Studies have attempted to determine whether there is a monetary loss for industries affected by copyright infringement by predicting what portion of pirated works would have been formally purchased if they had not been freely available. Other reports indicate that copyright infringement does not have an adverse effect on the entertainment industry, and can have a positive effect. In particular, a 2014 university study concluded that free music content, accessed on YouTube, does not necessarily hurt sales, instead has the potential to increase sales.\nAccording to the IP Commission Report the annual cost of intellectual property theft to the US economy \"continues to exceed $225 billion in counterfeit goods, pirated software, and theft of trade secrets and could be as high as $600 billion.\" A 2019 study sponsored by the US Chamber of Commerce Global Innovation Policy Center (GIPC), in partnership with NERA Economic Consulting \"estimates that global online piracy costs the U.S. economy at least $29.2 billion in lost revenue each year.\" An August 2021 report by the Digital Citizens Alliance states that \"online criminals who offer stolen movies, TV shows, games, and live events through websites and apps are reaping $1.34 billion in annual advertising revenues.\" This comes as a result of users visiting pirate websites who are then subjected to pirated content, malware, and fraud.\nRights granted.\nAccording to World Intellectual Property Organisation, copyright protects two types of rights. Economic rights allow right owners to derive financial reward from the use of their works by others. Moral rights allow authors and creators to take certain actions to preserve and protect their link with their work. The author or creator may be the owner of the economic rights or those rights may be transferred to one or more copyright owners. Many countries do not allow the transfer of moral rights.\nEconomic rights.\nWith any kind of property, its owner may decide how it is to be used, and others can use it lawfully only if they have the owner's permission, often through a license. The owner's use of the property must, however, respect the legally recognised rights and interests of other members of society. So the owner of a copyright-protected work may decide how to use the work, and may prevent others from using it without permission. National laws usually grant copyright owners exclusive rights to allow third parties to use their works, subject to the legally recognised rights and interests of others. Most copyright laws state that authors or other right owners have the right to authorise or prevent certain acts in relation to a work. Right owners can authorise or prohibit:\nMoral rights.\nMoral rights are concerned with the non-economic rights of a creator. They protect the creator's connection with a work as well as the integrity of the work. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. In some EU countries, such as France, moral rights last indefinitely. In the UK, however, moral rights are finite. That is, the right of attribution and the right of integrity last only as long as the work is in copyright. When the copyright term comes to an end, so too do the moral rights in that work. This is just one reason why the moral rights regime within the UK is often regarded as weaker or inferior to the protection of moral rights in continental Europe and elsewhere in the world. The Berne Convention, in Article 6bis, requires its members to grant authors the following rights:\nThese and other similar rights granted in national laws are generally known as the moral rights of authors. The Berne Convention requires these rights to be independent of authors\u2019 economic rights. Moral rights are only accorded to individual authors and in many national laws they remain with the authors even after the authors have transferred their economic rights. This means that even where, for example, a film producer or publisher owns the economic rights in a work, in many jurisdictions the individual author continues to have moral rights. Recently, as a part of the debates being held at the US Copyright Office on the question of inclusion of Moral Rights as a part of the framework of the Copyright Law in United States, the Copyright Office concluded that many diverse aspects of the current moral rights patchwork \u2013 including copyright law's derivative work right, state moral rights statutes, and contract law \u2013 are generally working well and should not be changed. Further, the Office concludes that there is no need for the creation of a blanket moral rights statute at this time. However, there are aspects of the US moral rights patchwork that could be improved to the benefit of individual authors and the copyright system as a whole.\nThe Copyright Law in the United States, several exclusive rights are granted to the holder of a copyright, as are listed below:\nThe basic right when a work is protected by copyright is that the holder may determine and decide how and under what conditions the protected work may be used by others. This includes the right to decide to distribute the work for free. This part of copyright is often overseen. The phrase \"exclusive right\" means that only the copyright holder is free to exercise those rights, and others are prohibited from using the work without the holder's permission. Copyright is sometimes called a \"negative right\", as it serves to prohibit certain people (e.g., readers, viewers, or listeners, and primarily publishers and would be publishers) from doing something they would otherwise be able to do, rather than permitting people (e.g., authors) to do something they would otherwise be unable to do. In this way it is similar to the unregistered design right in English law and European law. The rights of the copyright holder also permit him/her to not use or exploit their copyright, for some or all of the term. There is, however, a critique which rejects this assertion as being based on a philosophical interpretation of copyright law that is not universally shared. There is also debate on whether copyright should be considered a property right or a moral right.\nUK copyright law gives creators both economic rights and moral rights. While \u2018copying\u2019 someone else's work without permission may constitute an infringement of their economic rights, that is, the reproduction right or the right of communication to the public, whereas, \u2018mutilating\u2019 it might infringe the creator's moral rights. In the UK, moral rights include the right to be identified as the author of the work, which is generally identified as the right of attribution, and the right not to have your work subjected to \u2018derogatory treatment\u2019, that is the right of integrity.\nIndian copyright law is at parity with the international standards as contained in TRIPS. The Indian Copyright Act, 1957, pursuant to the amendments in 1999, 2002 and 2012, fully reflects the Berne Convention and the Universal Copyrights Convention, to which India is a party. India is also a party to the Geneva Convention for the Protection of Rights of Producers of Phonograms and is an active member of the World Intellectual Property Organization (WIPO) and United Nations Educational, Scientific and Cultural Organization (UNESCO). The Indian system provides both the economic and moral rights under different provisions of its Indian Copyright Act of 1957.\nDuration.\nCopyright subsists for a variety of lengths in different jurisdictions. The length of the term can depend on several factors, including the type of work (e.g. musical composition, novel), whether the work has been published, and whether the work was created by an individual or a corporation. In most of the world, the default length of copyright is the life of the author plus either 50 or 70 years. In the United States, the term for most existing works is a fixed number of years after the date of creation or publication. Under most countries' laws (for example, the United States and the United Kingdom), copyrights expire at the end of the calendar year in which they would otherwise expire.\nThe length and requirements for copyright duration are subject to change by legislation, and since the early 20th century there have been a number of adjustments made in various countries, which can make determining the duration of a given copyright somewhat difficult. For example, the United States used to require copyrights to be renewed after 28 years to stay in force, and formerly required a copyright notice upon first publication to gain coverage. In Italy and France, there were post-wartime extensions that could increase the term by approximately 6 years in Italy and up to about 14 in France. Many countries have extended the length of their copyright terms (sometimes retroactively). International treaties establish minimum terms for copyrights, but individual countries may enforce longer terms than those.\nIn the United States, all books and other works, except for sound recordings, published before 1928 have expired copyrights and are in the public domain. The applicable date for sound recordings in the United States is before 1923. In addition, works published before 1964 that did not have their copyrights renewed 28 years after first publication year also are in the public domain. Hirtle points out that the great majority of these works (including 93% of the books) were not renewed after 28 years and are in the public domain. Books originally published outside the US by non-Americans are exempt from this renewal requirement, if they are still under copyright in their home country.\nBut if the intended exploitation of the work includes publication (or distribution of derivative work, such as a film based on a book protected by copyright) outside the US, the terms of copyright around the world must be considered. If the author has been dead more than 70 years, the work is in the public domain in most, but not all, countries.\nIn 1998, the length of a copyright in the United States was increased by 20 years under the Copyright Term Extension Act. This legislation was strongly promoted by corporations which had valuable copyrights which otherwise would have expired, and has been the subject of substantial criticism on this point.\nLimitations and exceptions.\nIn many jurisdictions, copyright law makes exceptions to these restrictions when the work is copied for the purpose of commentary or other related uses. United States copyright law does not cover names, titles, short phrases or listings (such as ingredients, recipes, labels, or formulas). However, there are protections available for those areas copyright does not cover, such as trademarks and patents.\nIdea\u2013expression dichotomy and the merger doctrine.\nThe idea\u2013expression divide differentiates between ideas and expression, and states that copyright protects only the original expression of ideas, and not the ideas themselves. This principle, first clarified in the 1879 case of \"Baker v. Selden\", has since been codified by the Copyright Act of 1976 at 17 U.S.C. \u00a7\u00a0102(b).\nThe first-sale doctrine and exhaustion of rights.\nCopyright law does not restrict the owner of a copy from reselling legitimately obtained copies of copyrighted works, provided that those copies were originally produced by or with the permission of the copyright holder. It is therefore legal, for example, to resell a copyrighted book or CD. In the United States this is known as the first-sale doctrine, and was established by the courts to clarify the legality of reselling books in second-hand bookstores.\nSome countries may have parallel importation restrictions that allow the copyright holder to control the aftermarket. This may mean for example that a copy of a book that does not infringe copyright in the country where it was printed does infringe copyright in a country into which it is imported for retailing. The first-sale doctrine is known as exhaustion of rights in other countries and is a principle which also applies, though somewhat differently, to patent and trademark rights. It is important to note that the first-sale doctrine permits the transfer of the particular legitimate copy involved. It does not permit making or distributing additional copies.\nIn \"Kirtsaeng v. John Wiley &amp; Sons, Inc.\", in 2013, the United States Supreme Court held in a 6\u20133 decision that the first-sale doctrine applies to goods manufactured abroad with the copyright owner's permission and then imported into the US without such permission. The case involved a plaintiff who imported Asian editions of textbooks that had been manufactured abroad with the publisher-plaintiff's permission. The defendant, without permission from the publisher, imported the textbooks and resold on eBay. The Supreme Court's holding severely limits the ability of copyright holders to prevent such importation.\nIn addition, copyright, in most cases, does not prohibit one from acts such as modifying, defacing, or destroying one's own legitimately obtained copy of a copyrighted work, so long as duplication is not involved. However, in countries that implement moral rights, a copyright holder can in some cases successfully prevent the mutilation or destruction of a work that is publicly visible.\nFair use and fair dealing.\nCopyright does not prohibit all copying or replication. In the United States, the fair use doctrine, codified by the Copyright Act of 1976 as 17 U.S.C. Section 107, permits some copying and distribution without permission of the copyright holder or payment to same. The statute does not clearly define fair use, but instead gives four non-exclusive factors to consider in a fair use analysis. Those factors are:\nIn the United Kingdom and many other Commonwealth countries, a similar notion of fair dealing was established by the courts or through legislation. The concept is sometimes not well defined; however in Canada, private copying for personal use has been expressly permitted by statute since 1999. In \"Alberta (Education) v. Canadian Copyright Licensing Agency (Access Copyright)\", 2012 SCC 37, the Supreme Court of Canada concluded that limited copying for educational purposes could also be justified under the fair dealing exemption. In Australia, the fair dealing exceptions under the Copyright Act 1968 (Cth) are a limited set of circumstances under which copyrighted material can be legally copied or adapted without the copyright holder's consent. Fair dealing uses are research and study; review and critique; news reportage and the giving of professional advice (i.e. legal advice). Under current Australian law, although it is still a breach of copyright to copy, reproduce or adapt copyright material for personal or private use without permission from the copyright owner, owners of a legitimate copy are permitted to \"format shift\" that work from one medium to another for personal, private use, or to \"time shift\" a broadcast work for later, once and only once, viewing or listening. Other technical exemptions from infringement may also apply, such as the temporary reproduction of a work in machine readable form for a computer.\nIn the United States the AHRA (Audio Home Recording Act Codified in Section 10, 1992) prohibits action against consumers making noncommercial recordings of music, in return for royalties on both media and devices plus mandatory copy-control mechanisms on recorders.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Section 1008. Prohibition on certain infringement actions\"\n\"No action may be brought under this title alleging infringement of copyright based on the manufacture, importation, or distribution of a digital audio recording device, a digital audio recording medium, an analog recording device, or an analog recording medium, or based on the noncommercial use by a consumer of such a device or medium for making digital musical recordings or analog musical recordings.\"\nLater acts amended US Copyright law so that for certain purposes making 10 copies or more is construed to be commercial, but there is no general rule permitting such copying. Indeed, making one complete copy of a work, or in many cases using a portion of it, for commercial purposes will not be considered fair use. The Digital Millennium Copyright Act prohibits the manufacture, importation, or distribution of devices whose intended use, or only significant commercial use, is to bypass an access or copy control put in place by a copyright owner. An appellate court has held that fair use is not a defense to engaging in such distribution.\nEU copyright laws recognise the right of EU member states to implement some national exceptions to copyright. Examples of those exceptions are:\nAccessible copies.\nIt is legal in several countries including the United Kingdom and the United States to produce alternative versions (for example, in large print or braille) of a copyrighted work to provide improved access to a work for blind and visually impaired people without permission from the copyright holder.\nReligious Service Exemption.\nIn the US there is a Religious Service Exemption (1976 law, section 110[3]), namely \"performance of a non-dramatic literary or musical work or of a dramatico-musical work of a religious nature or display of a work, in the course of services at a place of worship or other religious assembly\" shall not constitute infringement of copyright.\nTransfer, assignment and licensing.\nA copyright, or aspects of it (e.g. reproduction alone, all but moral rights), may be assigned or transferred from one party to another. For example, a musician who records an album will often sign an agreement with a record company in which the musician agrees to transfer all copyright in the recordings in exchange for royalties and other considerations. The creator (and original copyright holder) benefits, or expects to, from production and marketing capabilities far beyond those of the author. In the digital age of music, music may be copied and distributed at minimal cost through the Internet; however, the record industry attempts to provide promotion and marketing for the artist and their work so it can reach a much larger audience. A copyright holder need not transfer all rights completely, though many publishers will insist. Some of the rights may be transferred, or else the copyright holder may grant another party a non-exclusive license to copy or distribute the work in a particular region or for a specified period of time.\nA transfer or licence may have to meet particular formal requirements in order to be effective, for example under the Australian Copyright Act 1968 the copyright itself must be expressly transferred in writing. Under the US Copyright Act, a transfer of ownership in copyright must be memorialized in a writing signed by the transferor. For that purpose, ownership in copyright includes exclusive licenses of rights. Thus exclusive licenses, to be effective, must be granted in a written instrument signed by the grantor. No special form of transfer or grant is required. A simple document that identifies the work involved and the rights being granted is sufficient. Non-exclusive grants (often called non-exclusive licenses) need not be in writing under US law. They can be oral or even implied by the behavior of the parties. Transfers of copyright ownership, including exclusive licenses, may and should be recorded in the U.S. Copyright Office. (Information on recording transfers is available on the Office's web site.) While recording is not required to make the grant effective, it offers important benefits, much like those obtained by recording a deed in a real estate transaction.\nCopyright may also be licensed. Some jurisdictions may provide that certain classes of copyrighted works be made available under a prescribed statutory license (e.g. musical works in the United States used for radio broadcast or performance). This is also called a compulsory license, because under this scheme, anyone who wishes to copy a covered work does not need the permission of the copyright holder, but instead merely files the proper notice and pays a set fee established by statute (or by an agency decision under statutory guidance) for every copy made. Failure to follow the proper procedures would place the copier at risk of an infringement suit. Because of the difficulty of following every individual work, copyright collectives or collecting societies and performing rights organizations (such as ASCAP, BMI, and SESAC) have been formed to collect royalties for hundreds (thousands and more) works at once. Though this market solution bypasses the statutory license, the availability of the statutory fee still helps dictate the price per work collective rights organizations charge, driving it down to what avoidance of procedural hassle would justify.\nFree licenses.\nCopyright licenses known as \"open\" or free licenses seek to grant several rights to licensees, either for a fee or not. \"Free\" in this context is not as much of a reference to price as it is to freedom. What constitutes free licensing has been characterised in a number of similar definitions, including by order of longevity the Free Software Definition, the Debian Free Software Guidelines, the Open Source Definition and the Definition of Free Cultural Works. Further refinements to these definitions have resulted in categories such as copyleft and permissive. Common examples of free licences are the GNU General Public License, BSD licenses and some Creative Commons licenses.\nFounded in 2001 by James Boyle, Lawrence Lessig, and Hal Abelson, the Creative Commons (CC) is a non-profit organization which aims to facilitate the legal sharing of creative works. To this end, the organization provides a number of generic copyright license options to the public, gratis. These licenses allow copyright holders to define conditions under which others may use a work and to specify what types of use are acceptable.\nTerms of use have traditionally been negotiated on an individual basis between copyright holder and potential licensee. Therefore, a general CC license outlining which rights the copyright holder is willing to waive enables the general public to use such works more freely. Six general types of CC licenses are available (although some of them are not properly free per the above definitions and per Creative Commons' own advice). These are based upon copyright-holder stipulations such as whether they are willing to allow modifications to the work, whether they permit the creation of derivative works and whether they are willing to permit commercial use of the work. As of 2009[ [update]] approximately 130 million individuals had received such licenses.\nCriticism.\nSome sources are critical of particular aspects of the copyright system. This is known as a debate over copynorms. Particularly to the background of uploading content to internet platforms and the digital exchange of original work, there is discussion about the copyright aspects of downloading and streaming, the copyright aspects of hyperlinking and framing.\nConcerns are often couched in the language of digital rights, digital freedom, database rights, open data or censorship. Discussions include \"Free Culture\", a 2004 book by Lawrence Lessig. Lessig coined the term permission culture to describe a worst-case system. \"Good Copy Bad Copy\" (documentary) and , discuss copyright. Some suggest an alternative compensation system. In Europe consumers are acting up against the raising costs of music, film and books, and as a result Pirate Parties have been created. Some groups reject copyright altogether, taking an anti-copyright stance. The perceived inability to enforce copyright online leads some to advocate ignoring legal statutes when on the web.\nPublic domain.\nCopyright, like other intellectual property rights, is subject to a statutorily determined term. Once the term of a copyright has expired, the formerly copyrighted work enters the public domain and may be used or exploited by anyone without obtaining permission, and normally without payment. However, in paying public domain regimes the user may still have to pay royalties to the state or to an authors' association. Courts in common law countries, such as the United States and the United Kingdom, have rejected the doctrine of a common law copyright. Public domain works should not be confused with works that are publicly available. Works posted in the internet, for example, are publicly available, but are not generally in the public domain. Copying such works may therefore violate the author's copyright.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5282", "revid": "43475265", "url": "https://en.wikipedia.org/wiki?curid=5282", "title": "Catalan language", "text": "Western Romance language\nCatalan (; autonym: , ]), known in the Valencian Community and Carche as \"Valencian\" (autonym: ), is a Western Romance language. It is the official language of Andorra, and an official language of two autonomous communities in eastern Spain: Catalonia and the Balearic Islands. The language is officially styled as Valencian in the Valencian Community. It also has semi-official status in the Italian \"comune\" of Alghero. It is also spoken in the Pyr\u00e9n\u00e9es-Orientales department of France and in two further areas in eastern Spain: the eastern strip of Aragon and the Carche area in the Region of Murcia. The Catalan-speaking territories are often called the or \"Catalan Countries\".\nThe language evolved from Vulgar Latin in the Middle Ages around the eastern Pyrenees. Nineteenth-century Spain saw a Catalan literary revival, culminating in the early 1900s.\nEtymology and pronunciation.\nThe word \"Catalan\" is derived from the territorial name of Catalonia, itself of disputed etymology. The main theory suggests that (Latin \"Gathia Launia\") derives from the name \"Gothia\" or \"Gauthia\" (\"Land of the Goths\"), since the origins of the Catalan counts, lords and people were found in the March of Gothia, whence \"Gothland\" &gt; \"Gothlandia\" &gt; \"Gothalania\" &gt; \"Catalonia\" theoretically derived.\nIn English, the term referring to a person first appears in the mid 14th century as \"Catelaner\", followed in the 15th century as \"Catellain\" (from French). It is attested a language name since at least 1652. The word \"Catalan\" can be pronounced in English as , or .\nThe endonym is pronounced ] in the Eastern Catalan dialects, and ] in the Western dialects. In the Valencian Community and Carche, the term ] is frequently used instead. Thus, the name \"Valencian\", although often employed for referring to the varieties specific to the Valencian Community and Carche, is also used by Valencians as a name for the language as a whole, synonymous with \"Catalan\". Both uses of the term have their respective entries in the dictionaries by the Acad\u00e8mia Valenciana de la Llengua and the Institut d'Estudis Catalans. See also status of Valencian below.\nHistory.\nMiddle Ages.\nBy the 9th century, Catalan had evolved from Vulgar Latin on both sides of the eastern end of the Pyrenees, as well as the territories of the Roman province of Hispania Tarraconensis to the south. From the 8th century onwards the Catalan counts extended their territory southwards and westwards at the expense of the Muslims, bringing their language with them. This process was given definitive impetus with the separation of the County of Barcelona from the Carolingian Empire in 988.\nIn the 11th century, documents written in macaronic Latin begin to show Catalan elements, with texts written almost completely in Romance appearing by 1080. Old Catalan shared many features with Gallo-Romance, diverging from Old Occitan between the 11th and 14th centuries.\nDuring the 11th and 12th centuries the Catalan rulers expanded southward to the Ebro river, and in the 13th century they conquered the Land of Valencia and the Balearic Islands. The city of Alghero in Sardinia was repopulated with Catalan speakers in the 14th century. The language also reached Murcia, which became Spanish-speaking in the 15th century.\nIn the Low Middle Ages, Catalan went through a golden age, reaching a peak of maturity and cultural richness. Examples include the work of Majorcan polymath Ramon Llull (1232\u20131315), the Four Great Chronicles (13th\u201314th centuries), and the Valencian school of poetry culminating in Ausi\u00e0s March (1397\u20131459). By the 15th century, the city of Valencia had become the sociocultural center of the Crown of Aragon, and Catalan was present all over the Mediterranean world. During this period, the Royal Chancery propagated a highly standardized language. Catalan was widely used as an official language in Sicily until the 15th century, and in Sardinia until the 17th. During this period, the language was what Costa Carreras terms \"one of the 'great languages' of medieval Europe\".\nMartorell's outstanding novel of chivalry \"Tirant lo Blanc\" (1490) shows a transition from Medieval to Renaissance values, something that can also be seen in Metge's work. The first book produced with movable type in the Iberian Peninsula was printed in Catalan.\nStart of the modern era.\nSpain.\nWith the union of the crowns of Castille and Aragon in 1479, the Spanish kings ruled over different kingdoms, each with its own cultural, linguistic and political particularities, and they had to swear by the Laws of each territory before the respective Parliaments. But after the War of the Spanish Succession, Spain became an Absolute monarchy under Philip V, which led to the assimilation of the Crown of Aragon by the Crown of Castile through the Nueva Planta decrees, as a first step in the creation of the Spanish nation-state; as in other contemporary European states, this meant the imposition of the political and cultural characteristics of the dominant groups. Since the political unification of 1714, Spanish assimilation policies towards national minorities have been a constant.\nThe process of assimilation began with secret instructions to the corregidores of the Catalan territory: they \"will take the utmost care to introduce the Castilian language, for which purpose he will give the most temperate and disguised measures so that the effect is achieved, without the care being noticed.\" From there, actions in the service of assimilation, discreet or aggressive, were continued, and reached to the last detail, such as, in 1799, the Royal Certificate forbidding anyone to \"represent, sing and dance pieces that were not in Spanish.\" Anyway, the use of Spanish gradually became more prestigious and marked the start of the decline of Catalan. Starting in the 16th century, Catalan literature came under the influence of Spanish, and the nobles, part of the urban and literary classes became bilingual.\nFrance.\nWith the Treaty of the Pyrenees (1659), Spain ceded the northern part of Catalonia to France, and soon thereafter the local Catalan varieties came under the influence of French, which in 1700 became the sole official language of the region.\nShortly after the French Revolution (1789), the French First Republic prohibited official use of, and enacted discriminating policies against, the regional languages of France, such as Catalan, Alsatian, Breton, Occitan, Flemish, and Basque.\nFrance: 19th to 20th century.\nFollowing the French establishment of the colony of Algeria from 1830 onward, it received several waves of Catalan-speaking settlers. People from the Spanish Alicante province settled around Oran, whereas Algiers received immigration from Northern Catalonia and Menorca.\nTheir speech was known as \"patuet\". By 1911, the number of Catalan speakers was around 100,000. After the declaration of independence of Algeria in 1962, almost all the Catalan speakers fled to Northern Catalonia (as \"Pieds-Noirs\") or Alacant.\nThe government of France formally recognizes only French as an official language. Nevertheless, on 10 December 2007, the General Council of the Pyr\u00e9n\u00e9es-Orientales officially recognized Catalan as one of the languages of the department and seeks to further promote it in public life and education.\nSpain: 18th to 20th century.\nIn 1807, the Statistics Office of the French Ministry of the Interior asked the prefects for an official survey on the limits of the French language. The survey found that in Roussillon, almost only Catalan was spoken, and since Napoleon wanted to incorporate Catalonia into France, as happened in 1812, the consul in Barcelona was also asked. He declared that Catalan \"is taught in schools, it is printed and spoken, not only among the lower class, but also among people of first quality, also in social gatherings, as in visits and congresses\", indicating that it was spoken everywhere \"with the exception of the royal courts\". He also indicated that Catalan was spoken \"in the Kingdom of Valencia, in the islands of Mallorca, Menorca, Ibiza, Sardinia, Corsica and much of Sicily, in the Vall d \"Aran and Cerda\u00f1a\".\nThe defeat of the pro-Habsburg coalition in the War of Spanish Succession (1714) initiated a series of laws which, among other centralizing measures, imposed the use of Spanish in legal documentation all over Spain. Because of this, use of the Catalan language declined into the 18th century.\nHowever, the 19th century saw a Catalan literary revival (), which has continued up to the present day. This period starts with Aribau's \"Ode to the Homeland\" (1833); followed in the second half of the 19th century, and the early 20th by the work of Verdaguer (poetry), Oller (realist novel), and Guimer\u00e0 (drama). In the 19th century, the region of Carche, in the province of Murcia was repopulated with Valencian speakers. Catalan spelling was standardized in 1913 and the language became official during the Second Spanish Republic (1931\u20131939). The Second Spanish Republic saw a brief period of tolerance, with most restrictions against Catalan lifted.\nThe Catalan language and culture were frowned upon during the Spanish Civil War (1936\u20131939) and the subsequent decades in Francoist Catalonia. The Francoist dictatorship (1939\u20131975) imposed the use of Spanish in schools and in public administration in all of Spain. However, in 1944, it became mandatory by law for universities with Romance Philology to include the subject of Catalan Philology. Numerous and prestigious cultural contests were created to reward works produced in Catalan. In January 1944, the \"Eugenio Nadal\" award was created. In 1945, with the sponsorship and subsidy of the Government, the centenary of Moss\u00e8n Cinto Verdaguer was celebrated. In 1947 the \"Joan Martorell\" prize for novels in Catalan was awarded. In 1949, the \"V\u00edctor Catal\u00e0\" award for short novels in Catalan and the \"Aedos\" awards for biographies, the \"Josep Ysart\" award for essays, and the \"Ossa Menor\" award, later renamed \"Carles Riba\", were created. In 1951, a national prize was awarded to poetry in Catalan with the same financial amount as Spanish poetry. That same year, \"Selecta Editions\" was founded for works written in Catalan. And the \"Joanot Martorell\" is awarded to Josep Pla for his work \"El carrer estret\". In subsequent years (50s, 60s and 70s) countless awards were born, such as the \"Lletra d'Or\", \"Amadeu Oller\" for poetry, the \"Sant Jordi\" for novels (endowed with 150,000 pesetas), the Honor Award of Catalan Letters, the \"Verdaguer\", the Josep Pla Prize, the Merc\u00e8 Rodoreda Prize for short stories and narratives. The first Catalan-language TV show was broadcast during the Franco period, in 1964. The Francoist dictatorship (1939\u20131975) banned the use of Catalan in schools and in public administration. At the same time, oppression of the Catalan language and identity was carried out in schools, through governmental bodies, and in religious centers. Franco's desire for a homogenous Spanish population resonated with some Catalans in favor of his regime, primarily members of the upper class, who began to reject the use of Catalan. Despite all of these hardships, Catalan continued to be used privately within households, and it was able to survive Francisco Franco's dictatorship. Several prominent Catalan authors resisted the suppression through literature.\nIn addition to the loss of prestige for Catalan and its prohibition in schools, migration during the 1950s into Catalonia from other parts of Spain also contributed to the diminished use of the language. These migrants were often unaware of the existence of Catalan, and thus felt no need to learn or use it. Catalonia was the economic powerhouse of Spain, so these migrations continued to occur from all corners of the country. Employment opportunities were reduced for those who were not bilingual.\nPresent day.\nSince the Spanish transition to democracy (1975\u20131982), Catalan has been institutionalized as an official language, language of education, and language of mass media; all of which have contributed to its increased prestige. In Catalonia, there is an unparalleled large bilingual European non-state linguistic community. The teaching of Catalan is mandatory in all schools, but it is possible to use Spanish for studying in the public education system of Catalonia in two situations \u2013 if the teacher assigned to a class chooses to use Spanish, or during the learning process of one or more recently arrived immigrant students. There is also some intergenerational shift towards Catalan.\nMore recently, several Spanish political forces have tried to increase the use of Spanish in the Catalan educational system. As a result, in May 2022 the Spanish Supreme Court urged the Catalan regional government to enforce a measure by which 25% of all lessons must be taught in Spanish.\nAccording to the Statistical Institute of Catalonia, in 2013 the Catalan language is the second most commonly used in Catalonia, after Spanish, as a native or self-defining language: 7% of the population self-identifies with both Catalan and Spanish equally, 36.4% with Catalan and 47.5% only Spanish. In 2003 the same studies concluded no language preference for self-identification within the population above 15 years old: 5% self-identified with both languages, 44.3% with Catalan and 47.5% with Spanish. To promote use of Catalan, the Generalitat de Catalunya (Catalonia's official Autonomous government) spends part of its annual budget on the promotion of the use of Catalan in Catalonia and in other territories, with entities such as Consorci per a la Normalitzaci\u00f3 Ling\u00fc\u00edstica (\"Consortium for Linguistic Normalization\")\nIn Andorra, Catalan has always been the sole official language. Since the promulgation of the 1993 constitution, several policies favoring Catalan have been enforced, like Catalan medium education.\nOn the other hand, there are several language shift processes currently taking place. In the Northern Catalonia area of France, Catalan has followed the same trend as the other minority languages of France, with most of its native speakers being 60 or older (as of 2004). Catalan is studied as a foreign language by 30% of the primary education students, and by 15% of the secondary. The cultural association promotes a network of community-run schools engaged in Catalan language immersion programs.\nIn Alicante province, Catalan is being replaced by Spanish and in Alghero by Italian. There is also well ingrained diglossia in the Valencian Community, Ibiza, and to a lesser extent, in the rest of the Balearic islands.\nDuring the 20th century many Catalans emigrated or went into exile to Venezuela, Mexico, Cuba, Argentina and other South American countries. They formed a large number of Catalan colonies that today continue to maintain the Catalan language. They also founded many Catalan casals (associations).\nClassification and relationship with other Romance languages.\nOne classification of Catalan is given by P\u00e8ire B\u00e8c:\nHowever, the ascription of Catalan to the Occitano-Romance branch of Gallo-Romance languages is not shared by all linguists and philologists, particularly among Spanish ones, such as Ram\u00f3n Men\u00e9ndez Pidal.\nCatalan bears varying degrees of similarity to the linguistic varieties subsumed under the cover term \"Occitan language\" (see also differences between Occitan and Catalan and Gallo-Romance languages). Thus, as it should be expected from closely related languages, Catalan today shares many traits with other Romance languages.\nRelationship with other Romance languages.\nSome include Catalan in Occitan, as the linguistic distance between this language and some Occitan dialects (such as the Gascon language) is similar to the distance among different Occitan dialects. Catalan was considered a dialect of Occitan until the end of the 19th century and still today remains its closest relative.\nCatalan shares many traits with the other neighboring Romance languages (Occitan, French, Italian, Sardinian as well as Spanish and Portuguese among others). However, despite being spoken mostly on the Iberian Peninsula, Catalan has marked differences with the Iberian Romance group (Spanish and Portuguese) in terms of pronunciation, grammar, and especially vocabulary; it shows instead its closest affinity with languages native to France and northern Italy, particularly Occitan and to a lesser extent Gallo-Romance (Franco-Proven\u00e7al, French, Gallo-Italian).\nAccording to Ethnologue, the lexical similarity between Catalan and other Romance languages is: 87% with Italian; 85% with Portuguese and Spanish; 76% with Ladin and Romansh; 75% with Sardinian; and 73% with Romanian.\nDuring much of its history, and especially during the Francoist dictatorship (1939\u20131975), the Catalan language was ridiculed as a mere dialect of Spanish. This view, based on political and ideological considerations, has no linguistic validity. Spanish and Catalan have important differences in their sound systems, lexicon, and grammatical features, placing the language in features closer to Occitan (and French).\nThere is evidence that, at least from the 2nd century a.d., the vocabulary and phonology of Roman Tarraconensis was different from the rest of Roman Hispania. Differentiation arose generally because Spanish, Asturian, and Galician-Portuguese share certain peripheral archaisms (Spanish , Asturian and Portuguese vs. Catalan , Occitan \"to boil\") and innovatory regionalisms (Sp , Ast vs. Cat , Oc \"bullock\"), while Catalan has a shared history with the Western Romance innovative core, especially Occitan.\nLike all Romance languages, Catalan has a handful of native words which are unique to it, or rare elsewhere. These include:\nThe Gothic superstrate produced different outcomes in Spanish and Catalan. For example, Catalan \"mud\" and \"to roast\", of Germanic origin, contrast with Spanish and , of Latin origin; whereas Catalan \"spinning wheel\" and \"temple\", of Latin origin, contrast with Spanish and , of Germanic origin.\nThe same happens with Arabic loanwords. Thus, Catalan \"large earthenware jar\" and \"tile\", of Arabic origin, contrast with Spanish and , of Latin origin; whereas Catalan \"oil\" and \"olive\", of Latin origin, contrast with Spanish and . However, the Arabic element in Spanish is generally much more prevalent.\nSituated between two large linguistic blocks (Iberian Romance and Gallo-Romance), Catalan has many unique lexical choices, such as \"to miss somebody\", \"to calm somebody down\", and \"reject\".\nGeographic distribution.\nCatalan-speaking territories.\nTraditionally Catalan-speaking territories are sometimes called the (Catalan Countries), a denomination based on cultural affinity and common heritage, that has also had a subsequent political interpretation but no official status. Various interpretations of the term may include some or all of these regions.\nNumber of speakers.\nThe number of people known to be fluent in Catalan varies depending on the sources used. A 2004 study did not count the total number of speakers, but estimated a total of 9\u20139.5 million by matching the percentage of speakers to the population of each area where Catalan is spoken. The web site of the Generalitat de Catalunya estimated that as of 2004 there were 9,118,882 speakers of Catalan. These figures only reflect potential speakers; today it is the native language of only 35.6% of the Catalan population. According to \"Ethnologue\", Catalan had 4.1 million native speakers and 5.1 million second-language speakers in 2021.\nAccording to a 2011 study the total number of Catalan speakers is over 9.8 million, with 5.9 million residing in Catalonia. More than half of them speak Catalan as a second language, with native speakers being about 4.4 million of those (more than 2.8 in Catalonia). Very few Catalan monoglots exist; basically, virtually all of the Catalan speakers in Spain are bilingual speakers of Catalan and Spanish, with a sizable population of Spanish-only speakers of immigrant origin (typically born outside Catalonia or whose parents were both born outside Catalonia) existing in the major Catalan urban areas as well.\nIn Roussillon, only a minority of French Catalans speak Catalan nowadays, with French being the majority language for the inhabitants after a continued process of language shift. According to a 2019 survey by the Catalan government, 31.5% of the inhabitants of Catalonia have Catalan as first language at home whereas 52.7% have Spanish, 2.8% both Catalan and Spanish and 10.8% other languages.\nSpanish is the most spoken language in Barcelona (according to the linguistic census held by the Government of Catalonia in 2013) and it is understood almost universally. According to this census of 2013 Catalan is also very commonly spoken in the city of 1,501,262: it is understood by 95% of the population, while 72.3% over the age of 2 can speak it (1,137,816), 79% can read it (1,246.555), and 53% can write it (835,080). The proportion in Barcelona who can speak it, 72.3%, is lower than that of the overall Catalan population, of whom 81.2% over the age of 15 speak the language. Knowledge of Catalan has increased significantly in recent decades thanks to a language immersion educational system. An important social characteristic of the Catalan language is that all the areas where it is spoken are bilingual in practice: together with the French language in Roussillon, with Italian in Alghero, with Spanish and French in Andorra and with Spanish in the rest of the territories.\n\nLevel of knowledge.\n(% of the population 15 years old and older).\nSocial use.\n(% of the population 15 years old and older).\nPhonology.\nCatalan phonology varies by dialect. Notable features include:\nIn contrast to other Romance languages, Catalan has many monosyllabic words, and these may end in a wide variety of consonants, including some consonant clusters. Additionally, Catalan has final obstruent devoicing, which gives rise to an abundance of such couplets as (\"male friend\") vs. (\"female friend\").\nCentral Catalan pronunciation is considered to be standard for the language. The descriptions below are mostly representative of this variety. For the differences in pronunciation between the different dialects, see the section on pronunciation of dialects in this article.\nVowels.\nCatalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: /a \u025b e i \u0254 o u/, a common feature in Western Romance, with the exception of Spanish. Balearic also has instances of stressed /\u0259/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /\u025b e/.\nIn Central Catalan, unstressed vowels reduce to three: /a e \u025b/ &gt; [\u0259]; /o \u0254 u/ &gt; [u]; /i/ remains distinct. The other dialects have different vowel reduction processes (see the section pronunciation of dialects in this article).\nConsonants.\nThe consonant system of Catalan is rather conservative.\nSociolinguistics.\nCatalan sociolinguistics studies the situation of Catalan in the world and the different varieties that this language presents. It is a subdiscipline of Catalan philology and other affine studies and has as an objective to analyze the relation between the Catalan language, the speakers and the close reality (including the one of other languages in contact).\nDialects.\nOverview.\nThe dialects of the Catalan language feature a relative uniformity, especially when compared to other Romance languages; both in terms of vocabulary, semantics, syntax, morphology, and phonology. Mutual intelligibility between dialects is very high, estimates ranging from 90% to 95%. The only exception is the isolated idiosyncratic Algherese dialect.\nCatalan is split in two major dialectal blocks: Eastern and Western. The main difference lies in the treatment of unstressed and ; which have merged to /\u0259/ in Eastern dialects, but which remain distinct as /a/ and /e/ in Western dialects. There are a few other differences in pronunciation, verbal morphology, and vocabulary.\nWestern Catalan comprises the two dialects of Northwestern Catalan and Valencian; the Eastern block comprises four dialects: Central Catalan, Balearic, Rossellonese, and Algherese. Each dialect can be further subdivided in several subdialects. The terms \"Catalan\" and \"Valencian\" (respectively used in Catalonia and the Valencian Community) refer to two varieties of the same language. There are two institutions regulating the two standard varieties, the Institute of Catalan Studies in Catalonia and the Valencian Academy of the Language in the Valencian Community.\nCentral Catalan is considered the standard pronunciation of the language and has the largest number of speakers. It is spoken in the densely populated regions of the Barcelona province, the eastern half of the province of Tarragona, and most of the province of Girona.\nCatalan has an inflectional grammar. Nouns have two genders (masculine, feminine), and two numbers (singular, plural). Pronouns additionally can have a neuter gender, and some are also inflected for case and politeness, and can be combined in very complex ways. Verbs are split in several paradigms and are inflected for person, number, tense, aspect, mood, and gender. In terms of pronunciation, Catalan has many words ending in a wide variety of consonants and some consonant clusters, in contrast with many other Romance languages.\nPronunciation.\nVowels.\nCatalan has inherited the typical vowel system of Vulgar Latin, with seven stressed phonemes: /a \u025b e i \u0254 o u/, a common feature in Western Romance, except Spanish. Balearic has also instances of stressed /\u0259/. Dialects differ in the different degrees of vowel reduction, and the incidence of the pair /\u025b e/.\nIn Eastern Catalan (except Majorcan), unstressed vowels reduce to three: /a e \u025b/ &gt; [\u0259]; /o \u0254 u/ &gt; [u]; /i/ remains distinct. There are a few instances of unreduced [e], [o] in some words. Algherese has lowered [\u0259] to [a].\nIn Majorcan, unstressed vowels reduce to four: /a e \u025b/ follow the Eastern Catalan reduction pattern; however /o \u0254/ reduce to [o], with /u/ remaining distinct, as in Western Catalan.\nIn Western Catalan, unstressed vowels reduce to five: /e \u025b/ &gt; [e]; /o \u0254/ &gt; [o]; /a u i/ remain distinct. This reduction pattern, inherited from Proto-Romance, is also found in Italian and Portuguese. Some Western dialects present further reduction or vowel harmony in some cases.\nCentral, Western, and Balearic differ in the lexical incidence of stressed /e/ and /\u025b/. Usually, words with /\u025b/ in Central Catalan correspond to /\u0259/ in Balearic and /e/ in Western Catalan. Words with /e/ in Balearic almost always have /e/ in Central and Western Catalan as well. As a result, Central Catalan has a much higher incidence of /\u025b/.\nMorphology.\nWestern Catalan: In verbs, the ending for 1st-person present indicative is in verbs of the 1st conjugation and -\u2205 in verbs of the 2nd and 3rd conjugations in most of the Valencian Community, or in all verb conjugations in the Northern Valencian Community and Western Catalonia.E.g. , , (Valencian); , , (Northwestern Catalan).\nEastern Catalan: In verbs, the ending for 1st-person present indicative is , , or -\u2205 in all conjugations. E.g. (Central), (Balearic), and (Northern), all meaning ('I speak').\nWestern Catalan: In verbs, the inchoative endings are /, , , /.\nEastern Catalan: In verbs, the inchoative endings are , , , .\nWestern Catalan: In nouns and adjectives, maintenance of /n/ of medieval plurals in proparoxytone words.E.g. 'men', 'youth'.\nEastern Catalan: In nouns and adjectives, loss of /n/ of medieval plurals in proparoxytone words.E.g. 'men', 'youth' (Ibicencan, however, follows the model of Western Catalan in this case).\nVocabulary.\nDespite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.\nStandards.\nStandard Catalan, virtually accepted by all speakers, is mostly based on Eastern Catalan, which is the most widely used dialect. Nevertheless, the standards of the Valencian Community and the Balearics admit alternative forms, mostly traditional ones, which are not current in eastern Catalonia.\nThe most notable difference between both standards is some tonic \u27e8e\u27e9 accentuation, for instance: (IEC) \u2013 (AVL). Nevertheless, AVL's standard keeps the grave accent \u27e8\u00e8\u27e9, while pronouncing it as /e/ rather than /\u025b/, in some words like: ('what'), or . Other divergences include the use of \u27e8tl\u27e9 (AVL) in some words instead of \u27e8tll\u27e9 like in / ('almond'), / ('back'), the use of elided demonstratives ( 'this', 'that') in the same level as reinforced ones () or the use of many verbal forms common in Valencian, and some of these common in the rest of Western Catalan too, like subjunctive mood or inchoative conjugation in at the same level as or the priority use of morpheme in 1st person singular in present indicative ( verbs): instead of ('I buy').\nIn the Balearic Islands, IEC's standard is used but adapted for the Balearic dialect by the University of the Balearic Islands's philological section. In this way, for instance, IEC says it is correct writing as much as ('we sing'), but the university says that the priority form in the Balearic Islands must be in all fields. Another feature of the Balearic standard is the non-ending in the 1st person singular present indicative: ('I buy'), ('I fear'), ('I sleep').\nIn Alghero, the IEC has adapted its standard to the Algherese dialect. In this standard one can find, among other features: the definite article instead of , special possessive pronouns and determinants ('mine'), ('his/her'), ('yours'), and so on, the use of /v/ in the imperfect tense in all conjugations: , , ; the use of many archaic words, usual words in Algherese: instead of ('less'), instead of ('someone'), instead of ('which'), and so on; and the adaptation of weak pronouns.\nIn 2011, the Aragonese government passed a decree approving the statutes of a new language regulator of Catalan in La Franja (the so-called Catalan-speaking areas of Aragon) as originally provided for by Law 10/2009. The new entity, designated as , shall allow a facultative education in Catalan and a standardization of the Catalan language in La Franja.\nStatus of Valencian.\nValencian is classified as a Western dialect, along with the northwestern varieties spoken in Western Catalonia (provinces of Lleida and the western half of Tarragona). Central Catalan has 90% to 95% inherent intelligibility for speakers of Valencian.\nLinguists, including Valencian scholars, deal with Catalan and Valencian as the same language. The official regulating body of the language of the Valencian Community, the Valencian Academy of Language (\"Acad\u00e8mia Valenciana de la Llengua\", AVL) declares the linguistic unity between Valencian and Catalan varieties.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n [T]he historical patrimonial language of the Valencian people, from a philological standpoint, is the same shared by the autonomous communities of Catalonia and Balearic islands, and Principality of Andorra. Additionally, it is the patrimonial historical language of other territories of the ancient Crown of Aragon [...] The different varieties of these territories constitute a language, that is, a \"linguistic system\" [...] From this group of varieties, Valencian has the same hierarchy and dignity as any other dialectal modality of that linguistic system [...]\nRuling of the Valencian Language Academy of 9 February 2005, extract of point 1.\nThe AVL, created by the Valencian parliament, is in charge of dictating the official rules governing the use of Valencian, and its standard is based on the Norms of Castell\u00f3 (\"Normes de Castell\u00f3\"). Currently, everyone who writes in Valencian uses this standard, except the Royal Academy of Valencian Culture (\"Acad\u00e8mia de Cultura Valenciana\", RACV), which uses for Valencian an independent standard.\nDespite the position of the official organizations, an opinion poll carried out between 2001 and 2004 showed that the majority of the Valencian people consider Valencian different from Catalan. This position is promoted by people who do not use Valencian regularly. Furthermore, the data indicates that younger generations educated in Valencian are much less likely to hold these views. A minority of Valencian scholars active in fields other than linguistics defends the position of the Royal Academy of Valencian Culture (\"Acad\u00e8mia de Cultura Valenciana\", RACV), which uses for Valencian a standard independent from Catalan.\nThis clash of opinions has sparked much controversy. For example, during the drafting of the European Constitution in 2004, the Spanish government supplied the EU with translations of the text into Basque, Galician, Catalan, and Valencian, but the latter two were identical.\nVocabulary.\nWord choices.\nDespite its relative lexical unity, the two dialectal blocks of Catalan (Eastern and Western) show some differences in word choices. Any lexical divergence within any of the two groups can be explained as an archaism. Also, usually Central Catalan acts as an innovative element.\nLiterary Catalan allows the use of words from different dialects, except those of very restricted use. However, from the 19th century onwards, there has been a tendency towards favoring words of Northern dialects to the detriment of others, \nLatin and Greek loanwords.\nLike other languages, Catalan has a large list of loanwords from Greek and Latin. This process started very early, and one can find such examples in Ramon Llull's work. In the 14th and 15th centuries Catalan had a far greater number of Greco-Latin loanwords than other Romance languages, as is attested for example in Ro\u00eds de Corella's writings. The incorporation of learned, or \"bookish\" words from its own ancestor language, Latin, into Catalan is arguably another form of lexical borrowing through the influence of written language and the liturgical language of the Church. Throughout the Middle Ages and into the early modern period, most literate Catalan speakers were also literate in Latin; and thus they easily adopted Latin words into their writing\u2014and eventually speech\u2014in Catalan.\nWord formation.\nThe process of morphological derivation in Catalan follows the same principles as the other Romance languages, where agglutination is common. Many times, several affixes are appended to a preexisting lexeme, and some sound alternations can occur, for example [\u0259\u02c8l\u025bktrik] (\"electrical\") vs. [\u0259l\u0259ktrisi\u02c8tat]. Prefixes are usually appended to verbs, as in (\"foresee\").\nThere is greater regularity in the process of word-compounding, where one can find compounded words formed much like those in English.\nWriting system.\nCatalan uses the Latin script, with some added symbols and digraphs. The Catalan orthography is systematic and largely phonologically based. Standardization of Catalan was among the topics discussed during the First International Congress of the Catalan Language, held in Barcelona October 1906. Subsequently, the Philological Section of the Institut d'Estudis Catalans (IEC, founded in 1911) published the \"Normes ortogr\u00e0fiques\" in 1913 under the direction of Antoni Maria Alcover and Pompeu Fabra. In 1932, Valencian writers and intellectuals gathered in Castell\u00f3 de la Plana to make a formal adoption of the so-called \"Normes de Castell\u00f3\", a set of guidelines following Pompeu Fabra's Catalan language norms.\nGrammar.\nThe grammar of Catalan is similar to other Romance languages. Features include:\nGender and number inflection.\nIn gender inflection, the most notable feature is (compared to Portuguese, Spanish or Italian), the loss of the typical masculine suffix . Thus, the alternance of /, has been replaced by \"\u00f8\"/. There are only a few exceptions, like / (\"scarce\"). Many not completely predictable morphological alternations may occur, such as:\nCatalan has few suppletive couplets, like Italian and Spanish, and unlike French. Thus, Catalan has / (\"boy\"/\"girl\") and / (\"cock\"/\"hen\"), whereas French has / and /.\nThere is a tendency to abandon traditionally gender-invariable adjectives in favor of marked ones, something prevalent in Occitan and French. Thus, one can find / (\"boiling\") in contrast with traditional /.\nAs in the other Western Romance languages, the main plural expression is the suffix , which may create morphological alternations similar to the ones found in gender inflection, albeit more rarely. The most important one is the addition of before certain consonant groups, a phonetic phenomenon that does not affect feminine forms: / (\"the pulse\"/\"the pulses\") vs. / (\"the dust\"/\"the dusts\").\nDeterminers.\nThe inflection of determinatives is complex, specially because of the high number of elisions, but is similar to the neighboring languages. Catalan has more contractions of preposition + article than Spanish, like (\"of + the [plural]\"), but not as many as Italian (which has , , , etc.).\nCentral Catalan has abandoned almost completely unstressed possessives (, etc.) in favor of constructions of article + stressed forms (, etc.), a feature shared with Italian.\nPersonal pronouns.\nThe morphology of Catalan personal pronouns is complex, especially in unstressed forms, which are numerous (13 distinct forms, compared to 11 in Spanish or 9 in Italian). Features include the gender-neutral and the great degree of freedom when combining different unstressed pronouns (65 combinations).\nCatalan pronouns exhibit T\u2013V distinction, like all other Romance languages (and most European languages, but not Modern English). This feature implies the use of a different set of second person pronouns for formality.\nThis flexibility allows Catalan to use extraposition extensively, much more than French or Spanish. Thus, Catalan can have (\"they recommended me to him\"), whereas in French one must say , and Spanish . This allows the placement of almost any nominal term as a sentence topic, without having to use so often the passive voice (as in French or English), or identifying the direct object with a preposition (as in Spanish).\nVerbs.\nLike all the Romance languages, Catalan verbal inflection is more complex than the nominal. Suffixation is omnipresent, whereas morphological alternations play a secondary role. Vowel alternances are active, as well as infixation and suppletion. However, these are not as productive as in Spanish, and are mostly restricted to irregular verbs.\nThe Catalan verbal system is basically common to all Western Romance, except that most dialects have replaced the synthetic indicative perfect with a periphrastic form of (\"to go\") + infinitive.\nCatalan verbs are traditionally divided into three conjugations, with vowel themes , , , the last two being split into two subtypes. However, this division is mostly theoretical. Only the first conjugation is nowadays productive (with about 3500 common verbs), whereas the third (the subtype of , with about 700 common verbs) is semiproductive. The verbs of the second conjugation are fewer than 100, and it is not possible to create new ones, except by compounding.\nSyntax.\nThe grammar of Catalan follows the general pattern of Western Romance languages. The primary word order is subject\u2013verb\u2013object. However, word order is very flexible. Commonly, verb-subject constructions are used to achieve a semantic effect. The sentence \"The train has arrived\" could be translated as or . Both sentences mean \"the train has arrived\", but the former puts a focus on the train, while the latter puts a focus on the arrival. This subtle distinction is described as \"what you might say while waiting in the station\" versus \"what you might say on the train.\"\nCatalan names.\nIn Spain, every person officially has two surnames, one of which is the father's first surname and the other is the mother's first surname. The law contemplates the possibility of joining both surnames with the Catalan conjunction \"i\" (\"and\").\nSample text.\nSelected text from Manuel de Pedrolo's 1970 novel (\"A love affair outside the city\").\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nInstitutions\nAbout the Catalan language\nMonolingual dictionaries\nBilingual and multilingual dictionaries\nAutomated translation systems\nPhrasebooks\nLearning resources\nCatalan-language online encyclopedia"}
{"id": "5283", "revid": "1110281609", "url": "https://en.wikipedia.org/wiki?curid=5283", "title": "Cryptomonads", "text": ""}
{"id": "5285", "revid": "597995", "url": "https://en.wikipedia.org/wiki?curid=5285", "title": "STS-51-F", "text": "1985 American crewed spaceflight\nSTS-51-F (also known as Spacelab 2) was the 19th flight of NASA's Space Shuttle program and the eighth flight of Space Shuttle \"Challenger\". It launched from Kennedy Space Center, Florida, on July 29, 1985, and landed eight days later on August 6, 1985.\nWhile STS-51-F's primary payload was the Spacelab 2 laboratory module, the payload that received the most publicity was the Carbonated Beverage Dispenser Evaluation, which was an experiment in which both Coca-Cola and Pepsi tried to make their carbonated drinks available to astronauts. A helium-cooled infrared telescope (IRT) was also flown on this mission, and while it did have some problems, it observed 60% of the galactic plane in infrared light.\nDuring launch, \"Challenger\" experienced multiple sensor failures in its Engine 1 Center SSME engine, which led to it shutting down and the shuttle had to perform an \"Abort to Orbit\" (ATO) emergency procedure. It is the only Shuttle mission to have carried out an abort after launching. As a result of the ATO, the mission was carried out at a slightly lower orbital altitude.\nCrew.\nCrew notes.\nAs with previous Spacelab missions, the crew was divided between two 12-hour shifts. Acton, Bridges and Henize made up the \"Red Team\" while Bartoe, England and Musgrave comprised the \"Blue Team\"; commander Fullerton could take either shift when needed. \"Challenger\" carried two Extravehicular Mobility Units (EMU) in the event of an emergency spacewalk, which would have been performed by England and Musgrave.\nLaunch.\nSTS-51-F's first launch attempt on July 12, 1985, was halted with the countdown at T\u22123 seconds after main engine ignition, when a malfunction of the number two RS-25 coolant valve caused an automatic launch abort. \"Challenger\" launched successfully on its second attempt on July 29, 1985, at 17:00\u00a0p.m. EDT, after a delay of 1 hour 37 minutes due to a problem with the table maintenance block update uplink.\nAt 3 minutes 31 seconds into the ascent, one of the center engine's two high-pressure fuel turbopump turbine discharge temperature sensors failed. Two minutes and twelve seconds later, the second sensor failed, causing the shutdown of the center engine. This was the only in-flight RS-25 failure of the Space Shuttle program. Approximately 8 minutes into the flight, one of the same temperature sensors in the right engine failed, and the remaining right-engine temperature sensor displayed readings near the redline for engine shutdown. Booster Systems Engineer Jenny M. Howard acted quickly to recommend that the crew inhibit any further automatic RS-25 shutdowns based on readings from the remaining sensors, preventing the potential shutdown of a second engine and a possible abort mode that may have resulted in the loss of crew and vehicle (LOCV).\nThe failed RS-25 resulted in an Abort to Orbit (ATO) trajectory, whereby the shuttle achieved a lower-than-planned orbital altitude. The plan had been for a by orbit, but the mission was carried out at by .\nMission summary.\nSTS-51-F's primary payload was the laboratory module Spacelab 2. A special part of the modular Spacelab system, the \"igloo\", which was located at the head of a three-pallet train, provided on-site support to instruments mounted on pallets. The main mission objective was to verify performance of Spacelab systems, determine the interface capability of the orbiter, and measure the environment created by the spacecraft. Experiments covered life sciences, plasma physics, astronomy, high-energy astrophysics, solar physics, atmospheric physics and technology research. Despite mission replanning necessitated by \"Challenger\"'s abort to orbit trajectory, the Spacelab mission was declared a success.\nThe flight marked the first time the European Space Agency (ESA) Instrument Pointing System (IPS) was tested in orbit. This unique pointing instrument was designed with an accuracy of one arcsecond. Initially, some problems were experienced when it was commanded to track the Sun, but a series of software fixes were made and the problem was corrected. In addition, Anthony W. England became the second amateur radio operator to transmit from space during the mission.\nSpacelab Infrared Telescope.\nThe Spacelab Infrared Telescope (IRT) was also flown on the mission. The IRT was a aperture helium-cooled infrared telescope, observing light between wavelengths of 1.7 to 118 \u03bcm. It was thought heat emissions from the Shuttle corrupting long-wavelength data, but it still returned useful astronomical data. Another problem was that a piece of mylar insulation broke loose and floated in the line-of-sight of the telescope. IRT collected infrared data on 60% of the galactic plane. (see also List of largest infrared telescopes) A later space mission that experienced a stray light problem from debris was \"Gaia\" astrometry spacecraft launch in 2013 by the ESA - the source of the stray light was later identified as the fibers of the sunshield, protruding beyond the edges of the shield.\nOther payloads.\nThe Plasma Diagnostics Package (PDP), which had been previously flown on STS-3, made its return on the mission, and was part of a set of plasma physics experiments designed to study the Earth's ionosphere. During the third day of the mission, it was grappled out of the payload bay by the Remote Manipulator System (Canadarm) and released for six hours. During this time, \"Challenger\" maneuvered around the PDP as part of a targeted proximity operations exercise. The PDP was successfully grappled by the Canadarm and returned to the payload bay at the beginning of the fourth day of the mission.\nIn a heavily publicized marketing experiment, astronauts aboard STS-51-F drank carbonated beverages from specially designed cans from Cola Wars competitors Coca-Cola and Pepsi. According to Acton, after Coke developed its experimental dispenser for an earlier shuttle flight, Pepsi insisted to American president Ronald Reagan that Coke should not be the first cola in space. The experiment was delayed until Pepsi could develop its own system, and the two companies' products were assigned to STS-51-F.\nRed Team tested Coke, and Blue Team tested Pepsi. As part of the experiment, each team was photographed with the cola logo. Acton said that while the sophisticated Coke system \"dispensed soda kind of like what we're used to drinking on Earth\", the Pepsi can was a shaving cream can with the Pepsi logo on a paper wrapper, which \"dispensed soda filled with bubbles\" that was \"not very drinkable\". Acton said that when he gives speeches in schools, audiences are much more interested in hearing about the cola experiment than in solar physics. Post-flight, the astronauts revealed that they preferred Tang, in part because it could be mixed on-orbit with existing chilled-water supplies, whereas there was no dedicated refrigeration equipment on board to chill the cans, which also fizzed excessively in microgravity.\nIn an experiment during the mission, thruster rockets were fired at a point over Tasmania and also above Boston to create two \"holes\" \u2013 plasma depletion regions \u2013 in the ionosphere. A worldwide group of geophysicists collaborated with the observations made from Spacelab 2.\nLanding.\n\"Challenger\" landed at Edwards Air Force Base, California, on August 6, 1985, at 12:45:26\u00a0p.m. PDT. Its rollout distance was . The mission had been extended by 17 orbits for additional payload activities due to the Abort to Orbit. The orbiter arrived back at Kennedy Space Center on August 11, 1985.\nMission insignia.\nThe mission insignia was designed by Houston, Texas artist Skip Bradley. Space Shuttle \"99 \" is depicted ascending toward the heavens in search of new knowledge in the field of solar and stellar astronomy, with its Spacelab 2 payload. The constellations Leo and Orion are shown in the positions they were in relative to the Sun during the flight. The nineteen stars indicate that the mission is the 19th shuttle flight.\nCrew bios.\nC. Gordon Fullerton died on August 21, 2013, aged 76.\nKarl Gordon Henize died October 5, 1993, aged 66, on an expedition to Mount Everest studying the effects of radiation from space.\nLegacy.\nOne of the purposes of the mission was to test how suitable the Shuttle was for conducting infrared observations, and the IRT was operated on this mission. However, the orbiter was found to have some draw-backs for infrared astronomy, and this led to later infrared telescopes being free-flying from the Shuttle orbiter.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5287", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5287", "title": "Classical Music", "text": ""}
{"id": "5288", "revid": "3236687", "url": "https://en.wikipedia.org/wiki?curid=5288", "title": "Classical period (music)", "text": "Era of classical music (c. 1730\u20131820)\nThe Classical period was an era of classical music between roughly 1750 and 1820.\nThe Classical period falls between the Baroque and the Romantic periods. Classical music has a lighter, clearer texture than Baroque music, but a more varying use of musical form, which is, in simpler terms, the rhythm and organization of any given piece of music. It is mainly homophonic, using a clear melody line over a subordinate chordal accompaniment, but counterpoint was by no means forgotten, especially in liturgical vocal music and, later in the period, secular instrumental music. It also makes use of \"style galant\" which emphasized light elegance in place of the Baroque's dignified seriousness and impressive grandeur. Variety and contrast within a piece became more pronounced than before and the orchestra increased in size, range, and power.\nThe harpsichord was replaced as the main keyboard instrument by the piano (or fortepiano). Unlike the harpsichord, which plucks strings with quills, pianos strike the strings with leather-covered hammers when the keys are pressed, which enables the performer to play louder or softer (hence the original name \"fortepiano,\" literally \"loud soft\") and play with more expression; in contrast, the force with which a performer plays the harpsichord keys does not change the sound. Instrumental music was considered important by Classical period composers. The main kinds of instrumental music were the sonata, trio, string quartet, quintet, symphony (performed by an orchestra) and the solo concerto, which featured a virtuoso solo performer playing a solo work for violin, piano, flute, or another instrument, accompanied by an orchestra. Vocal music, such as songs for a singer and piano (notably the work of Schubert), choral works, and opera (a staged dramatic work for singers and orchestra) were also important during this period.\nThe best-known composers from this period are Joseph Haydn, Wolfgang Amadeus Mozart, Ludwig van Beethoven, and Franz Schubert; other names in this period include: Carl Philipp Emanuel Bach, Johann Christian Bach, Luigi Boccherini, Domenico Cimarosa, Joseph Martin Kraus, Muzio Clementi, Christoph Willibald Gluck, Carl Ditters von Dittersdorf, Andr\u00e9 Gr\u00e9try, Pierre-Alexandre Monsigny, Leopold Mozart, Michael Haydn, Giovanni Paisiello, Johann Baptist Wanhal, Fran\u00e7ois-Andr\u00e9 Danican Philidor, Niccol\u00f2 Piccinni, Antonio Salieri, Etienne Nicolas Mehul, Georg Christoph Wagenseil, Georg Matthias Monn, Johann Gottlieb Graun, Carl Heinrich Graun, Franz Benda, Georg Anton Benda, Johann Georg Albrechtsberger, Mauro Giuliani, Christian Cannabich and the Chevalier de Saint-Georges. Beethoven is regarded either as a Romantic composer or a Classical period composer who was part of the transition to the Romantic era. Schubert is also a transitional figure, as were Johann Nepomuk Hummel, Luigi Cherubini, Gaspare Spontini, Gioachino Rossini, Carl Maria von Weber, Jan Ladislav Dussek and Niccol\u00f2 Paganini. The period is sometimes referred to as the era of \"Viennese Classicism\" (), since Gluck, Haydn, Salieri, Mozart, Beethoven, and Schubert all worked in Vienna.\nClassicism.\nIn the middle of the 18th century, Europe began to move toward a new style in architecture, literature, and the arts, generally known as Neoclassicism. This style sought to emulate the ideals of Classical antiquity, especially those of Classical Greece. Classical music used formality and emphasis on order and hierarchy, and a \"clearer\", \"cleaner\" style that used clearer divisions between parts (notably a clear, single melody accompanied by chords), brighter contrasts and \"tone colors\" (achieved by the use of dynamic changes and modulations to more keys). In contrast with the richly layered music of the Baroque era, Classical music moved towards simplicity rather than complexity. In addition, the typical size of orchestras began to increase, giving orchestras a more powerful sound.\nThe remarkable development of ideas in \"natural philosophy\" had already established itself in the public consciousness. In particular, Newton's physics was taken as a paradigm: structures should be well-founded in axioms and be both well-articulated and orderly. This taste for structural clarity began to affect music, which moved away from the layered polyphony of the Baroque period toward a style known as homophony, in which the melody is played over a subordinate harmony. This move meant that chords became a much more prevalent feature of music, even if they interrupted the melodic smoothness of a single part. As a result, the tonal structure of a piece of music became more audible.\nThe new style was also encouraged by changes in the economic order and social structure. As the 18th century progressed well, the nobility became the primary patrons of instrumental music, while public taste increasingly preferred lighter, funny comic operas. This led to changes in the way music was performed, the most crucial of which was the move to standard instrumental groups and the reduction in the importance of the \"continuo\"\u2014the rhythmic and harmonic groundwork of a piece of music, typically played by a keyboard (harpsichord or organ) and usually accompanied by a varied group of bass instruments, including cello, double bass, bass viol, and theorbo. One way to trace the decline of the continuo and its figured chords is to examine the disappearance of the term \"obbligato\", meaning a mandatory instrumental part in a work of chamber music. In Baroque compositions, additional instruments could be added to the continuo group according to the group or leader's preference; in Classical compositions, all parts were specifically noted, though not always \"notated\", so the term \"obbligato\" became redundant. By 1800, basso continuo was practically extinct, except for the occasional use of a pipe organ continuo part in a religious Mass in the early 1800s.\nEconomic changes also had the effect of altering the balance of availability and quality of musicians. While in the late Baroque, a major composer would have the entire musical resources of a town to draw on, the musical forces available at an aristocratic hunting lodge or small court were smaller and more fixed in their level of ability. This was a spur to having simpler parts for ensemble musicians to play, and in the case of a resident virtuoso group, a spur to writing spectacular, idiomatic parts for certain instruments, as in the case of the Mannheim orchestra, or virtuoso solo parts for particularly skilled violinists or flautists. In addition, the appetite by audiences for a continual supply of new music carried over from the Baroque. This meant that works had to be performable with, at best, one or two rehearsals. Even after 1790 Mozart writes about \"the rehearsal\", with the implication that his concerts would have only one rehearsal.\nSince there was a greater emphasis on a single melodic line, there was greater emphasis on notating that line for dynamics and phrasing. This contrasts with the Baroque era, when melodies were typically written with no dynamics, phrasing marks or ornaments, as it was assumed that the performer would improvise these elements on the spot. In the Classical era, it became more common for composers to indicate where they wanted performers to play ornaments such as trills or turns. The simplification of texture made such instrumental detail more important, and also made the use of characteristic rhythms, such as attention-getting opening fanfares, the funeral march rhythm, or the minuet genre, more important in establishing and unifying the tone of a single movement.\nThe Classical period also saw the gradual development of sonata form, a set of structural principles for music that reconciled the Classical preference for melodic material with harmonic development, which could be applied across musical genres. The sonata itself continued to be the principal form for solo and chamber music, while later in the Classical period the string quartet became a prominent genre. The symphony form for orchestra was created in this period (this is popularly attributed to Joseph Haydn). The \"concerto grosso\" (a concerto for more than one musician), a very popular form in the Baroque era, began to be replaced by the \"solo concerto\", featuring only one soloist. Composers began to place more importance on the particular soloist's ability to show off virtuoso skills, with challenging, fast scale and arpeggio runs. Nonetheless, some \"concerti grossi\" remained, the most famous of which being Mozart's Sinfonia Concertante for Violin and Viola in E-flat major.\nMain characteristics.\nIn the classical period, the theme consists of phrases with contrasting melodic figures and rhythms. These phrases are relatively brief, typically four bars in length, and can occasionally seem sparse or terse. The texture is mainly homophonic, with a clear melody above a subordinate chordal accompaniment, for instance an Alberti bass. This contrasts with the practice in Baroque music, where a piece or movement would typically have only one musical subject, which would then be worked out in a number of voices according to the principles of counterpoint, while maintaining a consistent rhythm or metre throughout. As a result, Classical music tends to have a lighter, clearer texture than the Baroque. The classical style draws on the \"style galant\", a musical style which emphasised light elegance in place of the Baroque's dignified seriousness and impressive grandeur.\nStructurally, Classical music generally has a clear musical form, with a well-defined contrast between tonic and dominant, introduced by clear cadences. Dynamics are used to highlight the structural characteristics of the piece. In particular, sonata form and its variants were developed during the early classical period and was frequently used. The Classical approach to structure again contrasts with the Baroque, where a composition would normally move between tonic and dominant and back again, but through a continual progress of chord changes and without a sense of \"arrival\" at the new key. While counterpoint was less emphasised in the classical period, it was by no means forgotten, especially later in the period, and composers still used counterpoint in \"serious\" works such as symphonies and string quartets, as well as religious pieces, such as Masses.\nThe classical musical style was supported by technical developments in instruments. The widespread adoption of equal temperament made classical musical structure possible, by ensuring that cadences in all keys sounded similar. The fortepiano and then the pianoforte replaced the harpsichord, enabling more dynamic contrast and more sustained melodies. Over the Classical period, keyboard instruments became richer, more sonorous and more powerful.\nThe orchestra increased in size and range, and became more standardised. The harpsichord or pipe organ basso continuo role in orchestra fell out of use between 1750 and 1775, leaving the string section. Woodwinds became a self-contained section, consisting of clarinets, oboes, flutes and bassoons.\nWhile vocal music such as comic opera was popular, great importance was given to instrumental music. The main kinds of instrumental music were the sonata, trio, string quartet, quintet, symphony, concerto (usually for a virtuoso solo instrument accompanied by orchestra), and light pieces such as serenades and divertimentos. Sonata form developed and became the most important form. It was used to build up the first movement of most large-scale works in symphonies and string quartets. Sonata form was also used in other movements and in single, standalone pieces such as overtures.\nHistory.\nBaroque/Classical transition c. 1750\u20131760.\nIn his book \"The Classical Style\", author and pianist Charles Rosen claims that from 1755 to 1775, composers groped for a new style that was more effectively dramatic. In the High Baroque period, dramatic expression was limited to the representation of individual \"affects\" (the \"doctrine of affections\", or what Rosen terms \"dramatic sentiment\"). For example, in Handel's oratorio \"Jephtha\", the composer renders four emotions separately, one for each character, in the quartet \"O, spare your daughter\". Eventually this depiction of individual emotions came to be seen as simplistic and unrealistic; composers sought to portray multiple emotions, simultaneously or progressively, within a single character or movement (\"dramatic action\"). Thus in the finale of act 2 of Mozart's \"Die Entf\u00fchrung aus dem Serail\", the lovers move \"from joy through suspicion and outrage to final reconciliation.\"\nMusically speaking, this \"dramatic action\" required more musical variety. Whereas Baroque music was characterized by seamless flow within individual movements and largely uniform textures, composers after the High Baroque sought to interrupt this flow with abrupt changes in texture, dynamic, harmony, or tempo. Among the stylistic developments which followed the High Baroque, the most dramatic came to be called \"Empfindsamkeit\", (roughly \"sensitive style\"), and its best-known practitioner was Carl Philipp Emanuel Bach. Composers of this style employed the above-discussed interruptions in the most abrupt manner, and the music can sound illogical at times. The Italian composer Domenico Scarlatti took these developments further. His more than five hundred single-movement keyboard sonatas also contain abrupt changes of texture, but these changes are organized into periods, balanced phrases that became a hallmark of the classical style. However, Scarlatti's changes in texture still sound sudden and unprepared. The outstanding achievement of the great classical composers (Haydn, Mozart and Beethoven) was their ability to make these dramatic surprises sound logically motivated, so that \"the expressive and the elegant could join hands.\"\nBetween the death of J. S. Bach and the maturity of Haydn and Mozart (roughly 1750\u20131770), composers experimented with these new ideas, which can be seen in the music of Bach's sons. Johann Christian developed a style which we now call \"Roccoco\", comprising simpler textures and harmonies, and which was \"charming, undramatic, and a little empty.\" As mentioned previously, Carl Philipp Emmanuel sought to increase drama, and his music was \"violent, expressive, brilliant, continuously surprising, and often incoherent.\" And finally Wilhelm Friedemann, J.S. Bach's eldest son, extended Baroque traditions in an idiomatic, unconventional way.\nAt first the new style took over Baroque forms\u2014the ternary \"da capo aria\", the \"sinfonia\" and the \"concerto\"\u2014but composed with simpler parts, more notated ornamentation, rather than the improvised ornaments that were common in the Baroque era, and more emphatic division of pieces into sections. However, over time, the new aesthetic caused radical changes in how pieces were put together, and the basic formal layouts changed. Composers from this period sought dramatic effects, striking melodies, and clearer textures. One of the big textural changes was a shift away from the complex, dense polyphonic style of the Baroque, in which multiple interweaving melodic lines were played simultaneously, and towards homophony, a lighter texture which uses a clear single melody line accompanied by chords.\nBaroque music generally uses many harmonic fantasies and polyphonic sections that focus less on the structure of the musical piece, and there was less emphasis on clear musical phrases. In the classical period, the harmonies became simpler. However, the structure of the piece, the phrases and small melodic or rhythmic motives, became much more important than in the Baroque period.\nAnother important break with the past was the radical overhaul of opera by Christoph Willibald Gluck, who cut away a great deal of the layering and improvisational ornaments and focused on the points of modulation and transition. By making these moments where the harmony changes more of a focus, he enabled powerful dramatic shifts in the emotional color of the music. To highlight these transitions, he used changes in instrumentation (orchestration), melody, and mode. Among the most successful composers of his time, Gluck spawned many emulators, including Antonio Salieri. Their emphasis on accessibility brought huge successes in opera, and in other vocal music such as songs, oratorios, and choruses. These were considered the most important kinds of music for performance and hence enjoyed greatest public success.\nThe phase between the Baroque and the rise of the Classical (around 1730), was home to various competing musical styles. The diversity of artistic paths are represented in the sons of Johann Sebastian Bach: Wilhelm Friedemann Bach, who continued the Baroque tradition in a personal way; Johann Christian Bach, who simplified textures of the Baroque and most clearly influenced Mozart; and Carl Philipp Emanuel Bach, who composed passionate and sometimes violently eccentric music of the \"Empfindsamkeit\" movement. Musical culture was caught at a crossroads: the masters of the older style had the technique, but the public hungered for the new. This is one of the reasons C. P. E. Bach was held in such high regard: he understood the older forms quite well and knew how to present them in new garb, with an enhanced variety of form.\n1750\u20131775.\nBy the late 1750s there were flourishing centers of the new style in Italy, Vienna, Mannheim, and Paris; dozens of symphonies were composed and there were bands of players associated with musical theatres. Opera or other vocal music accompanied by orchestra was the feature of most musical events, with concertos and symphonies (arising from the overture) serving as instrumental interludes and introductions for operas and church services. Over the course of the Classical period, symphonies and concertos developed and were presented independently of vocal music.\nThe \"normal\" orchestra ensemble\u2014a body of strings supplemented by winds\u2014and movements of particular rhythmic character were established by the late 1750s in Vienna. However, the length and weight of pieces was still set with some Baroque characteristics: individual movements still focused on one \"affect\" (musical mood) or had only one sharply contrasting middle section, and their length was not significantly greater than Baroque movements. There was not yet a clearly enunciated theory of how to compose in the new style. It was a moment ripe for a breakthrough.\nThe first great master of the style was the composer Joseph Haydn. In the late 1750s he began composing symphonies, and by 1761 he had composed a triptych (\"Morning\", \"Noon\", and \"Evening\") solidly in the contemporary mode. As a vice-Kapellmeister and later Kapellmeister, his output expanded: he composed over forty symphonies in the 1760s alone. And while his fame grew, as his orchestra was expanded and his compositions were copied and disseminated, his voice was only one among many.\nWhile some scholars suggest that Haydn was overshadowed by Mozart and Beethoven, it would be difficult to overstate Haydn's centrality to the new style, and therefore to the future of Western art music as a whole. At the time, before the pre-eminence of Mozart or Beethoven, and with Johann Sebastian Bach known primarily to connoisseurs of keyboard music, Haydn reached a place in music that set him above all other composers except perhaps the Baroque era's George Frideric Handel. Haydn took existing ideas, and radically altered how they functioned\u2014earning him the titles \"father of the symphony\" and \"father of the string quartet\".\nOne of the forces that worked as an impetus for his pressing forward was the first stirring of what would later be called Romanticism\u2014the \"Sturm und Drang\", or \"storm and stress\" phase in the arts, a short period where obvious and dramatic emotionalism was a stylistic preference. Haydn accordingly wanted more dramatic contrast and more emotionally appealing melodies, with sharpened character and individuality in his pieces. This period faded away in music and literature: however, it influenced what came afterward and would eventually be a component of aesthetic taste in later decades.\nThe \"Farewell Symphony\", No. 45 in F\u266f minor, exemplifies Haydn's integration of the differing demands of the new style, with surprising sharp turns and a long slow adagio to end the work. In 1772, Haydn completed his Opus 20 set of six string quartets, in which he deployed the polyphonic techniques he had gathered from the previous Baroque era to provide structural coherence capable of holding together his melodic ideas. For some, this marks the beginning of the \"mature\" Classical style, in which the period of reaction against late Baroque complexity yielded to a period of integration Baroque and Classical elements.\n1775\u20131790.\nHaydn, having worked for over a decade as the music director for a prince, had far more resources and scope for composing than most other composers. His position also gave him the ability to shape the forces that would play his music, as he could select skilled musicians. This opportunity was not wasted, as Haydn, beginning quite early on his career, sought to press forward the technique of building and developing ideas in his music. His next important breakthrough was in the Opus 33 string quartets (1781), in which the melodic and the harmonic roles segue among the instruments: it is often momentarily unclear what is melody and what is harmony. This changes the way the ensemble works its way between dramatic moments of transition and climactic sections: the music flows smoothly and without obvious interruption. He then took this integrated style and began applying it to orchestral and vocal music.\nHaydn's gift to music was a way of composing, a way of structuring works, which was at the same time in accord with the governing aesthetic of the new style. However, a younger contemporary, Wolfgang Amadeus Mozart, brought his genius to Haydn's ideas and applied them to two of the major genres of the day: opera, and the virtuoso concerto. Whereas Haydn spent much of his working life as a court composer, Mozart wanted public success in the concert life of cities, playing for the general public. This meant he needed to write operas and write and perform virtuoso pieces. Haydn was not a virtuoso at the international touring level; nor was he seeking to create operatic works that could play for many nights in front of a large audience. Mozart wanted to achieve both. Moreover, Mozart also had a taste for more chromatic chords (and greater contrasts in harmonic language generally), a greater love for creating a welter of melodies in a single work, and a more Italianate sensibility in music as a whole. He found, in Haydn's music and later in his study of the polyphony of J.S. Bach, the means to discipline and enrich his artistic gifts.\nMozart rapidly came to the attention of Haydn, who hailed the new composer, studied his works, and considered the younger man his only true peer in music. In Mozart, Haydn found a greater range of instrumentation, dramatic effect and melodic resource. The learning relationship moved in both directions. Mozart also had a great respect for the older, more experienced composer, and sought to learn from him.\nMozart's arrival in Vienna in 1780 brought an acceleration in the development of the Classical style. There, Mozart absorbed the fusion of Italianate brilliance and Germanic cohesiveness that had been brewing for the previous 20 years. His own taste for flashy brilliances, rhythmically complex melodies and figures, long cantilena melodies, and virtuoso flourishes was merged with an appreciation for formal coherence and internal connectedness. It is at this point that war and economic inflation halted a trend to larger orchestras and forced the disbanding or reduction of many theater orchestras. This pressed the Classical style inwards: toward seeking greater ensemble and technical challenges\u2014for example, scattering the melody across woodwinds, or using a melody harmonized in thirds. This process placed a premium on small ensemble music, called chamber music. It also led to a trend for more public performance, giving a further boost to the string quartet and other small ensemble groupings.\nIt was during this decade that public taste began, increasingly, to recognize that Haydn and Mozart had reached a high standard of composition. By the time Mozart arrived at age 25, in 1781, the dominant styles of Vienna were recognizably connected to the emergence in the 1750s of the early Classical style. By the end of the 1780s, changes in performance practice, the relative standing of instrumental and vocal music, technical demands on musicians, and stylistic unity had become established in the composers who imitated Mozart and Haydn. During this decade Mozart composed his most famous operas, his six late symphonies that helped to redefine the genre, and a string of piano concerti that still stand at the pinnacle of these forms.\nOne composer who was influential in spreading the more serious style that Mozart and Haydn had formed is Muzio Clementi, a gifted virtuoso pianist who tied with Mozart in a musical \"duel\" before the emperor in which they each improvised on the piano and performed their compositions. Clementi's sonatas for the piano circulated widely, and he became the most successful composer in London during the 1780s. Also in London at this time was Jan Ladislav Dussek, who, like Clementi, encouraged piano makers to extend the range and other features of their instruments, and then fully exploited the newly opened up possibilities. The importance of London in the Classical period is often overlooked, but it served as the home to the Broadwood's factory for piano manufacturing and as the base for composers who, while less notable than the \"Vienna School\", had a decisive influence on what came later. They were composers of many fine works, notable in their own right. London's taste for virtuosity may well have encouraged the complex passage work and extended statements on tonic and dominant.\nAround 1790\u20131820.\nWhen Haydn and Mozart began composing, symphonies were played as single movements\u2014before, between, or as interludes within other works\u2014and many of them lasted only ten or twelve minutes; instrumental groups had varying standards of playing, and the continuo was a central part of music-making.\nIn the intervening years, the social world of music had seen dramatic changes. International publication and touring had grown explosively, and concert societies formed. Notation became more specific, more descriptive\u2014and schematics for works had been simplified (yet became more varied in their exact working out). In 1790, just before Mozart's death, with his reputation spreading rapidly, Haydn was poised for a series of successes, notably his late oratorios and London symphonies. Composers in Paris, Rome, and all over Germany turned to Haydn and Mozart for their ideas on form.\nIn the 1790s, a new generation of composers, born around 1770, emerged. While they had grown up with the earlier styles, they heard in the recent works of Haydn and Mozart a vehicle for greater expression. In 1788 Luigi Cherubini settled in Paris and in 1791 composed \"Lodoiska\", an opera that raised him to fame. Its style is clearly reflective of the mature Haydn and Mozart, and its instrumentation gave it a weight that had not yet been felt in the grand opera. His contemporary \u00c9tienne M\u00e9hul extended instrumental effects with his 1790 opera \"Euphrosine et Coradin\", from which followed a series of successes. The final push towards change came from Gaspare Spontini, who was deeply admired by future romantic composers such as Weber, Berlioz and Wagner. The innovative harmonic language of his operas, their refined instrumentation and their \"enchained\" closed numbers (a structural pattern which was later adopted by Weber in Euryanthe and from him handed down, through Marschner, to Wagner), formed the basis from which French and German romantic opera had its beginnings.\nThe most fateful of the new generation was Ludwig van Beethoven, who launched his numbered works in 1794 with a set of three piano trios, which remain in the repertoire. Somewhat younger than the others, though equally accomplished because of his youthful study under Mozart and his native virtuosity, was Johann Nepomuk Hummel. Hummel studied under Haydn as well; he was a friend to Beethoven and Franz Schubert. He concentrated more on the piano than any other instrument, and his time in London in 1791 and 1792 generated the composition and publication in 1793 of three piano sonatas, opus 2, which idiomatically used Mozart's techniques of avoiding the expected cadence, and Clementi's sometimes modally uncertain virtuoso figuration. Taken together, these composers can be seen as the vanguard of a broad change in style and the center of music. They studied one another's works, copied one another's gestures in music, and on occasion behaved like quarrelsome rivals.\nThe crucial differences with the previous wave can be seen in the downward shift in melodies, increasing durations of movements, the acceptance of Mozart and Haydn as paradigmatic, the greater use of keyboard resources, the shift from \"vocal\" writing to \"pianistic\" writing, the growing pull of the minor and of modal ambiguity, and the increasing importance of varying accompanying figures to bring \"texture\" forward as an element in music. In short, the late Classical was seeking music that was internally more complex. The growth of concert societies and amateur orchestras, marking the importance of music as part of middle-class life, contributed to a booming market for pianos, piano music, and virtuosi to serve as exemplars. Hummel, Beethoven, and Clementi were all renowned for their improvising.\nThe direct influence of the Baroque continued to fade: the figured bass grew less prominent as a means of holding performance together, the performance practices of the mid-18th century continued to die out. However, at the same time, complete editions of Baroque masters began to become available, and the influence of Baroque style continued to grow, particularly in the ever more expansive use of brass. Another feature of the period is the growing number of performances where the composer was not present. This led to increased detail and specificity in notation; for example, there were fewer \"optional\" parts that stood separately from the main score.\nThe force of these shifts became apparent with Beethoven's 3rd Symphony, given the name \"Eroica\", which is Italian for \"heroic\", by the composer. As with Stravinsky's \"The Rite of Spring\", it may not have been the first in all of its innovations, but its aggressive use of every part of the Classical style set it apart from its contemporary works: in length, ambition, and harmonic resources as well.\nFirst Viennese School.\nThe First Viennese School is a name mostly used to refer to three composers of the Classical period in late-18th-century Vienna: Haydn, Mozart, and Beethoven. Franz Schubert is occasionally added to the list.\nIn German-speaking countries, the term \"Wiener Klassik\" (lit. \"Viennese classical era/art\") is used. That term is often more broadly applied to the Classical era in music as a whole, as a means to distinguish it from other periods that are colloquially referred to as \"classical\", namely Baroque and Romantic music.\nThe term \"Viennese School\" was first used by Austrian musicologist Raphael Georg Kiesewetter in 1834, although he only counted Haydn and Mozart as members of the school. Other writers followed suit, and eventually Beethoven was added to the list. The designation \"first\" is added today to avoid confusion with the Second Viennese School.\nWhilst, Schubert apart, these composers certainly knew each other (with Haydn and Mozart even being occasional chamber-music partners), there is no sense in which they were engaged in a collaborative effort in the sense that one would associate with 20th-century schools such as the Second Viennese School, or Les Six. Nor is there any significant sense in which one composer was \"schooled\" by another (in the way that Berg and Webern were taught by Schoenberg), though it is true that Beethoven for a time received lessons from Haydn.\nAttempts to extend the First Viennese School to include such later figures as Anton Bruckner, Johannes Brahms, and Gustav Mahler are merely journalistic, and never encountered in academic musicology.\nClassical influence on later composers.\nMusical eras and their prevalent styles, forms and instruments seldom disappear at once; instead, features are replaced over time, until the old approach is simply felt as \"old-fashioned\". The Classical style did not \"die\" suddenly; rather, it gradually got phased out under the weight of changes. To give just one example, while it is generally stated that the Classical era stopped using the harpsichord in orchestras, this did not happen all of a sudden at the start of the Classical era in 1750. Rather, orchestras slowly stopped using the harpsichord to play basso continuo until the practice was discontinued by the end of the 1700s.\nOne crucial change was the shift towards harmonies centering on \"flatward\" keys: shifts in the subdominant direction . In the Classical style, major key was far more common than minor, chromaticism being moderated through the use of \"sharpward\" modulation (e.g., a piece in C major modulating to G major, D major, or A major, all of which are keys with more sharps). As well, sections in the minor mode were often used for contrast. Beginning with Mozart and Clementi, there began a creeping colonization of the subdominant region (the ii or IV chord, which in the key of C major would be the keys of d minor or F major). With Schubert, subdominant modulations flourished after being introduced in contexts in which earlier composers would have confined themselves to dominant shifts (modulations to the dominant chord, e.g., in the key of C major, modulating to G major). This introduced darker colors to music, strengthened the minor mode, and made structure harder to maintain. Beethoven contributed to this by his increasing use of the fourth as a consonance, and modal ambiguity\u2014for example, the opening of the Symphony No. 9 in D minor.\nLudwig van Beethoven, Franz Schubert, Carl Maria von Weber, Johann Nepomuk Hummel, and John Field are among the most prominent in this generation of \"Proto-Romantics\", along with the young Felix Mendelssohn. Their sense of form was strongly influenced by the Classical style. While they were not yet \"learned\" composers (imitating rules which were codified by others), they directly responded to works by Haydn, Mozart, Clementi, and others, as they encountered them. The instrumental forces at their disposal in orchestras were also quite \"Classical\" in number and variety, permitting similarity with Classical works.\nHowever, the forces destined to end the hold of the Classical style gathered strength in the works of many of the above composers, particularly Beethoven. The most commonly cited one is harmonic innovation. Also important is the increasing focus on having a continuous and rhythmically uniform accompanying figuration: Beethoven's Moonlight Sonata was the model for hundreds of later pieces\u2014where the shifting movement of a rhythmic figure provides much of the drama and interest of the work, while a melody drifts above it. Greater knowledge of works, greater instrumental expertise, increasing variety of instruments, the growth of concert societies, and the unstoppable domination of the increasingly more powerful piano (which was given a bolder, louder tone by technological developments such as the use of steel strings, heavy cast-iron frames and sympathetically vibrating strings) all created a huge audience for sophisticated music. All of these trends contributed to the shift to the \"Romantic\" style.\nDrawing the line between these two styles is very difficult: some sections of Mozart's later works, taken alone, are indistinguishable in harmony and orchestration from music written 80 years later\u2014and some composers continued to write in normative Classical styles into the early 20th century. Even before Beethoven's death, composers such as Louis Spohr were self-described Romantics, incorporating, for example, more extravagant chromaticism in their works (e.g., using chromatic harmonies in a piece's chord progression). Conversely, works such as Schubert's Symphony No. 5, written during the chronological end of the Classical era and dawn of the Romantic era, exhibit a deliberately anachronistic artistic paradigm, harking back to the compositional style of several decades before.\nHowever, Vienna's fall as the most important musical center for orchestral composition during the late 1820s, precipitated by the deaths of Beethoven and Schubert, marked the Classical style's final eclipse\u2014and the end of its continuous organic development of one composer learning in close proximity to others. Franz Liszt and Fr\u00e9d\u00e9ric Chopin visited Vienna when they were young, but they then moved on to other cities. Composers such as Carl Czerny, while deeply influenced by Beethoven, also searched for new ideas and new forms to contain the larger world of musical expression and performance in which they lived.\nRenewed interest in the formal balance and restraint of 18th century classical music led in the early 20th century to the development of so-called Neoclassical style, which numbered Stravinsky and Prokofiev among its proponents, at least at certain times in their careers.\nClassical period instruments.\nGuitar.\nThe Baroque guitar, with four or five sets of double strings or \"courses\" and elaborately decorated soundhole, was a very different instrument from the early classical guitar which more closely resembles the modern instrument with the standard six strings. Judging by the number of instructional manuals published for the instrument \u2013 over three hundred texts were published by over two hundred authors between 1760 and 1860 \u2013 the classical period marked a golden age for guitar.\nStrings.\nIn the Baroque era, there was more variety in the bowed stringed instruments used in ensembles, with instruments such as the viola d'amore and a range of fretted viols being used, ranging from small viols to large bass viols. In the Classical period, the string section of the orchestra was standardized as just four instruments:\nIn the Baroque era, the double bass players were not usually given a separate part; instead, they typically played the same basso continuo bassline that the cellos and other low-pitched instruments (e.g., theorbo, serpent wind instrument, viols), albeit an octave below the cellos, because the double bass is a transposing instrument that sounds one octave lower than it is written. In the Classical era, some composers continued to write only one bass part for their symphony, labeled \"bassi\"; this bass part was played by cellists and double bassists. During the Classical era, some composers began to give the double basses their own part.\nWoodwinds.\nIt was commonplace for all orchestras to have at least 2 winds, usually oboes, flutes, clarinets, or sometimes english horns (see Symphony No. 22 (Haydn). Patrons also usually employed an ensemble of entirely winds, called the \"harmonie\", which would be employed for certain events. The harmonie would join the larger string orchestra sometimes to serve as the wind section.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5289", "revid": "788819460", "url": "https://en.wikipedia.org/wiki?curid=5289", "title": "Card games", "text": ""}
{"id": "5290", "revid": "890540264", "url": "https://en.wikipedia.org/wiki?curid=5290", "title": "Casino games", "text": ""}
{"id": "5291", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5291", "title": "Computer games", "text": ""}
{"id": "5292", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5292", "title": "Collectable card games", "text": ""}
{"id": "5295", "revid": "40721479", "url": "https://en.wikipedia.org/wiki?curid=5295", "title": "Character encoding", "text": "Using numbers to represent text characters\nCharacter encoding is the process of assigning numbers to graphical characters, especially the written characters of human language, allowing them to be stored, transmitted, and transformed using digital computers. The numerical values that make up a character encoding are known as \"code points\" and collectively comprise a \"code space\", a \"code page\", or a \"character map\".\nEarly character codes associated with the optical or electrical telegraph could only represent a subset of the characters used in written languages, sometimes restricted to upper case letters, numerals and some punctuation only. The low cost of digital representation of data in modern computer systems allows more elaborate character codes (such as Unicode) which represent most of the characters used in many written languages. Character encoding using internationally accepted standards permits worldwide interchange of text in electronic form.\nHistory.\nThe history of character codes illustrates the evolving need for machine-mediated character-based symbolic information over a distance, using once-novel electrical means. The earliest codes were based upon manual and hand-written encoding and cyphering systems, such as Bacon's cipher, Braille, international maritime signal flags, and the 4-digit encoding of Chinese characters for a Chinese telegraph code (Hans Schjellerup, 1869). With the adoption of electrical and electro-mechanical techniques these earliest codes were adapted to the new capabilities and limitations of the early machines. The earliest well-known electrically transmitted character code, Morse code, introduced in the 1840s, used a system of four \"symbols\" (short signal, long signal, short space, long space) to generate codes of variable length. Though some commercial use of Morse code was via machinery, it was often used as a manual code, generated by hand on a telegraph key and decipherable by ear, and persists in amateur radio and aeronautical use. Most codes are of fixed per-character length or variable-length sequences of fixed-length codes (e.g. Unicode).\nCommon examples of character encoding systems include Morse code, the Baudot code, the American Standard Code for Information Interchange (ASCII) and Unicode. Unicode, a well-defined and extensible encoding system, has supplanted most earlier character encodings, but the path of code development to the present is fairly well known.\nThe Baudot code, a five-bit encoding, was created by \u00c9mile Baudot in 1870, patented in 1874, modified by Donald Murray in 1901, and standardized by CCITT as International Telegraph Alphabet No.\u00a02 (ITA2) in 1930. The name \"baudot\" has been erroneously applied to ITA2 and its many variants. ITA2 suffered from many shortcomings and was often improved by many equipment manufacturers, sometimes creating compatibility issues. In 1959 the U.S. military defined its Fieldata code, a six-or seven-bit code, introduced by the U.S. Army Signal Corps. While Fieldata addressed many of the then-modern issues (e.g. letter and digit codes arranged for machine collation), it fell short of its goals and was short-lived. In 1963 the first ASCII code was released (X3.4-1963) by the ASCII committee (which contained at least one member of the Fieldata committee, W. F. Leubbert), which addressed most of the shortcomings of Fieldata, using a simpler code. Many of the changes were subtle, such as collatable character sets within certain numeric ranges. ASCII63 was a success, widely adopted by industry, and with the follow-up issue of the 1967 ASCII code (which added lower-case letters and fixed some \"control code\" issues) ASCII67 was adopted fairly widely. ASCII67's American-centric nature was somewhat addressed in the European ECMA-6 standard.\nHerman Hollerith invented punch card data encoding in the late 19th century to analyze census data. Initially, each hole position represented a different data element, but later, numeric information was encoded by numbering the lower rows 0 to 9, with a punch in a column representing its row number. Later alphabetic data was encoded by allowing more than one punch per column. Electromechanical tabulating machines represented date internally by the timing of pulses relative to the motion of the cards through the machine. When IBM went to electronic processing, starting with the IBM 603 Electronic Multiplier, it used a variety of binary encoding schemes that were tied to the punch card code.\nIBM's Binary Coded Decimal (BCD) was a six-bit encoding scheme used by IBM as early as 1953 in its 702 and 704 computers, and in its later 7000 Series and 1400 series, as well as in associated peripherals. Since the punched card code then in use only allowed digits, upper-case English letters and a few special characters, six bits were sufficient. BCD extended existing simple four-bit numeric encoding to include alphabetic and special characters, mapping it easily to punch-card encoding which was already in widespread use. IBMs codes were used primarily with IBM equipment; other computer vendors of the era had their own character codes, often six-bit, but usually had the ability to read tapes produced on IBM equipment. BCD was the precursor of IBM's Extended Binary-Coded Decimal Interchange Code (usually abbreviated as EBCDIC), an eight-bit encoding scheme developed in 1963 for the IBM System/360 that featured a larger character set, including lower case letters.\nThe limitations of such sets soon became apparent, and a number of \"ad hoc\" methods were developed to extend them. The need to support more writing systems for different languages, including the CJK family of East Asian scripts, required support for a far larger number of characters and demanded a systematic approach to character encoding rather than the previous \"ad hoc\" approaches.\nIn trying to develop universally interchangeable character encodings, researchers in the 1980s faced the dilemma that, on the one hand, it seemed necessary to add more bits to accommodate additional characters, but on the other hand, for the users of the relatively small character set of the Latin alphabet (who still constituted the majority of computer users), those additional bits were a colossal waste of then-scarce and expensive computing resources (as they would always be zeroed out for such users). In 1985, the average personal computer user's hard disk drive could store only about 10 megabytes, and it cost approximately US$250 on the wholesale market (and much higher if purchased separately at retail), so it was very important at the time to make every bit count.\nThe compromise solution that was eventually found and was to break the assumption (dating back to telegraph codes) that each character should always directly correspond to a particular sequence of bits. Instead, characters would first be mapped to a universal intermediate representation in the form of abstract numbers called code points. Code points would then be represented in a variety of ways and with various default numbers of bits per character (code units) depending on context. To encode code points higher than the length of the code unit, such as above 256 for eight-bit units, the solution was to implement variable-length encodings where an escape sequence would signal that subsequent bits should be parsed as a higher code point.\nTerminology.\nThe character repertoire is an abstract set of more than one million characters found in a wide variety of scripts including Latin, Cyrillic, Chinese, Korean, Japanese, Hebrew, and Aramaic.\nOther symbols such as musical notation are also included in the character repertoire. Both the Unicode and GB 18030 standards have a character repertoire. As new characters are added to one standard, the other standard also adds those characters, to maintain parity.\nThe code unit size is equivalent to the bit measurement for the particular encoding:\nConsider a string of the letters \"ab\u0332c\ud801\udc00\", that is, a string containing a Unicode combining character () as well a supplementary character (). This string has several representations which are logically equivalent, yet while each is suited to a diverse set of circumstances or range of requirements:\nNote in particular the last character, which is represented with either one \"1\" 32-bit value, \"2\" 16-bit values. or \"4\" 8-bit values. Although each of those forms uses the same total number of bits (32) to represent the glyph, the actual numeric byte values and their arrangement appear entirely unrelated.\nThe convention to refer to a character in Unicode is to start with 'U+' followed by the codepoint value in hexadecimal. The range of valid code points for the Unicode standard is U+0000 to U+10FFFF, inclusive, divided in 17 planes, identified by the numbers 0 to 16. Characters in the range U+0000 to U+FFFF are in plane 0, called the Basic Multilingual Plane (BMP). This plane contains most commonly-used characters. Characters in the range U+10000 to U+10FFFF in the other planes are called supplementary characters.\nThe following table shows examples of code point values:\nA code point is represented by a sequence of code units. The mapping is defined by the encoding. Thus, the number of code units required to represent a code point depends on the encoding:\nUnicode encoding model.\nUnicode and its parallel standard, the ISO/IEC 10646 Universal Character Set, together constitute a modern, unified character encoding. Rather than mapping characters directly to octets (bytes), they separately define what characters are available, corresponding natural numbers (code points), how those numbers are encoded as a series of fixed-size natural numbers (code units), and finally how those units are encoded as a stream of octets. The purpose of this decomposition is to establish a universal set of characters that can be encoded in a variety of ways. To describe this model correctly requires more precise terms than \"character set\" and \"character encoding.\" The terms used in the modern model follow:\nA character repertoire is the full set of abstract characters that a system supports. The repertoire may be closed, i.e. no additions are allowed without creating a new standard (as is the case with ASCII and most of the ISO-8859 series), or it may be open, allowing additions (as is the case with Unicode and to a limited extent the Windows code pages). The characters in a given repertoire reflect decisions that have been made about how to divide writing systems into basic information units. The basic variants of the Latin, Greek and Cyrillic alphabets can be broken down into letters, digits, punctuation, and a few \"special characters\" such as the space, which can all be arranged in simple linear sequences that are displayed in the same order they are read. But even with these alphabets, diacritics pose a complication: they can be regarded either as part of a single character containing a letter and diacritic (known as a precomposed character), or as separate characters. The former allows a far simpler text handling system but the latter allows any letter/diacritic combination to be used in text. Ligatures pose similar problems. Other writing systems, such as Arabic and Hebrew, are represented with more complex character repertoires due to the need to accommodate things like bidirectional text and glyphs that are joined in different ways for different situations.\nA coded character set (CCS) is a function that maps characters to \"code points\" (each code point represents one character). For example, in a given repertoire, the capital letter \"A\" in the Latin alphabet might be represented by the code point 65, the character \"B\" to 66, and so on. Multiple coded character sets may share the same repertoire; for example ISO/IEC 8859-1 and IBM code pages 037 and 500 all cover the same repertoire but map them to different code points.\nA character encoding form (CEF) is the mapping of code points to \"code units\" to facilitate storage in a system that represents numbers as bit sequences of fixed length (i.e. practically any computer system). For example, a system that stores numeric information in 16-bit units can only directly represent code points 0 to 65,535 in each unit, but larger code points (say, 65,536 to 1.4\u00a0million) could be represented by using multiple 16-bit units. This correspondence is defined by a CEF.\nNext, a character encoding scheme (CES) is the mapping of code units to a sequence of octets to facilitate storage on an octet-based file system or transmission over an octet-based network. Simple character encoding schemes include UTF-8, UTF-16BE, UTF-32BE, UTF-16LE or UTF-32LE; compound character encoding schemes, such as UTF-16, UTF-32 and ISO/IEC 2022, switch between several simple schemes by using a byte order mark or escape sequences; compressing schemes try to minimize the number of bytes used per code unit (such as SCSU, BOCU, and Punycode).\nAlthough UTF-32BE is a simpler CES, most systems working with Unicode use either UTF-8, which is backward compatible with fixed-length ASCII and maps Unicode code points to variable-length sequences of octets, or UTF-16BE, which is backward compatible with fixed-length UCS-2BE and maps Unicode code points to variable-length sequences of 16-bit words. See comparison of Unicode encodings for a detailed discussion.\nFinally, there may be a higher-level protocol which supplies additional information to select the particular variant of a Unicode character, particularly where there are regional variants that have been 'unified' in Unicode as the same character. An example is the XML attribute xml:lang.\nThe Unicode model uses the term character map for historical systems which directly assign a sequence of characters to a sequence of bytes, covering all of CCS, CEF and CES layers.\nCharacter sets, character maps and code pages.\nHistorically, the terms \"character encoding\", \"character map\", \"character set\" and \"code page\" were synonymous in computer science, as the same standard would specify a repertoire of characters and how they were to be encoded into a stream of code units \u2013 usually with a single character per code unit. But now the terms have related but distinct meanings, due to efforts by standards bodies to use precise terminology when writing about and unifying many different encoding systems. Regardless, the terms are still used interchangeably, with \"character set\" being nearly ubiquitous.\nA \"code page\" usually means a byte-oriented encoding, but with regard to some suite of encodings (covering different scripts), where many characters share the same codes in most or all those code pages. Well-known code page suites are \"Windows\" (based on Windows-1252) and \"IBM\"/\"DOS\" (based on code page 437), see Windows code page for details. Most, but not all, encodings referred to as code pages are single-byte encodings (but see octet on byte size.)\nIBM's Character Data Representation Architecture (CDRA) designates entities with coded character set identifiers (CCSIDs), each of which is variously called a \"charset\", \"character set\", \"code page\", or \"CHARMAP\".\nThe term \"code page\" does not occur in Unix or Linux where \"charmap\" is preferred, usually in the larger context of locales.\nIn contrast to a \"coded character set\", a \"character encoding\" is a map from abstract characters to code words. A \"character set\" in HTTP (and MIME) parlance is the same as a character encoding (but not the same as CCS).\n\"Legacy encoding\" is a term sometimes used to characterize old character encodings, but with an ambiguity of sense. Most of its use is in the context of Unicodification, where it refers to encodings that fail to cover all Unicode code points, or, more generally, using a somewhat different character repertoire: several code points representing one Unicode character, or versa (see e.g. code page 437). Some sources refer to an encoding as \"legacy\" only because it preceded Unicode. All Windows code pages are usually referred to as legacy, both because they antedate Unicode and because they are unable to represent all 221 possible Unicode code points.\nCharacter encoding translation.\nAs a result of having many character encoding methods in use (and the need for backward compatibility with archived data), many computer programs have been developed to translate data between encoding schemes as a form of data transcoding. Some of these are cited below.\nCross-platform:\nUnix-like: \nWindows:\nSee also.\nCommon character encodings.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5297", "revid": "35313544", "url": "https://en.wikipedia.org/wiki?curid=5297", "title": "Computer character", "text": ""}
{"id": "5298", "revid": "122189", "url": "https://en.wikipedia.org/wiki?curid=5298", "title": "Control character", "text": "Code point in a character set, that does not represent a written symbol\nIn computing and telecommunication, a control character or non-printing character (NPC) is a code point in a character set that does not represent a written character or symbol. They are used as in-band signaling to cause effects other than the addition of a symbol to the text. All other characters are mainly \"graphic characters\", also known as \"printing characters\" (or \"printable characters\"), except perhaps for \"space\" characters. In the ASCII standard there are 33 control characters, such as code 7, BEL, which rings a terminal bell.\nHistory.\nProcedural signs in Morse code are a form of control character.\nA form of control characters were introduced in the 1870 Baudot code: NUL and DEL.\nThe 1901 Murray code added the carriage return (CR) and line feed (LF), and other versions of the Baudot code included other control characters.\nThe bell character (BEL), which rang a bell to alert operators, was also an early teletype control character.\nControl characters have also been called \"format effectors\".\nIn ASCII.\nThere were quite a few control characters defined (33 in ASCII, and the ECMA-48 standard adds 32 more). This was because early terminals had very primitive mechanical or electrical controls that made any kind of state-remembering API quite expensive to implement, thus a different code for each and every function looked like a requirement. It quickly became possible and inexpensive to interpret sequences of codes to perform a function, and device makers found a way to send hundreds of device instructions. Specifically, they used ASCII code 2710 (escape), followed by a series of characters called a \"control sequence\" or \"escape sequence\". The mechanism was invented by Bob Bemer, the father of ASCII. For example, the sequence of code 2710, followed by the printable characters \"[2;10H\", would cause a Digital Equipment Corporation VT100 terminal to move its cursor to the 10th cell of the 2nd line of the screen. Several standards exist for these sequences, notably ANSI X3.64. But the number of non-standard variations in use is large, especially among printers, where technology has advanced far faster than any standards body can possibly keep up with.\nAll entries in the ASCII table below code 3210 (technically the C0 control code set) are of this kind, including CR and LF used to separate lines of text. The code 12710 (DEL) is also a control character. Extended ASCII sets defined by ISO 8859 added the codes 12810 through 15910 as control characters. This was primarily done so that if the high bit was stripped, it would not change a printing character to a C0 control code. This second set is called the C1 set.\nThese 65 control codes were carried over to Unicode. Unicode added more characters that could be considered controls, but it makes a distinction between these \"Formatting characters\" (such as the zero-width non-joiner) and the 65 control characters.\nThe Extended Binary Coded Decimal Interchange Code (EBCDIC) character set contains 65 control codes, including all of the ASCII control codes plus additional codes which are mostly used to control IBM peripherals.\nThe control characters in ASCII still in common use include:\nControl characters may be described as doing something when the user inputs them, such as code 3 (End-of-Text character, ETX, codice_31) to interrupt the running process, or code 4 (End-of-Transmission character, EOT, codice_32), used to end text input on Unix or to exit a Unix shell. These uses usually have little to do with their use when they are in text being output.\nIn Unicode.\nIn Unicode, \"Control-characters\" are U+0000\u2014U+001F (C0 controls), U+007F (delete), and U+0080\u2014U+009F (C1 controls). Their General Category is \"Cc\". Formatting codes are distinct, in General Category \"Cf\". The Cc control characters have no Name in Unicode, but are given labels such as \"&lt;control-001A&gt;\" instead.\nDisplay.\nThere are a number of techniques to display non-printing characters, which may be illustrated with the bell character in ASCII encoding:\nHow control characters map to keyboards.\nASCII-based keyboards have a key labelled \"Control\", \"Ctrl\", or (rarely) \"Cntl\" which is used much like a shift key, being pressed in combination with another letter or symbol key. In one implementation, the control key generates the code 64 places below the code for the (generally) uppercase letter it is pressed in combination with (i.e., subtract 0x40 from ASCII code value of the (generally) uppercase letter). The other implementation is to take the ASCII code produced by the key and bitwise AND it with 0x1F, forcing bits 6 and 7 to zero. For example, pressing \"control\" and the letter \"g\" (which is 0110 0111 in binary), produces the code 7 (BELL, 7 in base 10, or 0000 0111 in binary). The NULL character (code 0) is represented by Ctrl-@, \"@\" being the code immediately before \"A\" in the ASCII character set. For convenience, some terminals accept Ctrl-Space as an alias for Ctrl-@. In either case, this produces one of the 32 ASCII control codes between 0 and 31. Neither approach works to produce the DEL character because of its special location in the table and its value (code 12710), Ctrl-? is sometimes used for this character.\nWhen the control key is held down, letter keys produce the same control characters regardless of the state of the shift or caps lock keys. In other words, it does not matter whether the key would have produced an upper-case or a lower-case letter. The interpretation of the control key with the space, graphics character, and digit keys (ASCII codes 32 to 63) vary between systems. Some will produce the same character code as if the control key were not held down. Other systems translate these keys into control characters when the control key is held down. The interpretation of the control key with non-ASCII (\"foreign\") keys also varies between systems.\nControl characters are often rendered into a printable form known as caret notation by printing a caret (^) and then the ASCII character that has a value of the control character plus 64. Control characters generated using letter keys are thus displayed with the upper-case form of the letter. For example, ^G represents code 7, which is generated by pressing the G key when the control key is held down.\nKeyboards also typically have a few single keys which produce control character codes. For example, the key labelled \"Backspace\" typically produces code 8, \"Tab\" code 9, \"Enter\" or \"Return\" code 13 (though some keyboards might produce code 10 for \"Enter\").\nMany keyboards include keys that do not correspond to any ASCII printable or control character, for example cursor control arrows and word processing functions. The associated keypresses are communicated to computer programs by one of four methods: appropriating otherwise unused control characters; using some encoding other than ASCII; using multi-character control sequences; or using an additional mechanism outside of generating characters. \"Dumb\" computer terminals typically use control sequences. Keyboards attached to stand-alone personal computers made in the 1980s typically use one (or both) of the first two methods. Modern computer keyboards generate scancodes that identify the specific physical keys that are pressed; computer software then determines how to handle the keys that are pressed, including any of the four methods described above.\nThe design purpose.\nThe control characters were designed to fall into a few groups: printing and display control, data structuring, transmission control, and miscellaneous.\nPrinting and display control.\nPrinting control characters were first used to control the physical mechanism of printers, the earliest output device. An early example of this idea was the use of Figures (FIGS) and Letters (LTRS) in Baudot code to shift between two code pages. A later, but still early, example was the out-of-band ASA carriage control characters. Later, control characters were integrated into the stream of data to be printed.\nThe carriage return character (CR), when sent to such a device, causes it to put the character at the edge of the paper at which writing begins (it may, or may not, also move the printing position to the next line).\nThe line feed character (LF/NL) causes the device to put the printing position on the next line. It may (or may not), depending on the device and its configuration, also move the printing position to the start of the next line (which would be the leftmost position for left-to-right scripts, such as the alphabets used for Western languages, and the rightmost position for right-to-left scripts such as the Hebrew and Arabic alphabets).\nThe vertical and horizontal tab characters (VT and HT/TAB) cause the output device to move the printing position to the next tab stop in the direction of reading.\nThe form feed character (FF/NP) starts a new sheet of paper, and may or may not move to the start of the first line.\nThe backspace character (BS) moves the printing position one character space backwards. On printers, including hard-copy terminals, this is most often used so the printer can overprint characters to make other, not normally available, characters. On video terminals and other electronic output devices, there are often software (or hardware) configuration choices that allow a destructive backspace (e.g., a BS, SP, BS sequence), which erases, or a non-destructive one, which does not.\nThe shift in and shift out characters (SI and SO) selected alternate character sets, fonts, underlining, or other printing modes. Escape sequences were often used to do the same thing.\nWith the advent of computer terminals that did not physically print on paper and so offered more flexibility regarding screen placement, erasure, and so forth, printing control codes were adapted. Form feeds, for example, usually cleared the screen, there being no new paper page to move to. More complex escape sequences were developed to take advantage of the flexibility of the new terminals, and indeed of newer printers. The concept of a control character had always been somewhat limiting, and was extremely so when used with new, much more flexible, hardware. Control sequences (sometimes implemented as escape sequences) could match the new flexibility and power and became the standard method. However, there were, and remain, a large variety of standard sequences to choose from.\nData structuring.\nThe separators (File, Group, Record, and Unit: FS, GS, RS and US) were made to structure data, usually on a tape, in order to simulate punched cards.\nEnd of medium (EM) warns that the tape (or other recording medium) is ending.\nWhile many systems use CR/LF and TAB for structuring data, it is possible to encounter the separator control characters in data that needs to be structured. The separator control characters are not overloaded; there is no general use of them except to separate data into structured groupings. Their numeric values are contiguous with the space character, which can be considered a member of the group, as a word separator.\nFor example, the RS separator is used by (JSON Text Sequences) to encode a sequence of JSON elements. Each sequence item starts with a RS character and ends with a line feed. This allows to serialize open-ended JSON sequences. It is one of the JSON streaming protocols.\nTransmission control.\nThe transmission control characters were intended to structure a data stream, and to manage re-transmission or graceful failure, as needed, in the face of transmission errors.\nThe start of heading (SOH) character was to mark a non-data section of a data stream\u2014the part of a stream containing addresses and other housekeeping data. The start of text character (STX) marked the end of the header, and the start of the textual part of a stream. The end of text character (ETX) marked the end of the data of a message. A widely used convention is to make the two characters preceding ETX a checksum or CRC for error-detection purposes. The end of transmission block character (ETB) was used to indicate the end of a block of data, where data was divided into such blocks for transmission purposes.\nThe escape character (ESC) was intended to \"quote\" the next character, if it was another control character it would print it instead of performing the control function. It is almost never used for this purpose today. Various printable characters are used as visible \"escape characters\", depending on context.\nThe substitute character (SUB) was intended to request a translation of the next character from a printable character to another value, usually by setting bit 5 to zero. This is handy because some media (such as sheets of paper produced by typewriters) can transmit only printable characters. However, on MS-DOS systems with files opened in text mode, \"end of text\" or \"end of file\" is marked by this Ctrl-Z character, instead of the Ctrl-C or Ctrl-D, which are common on other operating systems.\nThe cancel character (CAN) signalled that the previous element should be discarded. The negative acknowledge character (NAK) is a definite flag for, usually, noting that reception was a problem, and, often, that the current element should be sent again. The acknowledge character (ACK) is normally used as a flag to indicate no problem detected with current element.\nWhen a transmission medium is half duplex (that is, it can transmit in only one direction at a time), there is usually a master station that can transmit at any time, and one or more slave stations that transmit when they have permission. The enquire character (ENQ) is generally used by a master station to ask a slave station to send its next message. A slave station indicates that it has completed its transmission by sending the end of transmission character (EOT).\nThe device control codes (DC1 to DC4) were originally generic, to be implemented as necessary by each device. However, a universal need in data transmission is to request the sender to stop transmitting when a receiver is temporarily unable to accept any more data. Digital Equipment Corporation invented a convention which used 19 (the device control 3 character (DC3), also known as control-S, or XOFF) to \"S\"top transmission, and 17 (the device control 1 character (DC1), a.k.a. control-Q, or XON) to start transmission. It has become so widely used that most don't realize it is not part of official ASCII. This technique, however implemented, avoids additional wires in the data cable devoted only to transmission management, which saves money. A sensible protocol for the use of such transmission flow control signals must be used, to avoid potential deadlock conditions, however.\nThe data link escape character (DLE) was intended to be a signal to the other end of a data link that the following character is a control character such as STX or ETX. For example a packet may be structured in the following way (DLE) &lt;STX&gt; &lt;PAYLOAD&gt; (DLE) &lt;ETX&gt;.\nMiscellaneous codes.\nCode 7 (BEL) is intended to cause an audible signal in the receiving terminal.\nMany of the ASCII control characters were designed for devices of the time that are not often seen today. For example, code 22, \"synchronous idle\" (SYN), was originally sent by synchronous modems (which have to send data constantly) when there was no actual data to send. (Modern systems typically use a start bit to announce the beginning of a transmitted word\u2014 this is a feature of \"asynchronous\" communication. \"Synchronous\" communication links were more often seen with mainframes, where they were typically run over corporate leased lines to connect a mainframe to another mainframe or perhaps a minicomputer.)\nCode 0 (ASCII code name NUL) is a special case. In paper tape, it is the case when there are no holes. It is convenient to treat this as a \"fill\" character with no meaning otherwise. Since the position of a NUL character has no holes punched, it can be replaced with any other character at a later time, so it was typically used to reserve space, either for correcting errors or for inserting information that would be available at a later time or in another place. In computing it is often used for padding in fixed length records and more commonly, to mark the end of a string.\nCode 127 (DEL, a.k.a. \"rubout\") is likewise a special case. Its 7-bit code is \"all-bits-on\" in binary, which essentially erased a character cell on a paper tape when overpunched. Paper tape was a common storage medium when ASCII was developed, with a computing history dating back to WWII code breaking equipment at Biuro Szyfr\u00f3w. Paper tape became obsolete in the 1970s, so this clever aspect of ASCII rarely saw any use after that. Some systems (such as the original Apples) converted it to a backspace. But because its code is in the range occupied by other printable characters, and because it had no official assigned glyph, many computer equipment vendors used it as an additional printable character (often an all-black \"box\" character useful for erasing text by overprinting with ink).\nNon-erasable programmable ROMs are typically implemented as arrays of fusible elements, each representing a bit, which can only be switched one way, usually from one to zero. In such PROMs, the DEL and NUL characters can be used in the same way that they were used on punched tape: one to reserve meaningless fill bytes that can be written later, and the other to convert written bytes to meaningless fill bytes. For PROMs that switch one to zero, the roles of NUL and DEL are reversed; also, DEL will only work with 7-bit characters, which are rarely used today; for 8-bit content, the character code 255, commonly defined as a nonbreaking space character, can be used instead of DEL.\nMany file systems do not allow control characters in filenames, as they may have reserved functions.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5299", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=5299", "title": "Carbon", "text": "Chemical element, symbol C and atomic number 6\nCarbon (from la\u00a0'coal') is a chemical element with the symbol C and atomic number 6. It is nonmetallic and tetravalent\u2014its atom making four electrons available to form covalent chemical bonds. It belongs to group 14 of the periodic table. Carbon makes up about 0.025 percent of Earth's crust. Three isotopes occur naturally, 12C and 13C being stable, while 14C is a radionuclide, decaying with a half-life of about 5,730\u00a0years. Carbon is one of the few elements known since antiquity.\nCarbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth, enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.\nThe atoms of carbon can bond together in diverse ways, resulting in various allotropes of carbon. Well-known allotropes include graphite, diamond, amorphous carbon, and fullerenes. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black, while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb \"\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\" which means \"to write\"), while diamond is the hardest naturally occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form at standard temperature and pressure. They are chemically resistant and require high temperature to react even with oxygen.\nThe most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, with about two hundred million having been described and indexed; and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions.\nCharacteristics.\nThe allotropes of carbon include graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms, including other carbon atoms, and is capable of forming multiple stable covalent bonds with suitable multivalent atoms. Carbon is a component element in the large majority of all chemical compounds, with about two hundred million examples having been described in the published chemical literature. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point, as its triple point is at and , so it sublimes at about . Graphite is much more reactive than diamond at standard conditions, despite being more thermodynamically stable, as its delocalised pi system is much more vulnerable to attack. For example, graphite can be oxidised by hot concentrated nitric acid at standard conditions to mellitic acid, C6(CO2H)6, which preserves the hexagonal units of graphite while breaking up the larger structure.\nCarbon sublimes in a carbon arc, which has a temperature of about 5800 K (5,530\u00a0\u00b0C or 9,980\u00a0\u00b0F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest-melting-point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper, which are weaker reducing agents at room temperature.\nCarbon is the sixth element, with a ground-state electron configuration of 1s22s22p2, of which the four outer electrons are valence electrons. Its first four ionisation energies, 1086.5, 2352.6, 4620.5 and 6222.7\u00a0kJ/mol, are much higher than those of the heavier group-14 elements. The electronegativity of carbon is 2.5, significantly higher than the heavier group-14 elements (1.8\u20131.9), but close to most of the nearby nonmetals, as well as some of the second- and third-row transition metals. Carbon's covalent radii are normally taken as 77.2\u00a0pm (C\u2212C), 66.7\u00a0pm (C=C) and 60.3\u00a0pm (C\u2261C), although these may vary depending on coordination number and what the carbon is bonded to. In general, covalent radius decreases with lower coordination number and higher bond order.\nCarbon-based compounds form the basis of all known life on Earth, and the carbon-nitrogen-oxygen cycle provides a small portion of the energy produced by the Sun, and most of the energy in larger stars (e.g. Sirius). Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:\nFe3O4 + 4 C(s) + 2 O2 \u2192 3 Fe(s) + 4 CO2(g).\nCarbon reacts with sulfur to form carbon disulfide, and it reacts with steam in the coal-gas reaction used in coal gasification:\nC(s) + H2O(g) \u2192 CO(g) + H2(g).\nCarbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.\nThe system of carbon allotropes spans a range of extremes:\nAllotropes.\nAtomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with diverse molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite, glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).\nGraphene is a two-dimensional sheet of carbon with the atoms arranged in a hexagonal lattice. As of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes. If successful, graphene could be used in the construction of a space elevator. It could also be used to safely store hydrogen for use in a hydrogen based engine in cars.\n The amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot), and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a \u03c0-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.\nAt very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that \"diamonds are forever\", they are thermodynamically unstable (\u0394f\"G\"\u00b0(diamond, 298\u00a0K) = 2.9\u00a0kJ/mol) under normal conditions (298\u00a0K, 105\u00a0Pa) and should theoretically transform into graphite. But due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. However, at very high temperatures diamond will turn into graphite, and diamonds can burn up in a house fire. The bottom left corner of the phase diagram for carbon has not been scrutinized experimentally. Although a computational study employing density functional theory methods reached the conclusion that as \"T\" \u2192 0 K and \"p\" \u2192 0 Pa, diamond becomes more stable than graphite by approximately 1.1\u00a0kJ/mol, more recent and definitive experimental and computational studies show that graphite is more stable than diamond for \"T\" &lt; 400 K, without applied pressure, by 2.7\u00a0kJ/mol at \"T\"\u00a0=\u00a00\u00a0K and 3.2\u00a0kJ/mol at \"T\"\u00a0=\u00a0298.15\u00a0K. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.\nFullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of flat hexagonal cells only, some of the cells of which fullerenes are formed may be pentagons, nonplanar hexagons, or even heptagons of carbon atoms. The sheets are thus warped into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names \"fullerene\" and \"buckyball\" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C60 buckminsterfullerene). Carbon nanotubes (buckytubes) are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid buckytube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.\nOf the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2\u00a0kg/m3. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure \u2212(C\u2261C)\u2212\u00a0. Carbon in this modification is linear with \"sp\" orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is 40\u00a0times that of the hardest known material\u00a0\u2013 diamond.\nIn 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high-energy low-duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagnetism, fluorescence, and a hardness superior to diamonds.\nIn the vapor phase, some of the carbon is in the form of highly reactive diatomic carbon dicarbon (). When excited, this gas glows green.\nOccurrence.\nCarbon is the fourth most abundant chemical element in the observable universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the Solar System was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.\nIn 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed \"a couple of billion years\" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIt has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is , this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).\nIn combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 900\u00a0gigatonnes of carbon \u2014 each ppm corresponds to 2.13\u00a0Gt) and dissolved in all water bodies (approximately 36,000\u00a0gigatonnes of carbon). Carbon in the biosphere has been estimated at 550\u00a0gigatonnes but with a large uncertainty, due mostly to a huge uncertainty in the amount of terrestrial deep subsurface bacteria. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal \"reserves\" (not \"resources\") amount to around 900\u00a0gigatonnes with perhaps 18,000 Gt of resources. Oil reserves are around 150\u00a0gigatonnes. Proven sources of natural gas are about (containing about 105 gigatonnes of carbon), but studies estimate another of \"unconventional\" deposits such as shale gas, representing about 540 gigatonnes of carbon.\nCarbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500 Gt, or 3,000 Gt.\nIn the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2,000 Gt.\nCarbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble, and others). Coal is very rich in carbon (anthracite contains 92\u201398%) and is the largest commercial source of mineral carbon, accounting for 4,000\u00a0gigatonnes or 80% of fossil fuel.\nAs for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic \"necks\", or \"pipes\". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.\nCarbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9\u201315\u00a0km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton. As such, of atmospheric carbon dioxide contains carbon-14.\nCarbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in the Solar System. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.\nIsotopes.\nIsotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (12C) forms 98.93% of the carbon on Earth, while carbon-13 (13C) forms the remaining 1.07%. The concentration of 12C is further increased in biological materials because biochemical reactions discriminate against 13C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in nuclear magnetic resonance (NMR) experiments is done with the isotope 13C.\nCarbon-14 (14C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of 1 part per trillion (0.0000000001%) or more, mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158\u00a0MeV \u03b2\u2212 emission. Because of its relatively short half-life of 5730\u00a0years, 14C is virtually absent in ancient rocks. The amount of 14C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000\u00a0years.\nThere are 15 known isotopes of carbon and the shortest-lived of these is 8C which decays through proton emission and alpha decay and has a half-life of 1.98739 \u00d7 10\u221221 s. The exotic 19C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.\nFormation in stars.\nFormation of the carbon atomic nucleus occurs within a giant or supergiant star through the triple-alpha process. This requires a nearly simultaneous collision of three alpha particles (helium nuclei), as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. The triple-alpha process happens in conditions of temperatures over 100 megakelvins and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.\nAccording to current physical cosmology theory, carbon is formed in the interiors of stars on the horizontal branch. When massive stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of the next-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it. It is the opinion of most scholars that all the carbon in the Solar System and the Milky Way comes from dying stars.\nThe CNO cycle is an additional hydrogen fusion mechanism that powers stars, wherein carbon operates as a catalyst.\nRotational transitions of various isotopic forms of carbon monoxide (for example, 12CO, 13CO, and 18CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.\nCarbon cycle.\nUnder terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.\nCompounds.\nOrganic compounds.\nCarbon can form very long chains of interconnecting carbon\u2013carbon bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon than do not. A similar claim can be made for hydrogen because most organic compounds contain hydrogen chemically bonded to carbon or another common element like oxygen or nitrogen.\nThe simplest form of an organic molecule is the hydrocarbon\u2014a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. A hydrocarbon backbone can be substituted by other atoms, known as heteroatoms. Common heteroatoms that appear in organic compounds include oxygen, nitrogen, sulfur, phosphorus, and the nonradioactive halogens, as well as the metals lithium and magnesium. Organic compounds containing bonds to metal are known as organometallic compounds (\"see below\"). Certain groupings of atoms, often including heteroatoms, recur in large numbers of organic compounds. These collections, known as \"functional groups\", confer common reactivity patterns and allow for the systematic study and categorization of organic compounds. Chain length, shape and functional groups all affect the properties of organic molecules.\nIn most stable compounds of carbon (and nearly all stable \"organic\" compounds), carbon obeys the octet rule and is \"tetravalent\", meaning that a carbon atom forms a total of four covalent bonds (which may include double and triple bonds). Exceptions include a small number of stabilized \"carbocations\" (three bonds, positive charge), \"radicals\" (three bonds, neutral), \"carbanions\" (three bonds, negative charge) and \"carbenes\" (two bonds, neutral), although these species are much more likely to be encountered as unstable, reactive intermediates.\nCarbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.\nWhen combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells. Norman Horowitz, head of the Mariner and Viking missions to Mars (1965-1976), considered that the unique characteristics of carbon made it unlikely that any other element could replace carbon, even on another planet, to generate the biochemistry necessary for life.\nInorganic compounds.\nCommonly carbon-containing compounds which are associated with minerals or which do not contain bonds to the other carbon atoms, halogens, or hydrogen, are treated separately from classical organic compounds; the definition is not rigid, and the classification of some compounds can vary from author to author (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (CO2). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (H2CO3), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide (CS2) is similar. Nevertheless, due to its physical properties and its association with organic synthesis, carbon disulfide is sometimes classified as an \"organic\" solvent.\nThe other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN\u2212), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)2), similar to diatomic halides. Likewise, the heavier analog of cyanide, cyaphide (CP\u2212), is also considered inorganic, though most simple derivatives are highly unstable. Other uncommon oxides are carbon suboxide (C3O2), the unstable dicarbon monoxide (C2O), carbon trioxide (CO3), cyclopentanepentone (C5O5), cyclohexanehexone (C6O6), and mellitic anhydride (C12O9). However, mellitic anhydride is the triple acyl anhydride of mellitic acid; moreover, it contains a benzene ring. Thus, many chemists consider it to be organic.\nWith reactive metals, such as tungsten, carbon forms either carbides (C4\u2212) or acetylides (C22-) to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond. Nevertheless, even the most polar and salt-like of carbides are not completely ionic compounds.\nOrganometallic compounds.\nOrganometallic compounds by definition contain at least one carbon-metal covalent bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), \u03b72-alkene compounds (for example, Zeise's salt), and \u03b73-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls and metal cyanides exist (for example, tetracarbonylnickel and potassium ferricyanide); some workers consider metal carbonyl and cyanide complexes without other carbon ligands to be purely inorganic, and not organometallic. However, most organometallic chemists consider metal complexes with any carbon ligand, even 'inorganic carbon' (e.g., carbonyls, cyanides, and certain types of carbides and acetylides) to be organometallic in nature. Metal complexes containing organic ligands without a carbon-metal covalent bond (e.g., metal carboxylates) are termed \"metalorganic\" compounds.\nWhile carbon is understood to strongly prefer formation of four covalent bonds, other exotic bonding schemes are also known. Carboranes are highly stable dodecahedral derivatives of the [B12H12]2- unit, with one BH replaced with a CH+. Thus, the carbon is bonded to five boron atoms and one hydrogen atom. The cation [(Ph3PAu)6C]2+ contains an octahedral carbon bound to six phosphine-gold fragments. This phenomenon has been attributed to the aurophilicity of the gold ligands, which provide additional stabilization of an otherwise labile species. In nature, the iron-molybdenum cofactor (FeMoco) responsible for microbial nitrogen fixation likewise has an octahedral carbon center (formally a carbide, C(-IV)) bonded to six iron atoms. In 2016, it was confirmed that, in line with earlier theoretical predictions, the hexamethylbenzene dication contains a carbon atom with six bonds. More specifically, the dication could be described structurally by the formulation [MeC(\u03b75-C5Me5)]2+, making it an \"organic metallocene\" in which a MeC3+ fragment is bonded to a \u03b75-C5Me5\u2212 fragment through all five of the carbons of the ring.\nIt is important to note that in the cases above, each of the bonds to carbon contain less than two formal electron pairs. Thus, the formal electron count of these species does not exceed an octet. This makes them hypercoordinate but not hypervalent. Even in cases of alleged 10-C-5 species (that is, a carbon with five ligands and a formal electron count of ten), as reported by Akiba and co-workers, electronic structure calculations conclude that the electron population around carbon is still less than eight, as is true for other compounds featuring four-electron three-center bonding.\nHistory and etymology.\nThe English name \"carbon\" comes from the Latin \"carbo\" for coal and charcoal, whence also comes the French \"charbon\", meaning charcoal. In German, Dutch and Danish, the names for carbon are \"Kohlenstoff\", \"koolstof\", and \"kulstof\" respectively, all literally meaning coal-substance.\nCarbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500\u00a0BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.\n In 1722, Ren\u00e9 Antoine Ferchault de R\u00e9aumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram. In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave \"aerial acid\" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name \"carbone\" (Latin \"carbonum\") for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.\nA new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers\u00a0\u2013 Robert Curl, Harold Kroto, and Richard Smalley\u00a0\u2013 received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms led to the discovery of further exotic allotropes, including glassy carbon, and the realization that \"amorphous carbon\" is not strictly amorphous.\nProduction.\nGraphite.\nCommercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil, and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica, and feldspars in schists, gneisses, and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.\nThere are three types of natural graphite\u2014amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry \"amorphous\" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.\nAccording to the USGS, world production of natural graphite was 1.1\u00a0million tonnes in 2010, to which China contributed 800,000\u00a0t, India 130,000 t, Brazil 76,000\u00a0t, North Korea 30,000\u00a0t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998\u00a0million was produced in 2009.\nDiamond.\nThe diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\nHistorically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.\nDiamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the diamond fields in South Africa. Production has increased over time and an accumulated total of over 4.5\u00a0billion carats have been mined since that date. Most commercially viable diamond deposits were in Russia, Botswana, Australia and the Democratic Republic of Congo. By 2005, Russia produced almost one-fifth of the global diamond output (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe) but the Argyle mine in Australia became the single largest source, producing 14 million carats in 2018. New finds, the Canadian mines at Diavik and Ekati, are expected to become even more valuable owing to their production of gem quality stones.\nIn the United States, diamonds have been found in Arkansas, Colorado, and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.\nApplications.\nCarbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere, and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.\nThe uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors, and as a neutron moderator in nuclear reactors.\nCharcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.\nCarbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.\nCarbon black is used as the black pigment in printing ink, artist's oil paint, and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron, and titanium are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.\nDiamonds.\nThe diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets function dramatically differently.\nUnlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.\nIndustrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20\u00a0tonnes annually) are unsuitable for use as gemstones are relegated for industrial use (known as \"bort)\". synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3\u00a0billion carats (600\u00a0tonnes) of synthetic diamond is produced annually.\nThe dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high-pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.\nPrecautions.\nPure carbon has extremely low toxicity to humans and can be handled safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and \u00d6tzi the Iceman was found to have carbon tattoos that survived during his life and for 5200\u00a0years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can be harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.\nCarbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to \"Drosophila\".\nCarbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.\nIn nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250\u00a0\u00b0C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.\nThe great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant \"Ricinus communis\", cyanide (CN\u2212), and carbon monoxide; and such essentials to life as glucose and protein.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5300", "revid": "45137862", "url": "https://en.wikipedia.org/wiki?curid=5300", "title": "Computer data storage", "text": "Storage of digital data readable by computers\nComputer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.\nThe central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy, which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally, the fast technologies are referred to as \"memory\", while slower persistent technologies are referred to as \"storage\".\nEven the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.\nFunctionality.\nWithout a significant amount of memory, a computer would merely be able to perform fixed operations and immediately output the result. It would have to be reconfigured to change its behavior. This is acceptable for devices such as desk calculators, digital signal processors, and other specialized devices. Von Neumann machines differ in having a memory in which they store their operating instructions and data. Such computers are more versatile in that they do not need to have their hardware reconfigured for each new program, but can simply be reprogrammed with new in-memory instructions; they also tend to be simpler to design, in that a relatively simple processor may keep state between successive computations to build up complex procedural results. Most modern computers are von Neumann machines.\nData organization and representation.\nA modern digital computer represents data using the binary numeral system. Text, numbers, pictures, audio, and nearly any other form of information can be converted into a string of bits, or binary digits, each of which has a value of 0\u00a0or\u00a01. The most common unit of storage is the byte, equal to 8 bits. A piece of information can be handled by any computer or device whose storage space is large enough to accommodate \"the binary representation of the piece of information\", or simply data. For example, the complete works of Shakespeare, about 1250\u00a0pages in print, can be stored in about five megabytes (40\u00a0million bits) with one byte per character.\nData are encoded by assigning a bit pattern to each character, digit, or multimedia object. Many standards exist for encoding (e.g. character encodings like ASCII, image encodings like JPEG, and video encodings like MPEG-4).\nBy adding bits to each encoded unit, redundancy allows the computer to both detect errors in coded data and correct them based on mathematical algorithms. Errors generally occur in low probabilities due to random bit value flipping, or \"physical bit fatigue\", loss of the physical bit in the storage of its ability to maintain a distinguishable value (0\u00a0or\u00a01), or due to errors in inter or intra-computer communication. A random bit flip (e.g. due to random radiation) is typically corrected upon detection. A bit or a group of malfunctioning physical bits (the specific defective bit is not always known; group definition depends on the specific storage device) is typically automatically fenced out, taken out of use by the device, and replaced with another functioning equivalent group in the device, where the corrected bit values are restored (if possible). The cyclic redundancy check (CRC) method is typically used in communications and storage for error detection. A detected error is then retried.\nData compression methods allow in many cases (such as a database) to represent a string of bits by a shorter bit string (\"compress\") and reconstruct the original string (\"decompress\") when needed. This utilizes substantially less storage (tens of percents) for many types of data at the cost of more computation (compress and decompress when needed). Analysis of the trade-off between storage cost saving and costs of related computations and possible delays in data availability is done before deciding whether to keep certain data compressed or not.\nFor security reasons, certain types of data (e.g. credit card information) may be kept encrypted in storage to prevent the possibility of unauthorized information reconstruction from chunks of storage snapshots.\nHierarchy of storage.\nGenerally, the lower a storage is in the hierarchy, the lesser its bandwidth and the greater its access latency is from the CPU. This traditional division of storage to primary, secondary, tertiary, and off-line storage is also guided by cost per bit.\nIn contemporary usage, \"memory\" is usually semiconductor storage read-write random-access memory, typically DRAM (dynamic RAM) or other forms of fast but temporary storage. \"Storage\" consists of storage devices and their media not directly accessible by the CPU (secondary or tertiary storage), typically hard disk drives, optical disc drives, and other devices slower than RAM but non-volatile (retaining contents when powered down).\nHistorically, \"memory\" has, depending on technology, been called \"central memory\", \"core memory\", \"core storage\", \"drum\", \"main memory\", \"real storage\", or \"internal memory\". Meanwhile, slower persistent storage devices have been referred to as \"secondary storage\", \"external memory\", or \"auxiliary/peripheral storage\".\nPrimary storage.\n\"Primary storage\" (also known as \"main memory\", \"internal memory\", or \"prime memory\"), often referred to simply as \"memory\", is the only one directly accessible to the CPU. The CPU continuously reads instructions stored there and executes them as required. Any data actively operated on is also stored there in uniform manner.\nHistorically, early computers used delay lines, Williams tubes, or rotating magnetic drums as primary storage. By 1954, those unreliable methods were mostly replaced by magnetic-core memory. Core memory remained dominant until the 1970s, when advances in integrated circuit technology allowed semiconductor memory to become economically competitive.\nThis led to modern random-access memory (RAM). It is small-sized, light, but quite expensive at the same time. The particular types of RAM used for primary storage are volatile, meaning that they lose the information when not powered. Besides storing opened programs, it serves as disk cache and write buffer to improve both reading and writing performance. Operating systems borrow RAM capacity for caching so long as not needed by running software. Spare memory can be utilized as RAM drive for temporary high-speed data storage.\nAs shown in the diagram, traditionally there are two more sub-layers of the primary storage, besides main large-capacity RAM:\nMain memory is directly or indirectly connected to the central processing unit via a \"memory bus\". It is actually two buses (not on the diagram): an address bus and a data bus. The CPU firstly sends a number through an address bus, a number called memory address, that indicates the desired location of data. Then it reads or writes the data in the memory cells using the data bus. Additionally, a memory management unit (MMU) is a small device between CPU and RAM recalculating the actual memory address, for example to provide an abstraction of virtual memory or other tasks.\nAs the RAM types used for primary storage are volatile (uninitialized at start up), a computer containing only such storage would not have a source to read instructions from, in order to start the computer. Hence, non-volatile primary storage containing a small startup program (BIOS) is used to bootstrap the computer, that is, to read a larger program from non-volatile \"secondary\" storage to RAM and start to execute it. A non-volatile technology used for this purpose is called ROM, for read-only memory (the terminology may be somewhat confusing as most ROM types are also capable of \"random access\").\nMany types of \"ROM\" are not literally \"read only\", as updates to them are possible; however it is slow and memory must be erased in large portions before it can be re-written. Some embedded systems run programs directly from ROM (or similar), because such programs are rarely changed. Standard computers do not store non-rudimentary programs in ROM, and rather, use large capacities of secondary storage, which is non-volatile as well, and not as costly.\nRecently, \"primary storage\" and \"secondary storage\" in some uses refer to what was historically called, respectively, \"secondary storage\" and \"tertiary storage\".\nSecondary storage.\n\"Secondary storage\" (also known as \"external memory\" or \"auxiliary storage\") differs from primary storage in that it is not directly accessible by the CPU. The computer usually uses its input/output channels to access secondary storage and transfer the desired data to primary storage. Secondary storage is non-volatile (retaining data when its power is shut off). Modern computer systems typically have two orders of magnitude more secondary storage than primary storage because secondary storage is less expensive.\nIn modern computers, hard disk drives (HDDs) or solid-state drives (SSDs) are usually used as secondary storage. The access time per byte for HDDs or SSDs is typically measured in milliseconds (thousandths of a second), while the access time per byte for primary storage is measured in nanoseconds (billionths of a second). Thus, secondary storage is significantly slower than primary storage. Rotating optical storage devices, such as CD and DVD drives, have even longer access times. Other examples of secondary storage technologies include USB flash drives, floppy disks, magnetic tape, paper tape, punched cards, and RAM disks.\nOnce the disk read/write head on HDDs reaches the proper placement and the data, subsequent data on the track are very fast to access. To reduce the seek time and rotational latency, data are transferred to and from disks in large contiguous blocks. Sequential or block access on disks is orders of magnitude faster than random access, and many sophisticated paradigms have been developed to design efficient algorithms based upon sequential and block access. Another way to reduce the I/O bottleneck is to use multiple disks in parallel in order to increase the bandwidth between primary and secondary memory.\nSecondary storage is often formatted according to a file system format, which provides the abstraction necessary to organize data into files and directories, while also providing metadata describing the owner of a certain file, the access time, the access permissions, and other information.\nMost computer operating systems use the concept of virtual memory, allowing utilization of more primary storage capacity than is physically available in the system. As the primary memory fills up, the system moves the least-used chunks (pages) to a swap file or page file on secondary storage, retrieving them later when needed. If a lot of pages are moved to slower secondary storage, the system performance is degraded.\nTertiary storage.\n\"Tertiary storage\" or \"tertiary memory\" is a level below secondary storage. Typically, it involves a robotic mechanism which will \"mount\" (insert) and \"dismount\" removable mass storage media into a storage device according to the system's demands; such data are often copied to secondary storage before use. It is primarily used for archiving rarely accessed information since it is much slower than secondary storage (e.g. 5\u201360 seconds vs. 1\u201310 milliseconds). This is primarily useful for extraordinarily large data stores, accessed without human operators. Typical examples include tape libraries and optical jukeboxes.\nWhen a computer needs to read information from the tertiary storage, it will first consult a catalog database to determine which tape or disc contains the information. Next, the computer will instruct a robotic arm to fetch the medium and place it in a drive. When the computer has finished reading the information, the robotic arm will return the medium to its place in the library.\nTertiary storage is also known as \"nearline storage\" because it is \"near to online\". The formal distinction between online, nearline, and offline storage is:\nFor example, always-on spinning hard disk drives are online storage, while spinning drives that spin down automatically, such as in massive arrays of idle disks (MAID), are nearline storage. Removable media such as tape cartridges that can be automatically loaded, as in tape libraries, are nearline storage, while tape cartridges that must be manually loaded are offline storage.\nOff-line storage.\n\"Off-line storage\" is a computer data storage on a medium or a device that is not under the control of a processing unit. The medium is recorded, usually in a secondary or tertiary storage device, and then physically removed or disconnected. It must be inserted or connected by a human operator before a computer can access it again. Unlike tertiary storage, it cannot be accessed without human interaction.\nOff-line storage is used to transfer information, since the detached medium can easily be physically transported. Additionally, it is useful for cases of disaster, where, for example, a fire destroys the original data, a medium in a remote location will be unaffected, enabling disaster recovery. Off-line storage increases general information security, since it is physically inaccessible from a computer, and data confidentiality or integrity cannot be affected by computer-based attack techniques. Also, if the information stored for archival purposes is rarely accessed, off-line storage is less expensive than tertiary storage.\nIn modern personal computers, most secondary and tertiary storage media are also used for off-line storage. Optical discs and flash memory devices are the most popular, and to a much lesser extent removable hard disk drives. In enterprise uses, magnetic tape is predominant. Older examples are floppy disks, Zip disks, or punched cards.\nCharacteristics of storage.\nStorage technologies at all levels of the storage hierarchy can be differentiated by evaluating certain core characteristics as well as measuring characteristics specific to a particular implementation. These core characteristics are volatility, mutability, accessibility, and addressability. For any particular implementation of any storage technology, the characteristics worth measuring are capacity and performance.\nVolatility.\nNon-volatile memory retains the stored information even if not constantly supplied with electric power. It is suitable for long-term storage of information. Volatile memory requires constant power to maintain the stored information. The fastest memory technologies are volatile ones, although that is not a universal rule. Since the primary storage is required to be very fast, it predominantly uses volatile memory.\nDynamic random-access memory is a form of volatile memory that also requires the stored information to be periodically reread and rewritten, or refreshed, otherwise it would vanish. Static random-access memory is a form of volatile memory similar to DRAM with the exception that it never needs to be refreshed as long as power is applied; it loses its content when the power supply is lost.\nAn uninterruptible power supply (UPS) can be used to give a computer a brief window of time to move information from primary volatile storage into non-volatile storage before the batteries are exhausted. Some systems, for example EMC Symmetrix, have integrated batteries that maintain volatile storage for several minutes.\nPerformance.\nUtilities such as hdparm and sar can be used to measure IO performance in Linux.\nSecurity.\nFull disk encryption, volume and virtual disk encryption, andor file/folder encryption is readily available for most storage devices.\nHardware memory encryption is available in Intel Architecture, supporting Total Memory Encryption (TME) and page granular memory encryption with multiple keys (MKTME). and in SPARC M7 generation since October 2015.\nVulnerability and reliability.\nDistinct types of data storage have different points of failure and various methods of predictive failure analysis.\nVulnerabilities that can instantly lead to total loss are head crashing on mechanical hard drives and failure of electronic components on flash storage.\nError detection.\nImpending failure on hard disk drives is estimable using S.M.A.R.T. diagnostic data that includes the hours of operation and the count of spin-ups, though its reliability is disputed.\nFlash storage may experience downspiking transfer rates as a result of accumulating errors, which the flash memory controller attempts to correct.\nThe health of optical media can be determined by measuring correctable minor errors, of which high counts signify deteriorating and/or low-quality media. Too many consecutive minor errors can lead to data corruption. Not all vendors and models of optical drives support error scanning.\nStorage media.\nAs of 2011[ [update]], the most commonly used data storage media are semiconductor, magnetic, and optical, while paper still sees some limited usage. Some other fundamental storage technologies, such as all-flash arrays (AFAs) are proposed for development.\nSemiconductor.\nSemiconductor memory uses semiconductor-based integrated circuit (IC) chips to store information. Data are typically stored in metal\u2013oxide\u2013semiconductor (MOS) memory cells. A semiconductor memory chip may contain millions of memory cells, consisting of tiny MOS field-effect transistors (MOSFETs) and/or MOS capacitors. Both \"volatile\" and \"non-volatile\" forms of semiconductor memory exist, the former using standard MOSFETs and the latter using floating-gate MOSFETs.\nIn modern computers, primary storage almost exclusively consists of dynamic volatile semiconductor random-access memory (RAM), particularly dynamic random-access memory (DRAM). Since the turn of the century, a type of non-volatile floating-gate semiconductor memory known as flash memory has steadily gained share as off-line storage for home computers. Non-volatile semiconductor memory is also used for secondary storage in various advanced electronic devices and specialized computers that are designed for them.\nAs early as 2006, notebook and desktop computer manufacturers started using flash-based solid-state drives (SSDs) as default configuration options for the secondary storage either in addition to or instead of the more traditional HDD.\nMagnetic.\nMagnetic storage uses different patterns of magnetization on a magnetically coated surface to store information. Magnetic storage is \"non-volatile\". The information is accessed using one or more read/write heads which may contain one or more recording transducers. A read/write head only covers a part of the surface so that the head or medium or both must be moved relative to another in order to access data. In modern computers, magnetic storage will take these forms:\nIn early computers, magnetic storage was also used as:\nMagnetic storage does not have a definite limit of rewriting cycles like flash storage and re-writeable optical media, as altering magnetic fields causes no physical wear. Rather, their life span is limited by mechanical parts.\nOptical.\nOptical storage, the typical optical disc, stores information in deformities on the surface of a circular disc and reads this information by illuminating the surface with a laser diode and observing the reflection. Optical disc storage is \"non-volatile\". The deformities may be permanent (read only media), formed once (write once media) or reversible (recordable or read/write media). The following forms are currently in common use:\nMagneto-optical disc storage is optical disc storage where the magnetic state on a ferromagnetic surface stores information. The information is read optically and written by combining magnetic and optical methods. Magneto-optical disc storage is \"non-volatile\", \"sequential access\", slow write, fast read storage used for tertiary and off-line storage.\n3D optical data storage has also been proposed.\nLight induced magnetization melting in magnetic photoconductors has also been proposed for high-speed low-energy consumption magneto-optical storage.\nPaper.\nPaper data storage, typically in the form of paper tape or punched cards, has long been used to store information for automatic processing, particularly before general-purpose computers existed. Information was recorded by punching holes into the paper or cardboard medium and was read mechanically (or later optically) to determine whether a particular location on the medium was solid or contained a hole. Barcodes make it possible for objects that are sold or transported to have some computer-readable information securely attached.\nRelatively small amounts of digital data (compared to other digital data storage) may be backed up on paper as a matrix barcode for very long-term storage, as the longevity of paper typically exceeds even magnetic data storage.\nRelated technologies.\nRedundancy.\nWhile a group of bits malfunction may be resolved by error detection and correction mechanisms (see above), storage device malfunction requires different solutions. The following solutions are commonly used and valid for most storage devices:\nDevice mirroring and typical RAID are designed to handle a single device failure in the RAID group of devices. However, if a second failure occurs before the RAID group is completely repaired from the first failure, then data can be lost. The probability of a single failure is typically small. Thus the probability of two failures in a same RAID group in time proximity is much smaller (approximately the probability squared, i.e., multiplied by itself). If a database cannot tolerate even such smaller probability of data loss, then the RAID group itself is replicated (mirrored). In many cases such mirroring is done geographically remotely, in a different storage array, to handle also recovery from disasters (see disaster recovery above).\nNetwork connectivity.\nA secondary or tertiary storage may connect to a computer utilizing computer networks. This concept does not pertain to the primary storage, which is shared between multiple processors to a lesser degree.\nRobotic storage.\nLarge quantities of individual magnetic tapes, and optical or magneto-optical discs may be stored in robotic tertiary storage devices. In tape storage field they are known as tape libraries, and in optical storage field optical jukeboxes, or optical disk libraries per analogy. The smallest forms of either technology containing just one drive device are referred to as autoloaders or autochangers.\nRobotic-access storage devices may have a number of slots, each holding individual media, and usually one or more picking robots that traverse the slots and load media to built-in drives. The arrangement of the slots and picking devices affects performance. Important characteristics of such storage are possible expansion options: adding slots, modules, drives, robots. Tape libraries may have from 10 to more than 100,000 slots, and provide terabytes or petabytes of near-line information. Optical jukeboxes are somewhat smaller solutions, up to 1,000 slots.\nRobotic storage is used for backups, and for high-capacity archives in imaging, medical, and video industries. Hierarchical storage management is a most known archiving strategy of automatically \"migrating\" long-unused files from fast hard disk storage to libraries or jukeboxes. If the files are needed, they are \"retrieved\" back to disk.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Citation/styles.css\"/&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5302", "revid": "42833664", "url": "https://en.wikipedia.org/wiki?curid=5302", "title": "Conditional", "text": "Conditional (if then) may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5303", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5303", "title": "Conic sections", "text": ""}
{"id": "5304", "revid": "1893265", "url": "https://en.wikipedia.org/wiki?curid=5304", "title": "Cone (disambiguation)", "text": "A cone is a basic geometrical shape.\nCone may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5306", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=5306", "title": "Chemical equilibrium", "text": "When the ratio of reactants to products of a chemical reaction is constant with time\nIn a chemical reaction, chemical equilibrium is the state in which both the reactants and products are present in concentrations which have no further tendency to change with time, so that there is no observable change in the properties of the system. This state results when the forward reaction proceeds at the same rate as the reverse reaction. The reaction rates of the forward and backward reactions are generally not zero, but they are equal. Thus, there are no net changes in the concentrations of the reactants and products. Such a state is known as dynamic equilibrium.\nHistorical introduction.\nThe concept of chemical equilibrium was developed in 1803, after Berthollet found that some chemical reactions are reversible. For any reaction mixture to exist at equilibrium, the rates of the forward and backward (reverse) reactions must be equal. In the following chemical equation, arrows point both ways to indicate equilibrium. A and B are reactant chemical species, S and T are product species, and \"\u03b1\", \"\u03b2\", \"\u03c3\", and \"\u03c4\" are the stoichiometric coefficients of the respective reactants and products:\n\"\u03b1\"\u00a0A + \"\u03b2\"\u00a0B \u21cc \"\u03c3\"\u00a0S + \"\u03c4\"\u00a0T\nThe equilibrium concentration position of a reaction is said to lie \"far to the right\" if, at equilibrium, nearly all the reactants are consumed. Conversely the equilibrium position is said to be \"far to the left\" if hardly any product is formed from the reactants.\nGuldberg and Waage (1865), building on Berthollet's ideas, proposed the law of mass action:\nformula_1\nwhere A, B, S and T are active masses and \"k\"+ and \"k\"\u2212 are rate constants. Since at equilibrium forward and backward rates are equal:\nformula_2\nand the ratio of the rate constants is also a constant, now known as an equilibrium constant.\nformula_3\nBy convention, the products form the numerator.\nHowever, the law of mass action is valid only for concerted one-step reactions that proceed through a single transition state and is not valid in general because rate equations do not, in general, follow the stoichiometry of the reaction as Guldberg and Waage had proposed (see, for example, nucleophilic aliphatic substitution by SN1 or reaction of hydrogen and bromine to form hydrogen bromide). Equality of forward and backward reaction rates, however, is a necessary condition for chemical equilibrium, though it is not sufficient to explain why equilibrium occurs.\nDespite the limitations of this derivation, the equilibrium constant for a reaction is indeed a constant, independent of the activities of the various species involved, though it does depend on temperature as observed by the van 't Hoff equation. Adding a catalyst will affect both the forward reaction and the reverse reaction in the same way and will not have an effect on the equilibrium constant. The catalyst will speed up both reactions thereby increasing the speed at which equilibrium is reached.\nAlthough the macroscopic equilibrium concentrations are constant in time, reactions do occur at the molecular level. For example, in the case of acetic acid dissolved in water and forming acetate and hydronium ions,\na proton may hop from one molecule of acetic acid onto a water molecule and then onto an acetate anion to form another molecule of acetic acid and leaving the number of acetic acid molecules unchanged. This is an example of dynamic equilibrium. Equilibria, like the rest of thermodynamics, are statistical phenomena, averages of microscopic behavior.\nLe Ch\u00e2telier's principle (1884) predicts the behavior of an equilibrium system when changes to its reaction conditions occur. \"If a dynamic equilibrium is disturbed by changing the conditions, the position of equilibrium moves to partially reverse the change\". For example, adding more S (to the chemical reaction above) from the outside will cause an excess of products, and the system will try to counteract this by increasing the reverse reaction and pushing the equilibrium point backward (though the equilibrium constant will stay the same).\nIf mineral acid is added to the acetic acid mixture, increasing the concentration of hydronium ion, the amount of dissociation must decrease as the reaction is driven to the left in accordance with this principle. This can also be deduced from the equilibrium constant expression for the reaction:\nformula_4\nIf {H3O+} increases {CH3CO2H} must increase and must decrease. The H2O is left out, as it is the solvent and its concentration remains high and nearly constant.\nA quantitative version is given by the reaction quotient.\nJ. W. Gibbs suggested in 1873 that equilibrium is attained when the Gibbs free energy of the system is at its minimum value (assuming the reaction is carried out at a constant temperature and pressure). What this means is that the derivative of the Gibbs energy with respect to reaction coordinate (a measure of the extent of reaction that has occurred, ranging from zero for all reactants to a maximum for all products) vanishes (because dG = 0), signaling a stationary point. This derivative is called the reaction Gibbs energy (or energy change) and corresponds to the difference between the chemical potentials of reactants and products at the composition of the reaction mixture. This criterion is both necessary and sufficient. If a mixture is not at equilibrium, the liberation of the excess Gibbs energy (or Helmholtz energy at constant volume reactions) is the \"driving force\" for the composition of the mixture to change until equilibrium is reached. The equilibrium constant can be related to the standard Gibbs free energy change for the reaction by the equation\nformula_5\nwhere \"R\" is the universal gas constant and \"T\" the temperature.\nWhen the reactants are dissolved in a medium of high ionic strength the quotient of activity coefficients may be taken to be constant. In that case the concentration quotient, \"K\"c,\nformula_6\nwhere [A] is the concentration of A, etc., is independent of the analytical concentration of the reactants. For this reason, equilibrium constants for solutions are usually determined in media of high ionic strength. \"Kc\" varies with ionic strength, temperature and pressure (or volume). Likewise \"Kp\" for gases depends on partial pressure. These constants are easier to measure and encountered in high-school chemistry courses.\nThermodynamics.\nAt constant temperature and pressure, one must consider the Gibbs free energy, \"G\", while at constant temperature and volume, one must consider the Helmholtz free energy, \"A\", for the reaction; and at constant internal energy and volume, one must consider the entropy, \"S\", for the reaction.\nThe constant volume case is important in geochemistry and atmospheric chemistry where pressure variations are significant. Note that, if reactants and products were in standard state (completely pure), then there would be no reversibility and no equilibrium. Indeed, they would necessarily occupy disjoint volumes of space. The mixing of the products and reactants contributes a large entropy increase (known as entropy of mixing) to states containing equal mixture of products and reactants and gives rise to a distinctive minimum in the Gibbs energy as a function of the extent of reaction. The standard Gibbs energy change, together with the Gibbs energy of mixing, determine the equilibrium state.\nIn this article only the constant pressure case is considered. The relation between the Gibbs free energy and the equilibrium constant can be found by considering chemical potentials.\nAt constant temperature and pressure in the absence of an applied voltage, the Gibbs free energy, \"G\", for the reaction depends only on the extent of reaction: \"\u03be\" (Greek letter xi), and can only decrease according to the second law of thermodynamics. It means that the derivative of \"G\" with respect to \"\u03be\" must be negative if the reaction happens; at the equilibrium this derivative is equal to zero.\nformula_7:equilibrium\nIn order to meet the thermodynamic condition for equilibrium, the Gibbs energy must be stationary, meaning that the derivative of \"G\" with respect to the extent of reaction, \"\u03be\", must be zero. It can be shown that in this case, the sum of chemical potentials times the stoichiometric coefficients of the products is equal to the sum of those corresponding to the reactants. Therefore, the sum of the Gibbs energies of the reactants must be the equal to the sum of the Gibbs energies of the products.\nformula_8\nwhere \"\u03bc\" is in this case a partial molar Gibbs energy, a chemical potential. The chemical potential of a reagent A is a function of the activity, {A} of that reagent.\nformula_9\n(where \"\u03bc\" is the standard chemical potential).\nThe definition of the Gibbs energy equation interacts with the fundamental thermodynamic relation to produce\nformula_10.\nInserting \"dNi\"\u00a0= \"\u03bdi\u00a0d\u03be\" into the above equation gives a stoichiometric coefficient (formula_11) and a differential that denotes the reaction occurring to an infinitesimal extent (\"d\u03be\"). At constant pressure and temperature the above equations can be written as\nformula_12 \nwhich is the \"Gibbs free energy change for the reaction. This results in:\nformula_13.\nBy substituting the chemical potentials:\nformula_14,\nthe relationship becomes:\nformula_15\nformula_16:\nwhich is the standard Gibbs energy change for the reaction that can be calculated using thermodynamical tables.\nThe reaction quotient is defined as:\nformula_17\nTherefore,\nformula_18\nAt equilibrium:\nformula_19\nleading to:\nformula_20\nand\nformula_21\nObtaining the value of the standard Gibbs energy change, allows the calculation of the equilibrium constant.\nAddition of reactants or products.\nFor a reactional system at equilibrium: \"Q\"r\u00a0=\u00a0\"K\"eq; \"\u03be\"\u00a0=\u00a0\"\u03be\"eq.\nNote that activities and equilibrium constants are dimensionless numbers.\nTreatment of activity.\nThe expression for the equilibrium constant can be rewritten as the product of a concentration quotient, \"K\"c and an activity coefficient quotient, \"\u0393\".\nformula_30\n[A] is the concentration of reagent A, etc. It is possible in principle to obtain values of the activity coefficients, \u03b3. For solutions, equations such as the Debye\u2013H\u00fcckel equation or extensions such as Davies equation Specific ion interaction theory or Pitzer equations may be used.Software (below) However this is not always possible. It is common practice to assume that \"\u0393\" is a constant, and to use the concentration quotient in place of the thermodynamic equilibrium constant. It is also general practice to use the term \"equilibrium constant\" instead of the more accurate \"concentration quotient\". This practice will be followed here.\nFor reactions in the gas phase partial pressure is used in place of concentration and fugacity coefficient in place of activity coefficient. In the real world, for example, when making ammonia in industry, fugacity coefficients must be taken into account. Fugacity, \"f\", is the product of partial pressure and fugacity coefficient. The chemical potential of a species in the real gas phase is given by\nformula_31\nso the general expression defining an equilibrium constant is valid for both solution and gas phases.\nConcentration quotients.\nIn aqueous solution, equilibrium constants are usually determined in the presence of an \"inert\" electrolyte such as sodium nitrate, NaNO3, or potassium perchlorate, KClO4. The ionic strength of a solution is given by\nformula_32\nwhere \"ci\" and \"zi\" stand for the concentration and ionic charge of ion type \"i\", and the sum is taken over all the \"N\" types of charged species in solution. When the concentration of dissolved salt is much higher than the analytical concentrations of the reagents, the ions originating from the dissolved salt determine the ionic strength, and the ionic strength is effectively constant. Since activity coefficients depend on ionic strength, the activity coefficients of the species are effectively independent of concentration. Thus, the assumption that \"\u0393\" is constant is justified. The concentration quotient is a simple multiple of the equilibrium constant.\nformula_33\nHowever, \"K\"c will vary with ionic strength. If it is measured at a series of different ionic strengths, the value can be extrapolated to zero ionic strength. The concentration quotient obtained in this manner is known, paradoxically, as a thermodynamic equilibrium constant.\nBefore using a published value of an equilibrium constant in conditions of ionic strength different from the conditions used in its determination, the value should be adjustedSoftware (below).\nMetastable mixtures.\nA mixture may appear to have no tendency to change, though it is not at equilibrium. For example, a mixture of SO2 and O2 is metastable as there is a kinetic barrier to formation of the product, SO3.\n2\u00a0SO2 + O2 \u21cc 2\u00a0SO3\nThe barrier can be overcome when a catalyst is also present in the mixture as in the contact process, but the catalyst does not affect the equilibrium concentrations.\nLikewise, the formation of bicarbonate from carbon dioxide and water is very slow under normal conditions\nbut almost instantaneous in the presence of the catalytic enzyme carbonic anhydrase.\nPure substances.\nWhen pure substances (liquids or solids) are involved in equilibria their activities do not appear in the equilibrium constant because their numerical values are considered one.\nApplying the general formula for an equilibrium constant to the specific case of a dilute solution of acetic acid in water one obtains\nCH3CO2H + H2O \u21cc CH3CO2\u2212 + H3O+\nformula_34\nFor all but very concentrated solutions, the water can be considered a \"pure\" liquid, and therefore it has an activity of one. The equilibrium constant expression is therefore usually written as\nformula_35.\nA particular case is the self-ionization of water\n2\u00a0H2O \u21cc H3O+ + OH\u2212\nBecause water is the solvent, and has an activity of one, the self-ionization constant of water is defined as\nformula_36\nIt is perfectly legitimate to write [H+] for the hydronium ion concentration, since the state of solvation of the proton is constant (in dilute solutions) and so does not affect the equilibrium concentrations. \"K\"w varies with variation in ionic strength and/or temperature.\nThe concentrations of H+ and OH\u2212 are not independent quantities. Most commonly [OH\u2212] is replaced by \"K\"w[H+]\u22121 in equilibrium constant expressions which would otherwise include hydroxide ion.\nSolids also do not appear in the equilibrium constant expression, if they are considered to be pure and thus their activities taken to be one. An example is the Boudouard reaction:\n2\u00a0CO \u21cc CO2 + C\nfor which the equation (without solid carbon) is written as:\nformula_37\nMultiple equilibria.\nConsider the case of a dibasic acid H2A. When dissolved in water, the mixture will contain H2A, HA\u2212 and A2\u2212. This equilibrium can be split into two steps in each of which one proton is liberated.\nformula_38\n\"K\"1 and\" K\"2 are examples of \"stepwise\" equilibrium constants. The \"overall\" equilibrium constant, \"\u03b2\"D, is product of the stepwise constants.\n&lt;chem&gt;{H2A} &lt;=&gt; {A^{2-}} + {2H+}&lt;/chem&gt;:formula_39\nNote that these constants are dissociation constants because the products on the right hand side of the equilibrium expression are dissociation products. In many systems, it is preferable to use association constants.\nformula_40\n\"\u03b2\"1 and \"\u03b2\"2 are examples of association constants. Clearly \"\u03b2\"1\u00a0=\u00a0 and \"\u03b2\"2\u00a0=\u00a0; log\u00a0\"\u03b2\"1\u00a0=\u00a0p\"K\"2 and log\u00a0\"\u03b2\"2\u00a0=\u00a0p\"K\"2\u00a0+\u00a0p\"K\"1\nFor multiple equilibrium systems, also see: theory of Response reactions.\nEffect of temperature.\nThe effect of changing temperature on an equilibrium constant is given by the van 't Hoff equation\nformula_41\nThus, for exothermic reactions (\u0394\"H\" is negative), \"K\" decreases with an increase in temperature, but, for endothermic reactions, (\u0394H is positive) \"K\" increases with an increase temperature. An alternative formulation is\nformula_42\nAt first sight this appears to offer a means of obtaining the standard molar enthalpy of the reaction by studying the variation of \"K\" with temperature. In practice, however, the method is unreliable because error propagation almost always gives very large errors on the values calculated in this way.\nEffect of electric and magnetic fields.\nThe effect of electric field on equilibrium has been studied by Manfred Eigen among others.\nTypes of equilibrium.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nHaber\u2013Bosch process\nEquilibrium can be broadly classified as heterogeneous and homogeneous equilibrium. Homogeneous equilibrium consists of reactants and products belonging in the same phase whereas heterogeneous equilibrium comes into play for reactants and products in different phases. \nIn these applications, terms such as stability constant, formation constant, binding constant, affinity constant, association constant and dissociation constant are used. In biochemistry, it is common to give units for binding constants, which serve to define the concentration units used when the constant's value was determined.\nComposition of a mixture.\nWhen the only equilibrium is that of the formation of a 1:1 adduct as the composition of a mixture, there are many ways that the composition of a mixture can be calculated. For example, see ICE table for a traditional method of calculating the pH of a solution of a weak acid.\nThere are three approaches to the general calculation of the composition of a mixture at equilibrium.\nMass-balance equations.\nIn general, the calculations are rather complicated or complex. For instance, in the case of a dibasic acid, H2A dissolved in water the two reactants can be specified as the conjugate base, A2\u2212, and the proton, H+. The following equations of mass-balance could apply equally well to a base such as 1,2-diaminoethane, in which case the base itself is designated as the reactant A:\nformula_43\nformula_44\nwith TA the total concentration of species A. Note that it is customary to omit the ionic charges when writing and using these equations.\nWhen the equilibrium constants are known and the total concentrations are specified there are two equations in two unknown \"free concentrations\" [A] and [H]. This follows from the fact that [HA]\u00a0=\u00a0\"\u03b2\"1[A][H], [H2A]\u00a0=\u00a0\"\u03b2\"2[A][H]2 and [OH]\u00a0=\u00a0\"K\"w[H]\u22121\nformula_45\nformula_46\nso the concentrations of the \"complexes\" are calculated from the free concentrations and the equilibrium constants.\nGeneral expressions applicable to all systems with two reagents, A and B would be\nformula_47\nformula_48\nIt is easy to see how this can be extended to three or more reagents.\nPolybasic acids.\nThe composition of solutions containing reactants A and H is easy to calculate as a function of p[H]. When [H] is known, the free concentration [A] is calculated from the mass-balance equation in A.\nThe diagram alongside, shows an example of the hydrolysis of the aluminium Lewis acid Al3+(aq) shows the species concentrations for a 5\u00a0\u00d7\u00a010\u22126\u00a0M solution of an aluminium salt as a function of pH. Each concentration is shown as a percentage of the total aluminium.\nSolution and precipitation.\nThe diagram above illustrates the point that a precipitate that is not one of the main species in the solution equilibrium may be formed. At pH just below 5.5 the main species present in a 5\u00a0\u03bcM solution of Al3+ are aluminium hydroxides Al(OH)2+, and , but on raising the pH Al(OH)3 precipitates from the solution. This occurs because Al(OH)3 has a very large lattice energy. As the pH rises more and more Al(OH)3 comes out of solution. This is an example of Le Ch\u00e2telier's principle in action: Increasing the concentration of the hydroxide ion causes more aluminium hydroxide to precipitate, which removes hydroxide from the solution. When the hydroxide concentration becomes sufficiently high the soluble aluminate, , is formed.\nAnother common instance where precipitation occurs is when a metal cation interacts with an anionic ligand to form an electrically neutral complex. If the complex is hydrophobic, it will precipitate out of water. This occurs with the nickel ion Ni2+ and dimethylglyoxime, (dmgH2): in this case the lattice energy of the solid is not particularly large, but it greatly exceeds the energy of solvation of the molecule Ni(dmgH)2.\nMinimization of Gibbs energy.\nAt equilibrium, at a specified temperature and pressure, and with no external forces, the Gibbs free energy \"G\" is at a minimum:\nformula_49\nwhere \u03bcj is the chemical potential of molecular species \"j\", and \"Nj\" is the amount of molecular species \"j\". It may be expressed in terms of thermodynamic activity as:\nformula_50\nwhere formula_51 is the chemical potential in the standard state, \"R\" is the gas constant \"T\" is the absolute temperature, and \"Aj\" is the activity.\nFor a closed system, no particles may enter or leave, although they may combine in various ways. The total number of atoms of each element will remain constant. This means that the minimization above must be subjected to the constraints:\nformula_52\nwhere \"aij\" is the number of atoms of element \"i\" in molecule \"j\" and \"b\" is the total number of atoms of element \"i\", which is a constant, since the system is closed. If there are a total of \"k\" types of atoms in the system, then there will be \"k\" such equations. If ions are involved, an additional row is added to the aij matrix specifying the respective charge on each molecule which will sum to zero.\nThis is a standard problem in optimisation, known as constrained minimisation. The most common method of solving it is using the method of Lagrange multipliers (although other methods may be used).\nDefine:\nformula_53\nwhere the \"\u03bbi\" are the Lagrange multipliers, one for each element. This allows each of the \"Nj\" and \"\u03bbj\" to be treated independently, and it can be shown using the tools of multivariate calculus that the equilibrium condition is given by\nformula_54\nformula_55\n(For proof see Lagrange multipliers.) This is a set of (\"m\"\u00a0+\u00a0\"k\") equations in (\"m\"\u00a0+\u00a0\"k\") unknowns (the \"Nj\" and the \"\u03bbi\") and may, therefore, be solved for the equilibrium concentrations \"Nj\" as long as the chemical activities are known as functions of the concentrations at the given temperature and pressure. (In the ideal case, activities are proportional to concentrations.) (See Thermodynamic databases for pure substances.) Note that the second equation is just the initial constraints for minimization.\nThis method of calculating equilibrium chemical concentrations is useful for systems with a large number of different molecules. The use of \"k\" atomic element conservation equations for the mass constraint is straightforward, and replaces the use of the stoichiometric coefficient equations. The results are consistent with those specified by chemical equations. For example, if equilibrium is specified by a single chemical equation:,\nformula_56\nwhere \u03bdj is the stoichiometric coefficient for the \"j\" th molecule (negative for reactants, positive for products) and \"Rj\" is the symbol for the \"j\" th molecule, a properly balanced equation will obey:\nformula_57\nMultiplying the first equilibrium condition by \u03bdj and using the above equation yields:\nformula_58\nAs above, defining \u0394G\nformula_59\nwhere \"Kc\" is the equilibrium constant, and \u0394G will be zero at equilibrium.\nAnalogous procedures exist for the minimization of other thermodynamic potentials.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5308", "revid": "13974845", "url": "https://en.wikipedia.org/wiki?curid=5308", "title": "Combination", "text": "Selection of items from a set\nIn mathematics, a combination is a selection of items from a set that has distinct members, such that the order of selection does not matter (unlike permutations). For example, given three fruits, say an apple, an orange and a pear, there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. More formally, a \"k\"-combination of a set \"S\" is a subset of \"k\" distinct elements of \"S\". So, two combinations are identical if and only if each combination has the same members. (The arrangement of the members in each set does not matter.) If the set has \"n\" elements, the number of \"k\"-combinations, denoted by formula_1 or formula_2, is equal to the binomial coefficient\nformula_3\nwhich can be written using factorials as formula_4 whenever formula_5, and which is zero when formula_6. This formula can be derived from the fact that each \"k\"-combination of a set \"S\" of \"n\" members has formula_7 permutations so formula_8 or formula_9. The set of all \"k\"-combinations of a set \"S\" is often denoted by formula_10.\nA combination is a combination of \"n\" things taken \"k\" at a time \"without repetition\". To refer to combinations in which repetition is allowed, the terms \"k\"-combination with repetition, \"k\"-multiset, or \"k\"-selection, are often used. If, in the above example, it were possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.\nAlthough the set of three fruits was small enough to write a complete list of combinations, this becomes impractical as the size of the set increases. For example, a poker hand can be described as a 5-combination (\"k\"\u00a0=\u00a05) of cards from a 52 card deck (\"n\"\u00a0=\u00a052). The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. There are 2,598,960 such combinations, and the chance of drawing any one hand at random is\u00a01\u00a0/\u00a02,598,960.\nNumber of \"k\"-combinations.\nThe number of \"k\"-combinations from a given set \"S\" of \"n\" elements is often denoted in elementary combinatorics texts by formula_1, or by a variation such as formula_2, formula_13, formula_14, formula_15 or even formula_16 (the last form is standard in French, Romanian, Russian, Chinese and Polish texts). The same number however occurs in many other mathematical contexts, where it is denoted by formula_17 (often read as \"\"n\" choose \"k\"\"); notably it occurs as a coefficient in the binomial formula, hence its name binomial coefficient. One can define formula_17 for all natural numbers \"k\" at once by the relation\nformula_19\nfrom which it is clear that\nformula_20\nand further,\nformula_21\nfor \"k\"\u00a0&gt;\u00a0\"n\".\nTo see that these coefficients count \"k\"-combinations from \"S\", one can first consider a collection of \"n\" distinct variables \"X\"\"s\" labeled by the elements \"s\" of \"S\", and expand the product over all elements of\u00a0\"S\":\nformula_22\nit has 2\"n\" distinct terms corresponding to all the subsets of \"S\", each subset giving the product of the corresponding variables \"X\"\"s\". Now setting all of the \"X\"\"s\" equal to the unlabeled variable \"X\", so that the product becomes (1 + \"X\")\"n\", the term for each \"k\"-combination from \"S\" becomes \"X\"\"k\", so that the coefficient of that power in the result equals the number of such \"k\"-combinations.\nBinomial coefficients can be computed explicitly in various ways. To get all of them for the expansions up to (1 + \"X\")\"n\", one can use (in addition to the basic cases already given) the recursion relation\nformula_23\nfor 0 &lt; \"k\" &lt; \"n\", which follows from (1 + \"X\")\"n\"=(1 + \"X\")\"n\" \u2212 1(1 + \"X\"); this leads to the construction of Pascal's triangle.\nFor determining an individual binomial coefficient, it is more practical to use the formula\nformula_24\nThe numerator gives the number of \"k\"-permutations of \"n\", i.e., of sequences of \"k\" distinct elements of \"S\", while the denominator gives the number of such \"k\"-permutations that give the same \"k\"-combination when the order is ignored.\nWhen \"k\" exceeds \"n\"/2, the above formula contains factors common to the numerator and the denominator, and canceling them out gives the relation\nformula_25\nfor 0 \u2264 \"k\" \u2264 \"n\". This expresses a symmetry that is evident from the binomial formula, and can also be understood in terms of \"k\"-combinations by taking the complement of such a combination, which is an (\"n\" \u2212 \"k\")-combination.\nFinally there is a formula which exhibits this symmetry directly, and has the merit of being easy to remember:\nformula_26\nwhere \"n\"! denotes the factorial of \"n\". It is obtained from the previous formula by multiplying denominator and numerator by (\"n\" \u2212 \"k\")!, so it is certainly computationally less efficient than that formula.\nThe last formula can be understood directly, by considering the \"n\"! permutations of all the elements of \"S\". Each such permutation gives a \"k\"-combination by selecting its first \"k\" elements. There are many duplicate selections: any combined permutation of the first \"k\" elements among each other, and of the final (\"n\"\u00a0\u2212\u00a0\"k\") elements among each other produces the same combination; this explains the division in the formula.\nFrom the above formulas follow relations between adjacent numbers in Pascal's triangle in all three directions:\nformula_27\nTogether with the basic cases formula_28, these allow successive computation of respectively all numbers of combinations from the same set (a row in Pascal's triangle), of \"k\"-combinations of sets of growing sizes, and of combinations with a complement of fixed size \"n\" \u2212 \"k\".\nExample of counting combinations.\nAs a specific example, one can compute the number of five-card hands possible from a standard fifty-two card deck as:\nformula_29\nAlternatively one may use the formula in terms of factorials and cancel the factors in the numerator against parts of the factors in the denominator, after which only multiplication of the remaining factors is required:\nformula_30\nAnother alternative computation, equivalent to the first, is based on writing\nformula_31\nwhich gives\nformula_32\nWhen evaluated in the following order, 52 \u00f7 1 \u00d7 51 \u00f7 2 \u00d7 50 \u00f7 3 \u00d7 49 \u00f7 4 \u00d7 48 \u00f7 5, this can be computed using only integer arithmetic. The reason is that when each division occurs, the intermediate result that is produced is itself a binomial coefficient, so no remainders ever occur.\nUsing the symmetric formula in terms of factorials without performing simplifications gives a rather extensive calculation:\nformula_33\nEnumerating \"k\"-combinations.\nOne can enumerate all \"k\"-combinations of a given set \"S\" of \"n\" elements in some fixed order, which establishes a bijection from an interval of formula_17 integers with the set of those \"k\"-combinations. Assuming \"S\" is itself ordered, for instance \"S\" = { 1, 2, ..., \"n\" }, there are two natural possibilities for ordering its \"k\"-combinations: by comparing their smallest elements first (as in the illustrations above) or by comparing their largest elements first. The latter option has the advantage that adding a new largest element to \"S\" will not change the initial part of the enumeration, but just add the new \"k\"-combinations of the larger set after the previous ones. Repeating this process, the enumeration can be extended indefinitely with \"k\"-combinations of ever larger sets. If moreover the intervals of the integers are taken to start at\u00a00, then the \"k\"-combination at a given place \"i\" in the enumeration can be computed easily from \"i\", and the bijection so obtained is known as the combinatorial number system. It is also known as \"rank\"/\"ranking\" and \"unranking\" in computational mathematics.\nThere are many ways to enumerate \"k\" combinations. One way is to visit all the binary numbers less than 2\"n\". Choose those numbers having \"k\" nonzero bits, although this is very inefficient even for small \"n\" (e.g. \"n\" = 20 would require visiting about one million numbers while the maximum number of allowed \"k\" combinations is about 186 thousand for \"k\" = 10). The positions of these 1 bits in such a number is a specific \"k\"-combination of the set { 1, ..., \"n\" }. Another simple, faster way is to track \"k\" index numbers of the elements selected, starting with {0 .. \"k\"\u22121} (zero-based) or {1 .. \"k\"} (one-based) as the first allowed \"k\"-combination and then repeatedly moving to the next allowed \"k\"-combination by incrementing the last index number if it is lower than \"n\"-1 (zero-based) or \"n\" (one-based) or the last index number \"x\" that is less than the index number following it minus one if such an index exists and resetting the index numbers after \"x\" to {\"x\"+1, \"x\"+2, ...}.\nNumber of combinations with repetition.\nA \"k\"-combination with repetitions, or \"k\"-multicombination, or multisubset of size \"k\" from a set \"S\" of size \"n\" is given by a set of \"k\" not necessarily distinct elements of \"S\", where order is not taken into account: two sequences define the same multiset if one can be obtained from the other by permuting the terms. In other words, it is a sample of \"k\" elements from a set of \"n\" elements allowing for duplicates (i.e., with replacement) but disregarding different orderings (e.g. {2,1,2} = {1,2,2}). Associate an index to each element of \"S\" and think of the elements of \"S\" as \"types\" of objects, then we can let formula_35 denote the number of elements of type \"i\" in a multisubset. The number of multisubsets of size \"k\" is then the number of nonnegative integer (so allowing zero) solutions of the Diophantine equation:\nformula_36\nIf \"S\" has \"n\" elements, the number of such \"k\"-multisubsets is denoted by\nformula_37\na notation that is analogous to the binomial coefficient which counts \"k\"-subsets. This expression, \"n\" multichoose \"k\", can also be given in terms of binomial coefficients:\nformula_38\nThis relationship can be easily proved using a representation known as stars and bars. \n&lt;templatestyles src=\"Template:Hidden begin/styles.css\"/&gt;Proof\nA solution of the above Diophantine equation can be represented by formula_39 \"stars\", a separator (a \"bar\"), then formula_40 more stars, another separator, and so on. The total number of stars in this representation is \"k\" and the number of bars is \"n\" - 1 (since a separation into n parts needs n-1 separators). Thus, a string of \"k\" + \"n\" - 1 (or \"n\" + \"k\" - 1) symbols (stars and bars) corresponds to a solution if there are \"k\" stars in the string. Any solution can be represented by choosing \"k\" out of \"k\" + \"n\" \u2212 1 positions to place stars and filling the remaining positions with bars. For example, the solution formula_41 of the equation formula_42 (\"n\" = 4 and \"k\" = 10) can be represented by\nformula_43\nThe number of such strings is the number of ways to place 10 stars in 13 positions, formula_44 which is the number of 10-multisubsets of a set with 4 elements.\nAs with binomial coefficients, there are several relationships between these multichoose expressions. For example, for formula_45,\nformula_46\nThis identity follows from interchanging the stars and bars in the above representation.\nExample of counting multisubsets.\nFor example, if you have four types of donuts (\"n\"\u00a0=\u00a04) on a menu to choose from and you want three donuts (\"k\"\u00a0=\u00a03), the number of ways to choose the donuts with repetition can be calculated as\nformula_47\nThis result can be verified by listing all the 3-multisubsets of the set \"S\" = {1,2,3,4}. This is displayed in the following table. The second column lists the donuts you actually chose, the third column shows the nonnegative integer solutions formula_48 of the equation formula_49 and the last column gives the stars and bars representation of the solutions.\nNumber of \"k\"-combinations for all \"k\".\nThe number of \"k\"-combinations for all \"k\" is the number of subsets of a set of \"n\" elements. There are several ways to see that this number is 2\"n\". In terms of combinations, formula_50, which is the sum of the \"n\"th row (counting from 0) of the binomial coefficients in Pascal's triangle. These combinations (subsets) are enumerated by the 1 digits of the set of base 2 numbers counting from 0 to 2\"n\"\u00a0\u2212\u00a01, where each digit position is an item from the set of \"n\".\nGiven 3 cards numbered 1 to 3, there are 8 distinct combinations (subsets), including the empty set:\nformula_51\nRepresenting these subsets (in the same order) as base 2 numerals:\nProbability: sampling a random combination.\nThere are various algorithms to pick out a random combination from a given set or list. Rejection sampling is extremely slow for large sample sizes. One way to select a \"k\"-combination efficiently from a population of size \"n\" is to iterate across each element of the population, and at each step pick that element with a dynamically changing probability of formula_52 (see Reservoir sampling). Another is to pick a random non-negative integer less than formula_53 and convert it into a combination using the combinatorial number system.\nNumber of ways to put objects into bins.\nA combination can also be thought of as a selection of \"two\" sets of items: those that go into the chosen bin and those that go into the unchosen bin. This can be generalized to any number of bins with the constraint that every item must go to exactly one bin. The number of ways to put objects into bins is given by the multinomial coefficient\nformula_54\nwhere \"n\" is the number of items, \"m\" is the number of bins, and formula_55 is the number of items that go into bin \"i\".\nOne way to see why this equation holds is to first number the objects arbitrarily from \"1\" to \"n\" and put the objects with numbers formula_56 into the first bin in order, the objects with numbers formula_57 into the second bin in order, and so on. There are formula_58 distinct numberings, but many of them are equivalent, because only the set of items in a bin matters, not their order in it. Every combined permutation of each bins' contents produces an equivalent way of putting items into bins. As a result, every equivalence class consists of formula_59 distinct numberings, and the number of equivalence classes is formula_60.\nThe binomial coefficient is the special case where \"k\" items go into the chosen bin and the remaining formula_61 items go into the unchosen bin:\nformula_62\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5309", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=5309", "title": "Software", "text": "Non-tangible executable component of a computer\nSoftware is a set of computer programs and associated documentation and data. This is in contrast to hardware, from which the system is built and which actually performs the work.\nAt the lowest programming level, executable code consists of machine language instructions supported by an individual processor\u2014typically a central processing unit (CPU) or a graphics processing unit (GPU). Machine language consists of groups of binary values signifying processor instructions that change the state of the computer from its preceding state. For example, an instruction may change the value stored in a particular storage location in the computer\u2014an effect that is not directly observable to the user. An instruction may also invoke one of many input or output operations, for example, displaying some text on a computer screen, causing state changes that should be visible to the user. The processor executes the instructions in the order they are provided, unless it is instructed to \"jump\" to a different instruction or is interrupted by the operating system. As of 2023[ [update]], most personal computers, smartphone devices, and servers have processors with multiple execution units, or multiple processors performing computation together, so computing has become a much more concurrent activity than in the past.\nThe majority of software is written in high-level programming languages. They are easier and more efficient for programmers because they are closer to natural languages than machine languages. High-level languages are translated into machine language using a compiler, an interpreter, or a combination of the two. Software may also be written in a low-level assembly language that has a strong correspondence to the computer's machine language instructions and is translated into machine language using an assembler.\nHistory.\nAn algorithm for what would have been the first piece of software was written by Ada Lovelace in the 19th century, for the planned Analytical Engine. She created proofs to show how the engine would calculate Bernoulli numbers. Because of the proofs and the algorithm, she is considered the first computer programmer.\nThe first theory about software, prior to the creation of computers as we know them today, was proposed by Alan Turing in his 1936 essay, \"On Computable Numbers, with an Application to the Entscheidungsproblem\" (decision problem). This eventually led to the creation of the academic fields of computer science and software engineering; both fields study software and its creation. Computer science is the theoretical study of computer and software (Turing's essay is an example of computer science), whereas software engineering is the application of engineering principles to development of software.\nIn 2000, Fred Shapiro, a librarian at the Yale Law School, published a letter revealing that John Wilder Tukey's 1958 paper \"The Teaching of Concrete Mathematics\" contained the earliest known usage of the term \"software\" found in a search of JSTOR's electronic archives, predating the \"Oxford English Dictionary\"'s citation by two years. This led many to credit Tukey with coining the term, particularly in obituaries published that same year, although Tukey never claimed credit for any such coinage. In 1995, Paul Niquette claimed he had originally coined the term in October 1953, although he could not find any documents supporting his claim. The earliest known publication of the term \"software\" in an engineering context was in August 1953 by Richard R. Carhart, in a Rand Corporation Research Memorandum.\nTypes.\nOn virtually all computer platforms, software can be grouped into a few broad categories.\nPurpose, or domain of use.\nBased on the goal, computer software can be divided into:\nProgramming tools.\nProgramming tools are also software in the form of programs or applications that developers use to create, debug, maintain, or otherwise support software.\nSoftware is written in one or more programming languages; there are many programming languages in existence, and each has at least one implementation, each of which consists of its own set of programming tools. These tools may be relatively self-contained programs such as compilers, debuggers, interpreters, linkers, and text editors, that can be combined to accomplish a task; or they may form an integrated development environment (IDE), which combines much or all of the functionality of such self-contained tools. IDEs may do this by either invoking the relevant individual tools or by re-implementing their functionality in a new way. An IDE can make it easier to do specific tasks, such as searching in files in a particular project. Many programming language implementations provide the option of using both individual tools or an IDE.\nTopics.\nArchitecture.\nPeople who use modern general purpose computers (as opposed to embedded systems, analog computers and supercomputers) usually see three layers of software performing a variety of tasks: platform, application, and user software.\nExecution.\nComputer software has to be \"loaded\" into the computer's storage (such as the hard drive or memory). Once the software has loaded, the computer is able to \"execute\" the software. This involves passing instructions from the application software, through the system software, to the hardware which ultimately receives the instruction as machine code. Each instruction causes the computer to carry out an operation\u2014moving data, carrying out a computation, or altering the control flow of instructions.\nData movement is typically from one place in memory to another. Sometimes it involves moving data between memory and registers which enable high-speed data access in the CPU. Moving data, especially large amounts of it, can be costly; this is sometimes avoided by using \"pointers\" to data instead. Computations include simple operations such as incrementing the value of a variable data element. More complex computations may involve many operations and data elements together.\nQuality and reliability.\nSoftware quality is very important, especially for commercial and system software. If software is faulty, it can delete a person's work, crash the computer and do other unexpected things. Faults and errors are called \"bugs\" which are often discovered during alpha and beta testing. Software is often also a victim to what is known as software aging, the progressive performance degradation resulting from a combination of unseen bugs.\nMany bugs are discovered and fixed through software testing. However, software testing rarely\u2014if ever\u2014eliminates every bug; some programmers say that \"every program has at least one more bug\" (Lubarsky's Law). In the waterfall method of software development, separate testing teams are typically employed, but in newer approaches, collectively termed agile software development, developers often do all their own testing, and demonstrate the software to users/clients regularly to obtain feedback. Software can be tested through unit testing, regression testing and other methods, which are done manually, or most commonly, automatically, since the amount of code to be tested can be large. Programs containing command software enable hardware engineering and system operations to function much easier together.\nLicense.\nThe software's license gives the user the right to use the software in the licensed environment, and in the case of free software licenses, also grants other rights such as the right to make copies.\nProprietary software can be divided into two types:\nOpen-source software comes with a free software license, granting the recipient the rights to modify and redistribute the software.\nPatents.\nSoftware patents, like other types of patents, are theoretically supposed to give an inventor an exclusive, time-limited license for a \"detailed idea (e.g. an algorithm) on how to implement\" a piece of software, or a component of a piece of software. Ideas for useful things that software could \"do\", and user \"requirements\", are not supposed to be patentable, and concrete implementations (i.e. the actual software packages implementing the patent) are not supposed to be patentable either\u2014the latter are already covered by copyright, generally automatically. So software patents are supposed to cover the middle area, between requirements and concrete implementation. In some countries, a requirement for the claimed invention to have an effect on the physical world may also be part of the requirements for a software patent to be held valid\u2014although since \"all\" useful software has effects on the physical world, this requirement may be open to debate. Meanwhile, American copyright law was applied to various aspects of the writing of the software code.\nSoftware patents are controversial in the software industry with many people holding different views about them. One of the sources of controversy is that the aforementioned split between initial ideas and patent does not seem to be honored in practice by patent lawyers\u2014for example the patent for aspect-oriented programming (AOP), which purported to claim rights over \"any\" programming tool implementing the idea of AOP, howsoever implemented. Another source of controversy is the effect on innovation, with many distinguished experts and companies arguing that software is such a fast-moving field that software patents merely create vast additional litigation costs and risks, and actually retard innovation. In the case of debates about software patents outside the United States, the argument has been made that large American corporations and patent lawyers are likely to be the primary beneficiaries of allowing or continue to allow software patents.\nDesign and implementation.\nDesign and implementation of software vary depending on the complexity of the software. For instance, the design and creation of Microsoft Word took much more time than designing and developing Microsoft Notepad because the former has much more basic functionality.\nSoftware is usually developed in integrated development environments (IDE) like Eclipse, IntelliJ and Microsoft Visual Studio that can simplify the process and compile the software. As noted in a different section, software is usually created on top of existing software and the application programming interface (API) that the underlying software provides like GTK+, JavaBeans or Swing. Libraries (APIs) can be categorized by their purpose. For instance, the Spring Framework is used for implementing enterprise applications, the Windows Forms library is used for designing graphical user interface (GUI) applications like Microsoft Word, and Windows Communication Foundation is used for designing web services. When a program is designed, it relies upon the API. For instance, a Microsoft Windows desktop application might call API functions in the .NET Windows Forms library like \"Form1.Close()\" and \"Form1.Show()\" to close or open the application. Without these APIs, the programmer needs to write these functionalities entirely themselves. Companies like Oracle and Microsoft provide their own APIs so that many applications are written using their software libraries that usually have numerous APIs in them.\nData structures such as hash tables, arrays, and binary trees, and algorithms such as quicksort, can be useful for creating software.\nComputer software has special economic characteristics that make its design, creation, and distribution different from most other economic goods.\nA person who creates software is called a programmer, software engineer or software developer, terms that all have a similar meaning. More informal terms for programmer also exist such as \"coder\" and \"hacker\"\u00a0\u2013 although use of the latter word may cause confusion, because it is more often used to mean someone who illegally breaks into computer systems.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5310", "revid": "17300358", "url": "https://en.wikipedia.org/wiki?curid=5310", "title": "Personal computer hardware", "text": ""}
{"id": "5311", "revid": "42335939", "url": "https://en.wikipedia.org/wiki?curid=5311", "title": "Computer programming", "text": "Process to create executable computer programs\nComputer programming is the process of performing particular computations (or more generally, accomplishing specific computing results), usually by designing and building executable computer programs. Programming involves tasks such as analysis, generating algorithms, profiling algorithms' accuracy and resource consumption, and the implementation of algorithms (usually in a particular programming language, commonly referred to as coding). The source code of a program is written in one or more languages that are intelligible to programmers, rather than machine code, which is directly executed by the central processing unit. The purpose of programming is to find a sequence of instructions that will automate the performance of a task (which can be as complex as an operating system) on a computer, often for solving a given problem. Proficient programming thus usually requires expertise in several different subjects, including knowledge of the application domain, specialized algorithms, and formal logic.\nTasks accompanying and related to programming include testing, debugging, source code maintenance, implementation of build systems, and management of derived artifacts, such as the machine code of computer programs. However, while these might be considered part of the programming process, often the term software development is more likely used for this larger overall process \u2013 whereas the terms \"programming\", \"implementation\", and \"coding\" tend to be focused on the actual writing of code. Relatedly, software engineering combines engineering techniques and principles with software development. Also, those involved with software development may at times engage in \"reverse engineering\", which is the practice of seeking to understand an existing program so as to re-implement its function in some way.\nHistory.\nProgrammable devices have existed for centuries. As early as the 9th century, a programmable music sequencer was invented by the Persian Banu Musa brothers, who described an automated mechanical flute player in the \"Book of Ingenious Devices\". In 1206, the Arab engineer Al-Jazari invented a programmable drum machine where a musical mechanical automaton could be made to play different rhythms and drum patterns, via pegs and cams. In 1801, the Jacquard loom could produce entirely different weaves by changing the \"program\" \u2013 a series of pasteboard cards with holes punched in them.\nCode-breaking algorithms have also existed for centuries. In the 9th century, the Arab mathematician Al-Kindi described a cryptographic algorithm for deciphering encrypted code, in \"A Manuscript on Deciphering Cryptographic Messages\". He gave the first description of cryptanalysis by frequency analysis, the earliest code-breaking algorithm.\nThe first computer program is generally dated to 1843, when mathematician Ada Lovelace published an algorithm to calculate a sequence of Bernoulli numbers, intended to be carried out by Charles Babbage's Analytical Engine. However, Charles Babbage had already written his first program for the Analytical Engine in 1837.\nIn the 1880s, Herman Hollerith invented the concept of storing \"data\" in machine-readable form. Later a control panel (plug board) added to his 1906 Type I Tabulator allowed it to be programmed for different jobs, and by the late 1940s, unit record equipment such as the IBM 602 and IBM 604, were programmed by control panels in a similar way, as were the first electronic computers. However, with the concept of the stored-program computer introduced in 1949, both programs and data were stored and manipulated in the same way in computer memory.\nMachine language.\nMachine code was the language of early programs, written in the instruction set of the particular machine, often in binary notation. Assembly languages were soon developed that let the programmer specify instruction in a text format (e.g., ADD X, TOTAL), with abbreviations for each operation code and meaningful names for specifying addresses. However, because an assembly language is little more than a different notation for a machine language, two machines with different instruction sets also have different assembly languages.\nCompiler languages.\nHigh-level languages made the process of developing a program simpler and more understandable, and less bound to the underlying hardware. \nThe first compiler related tool, the A-0 System, was developed in 1952 by Grace Hopper, who also coined the term 'compiler'. FORTRAN, the first widely used high-level language to have a functional implementation, came out in 1957, and many other languages were soon developed\u2014in particular, COBOL aimed at commercial data processing, and Lisp for computer research.\nThese compiled languages allow the programmer to write programs in terms that are syntactically richer, and more capable of abstracting the code, making it easy to target varying machine instruction sets via compilation declarations and heuristics. Compilers harnessed the power of computers to make programming easier by allowing programmers to specify calculations by entering a formula using infix notation.\nSource code entry.\nPrograms were mostly entered using punched cards or paper tape. By the late 1960s, data storage devices and computer terminals became inexpensive enough that programs could be created by typing directly into the computers. Text editors were also developed that allowed changes and corrections to be made much more easily than with punched cards.\nModern programming.\nQuality requirements.\nWhatever the approach to development may be, the final program must satisfy some fundamental properties. The following properties are among the most important:\nReadability of source code.\nIn computer programming, readability refers to the ease with which a human reader can comprehend the purpose, control flow, and operation of source code. It affects the aspects of quality above, including portability, usability and most importantly maintainability.\nReadability is important because programmers spend the majority of their time reading, trying to understand, reusing and modifying existing source code, rather than writing new source code. Unreadable code often leads to bugs, inefficiencies, and duplicated code. A study found that a few simple readability transformations made code shorter and drastically reduced the time to understand it.\nFollowing a consistent programming style often helps readability. However, readability is more than just programming style. Many factors, having little or nothing to do with the ability of the computer to efficiently compile and execute the code, contribute to readability. Some of these factors include:\nThe presentation aspects of this (such as indents, line breaks, color highlighting, and so on) are often handled by the source code editor, but the content aspects reflect the programmer's talent and skills.\nVarious visual programming languages have also been developed with the intent to resolve readability concerns by adopting non-traditional approaches to code structure and display. Integrated development environments (IDEs) aim to integrate all such help. Techniques like Code refactoring can enhance readability.\nAlgorithmic complexity.\nThe academic field and the engineering practice of computer programming are both largely concerned with discovering and implementing the most efficient algorithms for a given class of problems. For this purpose, algorithms are classified into \"orders\" using so-called Big O notation, which expresses resource use, such as execution time or memory consumption, in terms of the size of an input. Expert programmers are familiar with a variety of well-established algorithms and their respective complexities and use this knowledge to choose algorithms that are best suited to the circumstances.\nMethodologies.\nThe first step in most formal software development processes is requirements analysis, followed by testing to determine value modeling, implementation, and failure elimination (debugging). There exist a lot of different approaches for each of those tasks. One approach popular for requirements analysis is Use Case analysis. Many programmers use forms of Agile software development where the various stages of formal software development are more integrated together into short cycles that take a few weeks rather than years. There are many approaches to the Software development process.\nPopular modeling techniques include Object-Oriented Analysis and Design (OOAD) and Model-Driven Architecture (MDA). The Unified Modeling Language (UML) is a notation used for both the OOAD and MDA.\nA similar technique used for database design is Entity-Relationship Modeling (ER Modeling).\nImplementation techniques include imperative languages (object-oriented or procedural), functional languages, and logic languages.\nMeasuring language usage.\nIt is very difficult to determine what are the most popular modern programming languages. Methods of measuring programming language popularity include: counting the number of job advertisements that mention the language, the number of books sold and courses teaching the language (this overestimates the importance of newer languages), and estimates of the number of existing lines of code written in the language (this underestimates the number of users of business languages such as COBOL).\nSome languages are very popular for particular kinds of applications, while some languages are regularly used to write many different kinds of applications. For example, COBOL is still strong in corporate data centers often on large mainframe computers, Fortran in engineering applications, scripting languages in Web development, and C in embedded software. Many applications use a mix of several languages in their construction and use. New languages are generally designed around the syntax of a prior language with new functionality added, (for example C++ adds object-orientation to C, and Java adds memory management and bytecode to C++, but as a result, loses efficiency and the ability for low-level manipulation).\nDebugging.\nDebugging is a very important task in the software development process since having defects in a program can have significant consequences for its users. Some languages are more prone to some kinds of faults because their specification does not require compilers to perform as much checking as other languages. Use of a static code analysis tool can help detect some possible problems. Normally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, when a bug in a compiler can make it crash when parsing some large source file, a simplification of the test case that results in only few lines from the original source file can be sufficient to reproduce the same crash. Trial-and-error/divide-and-conquer is needed: the programmer will try to remove some parts of the original test case and check if the problem still exists. When debugging the problem in a GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear. Scripting and breakpointing is also part of this process.\nDebugging is often done with IDEs. Standalone debuggers like GDB are also used, and these often provide less of a visual environment, usually using a command line. Some text editors such as Emacs allow GDB to be invoked through them, to provide a visual environment.\nProgramming languages.\nDifferent programming languages support different styles of programming (called \"programming paradigms\"). The choice of language used is subject to many considerations, such as company policy, suitability to task, availability of third-party packages, or individual preference. Ideally, the programming language best suited for the task at hand will be selected. Trade-offs from this ideal involve finding enough programmers who know the language to build a team, the availability of compilers for that language, and the efficiency with which programs written in a given language execute. Languages form an approximate spectrum from \"low-level\" to \"high-level\"; \"low-level\" languages are typically more machine-oriented and faster to execute, whereas \"high-level\" languages are more abstract and easier to use but execute less quickly. It is usually easier to code in \"high-level\" languages than in \"low-level\" ones.\nProgramming languages are essential for software development. They are the building blocks for all software, from the simplest applications to the most sophisticated ones.\nAllen Downey, in his book \"How To Think Like A Computer Scientist\", writes:\nThe details look different in different languages, but a few basic instructions appear in just about every language:\n*Input: Gather data from the keyboard, a file, or some other device.\n*Output: Display data on the screen or send data to a file or other device.\n*Arithmetic: Perform basic arithmetical operations like addition and multiplication.\n*Conditional Execution: Check for certain conditions and execute the appropriate sequence of statements.\n*Repetition: Perform some action repeatedly, usually with some variation.\nMany computer languages provide a mechanism to call functions provided by shared libraries. Provided the functions in a library follow the appropriate run-time conventions (e.g., method of passing arguments), then these functions may be written in any other language.\nProgrammers.\nComputer programmers are those who write computer software. Their jobs usually involve:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nAlthough programming has been presented in the media as a somewhat mathematical subject, some research shows that good programmers have strong skills in natural human languages, and that learning to code is similar to learning a foreign language.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5312", "revid": "30746614", "url": "https://en.wikipedia.org/wiki?curid=5312", "title": "On the Consolation of Philosophy", "text": "Philosophical work by Boethius\nOn the Consolation of Philosophy (')\", often titled as The Consolation of Philosophy or simply the Consolation\"',\" is a philosophical work by the Roman philosopher Boethius. Written in 523 while he was imprisoned by the Ostrogothic King Theodoric, it is often described as the last great Western work of the Classical Period. Boethius' \"Consolation\" heavily influenced the philosophy of late antiquity, as well as Medieval and early Renaissance Christianity. \nDescription.\n\"On the Consolation of Philosophy\" was written in AD 523 during a one-year imprisonment Boethius served while awaiting trial\u2014and eventual execution\u2014for the alleged crime of treason under the Ostrogothic King Theodoric the Great. Boethius was at the very heights of power in Rome, holding the prestigious office of \"magister officiorum\", and was brought down by treachery. This experience inspired the text, which reflects on how evil can exist in a world governed by God (the problem of theodicy), and how happiness is still attainable amidst fickle fortune, while also considering the nature of happiness and God. In 1891, the academic Hugh Fraser Stewart described the work as \"by far the most interesting example of prison literature the world has ever seen.\"\nBoethius writes the book as a conversation between himself and a female personification of philosophy. Philosophy consoles Boethius by discussing the transitory nature of fame and wealth (\"no man can ever truly be secure until he has been forsaken by Fortune\"), and the ultimate superiority of things of the mind, which she calls the \"one true good\". She contends that happiness comes from within, and that virtue is all that one truly has because it is not imperiled by the vicissitudes of fortune.\nBoethius engages with the nature of predestination and free will, the problem of evil, human nature, virtue, and justice. He speaks about the nature of free will and determinism when he asks if God knows and sees all, or does man have free will. On human nature, Boethius says that humans are essentially good, and only when they give in to \"wickedness\" do they \"sink to the level of being an animal.\" On justice, he says criminals are not to be abused, but rather treated with sympathy and respect, using the analogy of doctor and patient to illustrate the ideal relationship between prosecutor and criminal.\nOutline.\n\"On the Consolation of Philosophy\" is laid out as follows:\nInterpretation.\nIn the \"Consolation\", Boethius answered religious questions without reference to Christianity, relying solely on natural philosophy and the Classical Greek tradition. He believed in the correspondence between faith and reason. The truths found in Christianity would be no different from the truths found in philosophy. In the words of Henry Chadwick, \"If the \"Consolation\" contains nothing distinctively Christian, it is also relevant that it contains nothing specifically pagan either...[it] is a work written by a Platonist who is also a Christian.\"\nBoethius repeats the Macrobius model of the Earth in the center of a spherical cosmos.\nThe philosophical message of the book fits well with the religious piety of the Middle Ages. Boethius encouraged readers not to pursue worldly goods such as money and power, but to seek internalized virtues. Evil had a purpose, to provide a lesson to help change for good; while suffering from evil was seen as virtuous. Because God ruled the universe through Love, prayer to God and the application of Love would lead to true happiness. The Middle Ages, with their vivid sense of an overruling fate, found in Boethius an interpretation of life closely akin to the spirit of Christianity. The \"Consolation\" stands, by its note of fatalism and its affinities with the Christian doctrine of humility, midway between the pagan philosophy of Seneca the Younger and the later Christian philosophy of consolation represented by Thomas \u00e0 Kempis.\nThe book is heavily influenced by Plato and his dialogues (as was Boethius himself). Its popularity can in part be explained by its Neoplatonic and Christian ethical messages, although current scholarly research is still far from clear exactly why and how the work became so vastly popular in the Middle Ages.\nInfluence.\nFrom the Carolingian epoch to the end of the Middle Ages and beyond, \"The Consolation of Philosophy\" was one of the most popular and influential philosophical works, read by statesmen, poets, historians, philosophers, and theologians. It is through Boethius that much of the thought of the Classical period was made available to the Western Medieval world. It has often been said Boethius was the \"last of the Romans and the first of the Scholastics\".\nTranslations into the vernacular were done by famous notables, including King Alfred (Old English), Jean de Meun (Old French), Geoffrey Chaucer (Middle English), Queen Elizabeth I (Early Modern English) and Notker Labeo (Old High German). Boethius's \"Consolation of Philosophy\" was translated into Italian by Alberto della Piagentina (1332), Anselmo Tanso (Milan, 1520), Lodovico Domenichi (Florence, 1550), Benedetto Varchi (Florence, 1551), Cosimo Bartoli (Florence, 1551) and Tommaso Tamburini (Palermo, 1657).\nFound within the \"Consolation\" are themes that have echoed throughout the Western canon: the female figure of wisdom that informs Dante, the ascent through the layered universe that is shared with Milton, the reconciliation of opposing forces that find their way into Chaucer in \"The Knight's Tale\", and the Wheel of Fortune so popular throughout the Middle Ages.\nCitations from it occur frequently in Dante's \"Divina Commedia\". Of Boethius, Dante remarked: \"The blessed soul who exposes the deceptive world to anyone who gives ear to him.\"\nBoethian influence can be found nearly everywhere in Geoffrey Chaucer's poetry, e.g. in \"Troilus and Criseyde\", \"The Knight's Tale\", \"The Clerk's Tale\", \"The Franklin's Tale\", \"The Parson's Tale\" and \"The Tale of Melibee\", in the character of Lady Nature in \"The Parliament of Fowls\" and some of the shorter poems, such as \"Truth\", \"The Former Age\" and \"Lak of Stedfastnesse\". Chaucer translated the work in his \"Boece\".\nThe Italian composer Luigi Dallapiccola used some of the text in his choral work \"Canti di prigionia\" (1938). The Australian composer Peter Sculthorpe quoted parts of it in his opera or music theatre work \"Rites of Passage\" (1972\u201373), which was commissioned for the opening of the Sydney Opera House but was not ready in time.\nTom Shippey in \"The Road to Middle-earth\" says how \"Boethian\" much of the treatment of evil is in Tolkien's \"The Lord of the Rings\". Shippey says that Tolkien knew well the translation of Boethius that was made by King Alfred and he quotes some \"Boethian\" remarks from Frodo, Treebeard, and Elrond.\nBoethius and \"Consolatio Philosophiae\" are cited frequently by the main character Ignatius J. Reilly in the Pulitzer Prize-winning \"A Confederacy of Dunces\" (1980).\nIt is a prosimetrical text, meaning that it is written in alternating sections of prose and metered verse. In the course of the text, Boethius displays a virtuosic command of the forms of Latin poetry. It is classified as a Menippean satire, a fusion of allegorical tale, platonic dialogue, and lyrical poetry.\nEdward Gibbon described the work as \"a golden volume not unworthy of the leisure of Plato or Tully.\"\nIn the 20th century, there were close to four hundred manuscripts still surviving, a testament to its popularity.\nOf the work, C. S. Lewis wrote: \"To acquire a taste for it is almost to become naturalised in the Middle Ages.\"\nReconstruction of lost songs.\nHundreds of Latin songs were recorded in neumes from the ninth century through to the thirteenth century, including settings of the poetic passages from Boethius's \"The Consolation of Philosophy\". The music of this song repertory had long been considered irretrievably lost because the notational signs indicated only melodic outlines, relying on now-lapsed oral traditions to fill in the missing details. However, research conducted by Sam Barrett at the University of Cambridge, extended in collaboration with medieval music ensemble Sequentia, has shown that principles of musical setting for this period can be identified, providing crucial information to enable modern realisations. Sequentia performed the world premiere of the reconstructed songs from Boethius's \"The Consolation of Philosophy\" at Pembroke College, Cambridge, in April 2016, bringing to life music not heard in over 1,000 years; a number of the songs were subsequently recorded on the CD \"Boethius: Songs of Consolation. Metra from 11th-Century Canterbury\" (Glossa, 2018). The detective story behind the recovery of these lost songs is told in a documentary film, and a website launched by the University of Cambridge in 2018 provides further details of the reconstruction process, bringing together manuscripts, reconstructions, and video resources.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
