{"id": "5313", "revid": "1160192415", "url": "https://en.wikipedia.org/wiki?curid=5313", "title": "Crouching Tiger, Hidden Dragon", "text": "2000 film by Ang Lee\nCrouching Tiger, Hidden Dragon is a 2000 wuxia martial arts adventure film directed by Ang Lee and written for the screen by Wang Hui-ling, James Schamus, and Tsai Kuo-jung. The film stars Chow Yun-fat, Michelle Yeoh, Zhang Ziyi, and Chang Chen. It is based on the Chinese novel of the same name serialized between 1941 and 1942 by Wang Dulu, the fourth part of his \"Crane Iron\" pentalogy.\nA multinational venture, the film was made on a US$17 million budget, and was produced by Edko Films and Zoom Hunt Productions in collaboration with China Film Co-productions Corporation and Asian Union Film &amp; Entertainment for Columbia Pictures Film Production Asia in association with Good Machine International. With dialogue in Standard Chinese, subtitled for various markets, \"Crouching Tiger, Hidden Dragon\" became a surprise international success, grossing $213.5 million worldwide. It grossed US$128 million in the United States, becoming the highest-grossing foreign-language film produced overseas in American history. The film was the first foreign-language film to break the $100 million mark in the United States.\nThe film premiered at the Cannes Film Festival on 18 May 2000, and was theatrically released in the United States on 8 December. Receiving universal acclaim and a box office hit, \"Crouching Tiger, Hidden Dragon\" won over 40 awards and was nominated for 10 Academy Awards in 2001, including Best Picture, and won Best Foreign Language Film, Best Art Direction, Best Original Score, and Best Cinematography, receiving the most nominations ever for a non-English-language film at the time, until 2018's \"Roma\" tied this record. The film also won four BAFTAs and two Golden Globe Awards, one for Best Foreign Film. Along with its numerous awards, \"Crouching Tiger\" is often cited as one of the finest wuxia films ever made. The film has been praised for its story, direction, cinematography, and martial arts sequences.\nPlot.\nIn Qing dynasty China, Li Mu Bai is a renowned Wudang swordsman, and his friend Yu Shu Lien, a female warrior, heads a private security company. Shu Lien and Mu Bai have long had feelings for each other, but because Shu Lien had been engaged to Mu Bai's close friend, Meng Sizhao before his death, Shu Lien and Mu Bai feel bound by loyalty to Meng Sizhao and have not revealed their feelings to each other. Mu Bai, choosing to retire, asks Shu Lien to give his fabled 400-year-old sword \"Green Destiny\" to their benefactor Sir Te in Beijing. Long ago, Mu Bai's teacher was killed by Jade Fox, a woman who sought to learn Wudang secrets. While at Sir Te's place, Shu Lien meets Yu Jiaolong, or Jen, who is the daughter of the rich and powerful Governor Yu and is about to get married.\nOne evening, a masked thief sneaks into Sir Te's estate and steals the Green Destiny. Sir Te's servant Master Bo and Shu Lien trace the theft to Governor Yu's compound, where Jade Fox had been posing as Jen's governess for many years. Soon after, Mu Bai arrives in Beijing and discusses the theft with Shu Lien. Master Bo makes the acquaintance of Inspector Tsai, a police investigator from the provinces, and his daughter May, who have come to Beijing in pursuit of Fox. Fox challenges the pair and Master Bo to a showdown that night. Following a protracted battle, the group is on the verge of defeat when Mu Bai arrives and outmaneuvers Fox. She reveals that she killed Mu Bai's teacher because he would sleep with her, but refuse to take a woman as a disciple, and she felt it poetic justice for him to die at a woman's hand. Just as Mu Bai is about to kill her, the masked thief reappears and helps Fox. Fox kills Tsai before fleeing with the thief (who is revealed to be Jen). After seeing Jen fight Mu Bai, Fox realizes Jen had been secretly studying the Wudang manual. Fox is illiterate and could only follow the diagrams, whereas Jen's ability to read the manual allowed her to surpass her teacher in martial arts.\nAt night, a bandit named Lo breaks into Jen's bedroom and asks her to leave with him. In the past, when Governor Yu and his family were traveling in the western deserts, Lo and his bandits raided Jen's caravan and Lo stole her comb. She pursued him to his desert cave to retrieve her comb. However, the pair soon fell in love. Lo eventually convinced Jen to return to her family, though not before telling her a legend of a man who jumped off a mountain to make his wishes come true. Because the man's heart was pure, his wish was granted and he was unharmed, but flew away never to be seen again. Lo has come now to Beijing to persuade Jen not to go through with her arranged marriage. However, Jen refuses to leave with him. Later, Lo interrupts Jen's wedding procession, begging her to leave with him. Shu Lien and Mu Bai convince Lo to wait for Jen at Mount Wudang, where he will be safe from Jen's family, who are furious with him. Jen runs away from her husband on their wedding night before the marriage can be consummated. Disguised in men's clothing, she is accosted at an inn by a large group of warriors; armed with the Green Destiny and her own superior combat skills, she emerges victorious.\nJen visits Shu Lien, who tells her that Lo is waiting for her at Mount Wudang. After an angry exchange, the two women engage in a duel. Shu Lien is the superior fighter, but Jen wields the Green Destiny and is able to destroy each weapon that Shu Lien wields, until Shu Lien finally manages to defeat Jen with a broken sword. When Shu Lien shows mercy, Jen wounds Shu Lien in the arm. Mu Bai arrives and pursues Jen into a bamboo forest, where he offers to take her as his student. Jen agrees if he can take Green Destiny from her in three moves. Mu Bai is able to take the sword in only one move, but Jen reneges on her promise, and Mu Bai throws the sword over a waterfall. Jen dives after the sword and is rescued by Fox. Fox puts Jen into a drugged sleep and places her in a cavern, where Mu Bai and Shu Lien discover her. Fox suddenly attacks them with poisoned needles. Mu Bai mortally wounds Fox, only to realize that one of the needles has hit him in the neck. Before dying, Fox confesses that her goal had been to kill Jen because Jen had hidden the secrets of Wudang's fighting techniques from her.\nContrite, Jen leaves to prepare an antidote for the poisoned dart. With his last breath, Mu Bai finally confesses his love for Shu Lien. He dies in her arms as Jen returns. Shu Lien forgives Jen, telling her to go to Lo and always be true to herself. The Green Destiny is returned to Sir Te. Jen goes to Mount Wudang and spends the night with Lo. The next morning, Lo finds Jen standing on a bridge overlooking the edge of the mountain. In an echo of the legend that they spoke about in the desert, she asks him to make a wish. Lo wishes for them to be together again, back in the desert. Jen leaps from the bridge, falling into the mists below.\nCast.\nCredits from British Film Institute:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nThemes and interpretations.\nTitle.\nThe title \"Crouching Tiger, Hidden Dragon\" is a literal translation of the Chinese idiom \"\u81e5\u864e\u85cf\u9f8d\" which describes a place or situation that is full of unnoticed masters. It is from a poem of the ancient Chinese poet Yu Xin (513\u2013581) that reads \"\u6697\u77f3\u7591\u85cf\u864e\uff0c\u76e4\u6839\u4f3c\u81e5\u9f8d\", which means \"behind the rock in the dark probably hides a tiger, and the coiling giant root resembles a crouching dragon\". The title also has several other layers of meaning. On one level, the Chinese characters in the title connect to the narrative that the last character in Xiaohu and Jiaolong's names mean \"tiger\" and \"dragon\", respectively. On another level, the Chinese idiomatic phrase is an expression referring to the undercurrents of emotion, passion, and secret desire that lie beneath the surface of polite society and civil behavior, which alludes to the film's storyline.\nGender roles.\nThe success of the Disney animated feature \"Mulan\" (1998) popularized the image of the Chinese woman warrior in the west. The storyline of \"Crouching Tiger, Hidden Dragon\" is mostly driven by the three female characters. In particular, Jen is driven by her desire to be free from the gender role imposed on her, while Shu Lien, herself oppressed by the gender role, tries to lead Jen back into the role deemed appropriate for her. Some prominent martial arts disciplines are traditionally held to have been originated by women, e.g., Wing Chun. The film's title refers to masters one does not notice, which necessarily includes mostly women, and therefore suggests the advantage of a female bodyguard.\nPoison.\nPoison is also a significant theme in the film. The Chinese word \"\u6bd2\" (\"d\u00fa\") means not only physical poison but also cruelty and sinfulness. In the world of martial arts, the use of poison is considered an act of one who is too cowardly and dishonorable to fight; and indeed, the only character who explicitly fits these characteristics is Jade Fox. The poison is a weapon of her bitterness and quest for vengeance: she poisons the master of Wudang, attempts to poison Jen, and succeeds in killing Mu Bai using a poisoned needle. In further play on this theme by the director, Jade Fox, as she dies, refers to the poison from a young child, \"the deceit of an eight-year-old girl\", referring to what she considers her own spiritual poisoning by her young apprentice Jen. Li Mu Bai himself warns that, without guidance, Jen could become a \"poison dragon\".\nChina of the imagination.\nThe story is set during the Qing dynasty (1644\u20131912), but it does not specify an exact time. Lee sought to present a \"China of the imagination\" rather than an accurate vision of Chinese history. At the same time, Lee also wanted to make a film that Western audiences would want to see. Thus, the film is shot for a balance between Eastern and Western aesthetics. There are some scenes showing uncommon artistry for the typical martial arts film such as an airborne battle among wispy bamboo plants.\nProduction.\nThe film was adapted from the novel \"Crouching Tiger, Hidden Dragon\" by Wang Dulu, serialized between 1941 and 1942 in \"Qingdao Xinmin News\". The novel is the fourth in a sequence of five. In the contract reached between Columbia Pictures and Ang Lee and Hsu Li-kong, they agreed to invest US$6 million in filming, but the stipulated recovery amount must be more than six times before the two parties will start to pay dividends.\nCasting.\nShu Qi was Ang Lee's first choice for the role of Jen, but she turned it down.\nFilming.\nAlthough its Academy Award for Best Foreign Language Film was presented to Taiwan, \"Crouching Tiger, Hidden Dragon\" was in fact an international co-production between companies in four regions: the Chinese company China Film Co-production Corporation, the American companies Columbia Pictures Film Production Asia, Sony Pictures Classics, and Good Machine, the Hong Kong company Edko Films, and the Taiwanese Zoom Hunt Productions, as well as the unspecified United China Vision and Asia Union Film &amp; Entertainment, created solely for this film.\nThe film was made in Beijing, with location shooting in the Anhui, Hebei, Jiangsu, and Xinjiang provinces of China. The first phase of shooting was in the Gobi Desert where it consistently rained. Director Ang Lee noted, \"I didn't take one break in eight months, not even for half a day. I was miserable\u2014I just didn't have the extra energy to be happy. Near the end, I could hardly breathe. I thought I was about to have a stroke.\" The stunt work was mostly performed by the actors themselves and Ang Lee stated in an interview that computers were used \"only to remove the safety wires that held the actors\" aloft. \"Most of the time you can see their faces,\" he added. \"That's really them in the trees.\"\nAnother compounding issue was the difference between accents of the four lead actors: Chow Yun-fat is from Hong Kong and speaks Cantonese natively; Michelle Yeoh is from Malaysia and grew up speaking English and Malay, so she learned the Standard Chinese lines phonetically; Chang Chen is from Taiwan and he speaks Standard Chinese in a Taiwanese accent. Only Zhang Ziyi spoke with a native Mandarin accent that Ang Lee wanted. Chow Yun Fat said, on \"the first day [of shooting], I had to do 28 takes just because of the language. That's never happened before in my life.\"\nThe film specifically targeted Western audiences rather than the domestic audiences who were already used to Wuxia films. As a result, high-quality English subtitles were needed. Ang Lee, who was educated in the West, personally edited the subtitles to ensure they were satisfactory for Western audiences.\nSoundtrack.\nThe score was composed by Dun TAN in 1999. It was played for the movie by the Shanghai Symphony Orchestra, the Shanghai National Orchestra and the Shanghai Percussion Ensemble. It features solo passages for cello played by Yo-Yo Ma. The \"last track\" (\"A Love Before Time\") features Coco Lee, who later sang it at the Academy Awards. The composer Chen Yuanlin also collaborated in the project. The music for the entire film was produced in two weeks. Tan the next year (2000) adapted his filmscore as a cello concerto called simply \"Crouching Tiger.\"\nRelease.\nMarketing.\nThe film was adapted into a video game and a series of comics, and it led to the original novel being adapted into a 34-episode Taiwanese television series. The latter was released in 2004 as \"New Crouching Tiger, Hidden Dragon\" for Northern American release.\nHome media.\nThe film was released on VHS and DVD on 5 June 2001 by Columbia TriStar Home Entertainment. It was also released on UMD on 26 June 2005. In the United Kingdom, it was watched by 3.5 million viewers on television in 2004, making it the year's most-watched foreign-language film on television.\nRestoration.\nThe film was re-released in a 4K restoration by Sony Pictures Classics in 2023.\nReception.\nBox office.\nThe film premiered in cinemas on 8 December 2000, in limited release within the United States. During its opening weekend, the film opened in 15th place, grossing $663,205 in business, showing at 16 locations. On 12 January 2001, \"Crouching Tiger, Hidden Dragon\" premiered in cinemas in wide release throughout the U.S., grossing $8,647,295 in business, ranking in sixth place. The film \"Save the Last Dance\" came in first place during that weekend, grossing $23,444,930. The film's revenue dropped by almost 30% in its second week of release, earning $6,080,357. For that particular weekend, the film fell to eighth place, screening in 837 theaters. \"Save the Last Dance\" remained unchanged in first place, grossing $15,366,047 in box-office revenue. During its final week in release, \"Crouching Tiger, Hidden Dragon\" opened in a distant 50th place with $37,233 in revenue. The film went on to top out domestically at $128,078,872 in total ticket sales through a 31-week theatrical run. Internationally, the film took in an additional $85,446,864 in box-office business for a combined worldwide total of $213,525,736. For 2000 as a whole, the film cumulatively ranked at a worldwide box-office performance position of 19.\nCritical response.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Crouching Tiger, Hidden Dragon\", which is based on an early 20th century novel by Wang Dulu, unfolds much like a comic book, with the characters and their circumstances being painted using wide brush strokes. Subtlety is not part of Lee's palette; he is going for something grand and melodramatic, and that's what he gets.\n\"Crouching Tiger, Hidden Dragon\" was widely acclaimed in the Western world, receiving numerous awards. On Rotten Tomatoes, the film holds an approval rating of 98% based on 168 reviews, with an average rating of 8.6/10. The site's critical consensus states: \"The movie that catapulted Ang Lee into the ranks of upper echelon Hollywood filmmakers, \"Crouching Tiger, Hidden Dragon\" features a deft mix of amazing martial arts battles, beautiful scenery, and tasteful drama.\" Metacritic reported the film had an average score of 94 out of 100, based on 32 reviews, indicating \"universal acclaim\".\nSome Chinese-speaking viewers were bothered by the accents of the leading actors. Neither Chow (a native Cantonese speaker) nor Yeoh (who was born and raised in Malaysia) spoke Mandarin Chinese as a mother tongue. All four main actors spoke Standard Chinese with vastly different accents: Chow speaks with a Cantonese accent, Yeoh with a Malaysian accent, Chang Chen with a Taiwanese accent, and Zhang Ziyi with a Beijing accent. Yeoh responded to this complaint in a 28 December 2000, interview with \"Cinescape\". She argued, \"My character lived outside of Beijing, and so I didn't have to do the Beijing accent.\" When the interviewer, Craig Reid, remarked, \"My mother-in-law has this strange Sichuan-Mandarin accent that's hard for me to understand,\" Yeoh responded: \"Yes, provinces all have their very own strong accents. When we first started the movie, Cheng Pei Pei was going to have her accent, and Chang Zhen was going to have his accent, and this person would have that accent. And in the end nobody could understand what they were saying. Forget about us, even the crew from Beijing thought this was all weird.\"\nThe film led to a boost in popularity of Chinese wuxia films in the western world, where they were previously little known, and led to films such as \"Hero\" and \"House of Flying Daggers\", both directed by Zhang Yimou, being marketed towards Western audiences. The film also provided the breakthrough role for Zhang Ziyi's career, who noted:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Because of movies like \"Crouching Tiger, Hidden Dragon\", \"Hero\", and \"Memoirs of a Geisha\", a lot of people in the United States have become interested not only in me but in Chinese and Asian actors in general. Because of these movies, maybe there will be more opportunities for Asian actors.\n\"Film Journal\" noted that \"Crouching Tiger, Hidden Dragon\" \"pulled off the rare trifecta of critical acclaim, boffo box-office and gestalt shift\", in reference to its ground-breaking success for a subtitled film in the American market.\nAccolades.\nGathering widespread critical acclaim at the Toronto and New York film festivals, the film also became a favorite when Academy Awards nominations were announced in 2001. The film was screened out of competition at the 2000 Cannes Film Festival. The film received ten Academy Award nominations, which was the highest ever for a non-English language film, up until it was tied by \"Roma\" (2018).\nThe film is ranked at number 497 on \"Empire\"'s 2008 list of the 500 greatest movies of all time. and at number 66 in the magazine's 100 Best Films of World Cinema, published in 2010.\nIn 2010, the Independent Film &amp; Television Alliance selected the film as one of the 30 Most Significant Independent Films of the last 30 years.\nIn 2016, it was voted the 35th-best film of the 21st century as picked by 177 film critics from around the world in a poll conducted by BBC.\nThe film was included in BBC's 2018 list of The 100 greatest foreign language films ranked by 209 critics from 43 countries around the world. In 2019, \"The Guardian\" ranked the film 51st in its 100 best films of the 21st century list.\nSequel.\nA sequel to the film, \"\", was released in 2016. It was directed by Yuen Wo-ping, who was the action choreographer for the first film. It is a co-production between Pegasus Media, China Film Group Corporation, and the Weinstein Company. Unlike the original film, the sequel was filmed in English for international release and dubbed into Chinese for Chinese releases.\n\"Sword of Destiny\" is based on \"Iron Knight, Silver Vase\", the next (and last) novel in the Crane\u2013Iron Pentalogy. It features a mostly new cast, headed by Donnie Yen. Michelle Yeoh reprised her role from the original. Zhang Ziyi was also approached to appear in \"Sword of Destiny\" but refused, stating that she would only appear in a sequel if Ang Lee were directing it.\nIn the West, the sequel was for the most part not shown in theaters, instead being distributed direct-to-video by the streaming service Netflix.\nPosterity.\nThe theme of Janet Jackson's song \"China Love\" was related to the film by MTV News, in which Jackson sings of the daughter of an emperor in love with a warrior, unable to sustain relations when forced to marry into royalty.\nThe names of the pterosaur genus \"Kryptodrakon\" and the ceratopsian genus \"Yinlong\" (both meaning \"hidden dragon\" in Greek and Chinese respectively) allude to the film.\nThe character of Lo, or \"Dark Cloud\" the desert bandit, influenced the development of the protagonist of the \"Prince of Persia\" series of video games.\nIn the video game there are two hybrid fighting styles that pay homage to this movie. Which have the following combinations: Crouching tiger (Martial Arts + Streetfighting + Submissions) and Hidden Dragon (Martial Arts + Streetfighting + Kickboxing).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5314", "revid": "12101203", "url": "https://en.wikipedia.org/wiki?curid=5314", "title": "Charlemagne", "text": "King of Franks, first Holy Roman Emperor\nCharlemagne ( ) or Charles the Great (, Frankish: \"Karl\"; 2 April 747 \u2013 28 January 814), a member of the Carolingian dynasty, was King of the Franks from 768, King of the Lombards from 774, and was crowned as the Emperor of the Romans by the Papacy in 800. Charlemagne succeeded in uniting the majority of western and central Europe and was the first recognized emperor to rule from western Europe after the fall of the Western Roman Empire approximately three centuries earlier. The expanded Frankish state that Charlemagne founded was the Carolingian Empire, which is considered the first phase in the history of the Holy Roman Empire. He was canonized by Antipope Paschal III\u2014an act later treated as invalid\u2014and he is now regarded by some as beatified (which is a step on the path to sainthood) in the Catholic Church.\nCharlemagne was the eldest son of Pepin the Short and Bertrada of Laon. He was born before their canonical marriage. He became king of the Franks in 768 following his father's death, and was initially co-ruler with his brother Carloman I until the latter's death in 771. As sole ruler, he continued his father's policy towards the protection of the papacy and became its sole defender, removing the Lombards from power in northern Italy and leading an incursion into Muslim Spain. He also campaigned against the Saxons to his east, Christianizing them (upon penalty of death) which led to events such as the Massacre of Verden. He reached the height of his power in 800 when he was crowned Emperor of the Romans by Pope Leo III on Christmas Day at Old St. Peter's Basilica in Rome.\nCharlemagne has been called the \"Father of Europe\" (\"Pater Europae\"), as he united most of Western Europe for the first time since the classical era of the Roman Empire, as well as uniting parts of Europe that had never been under Frankish or Roman rule. His reign spurred the Carolingian Renaissance, a period of energetic cultural and intellectual activity within the Western Church. The Eastern Orthodox Church viewed Charlemagne less favourably, due to his support of the filioque and the Pope's preference of him as emperor over the Eastern Roman Empire's first female monarch, Irene of Athens. These and other disputes led to the eventual split of Rome and Constantinople in the Great Schism of 1054.\nCharlemagne died in 814 after contracting an infectious lung disease. He was laid to rest in the Aachen Cathedral, in his imperial capital city of Aachen. He married at least four times, and three of his legitimate sons lived to adulthood. Only the youngest of them, Louis the Pious, survived to succeed him. Charlemagne is a direct ancestor of many of Europe's royal houses, including the Capetian dynasty, the Ottonian dynasty, the House of Luxembourg, the House of Ivrea and the House of Habsburg.\nNames and nicknames.\nThe name Charlemagne ( ), by which the emperor is normally known in English, comes from the French \"Charles-le-magne\", meaning \"Charles the Great\". In modern German, \"Karl der Gro\u00dfe\" has the same meaning. His given name in his native Frankish dialect was \"Karl\" (\"Charles\", Latin: \"Carolus\"; Old High German: \"Karlus\"; Gallo-Romance: \"Karlo\"). He was named after his grandfather, Charles Martel, a choice which intentionally marked him as Martel's true heir.\nThe nickname \"magnus\" (great) may have been associated with him already in his lifetime, but this is not certain. The contemporary Latin \"Royal Frankish Annals\" routinely call him \"Carolus magnus rex\", \"Charles the great king\". As a nickname, it is only certainly attested in the works of the Poeta Saxo around 900 and it only became standard in all the lands of his former empire around 1000.\nCharles' achievements gave a new meaning to his name. In many languages of Europe, the very word for \"king\" derives from his name; e.g., , , , , , , , , , , , . This development parallels that of the name of the Caesars in the original Roman Empire, which became \"kaiser\" and \"tsar\" (or \"czar\"), among others.\nPolitical background.\nBy the 6th century, the western Germanic tribe of the Franks had been Christianised, due in considerable measure to the Catholic conversion of Clovis\u00a0I. Francia, ruled by the Merovingians, was the most powerful of the kingdoms that succeeded the Western Roman Empire. Following the Battle of Tertry, the Merovingians declined into powerlessness, for which they have been dubbed the \"rois fain\u00e9ants\" (\"do-nothing kings\"). Almost all government powers were exercised by their chief officer, the mayor of the palace.\nIn 687, Pepin of Herstal, mayor of the palace of Austrasia, ended the strife between various kings and their mayors with his victory at Tertry. He became the sole governor of the entire Frankish kingdom. Pepin was the grandson of two important figures of the Austrasian Kingdom: Saint Arnulf of Metz and Pepin of Landen. Pepin of Herstal was eventually succeeded by his son Charles, later known as Charles Martel (Charles the Hammer).\nAfter 737, Charles governed the Franks in lieu of a king and declined to call himself \"king\". Charles was succeeded in 741 by his sons Carloman and Pepin the Short, the father of Charlemagne. In 743, the brothers placed Childeric III on the throne to curb separatism in the periphery. He was the last Merovingian king. Carloman resigned office in 746, preferring to enter the church as a monk. Pepin brought the question of the kingship before Pope Zachary, asking whether it was logical for a king to have no royal power. The pope handed down his decision in 749, decreeing that it was better for Pepin to be called king, as he had the powers of high office as Mayor, so as not to confuse the hierarchy. He, therefore, ordered him to become the \"true king\".&lt;ref name=\"France/Pippin\"&gt;&lt;/ref&gt;\nIn 750, Pepin was elected by an assembly of the Franks, anointed by the archbishop, and then raised to the office of king. The Pope branded Childeric III as \"the false king\" and ordered him into a monastery. The Merovingian dynasty was thereby replaced by the Carolingian dynasty, named after Charles Martel. In 753, Pope Stephen II fled from Italy to Francia, appealing to Pepin for assistance for the rights of St. Peter. He was supported in this appeal by Carloman, Charles' brother. In return, the pope could provide only legitimacy. He did this by again anointing and confirming Pepin, this time adding his young sons Carolus (Charlemagne) and Carloman to the royal patrimony. They thereby became heirs to the realm that already covered most of western Europe. In 754, Pepin accepted the Pope's invitation to visit Italy on behalf of St. Peter's rights, dealing successfully with the Lombards.\nUnder the Carolingians, the Frankish kingdom spread to encompass an area including most of Western Europe; the later east\u2013west division of the kingdom formed the basis for modern France and Germany. Orman portrays the Treaty of Verdun (843) between the warring grandsons of Charlemagne as the foundation event of an independent France under its first king Charles the Bald; an independent Germany under its first king Louis the German; and an independent intermediate state stretching from the Low Countries along the borderlands to south of Rome under Lothair I, who retained the title of emperor and the capitals Aachen and Rome without the jurisdiction. The middle kingdom had broken up by 890 and partly absorbed into the Western kingdom (later France) and the Eastern kingdom (Germany) and the rest developing into smaller \"buffer\" states that exist between France and Germany to this day, namely Benelux and Switzerland.\nRise to power.\nEarly life.\nThe most likely date of Charlemagne's birth is reconstructed from several sources. The date of 742\u2014calculated from Einhard's date of death of January 814 at age 72\u2014predates the marriage of his parents in 744. The year given in the \"Annales Petaviani\", 747, would be more likely, except that it contradicts Einhard and a few other sources in making Charlemagne sixty-seven years old at his death. The month and day of 2 April are based on a calendar from Lorsch Abbey. Charlemagne claimed descent from the Roman emperor, Constantine I.\nIn 747, Easter fell on 2 April, a coincidence that likely would have been remarked upon by chroniclers but was not. If Easter was being used as the beginning of the calendar year, then 2 April 747 could have been, by modern reckoning, April 748 (not on Easter). The date favoured by the preponderance of evidence is 2 April 742, based on Charlemagne's age at the time of his death. This date supports the concept that Charlemagne was technically an illegitimate child, although that is not mentioned by Einhard in either since he was born out of wedlock; Pepin and Bertrada were bound by a private contract or Friedelehe at the time of his birth, but did not marry until 744.\nCharlemagne's exact birthplace is unknown, although historians have suggested Aachen in modern-day Germany, and Li\u00e8ge (Herstal) in present-day Belgium as possible locations. Aachen and Li\u00e8ge are close to the region whence the Merovingian and Carolingian families originated. Other cities have been suggested, including D\u00fcren, Gauting, M\u00fcrlenbach, Quierzy, and Pr\u00fcm. No definitive evidence resolves the question.\nAncestry.\nCharlemagne was the eldest child of Pepin the Short (714\u00a0\u2013\u00a024 September\u00a0768, reigned from 751) and his wife Bertrada of Laon (720\u00a0\u2013\u00a012 July\u00a0783), daughter of Caribert of Laon. Many historians consider Charlemagne (Charles) to have been illegitimate, although some state that this is arguable, because Pepin did not marry Bertrada until 744, which was after Charles' birth; this status did not exclude him from the succession.\nRecords name only Carloman, Gisela, and three short-lived children named Pepin, Chrothais and Adelais as his younger siblings.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It would be folly, I think, to write a word concerning Charles' birth and infancy, or even his boyhood, for nothing has ever been written on the subject, and there is no one alive now who can give information on it.\nAmbiguous high office.\nThe most powerful officers of the Frankish people, the Mayor of the Palace (\"Maior Domus\") and one or more kings (\"rex\", \"reges\"), were appointed by the election of the people. Elections were not periodic, but were held as required to elect officers \"ad quos summa imperii pertinebat\", \"to whom the highest matters of state pertained\". Evidently, interim decisions could be made by the Pope, which ultimately needed to be ratified using an assembly of the people that met annually.\nBefore he was elected king in 751, Pepin was initially a mayor, a high office he held \"as though hereditary\" (\"velut hereditario fungebatur\"). Einhard explains that \"the honour\" was usually \"given by the people\" to the distinguished, but Pepin the Great and his brother Carloman the Wise received it as though hereditary, as had their father, Charles Martel. There was, however, a certain ambiguity about quasi-inheritance. The office was treated as joint property: one Mayorship held by two brothers jointly. Each, however, had his own geographic jurisdiction. When Carloman decided to resign, becoming ultimately a Benedictine at Monte Cassino, the question of the disposition of his quasi-share was settled by the pope. He converted the mayorship into a kingship and awarded the joint property to Pepin, who gained the right to pass it on by inheritance.\nThis decision was not accepted by all family members. Carloman had consented to the temporary tenancy of his own share, which he intended to pass on to his son, Drogo, when the inheritance should be settled at someone's death. By the Pope's decision, in which Pepin had a hand, Drogo was to be disqualified as an heir in favour of his cousin Charles. He took up arms in opposition to the decision and was joined by Grifo, a half-brother of Pepin and Carloman, who had been given a share by Charles Martel, but was stripped of it and held under loose arrest by his half-brothers after an attempt to seize their shares by military action. Grifo perished in combat in the Battle of Saint-Jean-de-Maurienne while Drogo was hunted down and taken into custody.\nAccording to the \"Life\", Pepin died in Paris on 24 September 768, whereupon the kingship passed jointly to his sons, \"with divine assent\" (\"divino nutu\"). The Franks \"in general assembly\" (\"generali conventu\") gave them both the rank of a king (\"reges\") but \"partitioned the whole body of the kingdom equally\" (\"totum regni corpus ex aequo partirentur\"). The \"annals\" tell a slightly different version, with the king dying at St-Denis, near Paris. The two \"lords\" (\"domni\") were \"elevated to kingship\" (\"elevati sunt in regnum\"), Charles on 9 October in Noyon, Carloman on an unspecified date in Soissons. If born in 742, Charles was 26 years old, but he had been campaigning at his father's right hand for several years, which may help to account for his military skill. Carloman was 17.\nThe language, in either case, suggests that there were not two inheritances, which would have created distinct kings ruling over distinct kingdoms, but a single joint inheritance and a joint kingship tenanted by two equal kings, Charles and his brother Carloman. As before, distinct jurisdictions were awarded. Charles received Pepin's original share as Mayor: the outer parts of the kingdom bordering on the sea, namely Neustria, western Aquitaine, and the northern parts of Austrasia; while Carloman was awarded his uncle's former share, the inner parts: southern Austrasia, Septimania, eastern Aquitaine, Burgundy, Provence, and Swabia, lands bordering Italy. The question of whether these jurisdictions were joint shares reverting to the other brother if one brother died or were inherited property passed on to the descendants of the brother who died was never definitely settled. It came up repeatedly over the succeeding decades until the grandsons of Charlemagne created distinct sovereign kingdoms.\nAquitainian rebellion.\nFormation of a new Aquitaine.\nIn southern Gaul, Aquitaine had been Romanised and people spoke a Romance language. Similarly, Hispania had been populated by peoples who spoke various languages, including Celtic, but these had now been mostly replaced by Romance languages. Between Aquitaine and Hispania were the Euskaldunak, Latinised to Vascones, or Basques, whose country, Vasconia, extended, according to the distributions of place names attributable to the Basques, mainly in the western Pyrenees but also as far south as the upper river Ebro in Spain and as far north as the river Garonne in France. The French name Gascony derives from Vasconia. The Romans were never able to subjugate the whole of Vasconia. The soldiers they recruited for the Roman legions from those parts they did submit and where they founded the region's first cities were valued for their fighting abilities. The border with Aquitaine was at Toulouse.\nIn about 660, the Duchy of Vasconia united with the Duchy of Aquitaine to form a single realm under Felix of Aquitaine, ruling from Toulouse. This was a joint kingship with a Basque Duke, Lupus I. \"Lupus\" is the Latin translation of Basque Otsoa, \"wolf\". At Felix's death in 670 the joint property of the kingship reverted entirely to Lupus. As the Basques had no law of joint inheritance but relied on primogeniture, Lupus in effect founded a hereditary dynasty of Basque rulers of an expanded Aquitaine.\nAcquisition of Aquitaine by the Carolingians.\nThe Latin chronicles of the end of Visigothic Hispania omit many details, such as identification of characters, filling in the gaps and reconciliation of numerous contradictions. Muslim sources, however, present a more coherent view, such as in the \"Ta'rikh iftitah al-Andalus\" (\"History of the Conquest of al-Andalus\") by Ibn al-Q\u016b\u1e6diyya (\"the son of the Gothic woman\", referring to the granddaughter of Wittiza, the last Visigothic king of a united Hispania, who married a Moor). Ibn al-Q\u016b\u1e6diyya, who had another, much longer name, must have been relying to some degree on family oral tradition.\nAccording to Ibn al-Q\u016b\u1e6diyya Wittiza, the last Visigothic king of a united Hispania, died before his three sons, Almund, Romulo, and Ardabast reached maturity. Their mother was queen regent at Toledo, but Roderic, army chief of staff, staged a rebellion, capturing C\u00f3rdoba. He chose to impose a joint rule over distinct jurisdictions on the true heirs. Evidence of a division of some sort can be found in the distribution of coins imprinted with the name of each king and in the king lists. Wittiza was succeeded by Roderic, who reigned for seven and a half years, followed by Achila (Aquila), who reigned three and a half years. If the reigns of both terminated with the incursion of the Saracens, then Roderic appears to have reigned a few years before the majority of Achila. The latter's kingdom was securely placed to the northeast, while Roderic seems to have taken the rest, notably modern Portugal.\nThe Saracens crossed the mountains to claim Ardo's Septimania, only to encounter the Basque dynasty of Aquitaine, always the allies of the Goths. Odo the Great of Aquitaine was at first victorious at the Battle of Toulouse in 721. Saracen troops gradually massed in Septimania and, in 732, an army under Emir Abdul Rahman Al Ghafiqi advanced into Vasconia, and Odo was defeated at the Battle of the River Garonne. They took Bordeaux and were advancing towards Tours when Odo, powerless to stop them, appealed to his arch-enemy, Charles Martel, mayor of the Franks. In one of the first of the lightning marches for which the Carolingian kings became famous, Charles and his army appeared in the path of the Saracens between Tours and Poitiers, and in the Battle of Tours decisively defeated and killed al-Ghafiqi. The Moors returned twice more, each time suffering defeat at Charles' hands\u2014at the River Berre near Narbonne in 737 and in the Dauphin\u00e9 in 740. Odo's price for salvation from the Saracens was incorporation into the Frankish kingdom, a decision that was repugnant to him and also to his heirs.\nLoss and recovery of Aquitaine.\nAfter the death of his father, Hunald I allied himself with free Lombardy. However, Odo had ambiguously left the kingdom jointly to his two sons, Hunald and Hatto. The latter, loyal to Francia, now went to war with his brother over full possession. Victorious, Hunald blinded and imprisoned his brother, only to be so stricken by conscience that he resigned and entered the church as a monk to do penance. The story is told in Annales Mettenses priores. His son Waifer took an early inheritance, becoming duke of Aquitaine and ratifying the alliance with Lombardy. Waifer, deciding to honour it, repeated his father's decision, which he justified by arguing that any agreements with Charles Martel became invalid on Martel's death. Since Aquitaine was now Pepin's inheritance because of the earlier assistance given by Charles Martel, according to some, the latter and his son, the young Charles, hunted down Waifer, who could only conduct a guerrilla war, and executed him.\nAmong the contingents of the Frankish army were Bavarians under Tassilo III, Duke of Bavaria, an Agilofing, the hereditary Bavarian ducal family. Grifo had installed himself as Duke of Bavaria, but Pepin replaced him with a member of the ducal family yet a child, Tassilo, whose protector he had become after the death of his father. The loyalty of the Agilolfings was perpetually in question, but Pepin exacted numerous oaths of loyalty from Tassilo. However, the latter had married Liutperga, a daughter of Desiderius, king of Lombardy. At a critical point in the campaign, Tassilo left the field with all his Bavarians. Out of reach of Pepin, he repudiated all loyalty to Francia. Pepin had no chance to respond as he grew ill and died within a few weeks after Waifer's execution.\nThe first event of the brothers' reign was the uprising of the Aquitainians and Gascons in 769, in that territory split between the two kings. One year earlier, Pepin had finally defeated Waifer, Duke of Aquitaine, after waging a destructive, ten-year war against Aquitaine. Now, Hunald II led the Aquitainians as far north as Angoul\u00eame. Charles met Carloman, but Carloman refused to participate and returned to Burgundy. Charles went to war, leading an army to Bordeaux, where he built a fortified camp on the mound at Fronsac. Hunald was forced to flee to the court of Duke Lupus\u00a0II of Gascony. Lupus, fearing Charles, turned Hunald over in exchange for peace, and Hunald was put in a monastery. Gascon lords also surrendered, and Aquitaine and Gascony were finally fully subdued by the Franks.\nMarriage to Desiderata.\nThe brothers maintained lukewarm relations with the assistance of their mother Bertrada, but in 770 Charles signed a treaty with Duke Tassilo III of Bavaria and married a Lombard Princess (commonly known today as Desiderata), the daughter of King Desiderius, to surround Carloman with his own allies. Though Pope Stephen III first opposed the marriage with the Lombard princess, he found little to fear from a Frankish-Lombard alliance.\nLess than a year after his marriage, Charlemagne repudiated Desiderata and married a 13-year-old Swabian named Hildegard. The repudiated Desiderata returned to her father's court at Pavia. Her father's wrath was now aroused, and he would have gladly allied with Carloman to defeat Charles. Before any open hostilities could be declared, however, Carloman died on 5 December 771, apparently of natural causes. Carloman's widow Gerberga fled to Desiderius' court with her sons for protection.\nWives, concubines, and children.\nCharlemagne had eighteen children with seven of his ten known wives or concubines. Nonetheless, he had only four legitimate grandsons, the four sons of his fourth son, Louis. In addition, he had a grandson (Bernard of Italy, the only son of his third son, Pepin of Italy), who was illegitimate but included in the line of inheritance. Among his descendants are several royal dynasties, including the Habsburg, and Capetian dynasties. By consequence, most if not all established European noble families ever since can genealogically trace some of their background to Charlemagne.\nChildren.\nDuring the first peace of any substantial length (780\u2013782), Charles began to appoint his sons to positions of authority. In 781, during a visit to Rome, he made his two youngest sons kings, crowned by the Pope. The elder of these two, Carloman, was made the king of Italy, taking the Iron Crown that his father had first worn in 774, and in the same ceremony was renamed \"Pepin\" (not to be confused with Charlemagne's eldest, possibly illegitimate son, Pepin the Hunchback). The younger of the two, Louis, became King of Aquitaine. Charlemagne ordered Pepin and Louis to be raised in the customs of their kingdoms, and he gave their regents some control of their subkingdoms, but kept the real power, though he intended his sons to inherit their realms. He did not tolerate insubordination in his sons: in 792, he banished Pepin the Hunchback to Pr\u00fcm Abbey because the young man had joined a rebellion against him.\nCharles was determined to have his children educated, including his daughters, as his parents had instilled the importance of learning in him at an early age. His children were also taught skills in accord with their aristocratic status, which included training in riding and weaponry for his sons, and embroidery, spinning and weaving for his daughters.\nThe sons fought many wars on behalf of their father. Charles was mostly preoccupied with the Bretons, whose border he shared and who insurrected on at least two occasions and were easily put down. He also fought the Saxons on multiple occasions. In 805 and 806, he was sent into the B\u00f6hmerwald (modern Bohemia) to deal with the Slavs living there (Bohemian tribes, ancestors of the modern Czechs). He subjected them to Frankish authority and devastated the valley of the Elbe, forcing tribute from them. Pippin had to hold the Avar and Beneventan borders and fought the Slavs to his north. He was uniquely poised to fight the Byzantine Empire when that conflict arose after Charlemagne's imperial coronation and a Venetian rebellion. Finally, Louis was in charge of the Spanish March and fought the Duke of Benevento in southern Italy on at least one occasion. He took Barcelona in a great siege in 801.\nCharlemagne kept his daughters at home with him and refused to allow them to contract sacramental marriages (though he originally condoned an engagement between his eldest daughter Rotrude and Constantine VI of Byzantium, this engagement was annulled when Rotrude was 11). Charlemagne's opposition to his daughters' marriages may possibly have intended to prevent the creation of cadet branches of the family to challenge the main line, as had been the case with Tassilo of Bavaria. However, he tolerated their extramarital relationships, even rewarding their common-law husbands and treasuring the illegitimate grandchildren they produced for him. He also refused to believe stories of their wild behaviour. After his death the surviving daughters were banished from the court by their brother, the pious Louis, to take up residence in the convents they had been bequeathed by their father. At least one of them, Bertha, had a recognised relationship, if not a marriage, with Angilbert, a member of Charlemagne's court circle.\nItalian campaigns.\nConquest of the Lombard kingdom.\nAt his succession in 772, Pope Adrian I demanded the return of certain cities in the former exarchate of Ravenna in accordance with a promise at the succession of Desiderius. Instead, Desiderius took over certain papal cities and invaded the Pentapolis, heading for Rome. Adrian sent ambassadors to Charlemagne in autumn requesting he enforce the policies of his father, Pepin. Desiderius sent his own ambassadors denying the pope's charges. The ambassadors met at Thionville, and Charlemagne upheld the pope's side. Charlemagne demanded what the pope had requested, but Desiderius swore never to comply. Charlemagne and his uncle Bernard crossed the Alps in 773 and chased the Lombards back to Pavia, which they then besieged. Charlemagne temporarily left the siege to deal with Adelchis, son of Desiderius, who was raising an army at Verona. The young prince was chased to the Adriatic littoral and fled to Constantinople to plead for assistance from Constantine V, who was waging war with Bulgaria.\nThe siege lasted until the spring of 774 when Charlemagne visited the pope in Rome. There he confirmed his father's grants of land, with some later chronicles falsely claiming that he also expanded them, granting Tuscany, Emilia, Venice and Corsica. The pope granted him the title \"patrician\". He then returned to Pavia, where the Lombards were on the verge of surrendering. In return for their lives, the Lombards surrendered and opened the gates in early summer. Desiderius was sent to the abbey of Corbie, and his son Adelchis died in Constantinople, a patrician. Charles, unusually, had himself crowned with the Iron Crown and made the magnates of Lombardy pay homage to him at Pavia. Only Duke Arechis II of Benevento refused to submit and proclaimed independence. Charlemagne was then master of Italy as king of the Lombards. He left Italy with a garrison in Pavia and a few Frankish counts in place the same year.\nInstability continued in Italy. In 776, Dukes Hrodgaud of Friuli and Hildeprand of Spoleto rebelled. Charlemagne rushed back from Saxony and defeated the Duke of Friuli in battle; the Duke was slain. The Duke of Spoleto signed a treaty. Their co-conspirator, Arechis, was not subdued, and Adelchis, their candidate in Byzantium, never left that city. Northern Italy was now faithfully his.\nSouthern Italy.\nIn 787, Charlemagne directed his attention towards the Duchy of Benevento, where Arechis II was reigning independently with the self-given title of Princeps. Charlemagne's siege of Salerno forced Arechis into submission, and in return for peace, Arechis recognized Charlemagne's suzerainty and handed his son Grimoald III over as a hostage. After Arechis' death in 787, Grimoald was allowed to return to Benevento. In 788, the principality was invaded by Byzantine troops led by Adelchis, but his attempts were thwarted by Grimoald. The Franks assisted in the repulsion of Adelchis, but, in turn, attacked Benevento's territories several times, obtaining small gains, notably the annexation of Chieti to the duchy of Spoleto. Later, Grimoald tried to throw off Frankish suzerainty, but Charles' sons, Pepin of Italy and Charles the Younger, forced him to submit in 792.\nCarolingian expansion to the south.\nVasconia and the Pyrenees.\nThe destructive war led by Pepin in Aquitaine, although brought to a satisfactory conclusion for the Franks, proved the Frankish power structure south of the Loire was feeble and unreliable. After the defeat and death of Waifer in 768, while Aquitaine submitted again to the Carolingian dynasty, a new rebellion broke out in 769 led by Hunald II, a possible son of Waifer. He took refuge with the ally Duke Lupus II of Gascony, but probably out of fear of Charlemagne's reprisal, Lupus handed him over to the new King of the Franks to whom he pledged loyalty, which seemed to confirm the peace in the Basque area south of the Garonne. In the campaign of 769, Charlemagne seems to have followed a policy of \"overwhelming force\" and avoided a major pitched battle\nWary of new Basque uprisings, Charlemagne seems to have tried to contain Duke Lupus's power by appointing Seguin as the Count of Bordeaux (778) and other counts of Frankish background in bordering areas (, County of F\u00e9zensac). The Basque Duke, in turn, seems to have contributed decisively or schemed the Battle of Roncevaux Pass (referred to as \"Basque treachery\"). The defeat of Charlemagne's army in Roncevaux (778) confirmed his determination to rule directly by establishing the Kingdom of Aquitaine (ruled by Louis the Pious) based on a power base of Frankish officials, distributing lands among colonisers and allocating lands to the Church, which he took as an ally. A Christianisation programme was put in place across the high Pyrenees (778).\nThe new political arrangement for Vasconia did not sit well with local lords. As of 788 Adalric was fighting and capturing Chorson, Carolingian Count of Toulouse. He was eventually released, but Charlemagne, enraged at the compromise, decided to depose him and appointed his trustee William of Gellone. William, in turn, fought the Basques and defeated them after banishing Adalric (790).\nFrom 781 (Pallars, Ribagor\u00e7a) to 806 (Pamplona under Frankish influence), taking the County of Toulouse for a power base, Charlemagne asserted Frankish authority over the Pyrenees by subduing the south-western marches of Toulouse (790) and establishing vassal counties on the southern Pyrenees that were to make up the Marca Hispanica. As of 794, a Frankish vassal, the Basque lord Belasko (\"al-Galashki\", 'the Gaul') ruled \u00c1lava, but Pamplona remained under Cordovan and local control up to 806. Belasko and the counties in the Marca Hisp\u00e1nica provided the necessary base to attack the Andalusians (an expedition led by William Count of Toulouse and Louis the Pious to capture Barcelona in 801). Events in the Duchy of Vasconia (rebellion in Pamplona, count overthrown in Aragon, Duke Seguin of Bordeaux deposed, uprising of the Basque lords, etc.) were to prove it ephemeral upon Charlemagne's death.\nRoncesvalles campaign.\nAccording to the Muslim historian Ibn al-Athir, the Diet of Paderborn had received the representatives of the Muslim rulers of Zaragoza, Girona, Barcelona and Huesca. Their masters had been cornered in the Iberian peninsula by Abd ar-Rahman I, the Umayyad emir of Cordova. These \"Saracen\" (Moorish and Muwallad) rulers offered their homage to the king of the Franks in return for military support. Seeing an opportunity to extend Christendom and his own power, and believing the Saxons to be a fully conquered nation, Charlemagne agreed to go to Spain.\nIn 778, he led the Neustrian army across the Western Pyrenees, while the Austrasians, Lombards, and Burgundians passed over the Eastern Pyrenees. The armies met at Saragossa and Charlemagne received the homage of the Muslim rulers, Sulayman al-Arabi and Kasmin ibn Yusuf, but the city did not fall for him. Indeed, Charlemagne faced the toughest battle of his career. The Muslims forced him to retreat, so he decided to go home, as he could not trust the Basques, whom he had subdued by conquering Pamplona. He turned to leave Iberia, but as his army was crossing back through the Pass of Roncesvalles, one of the most famous events of his reign occurred: the Basques attacked and destroyed his rearguard and baggage train. The Battle of Roncevaux Pass, though less a battle than a skirmish, left many famous dead, including the seneschal Eggihard, the count of the palace Anselm, and the warden of the Breton March, Roland, inspiring the subsequent creation of \"The Song of Roland\" (\"La Chanson de Roland\"), regarded as the first major work in the French language.\nContact with Muslims.\nThe conquest of Italy brought Charlemagne in contact with Muslims who, at the time, controlled the Mediterranean. Charlemagne's eldest son, Pepin the Hunchback, was much occupied with Muslims in Italy. Charlemagne conquered Corsica and Sardinia at an unknown date and in 799 the Balearic Islands. The islands were often attacked by Muslim pirates, but the counts of Genoa and Tuscany (Boniface) controlled them with large fleets until the end of Charlemagne's reign. Charlemagne even had contact with the caliphal court in Baghdad. In 797 (or possibly 801), the caliph of Baghdad, Harun al-Rashid, presented Charlemagne with an Asian elephant named Abul-Abbas and a clock.\nWars with the Moors.\nIn Hispania, the struggle against Islam continued unabated throughout the latter half of his reign. Louis was in charge of the Spanish border. In 785, his men captured Girona permanently and extended Frankish control into the Catalan littoral for the duration of Charlemagne's reign (the area remained nominally Frankish until the Treaty of Corbeil in 1258). The Muslim chiefs in the northeast of Islamic Spain were constantly rebelling against Cordovan authority, and they often turned to the Franks for help. The Frankish border was slowly extended until 795, when Girona, Cardona, Ausona and Urgell were united into the new Spanish March, within the old duchy of Septimania.\nIn 797, Barcelona, the greatest city of the region, fell to the Franks when Zeid, its governor, rebelled against Cordova and, failing, handed it to them. The Umayyad authority recaptured it in 799. However, Louis of Aquitaine marched the entire army of his kingdom over the Pyrenees and besieged it for two years, wintering there from 800 to 801, when it capitulated. The Franks continued to press forward against the emir. They probably took Tarragona and forced the submission of Tortosa in 809. The last conquest brought them to the mouth of the Ebro and gave them raiding access to Valencia, prompting the Emir al-Hakam I to recognise their conquests in 813.\nEastern campaigns.\nSaxon Wars.\nCharlemagne was engaged in almost constant warfare throughout his reign, often at the head of his elite \"scara\" bodyguard squadrons. In the Saxon Wars, spanning thirty years and eighteen battles, he conquered Saxonia and proceeded to convert it to Christianity.\nThe Germanic Saxons were divided into four subgroups in four regions. Nearest to Austrasia was Westphalia and farthest away was Eastphalia. Between them was Engria and north of these three, at the base of the Jutland peninsula, was Nordalbingia.\nIn his first campaign, in 773, Charlemagne forced the Engrians to submit and cut down an Irminsul pillar near Paderborn. The campaign was cut short by his first expedition to Italy. He returned in 775, marching through Westphalia and conquering the Saxon fort at Sigiburg. He then crossed Engria, where he defeated the Saxons again. Finally, in Eastphalia, he defeated a Saxon force, and its leader Hessi converted to Christianity. Charlemagne returned through Westphalia, leaving encampments at Sigiburg and Eresburg, which had been important Saxon bastions. He then controlled Saxony with the exception of Nordalbingia, but Saxon resistance had not ended.\nFollowing his subjugation of the Dukes of Friuli and Spoleto, Charlemagne returned rapidly to Saxony in 776, where a rebellion had destroyed his fortress at Eresburg. The Saxons were once again defeated, but their main leader, Widukind, escaped to Denmark, his wife's home. Charlemagne built a new camp at Karlstadt. In 777, he called a national diet at Paderborn to integrate Saxony fully into the Frankish kingdom. Many Saxons were baptised as Christians.\nIn the summer of 779, he again invaded Saxony and reconquered Eastphalia, Engria and Westphalia. At a diet near Lippe, he divided the land into missionary districts and himself assisted in several mass baptisms (780). He then returned to Italy and, for the first time, the Saxons did not immediately revolt. Saxony was peaceful from 780 to 782.\nHe returned to Saxony in 782 and instituted a code of law and appointed counts, both Saxon and Frank. The laws were draconian on religious issues; for example, the \"Capitulatio de partibus Saxoniae\" prescribed death to Saxon pagans who refused to convert to Christianity. This led to renewed conflict. That year, in autumn, Widukind returned and led a new revolt. In response, at Verden in Lower Saxony, Charlemagne is recorded as having ordered the execution of 4,500 Saxon prisoners by beheading, known as the Massacre of Verden (\"Verdener Blutgericht\"). The killings triggered three years of renewed bloody warfare. During this war, the East Frisians between the Lauwers and the Weser joined the Saxons in revolt and were finally subdued. The war ended with Widukind accepting baptism. The Frisians afterwards asked for missionaries to be sent to them and a bishop of their own nation, Ludger, was sent. Charlemagne also promulgated a law code, the \"Lex Frisonum\", as he did for most subject peoples.\nThereafter, the Saxons maintained the peace for seven years, but in 792 Westphalia again rebelled. The Eastphalians and Nordalbingians joined them in 793, but the insurrection was unpopular and was put down by 794. An Engrian rebellion followed in 796, but the presence of Charlemagne, Christian Saxons and Slavs quickly crushed it. The last insurrection occurred in 804, more than thirty years after Charlemagne's first campaign against them, but also failed. According to Einhard:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The war that had lasted so many years was at length ended by their acceding to the terms offered by the King; which were renunciation of their national religious customs and the worship of devils, acceptance of the sacraments of the Christian faith and religion, and union with the Franks to form one people.\nSubmission of Bavaria.\nBy 774, Charlemagne had invaded the Kingdom of Lombardy, and he later annexed the Lombardian territories and assumed its crown, placing the Papal States under Frankish protection. The Duchy of Spoleto south of Rome was acquired in 774, while in the central western parts of Europe, the Duchy of Bavaria was absorbed and the Bavarian policy continued of establishing tributary marches, (borders protected in return for tribute or taxes) among the Slavic Sorbs and Czechs. The remaining power confronting the Franks in the east were the Avars. However, Charlemagne acquired other Slavic areas, including Bohemia, Moravia, Austria and Croatia.\nIn 789, Charlemagne turned to Bavaria. He claimed that Tassilo III, Duke of Bavaria was an unfit ruler, due to his oath-breaking. The charges were exaggerated, but Tassilo was deposed anyway and put in the monastery of Jumi\u00e8ges. In 794, Tassilo was made to renounce any claim to Bavaria for himself and his family (the Agilolfings) at the synod of Frankfurt; he formally handed over to the king all of the rights he had held. Bavaria was subdivided into Frankish counties, as had been done with Saxony.\nAvar campaigns.\nIn 788, the Avars, an Asian nomadic group that had settled down in what is today Hungary (Einhard called them Huns), invaded Friuli and Bavaria. Charlemagne was preoccupied with other matters until 790 when he marched down the Danube and ravaged Avar territory to the Gy\u0151r. A Lombard army under Pippin then marched into the Drava valley and ravaged Pannonia. The campaigns ended when the Saxons revolted again in 792.\nFor the next two years, Charlemagne was occupied, along with the Slavs, against the Saxons. Pippin and Duke Eric of Friuli continued, however, to assault the Avars' ring-shaped strongholds. The great Ring of the Avars, their capital fortress, was taken twice. The booty was sent to Charlemagne at his capital, Aachen, and redistributed to his followers and to foreign rulers, including King Offa of Mercia. Soon the Avar tuduns had lost the will to fight and travelled to Aachen to become vassals to Charlemagne and to become Christians. Charlemagne accepted their surrender and sent one native chief, baptised Abraham, back to Avaria with the ancient title of khagan. Abraham kept his people in line, but in 800, the Bulgarians under Khan Krum attacked the remains of the Avar state.\nIn 803, Charlemagne sent a Bavarian army into Pannonia, defeating and bringing an end to the Avar confederation.\nIn November of the same year, Charlemagne went to Regensburg where the Avar leaders acknowledged him as their ruler. In 805, the Avar khagan, who had already been baptised, went to Aachen to ask permission to settle with his people south-eastward from Vienna. The Transdanubian territories became integral parts of the Frankish realm, which was abolished by the Magyars in 899\u2013900.\nNortheast Slav expeditions.\nIn 789, in recognition of his new pagan neighbours, the Slavs, Charlemagne marched an Austrasian-Saxon army across the Elbe into Obotrite territory. The Slavs ultimately submitted, led by their leader Witzin. Charlemagne then accepted the surrender of the Veleti under Dragovit and demanded many hostages. He also demanded permission to send missionaries into this pagan region unmolested. The army marched to the Baltic before turning around and marching to the Rhine, winning much booty with no harassment. The tributary Slavs became loyal allies. In 795, when the Saxons broke the peace, the Abotrites and Veleti rebelled with their new ruler against the Saxons. Witzin died in battle and Charlemagne avenged him by harrying the Eastphalians on the Elbe. Thrasuco, his successor, led his men to conquest over the Nordalbingians and handed their leaders over to Charlemagne, who honoured him. The Abotrites remained loyal until Charles' death and fought later against the Danes.\nSoutheast Slav expeditions.\nWhen Charlemagne incorporated much of Central Europe, he brought the Frankish state face to face with the Avars and Slavs in the southeast. The most southeast Frankish neighbours were Croats, who settled in Lower Pannonia and Duchy of Croatia. While fighting the Avars, the Franks had called for their support. During the 790s, he won a major victory over them in 796. Duke Vojnomir of Lower Pannonia aided Charlemagne, and the Franks made themselves overlords over the Croats of northern Dalmatia, Slavonia and Pannonia.\nThe Frankish commander Eric of Friuli wanted to extend his dominion by conquering the Littoral Croat Duchy. During that time, Dalmatian Croatia was ruled by Duke Vi\u0161eslav of Croatia. In the Battle of Trsat, the forces of Eric fled their positions and were routed by the forces of Vi\u0161eslav. Eric was among those killed which was a great blow for the Carolingian Empire.\nCharlemagne also directed his attention to the Slavs to the west of the Avar khaganate: the Carantanians and Carniolans. These people were subdued by the Lombards and Bavarii and made tributaries, but were never fully incorporated into the Frankish state.\nImperium.\nCoronation.\nIn 799, Pope Leo III had been assaulted by some of the Romans, who tried to pull out his eyes and tear out his tongue. Leo escaped and fled to Charlemagne at Paderborn. Charlemagne, advised by scholar Alcuin, travelled to Rome, in November 800 and held a synod. On 23 December, Leo swore an oath of innocence to Charlemagne. His position having thereby been weakened, the Pope sought to restore his status. Two days later, at Mass, on Christmas Day (25 December), when Charlemagne knelt at the altar to pray, the Pope crowned him \"Imperator Romanorum\" (\"Emperor of the Romans\") in Saint Peter's Basilica. In so doing, the Pope rejected the legitimacy of Empress Irene of Constantinople:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;When Odoacer compelled the abdication of Romulus Augustulus, he did not abolish the Western Empire as a separate power, but caused it to be reunited with or sink into the Eastern, so that from that time there was a single undivided Roman Empire\u00a0... [Pope Leo III and Charlemagne], like their predecessors, held the Roman Empire to be one and indivisible, and proposed by the coronation of [Charlemagne] not to proclaim a severance of the East and West\u00a0... they were not revolting against a reigning sovereign, but legitimately filling up the place of the deposed Constantine VI\u00a0... [Charlemagne] was held to be the legitimate successor, not of Romulus Augustulus, but of Constantine VI\u00a0...Charlemagne's coronation as Emperor, though intended to represent the continuation of the unbroken line of Emperors from Augustus to Constantine VI, had the effect of setting up two separate (and often opposing) Empires and two separate claims to imperial authority. It led to war in 802, and for centuries to come, the Emperors of both West and East would make competing claims of sovereignty over the whole.\nEinhard says that Charlemagne was ignorant of the Pope's intent and did not want any such coronation:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[H]e at first had such an aversion that he declared that he would not have set foot in the Church the day that they [the imperial titles] were conferred, although it was a great feast-day, if he could have foreseen the design of the Pope.\nA number of modern scholars, however, suggest that Charlemagne was indeed aware of the coronation; certainly, he cannot have missed the bejewelled crown waiting on the altar when he came to pray\u2014something even contemporary sources support.\nDebate.\nHistorians have debated for centuries whether Charlemagne was aware before the coronation of the Pope's intention to crown him Emperor (Charlemagne declared that he would not have entered Saint Peter's had he known, according to chapter twenty-eight of Einhard's \"Vita Karoli Magni\"), but that debate obscured the more significant question of \"why\" the Pope granted the title and why Charlemagne accepted it.\nCollins points out \"[t]hat the motivation behind the acceptance of the imperial title was a romantic and antiquarian interest in reviving the Roman Empire is highly unlikely.\" For one thing, such romance would not have appealed either to Franks or Roman Catholics at the turn of the ninth century, both of whom viewed the Classical heritage of the Roman Empire with distrust. The Franks took pride in having \"fought against and thrown from their shoulders the heavy yoke of the Romans\" and \"from the knowledge gained in baptism, clothed in gold and precious stones the bodies of the holy martyrs whom the Romans had killed by fire, by the sword and by wild animals\", as Pepin\u00a0III described it in a law of 763 or 764.\nFurthermore, the new title\u2014carrying with it the risk that the new emperor would \"make drastic changes to the traditional styles and procedures of government\" or \"concentrate his attentions on Italy or on Mediterranean concerns more generally\"\u2014risked alienating the Frankish leadership.\nFor both the Pope and Charlemagne, the Roman Empire remained a significant power in European politics at this time. The Byzantine Empire, based in Constantinople, continued to hold a substantial portion of Italy, with borders not far south of Rome. Charles' sitting in judgment of the Pope could be seen as usurping the prerogatives of the Emperor in Constantinople:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;By whom, however, could he [the Pope] be tried? Who, in other words, was qualified to pass judgement on the Vicar of Christ? In normal circumstances the only conceivable answer to that question would have been the Emperor at Constantinople; but the imperial throne was at this moment occupied by Irene. That the Empress was notorious for having blinded and murdered her own son was, in the minds of both Leo and Charles, almost immaterial: it was enough that she was a woman. The female sex was known to be incapable of governing, and by the old Salic tradition was debarred from doing so. As far as Western Europe was concerned, the Throne of the Emperors was vacant: Irene's claim to it was merely an additional proof, if any were needed, of the degradation into which the so-called Roman Empire had fallen.\nFor the Pope, then, there was \"no living Emperor at that time\" though Henri Pirenne disputes this saying that the coronation \"was not in any sense explained by the fact that at this moment a woman was reigning in Constantinople\". Nonetheless, the Pope took the extraordinary step of creating one. The papacy had since 727 been in conflict with Irene's predecessors in Constantinople over a number of issues, chiefly the continued Byzantine adherence to the doctrine of iconoclasm, the destruction of Christian images; while from 750, the secular power of the Byzantine Empire in central Italy had been nullified.\nBy bestowing the Imperial crown upon Charlemagne, the Pope arrogated to himself \"the right to appoint\u00a0... the Emperor of the Romans,\u00a0... establishing the imperial crown as his own personal gift but simultaneously granting himself implicit superiority over the Emperor whom he had created.\" And \"because the Byzantines had proved so unsatisfactory from every point of view\u2014political, military and doctrinal\u2014he would select a westerner: the one man who by his wisdom and statesmanship and the vastness of his dominions\u00a0... stood out head and shoulders above his contemporaries.\"\nWith Charlemagne's coronation, therefore, \"the Roman Empire remained, so far as either of them [Charlemagne and Leo] were concerned, one and indivisible, with Charles as its Emperor\", though there can have been \"little doubt that the coronation, with all that it implied, would be furiously contested in Constantinople\".\nAlcuin writes hopefully in his letters of an \"Imperium Christianum\" (\"Christian Empire\"), wherein, \"just as the inhabitants of the [Roman Empire] had been united by a common Roman citizenship\", presumably this new empire would be united by a common Christian faith. This is the view of Pirenne when he says \"Charles was the Emperor of the \"ecclesia\" as the Pope conceived it, of the Roman Church, regarded as the universal Church\". The \"Imperium Christianum\" was further supported at a number of synods all across Europe by Paulinus of Aquileia.\nWhat is known, from the Byzantine chronicler Theophanes, is that Charlemagne's reaction to his coronation was to take the initial steps towards securing the Constantinopolitan throne by sending envoys of marriage to Irene, and that Irene reacted somewhat favourably to them.\nDistinctions between the universalist and localist conceptions of the empire remain controversial among historians. According to the former, the empire was a universal monarchy, a \"commonwealth of the whole world, whose sublime unity transcended every minor distinction\"; and the emperor \"was entitled to the obedience of Christendom\". According to the latter, the emperor had no ambition for universal dominion; his realm was limited in the same way as that of every other ruler, and when he made more far-reaching claims his object was normally to ward off the attacks either of the Pope or of the Byzantine emperor. According to this view, also, the origin of the empire is to be explained by specific local circumstances rather than by overarching theories.\nAccording to Ohnsorge, for a long time, it had been the custom of Byzantium to designate the German princes as spiritual \"sons\" of the Romans. What might have been acceptable in the fifth century had become provoking and insulting to the Franks in the eighth century. Charles came to believe that the Roman emperor, who claimed to head the world hierarchy of states, was, in reality, no greater than Charles himself, a king as other kings, since beginning in 629 he had entitled himself \"Basileus\" (translated literally as \"king\"). Ohnsorge finds it significant that the chief wax seal of Charles, which bore only the inscription: \"Christe, protege Carolum regem Francorum\" [Christ, protect Charles, king of the Franks], was used from 772 to 813, even during the imperial period and was not replaced by a special imperial seal; indicating that Charles felt himself to be just the king of the Franks. Finally, Ohnsorge points out that in the spring of 813 at Aachen Charles crowned his only surviving son, Louis, as the emperor without recourse to Rome with only the acclamation of his Franks. The form in which this acclamation was offered was Frankish-Christian rather than Roman. This implies both independence from Rome and a Frankish (non-Roman) understanding of empire.\nMayr-Harting argues that the Imperial title was Charlemagne's face-saving offer to incorporate the recently conquered Saxons. Since the Saxons did not have an institution of kingship for their own ethnicity, claiming the right to rule them as King of the Saxons was not possible. Hence, it is argued, Charlemagne used the supra-ethnic Imperial title to incorporate the Saxons, which helped to cement the diverse peoples under his rule.\nImperial title.\nCharlemagne used these circumstances to claim that he was the \"renewer of the Roman Empire\", which had declined under the Byzantines. In his official charters, Charles preferred the style \"Karolus serenissimus Augustus a Deo coronatus magnus pacificus imperator Romanum gubernans imperium\" (\"Charles, most serene Augustus crowned by God, the great, peaceful emperor ruling the Roman empire\") to the more direct \"Imperator Romanorum\" (\"Emperor of the Romans\").\nThe title of Emperor remained in the Carolingian family for years to come, but divisions of territory and in-fighting over supremacy of the Frankish state weakened its significance. The papacy itself never forgot the title nor abandoned the right to bestow it. When the family of Charles ceased to produce worthy heirs, the Pope gladly crowned whichever Italian magnate could best protect him from his local enemies. The empire would remain in continuous existence for over a millennium, as the Holy Roman Empire, a true imperial successor to Charles.\nImperial diplomacy.\nThe iconoclasm of the Byzantine Isaurian Dynasty was endorsed by the Franks. The Second Council of Nicaea reintroduced the veneration of icons under Empress Irene. The council was not recognised by Charlemagne since no Frankish emissaries had been invited, even though Charlemagne ruled more than three provinces of the classical Roman empire and was considered equal in rank to the Byzantine emperor. And while the Pope supported the reintroduction of the iconic veneration, he politically digressed from Byzantium. He certainly desired to increase the influence of the papacy, to honour his saviour Charlemagne, and to solve the constitutional issues then most troubling to European jurists in an era when Rome was not in the hands of an emperor. Thus, Charlemagne's assumption of the imperial title was not a usurpation in the eyes of the Franks or Italians. It was, however, seen as such in Byzantium, where it was protested by Irene and her successor Nikephoros I\u2014neither of whom had any great effect in enforcing their protests.\nThe East Romans, however, still held several territories in Italy: Venice (what was left of the Exarchate of Ravenna), Reggio (in Calabria), Otranto (in Apulia), and Naples (the \"Ducatus Neapolitanus\"). These regions remained outside of Frankish hands until 804, when the Venetians, torn by infighting, transferred their allegiance to the Iron Crown of Pippin, Charles' son. The \"Pax Nicephori\" ended. Nicephorus ravaged the coasts with a fleet, initiating the only instance of war between the Byzantines and the Franks. The conflict lasted until 810 when the pro-Byzantine party in Venice gave their city back to the Byzantine Emperor, and the two emperors of Europe made peace: Charlemagne received the Istrian peninsula and in 812 the emperor Michael I Rangabe recognised his status as Emperor, although not necessarily as \"Emperor of the Romans\".\nDanish attacks.\nAfter the conquest of Nordalbingia, the Frankish frontier was brought into contact with Scandinavia. The pagan Danes, \"a\u00a0race almost unknown to his ancestors, but destined to be only too well known to his sons\" as Charles Oman described them, inhabiting the Jutland peninsula, had heard many stories from Widukind and his allies who had taken refuge with them about the dangers of the Franks and the fury which their Christian king could direct against pagan neighbours.\nIn 808, the king of the Danes, Godfred, expanded the vast Danevirke across the isthmus of Schleswig. This defence, last employed in the Danish-Prussian War of 1864, was at its beginning a long earthenwork rampart. The Danevirke protected Danish land and gave Godfred the opportunity to harass Frisia and Flanders with pirate raids. He also subdued the Frank-allied Veleti and fought the Abotrites.\nGodfred invaded Frisia, joked of visiting Aachen, but was murdered before he could do any more, either by a Frankish assassin or by one of his own men. Godfred was succeeded by his nephew Hemming, who concluded the Treaty of Heiligen with Charlemagne in late 811.\nDeath.\nIn 813, Charlemagne called Louis the Pious, king of Aquitaine, his only surviving legitimate son, to his court. There Charlemagne crowned his son as co-emperor and sent him back to Aquitaine. He then spent the autumn hunting before returning to Aachen on 1 November. In January, he fell ill with pleurisy. In deep depression (mostly because many of his plans were not yet realised), he took to his bed on 21 January and as Einhard tells it:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He died January twenty-eighth, the seventh day from the time that he took to his bed, at nine o'clock in the morning, after partaking of the Holy Communion, in the seventy-second year of his age and the forty-seventh of his reign.\nHe was buried that same day, in Aachen Cathedral. The earliest surviving \"planctus\", the \"Planctus de obitu Karoli\", was composed by a monk of Bobbio, which he had patronised. A later story, told by Otho of Lomello, Count of the Palace at Aachen in the time of Emperor Otto III, would claim that he and Otto had discovered Charlemagne's tomb: Charlemagne, they claimed, was seated upon a throne, wearing a crown and holding a sceptre, his flesh almost entirely incorrupt. In 1165, Emperor Frederick I re-opened the tomb again and placed the emperor in a sarcophagus beneath the floor of the cathedral. In 1215 Emperor Frederick II re-interred him in a casket made of gold and silver known as the Karlsschrein.\nCharlemagne's death emotionally affected many of his subjects, particularly those of the literary clique who had surrounded him at Aachen. An anonymous monk of Bobbio lamented:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;From the lands where the sun rises to western shores, people are crying and wailing\u00a0... the Franks, the Romans, all Christians, are stung with mourning and great worry\u00a0... the young and old, glorious nobles, all lament the loss of their Caesar\u00a0... the world laments the death of Charles\u00a0... O Christ, you who govern the heavenly host, grant a peaceful place to Charles in your kingdom. Alas for miserable me.\nLouis succeeded him as Charles had intended. He left a testament allocating his assets in 811 that was not updated prior to his death. He left most of his wealth to the Church, to be used for charity. His empire lasted only another generation in its entirety; its division, according to custom, between Louis's own sons after their father's death laid the foundation for the modern states of Germany and France.\nAdministration.\nOrganisation.\nThe Carolingian king exercised the \"bannum\", the right to rule and command. Under the Franks, it was a royal prerogative but could be delegated. He had supreme jurisdiction in judicial matters, made legislation, led the army, and protected both the Church and the poor. His administration was an attempt to organise the kingdom, church and nobility around him. As an administrator, Charlemagne stands out for his many reforms: monetary, governmental, military, cultural and ecclesiastical. He is the main protagonist of the \"Carolingian Renaissance\".\nMilitary.\nCharlemagne's success rested primarily on novel siege technologies and excellent logistics rather than the long-claimed \"cavalry revolution\" led by Charles Martel in 730s. However, the stirrup, which made the \"shock cavalry\" lance charge possible, was not introduced to the Frankish kingdom until the late eighth century.\nHorses were used extensively by the Frankish military because they provided a quick, long-distance method of transporting troops, which was critical to building and maintaining the large empire.\nEconomic and monetary reforms.\nCharlemagne had an important role in determining Europe's immediate economic future. Pursuing his father's reforms, Charlemagne abolished the monetary system based on the gold . Instead, he and the Anglo-Saxon King Offa of Mercia took up Pippin's system for pragmatic reasons, notably a shortage of the metal.\nThe gold shortage was a direct consequence of the conclusion of peace with Byzantium, which resulted in ceding Venice and Sicily to the East and losing their trade routes to Africa. The resulting standardisation economically harmonised and unified the complex array of currencies that had been in use at the commencement of his reign, thus simplifying trade and commerce.\nCharlemagne established a new standard, the (from the Latin , the modern pound), which was based upon a pound of silver\u2014a unit of both money and weight\u2014worth 20 sous (from the Latin [which was primarily an accounting device and never actually minted], the modern shilling) or 240 (from the Latin , the modern penny). During this period, the and the were counting units; only the was a coin of the realm.\nCharlemagne instituted principles for accounting practice by means of the Capitulare de villis of 802, which laid down strict rules for the way in which incomes and expenses were to be recorded.\nCharlemagne applied this system to much of the European continent, and Offa's standard was voluntarily adopted by much of England. After Charlemagne's death, continental coinage degraded, and most of Europe resorted to using the continued high-quality English coin until about 1100.\nJews in Charlemagne's realm.\nEarly in Charlemagne's rule he tacitly allowed Jews to monopolise money lending. He invited Italian Jews to immigrate, as royal clients independent of the feudal landowners, and form trading communities in the agricultural regions of Provence and the Rhineland. Their trading activities augmented the otherwise almost exclusively agricultural economies of these regions. His personal physician was Jewish, and he employed a Jew named Isaac as his personal representative to the Muslim caliphate of Baghdad.\nEducation reforms.\nPart of Charlemagne's success as a warrior, an administrator and ruler can be traced to his admiration for learning and education. His reign is often referred to as the Carolingian Renaissance because of the flowering of scholarship, literature, art and architecture that characterise it. Charlemagne came into contact with the culture and learning of other countries (especially Moorish Spain, Anglo-Saxon England, and Lombard Italy) due to his vast conquests. He greatly increased the provision of monastic schools and scriptoria (centres for book-copying) in Francia.\nCharlemagne was a lover of books, sometimes having them read to him during meals. He was thought to enjoy the works of Augustine of Hippo. His court played a key role in producing books that taught elementary Latin and different aspects of the church. It also played a part in creating a royal library that contained in-depth works on language and Christian faith.\nCharlemagne encouraged clerics to translate Christian creeds and prayers into their respective vernaculars as well to teach grammar and music. Due to the increased interest of intellectual pursuits and the urging of their king, the monks accomplished so much copying that almost every manuscript from that time was preserved. At the same time, at the urging of their king, scholars were producing more secular books on many subjects, including history, poetry, art, music, law, theology, etc. Due to the increased number of titles, private libraries flourished. These were mainly supported by aristocrats and churchmen who could afford to sustain them. At Charlemagne's court, a library was founded and a number of copies of books were produced, to be distributed by Charlemagne. Book production was completed slowly by hand and took place mainly in large monastic libraries. Books were so in demand during Charlemagne's time that these libraries lent out some books, but only if that borrower offered valuable collateral in return.\nMost of the surviving works of classical Latin were copied and preserved by Carolingian scholars. Indeed, the earliest manuscripts available for many ancient texts are Carolingian. It is almost certain that a text which survived to the Carolingian age survives still.\nThe pan-European nature of Charlemagne's influence is indicated by the origins of many of the men who worked for him: Alcuin, an Anglo-Saxon from York; Theodulf, a Visigoth, probably from Septimania; Paul the Deacon, Lombard; Italians Peter of Pisa and Paulinus of Aquileia; and Franks Angilbert, Angilram, Einhard and Waldo of Reichenau.\nCharlemagne promoted the liberal arts at court, ordering that his children and grandchildren be well-educated, and even studying himself (in a time when even leaders who promoted education did not take time to learn themselves) under the tutelage of Peter of Pisa, from whom he learned grammar; Alcuin, with whom he studied rhetoric, dialectic (logic), and astronomy (he was particularly interested in the movements of the stars); and Einhard, who tutored him in arithmetic.\nHis great scholarly failure, as Einhard relates, was his inability to write: when in his old age he attempted to learn\u2014practising the formation of letters in his bed during his free time on books and wax tablets he hid under his pillow\u2014\"his effort came too late in life and achieved little success\", and his ability to read\u2014which Einhard is silent about, and which no contemporary source supports\u2014has also been called into question.\nIn 800, Charlemagne enlarged the hostel at the Muristan in Jerusalem and added a library to it. He certainly had not been personally in Jerusalem.\nChurch reforms.\nCharlemagne expanded the reform Church's programme unlike his father, Pippin, and uncle, Carloman. The deepening of the spiritual life was later to be seen as central to public policy and royal governance. His reform focused on strengthening the church's power structure, improving clergy's skill and moral quality, standardising liturgical practices, improvements on the basic tenets of the faith and the rooting out of paganism. His authority extended over church and state. He could discipline clerics, control ecclesiastical property and define orthodox doctrine. Despite the harsh legislation and sudden change, he had developed support from clergy who approved his desire to deepen the piety and morals of his subjects.\nIn 809\u2013810, Charlemagne called a church council in Aachen, which confirmed the unanimous belief in the West that the Holy Spirit proceeds from the Father and the Son (\"ex Patre Filioque\") and sanctioned inclusion in the Nicene Creed of the phrase \"Filioque\" (and the Son). For this Charlemagne sought the approval of Pope Leo III. The Pope, while affirming the doctrine and approving its use in teaching, opposed its inclusion in the text of the Creed as adopted in the 381 First Council of Constantinople. This spoke of the procession of the Holy Spirit from the Father, without adding phrases such as \"and the Son\", \"through the Son\", or \"alone\". Stressing his opposition, the Pope had the original text inscribed in Greek and Latin on two heavy shields that were displayed in Saint Peter's Basilica.\nWriting reforms.\nDuring Charles' reign, the Roman half uncial script and its cursive version, which had given rise to various continental minuscule scripts, were combined with features from the insular scripts in use in Irish and English monasteries. Carolingian minuscule was created partly under the patronage of Charlemagne. Alcuin, who ran the palace school and scriptorium at Aachen, was probably a chief influence.\nThe revolutionary character of the Carolingian reform, however, can be overemphasised; efforts at taming Merovingian and Germanic influence had been underway before Alcuin arrived at Aachen. The new minuscule was disseminated first from Aachen and later from the influential scriptorium at Tours, where Alcuin retired as an abbot.\nPolitical reforms.\nCharlemagne engaged in many reforms of Frankish governance while continuing many traditional practices, such as the division of the kingdom among sons.\nDivisio regnorum.\nIn 806, Charlemagne first made provision for the traditional division of the empire on his death. For Charles the Younger he designated Austrasia and Neustria, Saxony, Burgundy and Thuringia. To Pippin, he gave Italy, Bavaria, and Swabia. Louis received Aquitaine, the Spanish March and Provence. The imperial title was not mentioned, which led to the suggestion that, at that particular time, Charlemagne regarded the title as an honorary achievement that held no hereditary significance.\nPepin died in 810 and Charles in 811. Charlemagne then reconsidered the matter, and in 813, crowned his youngest son, Louis, co-emperor and co-King of the Franks, granting him a half-share of the empire and the rest upon Charlemagne's own death. The only part of the Empire that Louis was not promised was Italy, which Charlemagne specifically bestowed upon Pippin's illegitimate son Bernard.\nAppearance.\nManner.\nEinhard tells in his twenty-fourth chapter: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Charles was temperate in eating, and particularly so in drinking, for he abominated drunkenness in anybody, much more in himself and those of his household; but he could not easily abstain from food, and often complained that fasts injured his health. He very rarely gave entertainments, only on great feast-days, and then to large numbers of people. His meals ordinarily consisted of four courses, not counting the roast, which his huntsmen used to bring in on the spit; he was more fond of this than of any other dish. While at table, he listened to reading or music. The subjects of the readings were the stories and deeds of olden time: he was fond, too, of St. Augustine's books, and especially of the one titled \"The City of God\".Charlemagne threw grand banquets and feasts for special occasions such as religious holidays and four of his weddings. When he was not working, he loved Christian books, horseback riding, swimming, bathing in natural hot springs with his friends and family, and hunting. Franks were well known for horsemanship and hunting skills. Charles was a light sleeper and would stay in his bed chambers for entire days at a time due to restless nights. During these days, he would not get out of bed when a quarrel occurred in his kingdom, instead summoning all members of the situation into his bedroom to be given orders. Einhard tells again in the twenty-fourth chapter: \"In summer after the midday meal, he would eat some fruit, drain a single cup, put off his clothes and shoes, just as he did for the night, and rest for two or three hours. He was in the habit of awaking and rising from bed four or five times during the night.\"\nLanguage.\nCharlemagne probably spoke a Rhenish Franconian dialect.\nHe also spoke Latin and had at least some understanding of Greek, according to Einhard (\"Grecam vero melius intellegere quam pronuntiare poterat\", \"he could understand Greek better than he could speak it\").\nThe largely fictional account of Charlemagne's Iberian campaigns by Pseudo-Turpin, written some three centuries after his death, gave rise to the legend that the king also spoke Arabic.\nPhysical appearance.\nCharlemagne's personal appearance is known from a good description by Einhard after his death in the biography \"Vita Karoli Magni\". Einhard states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He was heavily built, sturdy, and of considerable stature, although not exceptionally so, since his height was seven times the length of his own foot. He had a round head, large and lively eyes, a slightly larger nose than usual, white but still attractive hair, a bright and cheerful expression, a short and fat neck, and he enjoyed good health, except for the fevers that affected him in the last few years of his life. Towards the end, he dragged one leg. Even then, he stubbornly did what he wanted and refused to listen to doctors, indeed he detested them, because they wanted to persuade him to stop eating roast meat, as was his wont, and to be content with boiled meat.\nThe physical portrait provided by Einhard is confirmed by contemporary depictions such as coins and his bronze statuette kept in the Louvre. In 1861, Charlemagne's tomb was opened by scientists who reconstructed his skeleton and estimated it to be measured . A 2010 estimate of his height from an X-ray and CT scan of his tibia was . This puts him in the 99th percentile of height for his period, given that average male height of his time was . The width of the bone suggested he was gracile in body build.\nDress.\nCharlemagne wore the traditional costume of the Frankish people, described by Einhard thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He used to wear the national, that is to say, the Frank, dress\u2014next his skin a linen shirt and linen breeches, and above these a tunic fringed with silk; while hose fastened by bands covered his lower limbs, and shoes his feet, and he protected his shoulders and chest in winter by a close-fitting coat of otter or marten skins.\nHe wore a blue cloak and always carried a sword typically of a golden or silver hilt. He wore intricately jeweled swords to banquets or ambassadorial receptions. Nevertheless:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;He despised foreign costumes, however handsome, and never allowed himself to be robed in them, except twice in Rome, when he donned the Roman tunic, chlamys, and shoes; the first time at the request of Pope Hadrian, the second to gratify Leo, Hadrian's successor.\nOn great feast days, he wore embroidery and jewels on his clothing and shoes. He had a golden buckle for his cloak on such occasions and would appear with his great diadem, but he despised such apparel according to Einhard, and usually dressed like the common people.\nHomes.\nCharlemagne had residences across his kingdom, including numerous private estates that were governed in accordance with the Capitulare de villis. A 9th-century document detailing the inventory of an estate at Asnapium listed amounts of livestock, plants and vegetables and kitchenware including cauldrons, drinking cups, brass kettles and firewood. The manor contained seventeen houses built inside the courtyard for nobles and family members and was separated from its supporting villas.\nBeatification.\nCharlemagne was revered as a saint in the Holy Roman Empire and some other locations after the twelfth century. The Apostolic See did not recognise his invalid canonisation by Antipope Paschal III, done to gain the favour of Frederick Barbarossa in 1165. The Apostolic See annulled all of Paschal's ordinances at the Third Lateran Council in 1179. He is not enumerated among the 28 saints named \"Charles\" in the \"Roman Martyrology\". His beatification has been acknowledged as \"cultus confirmed\" and is celebrated on 28 January.\nCultural impact.\nMiddle Ages.\nThe author of the \"Visio Karoli Magni\" written around 865 uses facts gathered apparently from Einhard and his own observations on the decline of Charlemagne's family after the dissensions war (840\u201343) as the basis for a visionary tale of Charles' meeting with a prophetic spectre in a dream.\nCharlemagne was a model knight as one of the Nine Worthies who enjoyed an important legacy in European culture. One of the great medieval literary cycles, the Charlemagne cycle or the \"Matter of France\", centres on his deeds\u2014the Emperor with the Flowing Beard of \"Roland\" fame\u2014and his historical commander of the border with Brittany, Roland, and the 12 paladins. These are analogous to, and inspired the myth of, the Knights of the Round Table of King Arthur's court. Their tales constitute the first \"chansons de geste\".\nIn the 12th century, Geoffrey of Monmouth based his stories of Arthur largely on stories of Charlemagne. During the Hundred Years' War in the 14th century, there was considerable cultural conflict in England, where the Norman rulers were aware of their French roots and identified with Charlemagne, Anglo-Saxon natives felt more affinity for Arthur, whose own legends were relatively primitive. Therefore, storytellers in England adapted legends of Charlemagne and his 12 Peers to the Arthurian tales.\nIn the \"Divine Comedy\", the spirit of Charlemagne appears to Dante in the , among the other \"warriors of the faith\".\n19th century.\nCharlemagne's capitularies were quoted by Pope Benedict XIV in his apostolic constitution 'Providas' against freemasonry: \"For in no way are we able to understand how they can be faithful to us, who have shown themselves unfaithful to God and disobedient to their Priests\".\nCharlemagne appears in \"Adelchi\", the second tragedy by Italian writer Alessandro Manzoni, first published in 1822.\nIn 1867, an equestrian statue of Charlemagne was made by Louis Jehotte and was inaugurated in 1868 on the Boulevard d'Avroy in Li\u00e8ge. In the niches of the neo-roman pedestal are six statues of Charlemagne's ancestors (Sainte Begge, P\u00e9pin de Herstal, Charles Martel, Bertrude, P\u00e9pin de Landen and P\u00e9pin le Bref).\nThe North Wall Frieze in the courtroom of the Supreme Court of the United States depicts Charlemagne as a legal reformer.\n20th century.\nThe city of Aachen has, since 1949, awarded an international prize (called the \"Karlspreis der Stadt Aachen\") in honour of Charlemagne. It is awarded annually to \"personages of merit who have promoted the idea of Western unity by their political, economic and literary endeavours.\" Winners of the prize include Richard von Coudenhove-Kalergi, the founder of the pan-European movement, Alcide De Gasperi, and Winston Churchill.\nIn its national anthem, \"El Gran Carlemany\", the microstate of Andorra credits Charlemagne with its independence.\nIn 1964, young French singer France Gall released the hit song \"Sacr\u00e9 Charlemagne\" in which the lyrics blame the great king for imposing the burden of compulsory education on French children.\nCharlemagne is quoted by Dr Henry Jones, Sr. in \"Indiana Jones and the Last Crusade\". After using his umbrella to induce a flock of seagulls to smash through the glass cockpit of a pursuing German fighter plane, Henry Jones remarks, \"I suddenly remembered my Charlemagne: 'Let my armies be the rocks and the trees and the birds in the sky.'\" Despite the quote's popularity since the movie, there is no evidence that Charlemagne actually said this.\n21st century.\nA 2010 episode of \"QI\" discussed the mathematics completed by Mark Humphrys that calculated that all modern Europeans are highly likely to share Charlemagne as a common ancestor (see most recent common ancestor).\n\"The Economist\" featured a weekly column entitled \"Charlemagne\", focusing generally on European affairs and, more usually and specifically, on the European Union and its politics.\nActor and singer Christopher Lee's symphonic metal concept album ' and its heavy metal follow-up ' feature the events of Charlemagne's life.\nIn April 2014, on the occasion of the 1200th anniversary of Charlemagne's death, public art \"Mein Karl\" by Ottmar H\u00f6rl at Katschhof place was installed between city hall and the Aachen cathedral, displaying 500 Charlemagne statues.\nCharlemagne features as a playable character in the 2014 \"Charlemagne\" expansion for the grand strategy video game \"Crusader Kings 2\".\nCharlemagne is a playable character in the Mobile/PC Game Rise of Kingdoms.\nIn the 2018 video game \"Fate/Extella Link\", Charlemagne appears as a Heroic Spirit separated into two Saint Graphs: the adventurous hero Charlemagne, who embodies the fantasy aspect as leader of the Twelve Paladins, and the villain Karl de Gro\u00dfe, who embodies the historical aspect as Holy Roman Emperor.\nIn July 2022, Charlemagne featured as a character in an episode of The Family Histories Podcast, and it references his role as an ancestor of all modern Europeans. He is portrayed here in later life, and is speaking Latin, which is translated by a device. He is returned to 9th Century Aquitaine by the end of the episode after a DNA sample has been extracted.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5315", "revid": "374440", "url": "https://en.wikipedia.org/wiki?curid=5315", "title": "Character encodings in HTML", "text": "Use of encoding systems for international characters in HTML\nWhile Hypertext Markup Language (HTML) has been in use since 1991, HTML 4.0 from December 1997 was the first standardized version where international characters were given reasonably complete treatment. When an HTML document includes special characters outside the range of seven-bit ASCII, two goals are worth considering: the information's integrity, and universal browser display.\nSpecifying the document's character encoding.\nThere are two general ways to specify which character encoding is used in the document.\nFirst, the web server can include the character encoding or \"codice_1\" in the Hypertext Transfer Protocol (HTTP) codice_2 header, which would typically look like this:\nContent-Type: text/html; charset=utf-8\nThis method gives the HTTP server a convenient way to alter document's encoding according to content negotiation; certain HTTP server software can do it, for example Apache with the module codice_3.\nSecond, a declaration can be included within the document itself.\nFor HTML it is possible to include this information inside the codice_4 element near the top of the document:\n&lt;meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\"&gt;\nHTML5 also allows the following syntax to mean exactly the same:\n&lt;meta charset=\"utf-8\"&gt;\nXHTML documents have a third option: to express the character encoding via XML declaration, as follows:\n&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;\nWith this second approach, because the character encoding cannot be known until the declaration is parsed, there is a problem knowing which character encoding is used in the document up to and including the declaration itself. If the character encoding is an ASCII extension then the content up to and including the declaration itself should be pure ASCII and this will work correctly. For character encodings that are not ASCII extensions (i.e. not a superset of ASCII), such as UTF-16BE and UTF-16LE, a processor of HTML, such as a web browser, should be able to parse the declaration in some cases through the use of heuristics.\nEncoding detection algorithm.\nAs of HTML5 the recommended charset is UTF-8. An \"encoding sniffing algorithm\" is defined in the specification to determine the character encoding of the document based on multiple sources of input, including:\nCharacters outside of the printable ASCII range (32 to 126) usually appear incorrectly. This presents few problems for English-speaking users, but other languages regularly\u2014in some cases, always\u2014require characters outside that range. In Chinese, Japanese, and Korean (CJK) language environments where there are several different multi-byte encodings in use, auto-detection is also often employed. Finally, browsers usually permit the user to override \"incorrect\" charset label manually as well.\nIt is increasingly common for multilingual websites and websites in non-Western languages to use UTF-8, which allows use of the same encoding for all languages. UTF-16 or UTF-32, which can be used for all languages as well, are less widely used because they can be harder to handle in programming languages that assume a byte-oriented ASCII superset encoding, and they are less efficient for text with a high frequency of ASCII characters, which is usually the case for HTML documents.\nSuccessful viewing of a page is not necessarily an indication that its encoding is specified correctly. If the page's creator and reader are both assuming some platform-specific character encoding, and the server does not send any identifying information, then the reader will nonetheless see the page as the creator intended, but other readers on different platforms or with different native languages will not see the page as intended.\nPermitted encodings.\nThe WHATWG Encoding Standard, referenced by recent HTML standards (the current WHATWG HTML Living Standard, as well as the formerly competing W3C HTML 5.0 and 5.1) specifies a list of encodings which browsers must support. The HTML standards forbid support of other encodings. The Encoding Standard further stipulates that new formats, new protocols (even when existing formats are used) and authors of new documents are required to use UTF-8 exclusively.\nBesides UTF-8, the following encodings are explicitly listed in the HTML standard itself, with reference to the Encoding Standard:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nThe following additional encodings are listed in the Encoding Standard, and support for them is therefore also required:\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nThe following encodings are listed as explicit examples of forbidden encodings:\nThe standard also defines a \"replacement\" decoder, which maps all content labelled as certain encodings to the replacement character (\ufffd), refusing to process it at all. This is intended to prevent attacks (e.g. cross site scripting) which may exploit a difference between the client and server in what encodings are supported in order to mask malicious content. Although the same security concern applies to ISO-2022-JP and UTF-16, which also allow sequences of ASCII bytes to be interpreted differently, this approach was not seen as feasible for them since they are comparatively more frequently used in deployed content. The following encodings receive this treatment:\nCharacter references.\nIn addition to native character encodings, characters can also be encoded as \"character references\", which can be \"numeric character references\" (decimal or hexadecimal) or \"character entity references\". Character entity references are also sometimes referred to as \"named entities\", or \"HTML entities\" for HTML. HTML's usage of character references derives from SGML.\nHTML character references.\nA \"numeric character reference\" in HTML refers to a character by its Universal Character Set/Unicode \"code point\", and uses the format\ncodice_5\nor\ncodice_6\nwhere \"nnnn\" is the code point in decimal form, and \"hhhh\" is the code point in hexadecimal form. The \"x\" must be lowercase in XML documents. The \"nnnn\" or \"hhhh\" may be any number of digits and may include leading zeros. The \"hhhh\" may mix uppercase and lowercase, though uppercase is the usual style.\nNot all web browsers or email clients used by receivers of HTML documents, or text editors used by authors of HTML documents, will be able to render all HTML characters. Most modern software is able to display most or all of the characters for the user's language, and will draw a box or other clear indicator for characters they cannot render.\nFor codes from 0 to 127, the original 7-bit ASCII standard set, most of these characters can be used without a character reference. Codes from 160 to 255 can all be created using character entity names. Only a few higher-numbered codes can be created using entity names, but all can be created by decimal number character reference.\nCharacter entity references can also have the format codice_7 where \"name\" is a case-sensitive alphanumeric string. For example, \"\u03bb\" can also be encoded as codice_8 in an HTML document. The character entity references codice_9, codice_10, codice_11 and codice_12 are predefined in HTML and SGML, because codice_13, codice_14, codice_15 and codice_16 are already used to delimit markup. This notably did not include XML's codice_17 (') entity prior to HTML5. For a list of all named HTML character entity references along with the versions in which they were introduced, see List of XML and HTML character entity references.\nUnnecessary use of HTML character references may significantly reduce HTML readability. If the character encoding for a web page is chosen appropriately, then HTML character references are usually only required for markup delimiting characters as mentioned above, and for a few special characters (or none at all if a native Unicode encoding like UTF-8 is used). Incorrect HTML entity escaping may also open up security vulnerabilities for injection attacks such as cross-site scripting. If HTML attributes are left unquoted, certain characters, most importantly whitespace, such as space and tab, must be escaped using entities. Other languages related to HTML have their own methods of escaping characters.\nXML character references.\nUnlike traditional HTML with its large range of character entity references, in XML there are only five predefined character entity references. These are used to escape characters that are markup sensitive in certain contexts:\nAll other character entity references have to be defined before they can be used. For example, use of codice_23 (which gives \u00e9, Latin lower-case E with acute accent, U+00E9 in Unicode) in an XML document will generate an error unless the entity has already been defined. XML also requires that the codice_24 in hexadecimal numeric references be in lowercase: for example codice_25 rather than codice_26. XHTML, which is an XML application, supports the HTML entity set, along with XML's predefined entities.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5318", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=5318", "title": "Computer/Time-sharing", "text": ""}
{"id": "5319", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=5319", "title": "Computer/Multitasking", "text": ""}
{"id": "5320", "revid": "18872885", "url": "https://en.wikipedia.org/wiki?curid=5320", "title": "Carbon nanotube", "text": "Allotropes of carbon with a cylindrical nanostructure\nA carbon nanotube (CNT) is a tube made of carbon with a diameter in the nanometer range (nanoscale). They are one of the allotropes of carbon.\n\"Single-walled carbon nanotubes\" (\"SWCNTs\") have diameters around 0.5-2 nanometers, about 100,000 times smaller than the width of a human hair. They resemble a single sheet of graphite rolled up into a tube. They can be idealized as cutouts from a two-dimensional graphene sheet rolled up to form a hollow cylinder.\n\"Multi-walled carbon nanotubes\" (\"MWCNTs\") consist of nested single-wall carbon nanotubes in a nested, tube-in-tube structure. Multi-wall carbon nanotubes are also sometimes used to refer to double- and triple-walled carbon nanotubes.\nCarbon nanotubes can exhibit remarkable properties, such as exceptional tensile strength and thermal conductivity because of their nanostructure and strength of the bonds between carbon atoms. Some carbon nanotube structures exhibit high electrical conductivity while others are semiconductors. In addition, they can be chemically modified. These properties are expected to be valuable in many areas of technology, such as electronics, optics, composite materials (replacing or complementing carbon fibers), nanotechnology, and other applications of materials science.\nA path to synthesizing carbon nanotubes was lacking until the 1990s. Iijima and Ichihashi at NEC and Bethune et al. at IBM independently discovered in 1993 that co-vaporizing carbon and transition metals such as iron and cobalt could specifically catalyze SWCNT formation. These discoveries triggered research that succeeded in greatly increasing the efficiency of the catalytic production technique, and led to an explosion of work to characterize and find applications for SWCNTs.\nStructure of SWNTs.\nBasic details.\nThe structure of an ideal (infinitely long) single-walled carbon nanotube is that of a regular hexagonal lattice drawn on an infinite cylindrical surface, whose vertices are the positions of the carbon atoms. Since the length of the carbon-carbon bonds is fairly fixed, there are constraints on the diameter of the cylinder and the arrangement of the atoms on it.\nIn the study of nanotubes, one defines a zigzag path on a graphene-like lattice as a path that turns 60 degrees, alternating left and right, after stepping through each bond. It is also conventional to define an armchair path as one that makes two left turns of 60 degrees followed by two right turns every four steps. On some carbon nanotubes, there is a closed zigzag path that goes around the tube. One says that the tube is of the zigzag type or configuration, or simply is a zigzag nanotube. If the tube is instead encircled by a closed armchair path, it is said to be of the armchair type, or an armchair nanotube. An infinite nanotube that is of the zigzag (or armchair) type consists entirely of closed zigzag (or armchair) paths, connected to each other.\nThe zigzag and armchair configurations are not the only structures that a single-walled nanotube can have. To describe the structure of a general infinitely long tube, one should imagine it being sliced open by a cut parallel to its axis, that goes through some atom \"A\", and then unrolled flat on the plane, so that its atoms and bonds coincide with those of an imaginary graphene sheet\u2014more precisely, with an infinitely long strip of that sheet. The two halves of the atom \"A\" will end up on opposite edges of the strip, over two atoms \"A1\" and \"A2\" of the graphene. The line from \"A1\" to \"A2\" will correspond to the circumference of the cylinder that went through the atom \"A\", and will be perpendicular to the edges of the strip. In the graphene lattice, the atoms can be split into two classes, depending on the directions of their three bonds. Half the atoms have their three bonds directed the same way, and half have their three bonds rotated 180 degrees relative to the first half. The atoms \"A1\" and \"A2\", which correspond to the same atom \"A\" on the cylinder, must be in the same class. It follows that the circumference of the tube and the angle of the strip are not arbitrary, because they are constrained to the lengths and directions of the lines that connect pairs of graphene atoms in the same class.\nLet u and v be two linearly independent vectors that connect the graphene atom \"A1\" to two of its nearest atoms with the same bond directions. That is, if one numbers consecutive carbons around a graphene cell with C1 to C6, then u can be the vector from C1 to C3, and v be the vector from C1 to C5. Then, for any other atom \"A2\" with same class as \"A1\", the vector from \"A1\" to \"A2\" can be written as a linear combination \"n\" u + \"m\" v, where \"n\" and \"m\" are integers. And, conversely, each pair of integers (\"n\",\"m\") defines a possible position for \"A2\". Given \"n\" and \"m\", one can reverse this theoretical operation by drawing the vector w on the graphene lattice, cutting a strip of the latter along lines perpendicular to w through its endpoints \"A1\" and \"A2\", and rolling the strip into a cylinder so as to bring those two points together. If this construction is applied to a pair (\"k\",0), the result is a zigzag nanotube, with closed zigzag paths of 2\"k\" atoms. If it is applied to a pair (\"k\",\"k\"), one obtains an armchair tube, with closed armchair paths of 4\"k\" atoms.\nTypes.\nThe structure of the nanotube is not changed if the strip is rotated by 60 degrees clockwise around \"A1\" before applying the hypothetical reconstruction above. Such a rotation changes the corresponding pair (\"n\",\"m\") to the pair (\u22122\"m\",\"n\"+\"m\"). It follows that many possible positions of \"A2\" relative to \"A1\" \u2014 that is, many pairs (\"n\",\"m\") \u2014 correspond to the same arrangement of atoms on the nanotube. That is the case, for example, of the six pairs (1,2), (\u22122,3), (\u22123,1), (\u22121,\u22122), (2,\u22123), and (3,\u22121). In particular, the pairs (\"k\",0) and (0,\"k\") describe the same nanotube geometry. These redundancies can be avoided by considering only pairs (\"n\",\"m\") such that \"n\" &gt; 0 and \"m\" \u2265 0; that is, where the direction of the vector w lies between those of u (inclusive) and v (exclusive). It can be verified that every nanotube has exactly one pair (\"n\",\"m\") that satisfies those conditions, which is called the tube's type. Conversely, for every type there is a hypothetical nanotube. In fact, two nanotubes have the same type if and only if one can be conceptually rotated and translated so as to match the other exactly. Instead of the type (\"n\",\"m\"), the structure of a carbon nanotube can be specified by giving the length of the vector w (that is, the circumference of the nanotube), and the angle \"\u03b1\" between the directions of u and w, \nmay range from 0 (inclusive) to 60 degrees clockwise (exclusive). If the diagram is drawn with u horizontal, the latter is the tilt of the strip away from the vertical.\nChirality and mirror symmetry.\nA nanotube is chiral if it has type (\"n\",\"m\"), with \"m\" &gt; 0 and \"m\" \u2260 \"n\"; then its enantiomer (mirror image) has type (\"m\",\"n\"), which is different from (\"n\",\"m\"). This operation corresponds to mirroring the unrolled strip about the line \"L\" through \"A1\" that makes an angle of 30 degrees clockwise from the direction of the u vector (that is, with the direction of the vector u+v). The only types of nanotubes that are achiral are the (\"k\",0) \"zigzag\" tubes and the (\"k\",\"k\") \"armchair\" tubes. If two enantiomers are to be considered the same structure, then one may consider only types (\"n\",\"m\") with 0 \u2264 \"m\" \u2264 \"n\" and \"n\" &gt; 0. Then the angle \"\u03b1\" between u and w, which may range from 0 to 30 degrees (inclusive both), is called the \"chiral angle\" of the nanotube.\nCircumference and diameter.\nFrom \"n\" and \"m\" one can also compute the circumference \"c\", which is the length of the vector w, which turns out to be:\n formula_1\nin picometres. The diameter formula_2 of the tube is then formula_3, that is\n formula_4\nalso in picometres. (These formulas are only approximate, especially for small \"n\" and \"m\" where the bonds are strained; and they do not take into account the thickness of the wall.)\nThe tilt angle \"\u03b1\" between u and w and the circumference \"c\" are related to the type indices \"n\" and \"m\" by:\n formula_5\nwhere arg(\"x\",\"y\") is the clockwise angle between the \"X\"-axis and the vector (\"x\",\"y\"); a function that is available in many programming languages as codice_1(\"y\",\"x\"). Conversely, given \"c\" and \"\u03b1\", one can get the type (\"n\",\"m\") by the formulas:\n formula_6\nwhich must evaluate to integers.\nPhysical limits.\nNarrowest examples.\nIf \"n\" and \"m\" are too small, the structure described by the pair (\"n\",\"m\") will describe a molecule that cannot be reasonably called a \"tube\", and may not even be stable. For example, the structure theoretically described by the pair (1,0) (the limiting \"zigzag\" type) would be just a chain of carbons. That is a real molecule, the carbyne; which has some characteristics of nanotubes (such as orbital hybridization, high tensile strength, etc.) \u2014 but has no hollow space, and may not be obtainable as a condensed phase. The pair (2,0) would theoretically yield a chain of fused 4-cycles; and (1,1), the limiting \"armchair\" structure, would yield a chain of bi-connected 4-rings. These structures may not be realizable.\nThe thinnest carbon nanotube proper is the armchair structure with type (2,2), which has a diameter of 0.3\u00a0nm. This nanotube was grown inside a multi-walled carbon nanotube. Assigning of the carbon nanotube type was done by a combination of high-resolution transmission electron microscopy (HRTEM), Raman spectroscopy, and density functional theory (DFT) calculations.\nThe thinnest \"freestanding\" single-walled carbon nanotube is about 0.43\u00a0nm in diameter. Researchers suggested that it can be either (5,1) or (4,2) SWCNT, but the exact type of the carbon nanotube remains questionable. (3,3), (4,3), and (5,1) carbon nanotubes (all about 0.4\u00a0nm in diameter) were unambiguously identified using aberration-corrected high-resolution transmission electron microscopy inside double-walled CNTs.\nLength.\nThe observation of the \"longest\" carbon nanotubes grown so far, around 0.5\u00a0metre (550\u00a0mm) long, was reported in 2013. These nanotubes were grown on silicon substrates using an improved chemical vapor deposition (CVD) method and represent electrically uniform arrays of single-walled carbon nanotubes.\nThe \"shortest\" carbon nanotube can be considered to be the organic compound cycloparaphenylene, which was synthesized in 2008 by Ramesh Jasti. Other small molecule carbon nanotubes have been synthesized since.\nDensity.\nThe \"highest density\" of CNTs was achieved in 2013, grown on a conductive titanium-coated copper surface that was coated with co-catalysts cobalt and molybdenum at lower than typical temperatures of 450\u00a0\u00b0C. The tubes averaged a height of 380\u00a0nm and a mass density of 1.6\u2009g cm\u22123. The material showed ohmic conductivity (lowest resistance ~22\u2009k\u03a9).\nVariants.\nThere is no consensus on some terms describing carbon nanotubes in scientific literature: both \"-wall\" and \"-walled\" are being used in combination with \"single\", \"double\", \"triple\", or \"multi\", and the letter C is often omitted in the abbreviation, for example, multi-walled carbon nanotube (MWNT). The International Standards Organization uses single-wall or multi-wall in its documents.\nMulti-walled.\nMulti-walled nanotubes (MWNTs) consist of multiple rolled layers (concentric tubes) of graphene. There are two models that can be used to describe the structures of multi-walled nanotubes. In the \"Russian Doll\" model, sheets of graphite are arranged in concentric cylinders, e.g., a (0,8) single-walled nanotube (SWNT) within a larger (0,17) single-walled nanotube. In the \"Parchment\" model, a single sheet of graphite is rolled in around itself, resembling a scroll of parchment or a rolled newspaper. The interlayer distance in multi-walled nanotubes is close to the distance between graphene layers in graphite, approximately 3.4 \u00c5. The Russian Doll structure is observed more commonly. Its individual shells can be described as SWNTs, which can be metallic or semiconducting. Because of statistical probability and restrictions on the relative diameters of the individual tubes, one of the shells, and thus the whole MWNT, is usually a zero-gap metal.\nDouble-walled carbon nanotubes (DWNTs) form a special class of nanotubes because their morphology and properties are similar to those of SWNTs but they are more resistant to attacks by chemicals. This is especially important when it is necessary to graft chemical functions to the surface of the nanotubes (functionalization) to add properties to the CNT. Covalent functionalization of SWNTs will break some C=C double bonds, leaving \"holes\" in the structure on the nanotube and thus modifying both its mechanical and electrical properties. In the case of DWNTs, only the outer wall is modified. DWNT synthesis on the gram-scale by the CCVD technique was first proposed in 2003 from the selective reduction of oxide solutions in methane and hydrogen.\nThe telescopic motion ability of inner shells and their unique mechanical properties will permit the use of multi-walled nanotubes as the main movable arms in upcoming nanomechanical devices. The retraction force that occurs to telescopic motion is caused by the Lennard-Jones interaction between shells, and its value is about 1.5\u00a0nN.\nJunctions and crosslinking.\n Junctions between two or more nanotubes have been widely discussed theoretically. Such junctions are quite frequently observed in samples prepared by arc discharge as well as by chemical vapor deposition. The electronic properties of such junctions were first considered theoretically by Lambin et al., who pointed out that a connection between a metallic tube and a semiconducting one would represent a nanoscale heterojunction. Such a junction could therefore form a component of a nanotube-based electronic circuit. The adjacent image shows a junction between two multiwalled nanotubes.\nJunctions between nanotubes and graphene have been considered theoretically and studied experimentally. Nanotube-graphene junctions form the basis of pillared graphene, in which parallel graphene sheets are separated by short nanotubes. Pillared graphene represents a class of three-dimensional carbon nanotube architectures.\nRecently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (&gt;100\u00a0nm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical-initiated thermal crosslinking method to fabricate macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano-structured pores, and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices, implants, and sensors.\nOther morphologies.\nCarbon nanobuds are a newly created material combining two previously discovered allotropes of carbon: carbon nanotubes and fullerenes. In this new material, fullerene-like \"buds\" are covalently bonded to the outer sidewalls of the underlying carbon nanotube. This hybrid material has useful properties of both fullerenes and carbon nanotubes. In particular, they have been found to be exceptionally good field emitters. In composite materials, the attached fullerene molecules may function as molecular anchors preventing slipping of the nanotubes, thus improving the composite's mechanical properties.\nA carbon peapod is a novel hybrid carbon material which traps fullerene inside a carbon nanotube. It can possess interesting magnetic properties with heating and irradiation. It can also be applied as an oscillator during theoretical investigations and predictions.\nIn theory, a nanotorus is a carbon nanotube bent into a torus (doughnut shape). Nanotori are predicted to have many unique properties, such as magnetic moments 1000 times larger than that previously expected for certain specific radii. Properties such as magnetic moment, thermal stability, etc. vary widely depending on the radius of the torus and the radius of the tube.\nGraphenated carbon nanotubes are a relatively new hybrid that combines graphitic foliates grown along the sidewalls of multiwalled or bamboo style CNTs. The foliate density can vary as a function of deposition conditions (e.g., temperature and time) with their structure ranging from a few layers of graphene (&lt; 10) to thicker, more graphite-like. The fundamental advantage of an integrated graphene-CNT structure is the high surface area three-dimensional framework of the CNTs coupled with the high edge density of graphene. Depositing a high density of graphene foliates along the length of aligned CNTs can significantly increase the total charge capacity per unit of nominal area as compared to other carbon nanostructures.\nCup-stacked carbon nanotubes (CSCNTs) differ from other quasi-1D carbon structures, which normally behave as quasi-metallic conductors of electrons. CSCNTs exhibit semiconducting behavior because of the stacking microstructure of graphene layers.\nProperties.\nMany properties of single-walled carbon nanotubes depend significantly on the (\"n\",\"m\") type, and this dependence is non-monotonic (see Kataura plot). In particular, the band gap can vary from zero to about 2 eV and the electrical conductivity can show metallic or semiconducting behavior.\nMechanical.\nCarbon nanotubes are the strongest and stiffest materials yet discovered in terms of tensile strength and elastic modulus. This strength results from the covalent sp2 bonds formed between the individual carbon atoms. In 2000, a multiwalled carbon nanotube was tested to have a tensile strength of . (For illustration, this translates into the ability to endure tension of a weight equivalent to on a cable with cross-section of ). Further studies, such as one conducted in 2008, revealed that individual CNT shells have strengths of up to \u2248, which is in agreement with quantum/atomistic models. Because carbon nanotubes have a low density for a solid of 1.3 to 1.4\u00a0g/cm3, its specific strength of up to 48,000\u00a0kN\u00b7m\u00b7kg\u22121 is the best of known materials, compared to high-carbon steel's 154\u00a0kN\u00b7m\u00b7kg\u22121.\nAlthough the strength of individual CNT shells is extremely high, weak shear interactions between adjacent shells and tubes lead to significant reduction in the effective strength of multiwalled carbon nanotubes and carbon nanotube bundles down to only a few GPa. This limitation has been recently addressed by applying high-energy electron irradiation, which crosslinks inner shells and tubes, and effectively increases the strength of these materials to \u224860 GPa for multiwalled carbon nanotubes and \u224817 GPa for double-walled carbon nanotube bundles. CNTs are not nearly as strong under compression. Because of their hollow structure and high aspect ratio, they tend to undergo buckling when placed under compressive, torsional, or bending stress.\nOn the other hand, there was evidence that in the radial direction they are rather soft. The first transmission electron microscope observation of radial elasticity suggested that even van der Waals forces can deform two adjacent nanotubes. Later, nanoindentations with an atomic force microscope were performed by several groups to quantitatively measure radial elasticity of multiwalled carbon nanotubes and tapping/contact mode atomic force microscopy was also performed on single-walled carbon nanotubes. Young's modulus of on the order of several GPa showed that CNTs are in fact very soft in the radial direction.\nIt was reported in 2020, CNT-filled polymer nanocomposites with 4 wt% and 6 wt% loadings are the most optimal concentrations, as they provide a good balance between mechanical properties and resilience of mechanical properties against UV exposure for the offshore umbilical sheathing layer.\nElectrical.\nUnlike graphene, which is a two-dimensional semimetal, carbon nanotubes are either metallic or semiconducting along the tubular axis. For a given (\"n\",\"m\") nanotube, if \"n\" = \"m\", the nanotube is metallic; if \"n\" \u2212 \"m\" is a multiple of 3 and n \u2260 m, then the nanotube is quasi-metallic with a very small band gap, otherwise the nanotube is a moderate semiconductor.\nThus, all armchair (\"n\" = \"m\") nanotubes are metallic, and nanotubes (6,4), (9,1), etc. are semiconducting.\nCarbon nanotubes are not semimetallic because the degenerate point (the point where the \u03c0 [bonding] band meets the \u03c0* [anti-bonding] band, at which the energy goes to zero) is slightly shifted away from the \"K\" point in the Brillouin zone because of the curvature of the tube surface, causing hybridization between the \u03c3* and \u03c0* anti-bonding bands, modifying the band dispersion.\nThe rule regarding metallic versus semiconductor behavior has exceptions because curvature effects in small-diameter tubes can strongly influence electrical properties. Thus, a (5,0) SWCNT that should be semiconducting in fact is metallic according to the calculations. Likewise, zigzag and chiral SWCNTs with small diameters that should be metallic have a finite gap (armchair nanotubes remain metallic). In theory, metallic nanotubes can carry an electric current density of 4 \u00d7 109 A/cm2, which is more than 1,000 times greater than those of metals such as copper, where for copper interconnects, current densities are limited by electromigration. Carbon nanotubes are thus being explored as interconnects and conductivity-enhancing components in composite materials, and many groups are attempting to commercialize highly conducting electrical wire assembled from individual carbon nanotubes. There are significant challenges to be overcome however, such as undesired current saturation under voltage, and the much more resistive nanotube-to-nanotube junctions and impurities, all of which lower the electrical conductivity of the macroscopic nanotube wires by orders of magnitude, as compared to the conductivity of the individual nanotubes.\nBecause of its nanoscale cross-section, electrons propagate only along the tube's axis. As a result, carbon nanotubes are frequently referred to as one-dimensional conductors. The maximum electrical conductance of a single-walled carbon nanotube is 2\"G\"0, where \"G\"0 = 2\"e\"2/\"h\" is the conductance of a single ballistic quantum channel.\nBecause of the role of the \u03c0-electron system in determining the electronic properties of graphene, doping in carbon nanotubes differs from that of bulk crystalline semiconductors from the same group of the periodic table (e.g., silicon). Graphitic substitution of carbon atoms in the nanotube wall by boron or nitrogen dopants leads to p-type and n-type behavior, respectively, as would be expected in silicon. However, some non-substitutional (intercalated or adsorbed) dopants introduced into a carbon nanotube, such as alkali metals and electron-rich metallocenes, result in n-type conduction because they donate electrons to the \u03c0-electron system of the nanotube. By contrast, \u03c0-electron acceptors such as FeCl3 or electron-deficient metallocenes function as p-type dopants because they draw \u03c0-electrons away from the top of the valence band.\nIntrinsic superconductivity has been reported, although other experiments found no evidence of this, leaving the claim a subject of debate.\nIn 2021, Michael Strano, the Carbon P. Dubbs Professor of Chemical Engineering at MIT, published department findings on the use of carbon nanotubes to create an electrical current. By immersing the structures in an organic solvent, the liquid drew electrons out of the carbon particles. Strano was quoted as saying, \"This allows you to do electrochemistry, but with no wires,\" and represents a significant breakthrough in the technology. Future applications include powering micro- or nanoscale robots, as well as driving alcohol oxidation reactions, which are important in the chemicals industry.\nCrystallographic defects also affect the tube's electrical properties. A common result is lowered conductivity through the defective region of the tube. A defect in metallic armchair-type tubes (which can conduct electricity) can cause the surrounding region to become semiconducting, and single monatomic vacancies induce magnetic properties.\nOptical.\nCarbon nanotubes have useful absorption, photoluminescence (fluorescence), and Raman spectroscopy properties. Spectroscopic methods offer the possibility of quick and non-destructive characterization of relatively large amounts of carbon nanotubes. There is a strong demand for such characterization from the industrial point of view: numerous parameters of nanotube synthesis can be changed, intentionally or unintentionally, to alter the nanotube quality, such as the non-tubular carbon content, structure (chirality) of the produced nanotubes, and structural defects. These features then determine nearly all other significant optical, mechanical, and electrical properties.\nCarbon nanotube optical properties have been explored for use in applications such as for light-emitting diodes (LEDs) and photo-detectors based on a single nanotube have been produced in the lab. Their unique feature is not the efficiency, which is yet relatively low, but the narrow selectivity in the wavelength of emission and detection of light and the possibility of its fine tuning through the nanotube structure. In addition, bolometer and optoelectronic memory devices have been realised on ensembles of single-walled carbon nanotubes. Nanotube fluorescence has been investigated for the purposes of imaging and sensing in biomedical applications.\nThermal.\nAll nanotubes are expected to be very good thermal conductors along the tube, exhibiting a property known as \"ballistic conduction\", but good insulators lateral to the tube axis. Measurements show that an individual SWNT has a room-temperature thermal conductivity along its axis of about 3500 W\u00b7m\u22121\u00b7K\u22121; compare this to copper, a metal well known for its good thermal conductivity, which transmits 385 W\u00b7m\u22121\u00b7K\u22121. An individual SWNT has a room-temperature thermal conductivity lateral to its axis (in the radial direction) of about 1.52 W\u00b7m\u22121\u00b7K\u22121, which is about as thermally conductive as soil. Macroscopic assemblies of nanotubes such as films or fibres have reached up to 1500 W\u00b7m\u22121\u00b7K\u22121 so far. Networks composed of nanotubes demonstrate different values of thermal conductivity, from the level of thermal insulation with the thermal conductivity of 0.1 W\u00b7m\u22121\u00b7K\u22121 to such high values. That is dependent on the amount of contribution to the thermal resistance of the system caused by the presence of impurities, misalignments and other factors. The temperature stability of carbon nanotubes is estimated to be up to 2800\u00a0\u00b0C in vacuum and about 750\u00a0\u00b0C in air.\nCrystallographic defects strongly affect the tube's thermal properties. Such defects lead to phonon scattering, which in turn increases the relaxation rate of the phonons. This reduces the mean free path and reduces the thermal conductivity of nanotube structures. Phonon transport simulations indicate that substitutional defects such as nitrogen or boron will primarily lead to scattering of high-frequency optical phonons. However, larger-scale defects such as Stone\u2013Wales defects cause phonon scattering over a wide range of frequencies, leading to a greater reduction in thermal conductivity.\nSynthesis.\nTechniques have been developed to produce nanotubes in sizeable quantities, including arc discharge, laser ablation, chemical vapor deposition (CVD) and high-pressure carbon monoxide disproportionation (HiPCO). Among these arc discharge, laser ablation are batch by batch process, Chemical Vapor Deposition can bu used both for batch by batch or continuous processes, and HiPCO is gas phase continuous process. Most of these processes take place in a vacuum or with process gases. The CVD growth method is popular, as it yields high quantity and has a degree of control over diameter, length and morphology. Using particulate catalysts, large quantities of nanotubes can be synthesized by these methods, and industrialisation is well on it's way, with several CNT and CNT fibers factory in the world. One problem of CVD processes is the high variability in the nanotube's caracteristics The HiPCO process advances in catalysis and continuous growth are making CNTs more commercially viable. The HiPCO process helps in producing high purity single walled carbon nanotubes in higher quantity. The HiPCO reactor operates at high temperature 900-1100\u00a0\u00b0C and high pressure ~30-50 bar. It uses carbon monoxide as the carbon source and iron pentacarbonyl or nickel tetracarbonyl as a catalyst. These catalysts provide a nucleation site for the nanotubes to growc, while cheaper iron based catalysts like Ferrocene can be used for CVD process.\nVertically aligned carbon nanotube arrays are also grown by thermal chemical vapor deposition. A substrate (quartz, silicon, stainless steel, carbon fibers, etc.) is coated with a catalytic metal (Fe, Co, Ni) layer. Typically that layer is iron and is deposited via sputtering to a thickness of 1\u20135\u00a0nm. A 10\u201350\u00a0nm underlayer of alumina is often also put down on the substrate first. This imparts controllable wetting and good interfacial properties.\nWhen the substrate is heated to the growth temperature (~600 to 850\u00a0\u00b0C), the continuous iron film breaks up into small islands with each island then nucleating a carbon nanotube. The sputtered thickness controls the island size and this in turn determines the nanotube diameter. Thinner iron layers drive down the diameter of the islands and drive down the diameter of the nanotubes grown. The amount of time the metal island can sit at the growth temperature is limited as they are mobile and can merge into larger (but fewer) islands. Annealing at the growth temperature reduces the site density (number of CNT/mm2) while increasing the catalyst diameter. \nThe as-prepared carbon nanotubes always have impurities such as other forms of carbon (amorphous carbon, fullerene, etc.) and non-carbonaceous impurities (metal used for catalyst). These impurities need to be removed to make use of the carbon nanotubes in applications.\nFunctionalization.\nCNTs are known to have weak dispersibility in many solvents such as water as a consequence of strong intermolecular p\u2013p interactions. This hinders the processability of CNTs in industrial applications. In order to tackle the issue, various techniques have been developed to modify the surface of CNTs in order to improve their stability and solubility in water. This enhances the processing and manipulation of insoluble CNTs rendering them useful for synthesizing innovative CNT nanofluids with impressive properties that are tunable for a wide range of applications.\nChemical routes such as covalent functionalization have been studied extensively, which involves the oxidation of CNTs via strong acids (e.g. sulfuric acid, nitric acid, or a mixture of both) in order to set the carboxylic groups onto the surface of the CNTs as the final product or for further modification by esterification or amination. Free radical grafting is a promising technique among covalent functionalization methods, in which alkyl or aryl peroxides, substituted anilines, and diazonium salts are used as the starting agents.\nFree radical grafting of macromolecules (as the functional group) onto the surface of CNTs can improve the solubility of CNTs compared to common acid treatments which involve the attachment of small molecules such as hydroxyl onto the surface of CNTs. The solubility of CNTs can be improved significantly by free-radical grafting because the large functional molecules facilitate the dispersion of CNTs in a variety of solvents even at a low degree of functionalization. Recently an innovative environmentally friendly approach has been developed for the covalent functionalization of multi-walled carbon nanotubes (MWCNTs) using clove buds. This approach is innovative and green because it does not use toxic and hazardous acids which are typically used in common carbon nanomaterial functionalization procedures. The MWCNTs are functionalized in one pot using a free radical grafting reaction. The clove-functionalized MWCNTs are then dispersed in water producing a highly stable multi-walled carbon nanotube aqueous suspension (nanofluids).\nModeling.\nCarbon nanotubes are modelled in a similar manner as traditional composites in which a reinforcement phase is surrounded by a matrix phase. Ideal models such as cylindrical, hexagonal and square models are common. The size of the micromechanics model is highly function of the studied mechanical properties. The concept of representative volume element (RVE) is used to determine the appropriate size and configuration of computer model to replicate the actual behavior of CNT reinforced nanocomposite. Depending on the material property of interest (thermal, electrical, modulus, creep), one RVE might predict the property better than the alternatives. While the implementation of ideal model is computationally efficient, they do not represent microstructural features observed in scanning electron microscopy of actual nanocomposites. To incorporate realistic modeling, computer models are also generated to incorporate variability such as waviness, orientation and agglomeration of multiwall or single wall carbon nanotubes.\nMetrology.\nThere are many metrology standards and reference materials available for carbon nanotubes.\nFor single-wall carbon nanotubes, ISO/TS 10868 describes a measurement method for the diameter, purity, and fraction of metallic nanotubes through optical absorption spectroscopy, while ISO/TS 10797 and ISO/TS 10798 establish methods to characterize the morphology and elemental composition of single-wall carbon nanotubes, using transmission electron microscopy and scanning electron microscopy respectively, coupled with energy dispersive X-ray spectrometry analysis.\nNIST SRM 2483 is a soot of single-wall carbon nanotubes used as a reference material for elemental analysis, and was characterized using thermogravimetric analysis, prompt gamma activation analysis, induced neutron activation analysis, inductively coupled plasma mass spectroscopy, resonant Raman scattering, UV-visible-near infrared fluorescence spectroscopy and absorption spectroscopy, scanning electron microscopy, and transmission electron microscopy. The Canadian National Research Council also offers a certified reference material SWCNT-1 for elemental analysis using neutron activation analysis and inductively coupled plasma mass spectroscopy. NIST RM 8281 is a mixture of three lengths of single-wall carbon nanotube.\nFor multiwall carbon nanotubes, ISO/TR 10929 identifies the basic properties and the content of impurities, while ISO/TS 11888 describes morphology using scanning electron microscopy, transmission electron microscopy, viscometry, and light scattering analysis. ISO/TS 10798 is also valid for multiwall carbon nanotubes.\nChemical modification.\nCarbon nanotubes can be functionalized to attain desired properties that can be used in a wide variety of applications. The two main methods of carbon nanotube functionalization are covalent and non-covalent modifications. Because of their apparent hydrophobic nature, carbon nanotubes tend to agglomerate hindering their dispersion in solvents or viscous polymer melts. The resulting nanotube bundles or aggregates reduce the mechanical performance of the final composite. The surface of the carbon nanotubes can be modified to reduce the hydrophobicity and improve interfacial adhesion to a bulk polymer through chemical attachment.\nThe surface of carbon nanotubes can be chemically modified by coating spinel nanoparticles by hydrothermal synthesis and can be used for water oxidation purposes.\nIn addition, the surface of carbon nanotubes can be fluorinated or halofluorinated by heating while in contact with a fluoroorganic substance, thereby forming partially fluorinated carbons (so called Fluocar materials) with grafted (halo)fluoroalkyl functionality.\nApplications.\nCarbon nanotubes are currently used in multiple industrial and consumer applications. These include battery components, polymer composites, to improve the mechanical, thermal and electrical properties of the bulk product, and as a highly absorptive black paint. Many other applications are under development, including field effect transistors for electronics, high-strength fabrics, biosensors for biomedical and agricultural applications, and many others.\nCurrent industrial applications.\nApplications under development.\nApplications of nanotubes in development in academia and industry include:\nCarbon nanotubes can serve as additives to various structural materials. For instance, nanotubes form a tiny portion of the material(s) in some (primarily carbon fiber) baseball bats, golf clubs, car parts, or damascus steel.\nIBM expected carbon nanotube transistors to be used on Integrated Circuits by 2020.\nPotential/Future.\nThe strength and flexibility of carbon nanotubes makes them of potential use in controlling other nanoscale structures, which suggests they will have an important role in nanotechnology engineering. The highest tensile strength of an individual multi-walled carbon nanotube has been tested to be 63\u00a0GPa. Carbon nanotubes were found in Damascus steel from the 17th century, possibly helping to account for the legendary strength of the swords made of it. Recently, several studies have highlighted the prospect of using carbon nanotubes as building blocks to fabricate three-dimensional macroscopic (&gt;1mm in all three dimensions) all-carbon devices. Lalwani et al. have reported a novel radical initiated thermal crosslinking method to fabricated macroscopic, free-standing, porous, all-carbon scaffolds using single- and multi-walled carbon nanotubes as building blocks. These scaffolds possess macro-, micro-, and nano- structured pores and the porosity can be tailored for specific applications. These 3D all-carbon scaffolds/architectures may be used for the fabrication of the next generation of energy storage, supercapacitors, field emission transistors, high-performance catalysis, photovoltaics, and biomedical devices and implants.\nCNTs are potential candidates for future via and wire material in nano-scale VLSI circuits. Eliminating electromigration reliability concerns that plague today's Cu interconnects, isolated (single and multi-wall) CNTs can carry current densities in excess of 1000\u00a0MA/cm2 without electromigration damage.\nSingle-walled nanotubes are likely candidates for miniaturizing electronics. The most basic building block of these systems is an electric wire, and SWNTs with diameters of an order of a nanometre can be excellent conductors. One useful application of SWNTs is in the development of the first intermolecular field-effect transistors (FET). The first intermolecular logic gate using SWCNT FETs was made in 2001. A logic gate requires both a p-FET and an n-FET. Because SWNTs are p-FETs when exposed to oxygen and n-FETs otherwise, it is possible to expose half of an SWNT to oxygen and protect the other half from it. The resulting SWNT acts as a \"not\" logic gate with both p- and n-type FETs in the same molecule.\nLarge quantities of pure CNTs can be made into a freestanding sheet or film by surface-engineered tape-casting (SETC) fabrication technique which is a scalable method to fabricate flexible and foldable sheets with superior properties. Another reported form factor is CNT fiber (a.k.a. filament) by wet spinning. The fiber is either directly spun from the synthesis pot or spun from pre-made dissolved CNTs. Individual fibers can be turned into a yarn. Apart from its strength and flexibility, the main advantage is making an electrically conducting yarn. The electronic properties of individual CNT fibers (i.e. bundle of individual CNT) are governed by the two-dimensional structure of CNTs. The fibers were measured to have a resistivity only one order of magnitude higher than metallic conductors at 300K. By further optimizing the CNTs and CNT fibers, CNT fibers with improved electrical properties could be developed.\nCNT-based yarns are suitable for applications in energy and electrochemical water treatment when coated with an ion-exchange membrane. Also, CNT-based yarns could replace copper as a winding material. Pyrh\u00f6nen et al. (2015) have built a motor using CNT winding.\nSafety and health.\nThe National Institute for Occupational Safety and Health (NIOSH) is the leading United States federal agency conducting research and providing guidance on the occupational safety and health implications and applications of nanomaterials. Early scientific studies have indicated that nanoscale particles may pose a greater health risk than bulk materials due to a relative increase in surface area per unit mass. Increase in\u00a0length and diameter of CNT is correlated to increased toxicity and pathological alterations in lung. The biological interactions of nanotubes are not well understood, and the field is open to continued toxicological studies. It is often difficult to separate confounding factors, and since carbon is relatively biologically inert, some of the toxicity attributed to carbon nanotubes may be instead due to residual metal catalyst contamination. In previous studies, only Mitsui-7 was reliably demonstrated to be carcinogenic, although for unclear/unknown reasons. Unlike many common mineral fibers (such as asbestos), most SWCNTs and MWCNTs do not fit the size and aspect-ratio criteria to be classified as respirable fibers. In 2013, given that the long-term health effects have not yet been measured, NIOSH published a Current Intelligence Bulletin detailing the potential hazards and recommended exposure limit for carbon nanotubes and fibers. The U.S. National Institute for Occupational Safety and Health has determined non-regulatory recommended exposure limits (RELs) of 1 \u03bcg/m3 for carbon nanotubes and carbon nanofibers as background-corrected elemental carbon as an 8-hour time-weighted average (TWA) respirable mass concentration. Although CNT caused pulmonary inflammation and toxicity in mice, exposure to aerosols generated from sanding of composites containing polymer-coated MWCNTs, representative of the actual end-product, did not exert such toxicity.\nAs of October 2016, single wall carbon nanotubes have been registered through the European Union's Registration, Evaluation, Authorization and Restriction of Chemicals (REACH) regulations, based on evaluation of the potentially hazardous properties of SWCNT. Based on this registration, SWCNT commercialization is allowed in the EU up to 10 metric tons. Currently, the type of SWCNT registered through REACH is limited to the specific type of single wall carbon nanotubes manufactured by OCSiAl, which submitted the application.\nHistory.\nThe true identity of the discoverers of carbon nanotubes is a subject of some controversy. A 2006 editorial written by Marc Monthioux and Vladimir Kuznetsov in the journal \"Carbon\" described the origin of the carbon nanotube. A large percentage of academic and popular literature attributes the discovery of hollow, nanometre-size tubes composed of graphitic carbon to Sumio Iijima of NEC in 1991. His paper initiated a flurry of excitement and could be credited with inspiring the many scientists now studying applications of carbon nanotubes. Though Iijima has been given much of the credit for discovering carbon nanotubes, it turns out that the timeline of carbon nanotubes goes back much further than 1991.\nIn 1952, L. V. Radushkevich and V. M. Lukyanovich published clear images of 50 nanometre diameter tubes made of carbon in the \"Journal of Physical Chemistry Of Russia\". This discovery was largely unnoticed, as the article was published in Russian, and Western scientists' access to Soviet press was limited during the Cold War. Monthioux and Kuznetsov mentioned in their \"Carbon\" editorial: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The fact is, Radushkevich and Lukyanovich [...] should be credited for the discovery that carbon filaments could be hollow and have a nanometre-size diameter, that is to say for the discovery of carbon nanotubes.\nIn 1976, Morinobu Endo of CNRS observed hollow tubes of rolled up graphite sheets synthesised by a chemical vapour-growth technique. The first specimens observed would later come to be known as single-walled carbon nanotubes (SWNTs). Endo, in his early review of vapor-phase-grown carbon fibers (VPCF), also reminded us that he had observed a hollow tube, linearly extended with parallel carbon layer faces near the fiber core. This appears to be the observation of multi-walled carbon nanotubes at the center of the fiber. The mass-produced MWCNTs today are strongly related to the VPGCF developed by Endo. In fact, they call it the \"Endo-process\", out of respect for his early work and patents. In 1979, John Abrahamson presented evidence of carbon nanotubes at the 14th Biennial Conference of Carbon at Pennsylvania State University. The conference paper described carbon nanotubes as carbon fibers that were produced on carbon anodes during arc discharge. A characterization of these fibers was given, as well as hypotheses for their growth in a nitrogen atmosphere at low pressures.\nIn 1981, a group of Soviet scientists published the results of chemical and structural characterization of carbon nanoparticles produced by a thermocatalytic disproportionation of carbon monoxide. Using TEM images and XRD patterns, the authors suggested that their \"carbon multi-layer tubular crystals\" were formed by rolling graphene layers into cylinders. They speculated that via this rolling, many different arrangements of graphene hexagonal nets are possible. They suggested two such possible arrangements: circular arrangement (armchair nanotube); and a spiral, helical arrangement (chiral tube).\nIn 1987, Howard G. Tennent of Hyperion Catalysis was issued a U.S. patent for the production of \"cylindrical discrete carbon fibrils\" with a \"constant diameter between about 3.5 and about 70\u00a0nanometers..., length 102 times the diameter, and an outer region of multiple essentially continuous layers of ordered carbon atoms and a distinct inner core...\"\nHelping to create the initial excitement associated with carbon nanotubes were Iijima's 1991 discovery of multi-walled carbon nanotubes in the insoluble material of arc-burned graphite rods; and Mintmire, Dunlap, and White's independent prediction that if single-walled carbon nanotubes could be made, they would exhibit remarkable conducting properties. Nanotube research accelerated greatly following the independent discoveries by Iijima and Ichihashi at NEC and Bethune \"et al.\" at IBM of methods to specifically produce \"single-walled\" carbon nanotubes by adding transition-metal catalysts to the carbon in an arc discharge. Thess et al. refined this catalytic method by vaporizing the carbon/transition-metal combination in a high temperature furnace, which greatly improved the yield and purity of the SWNTs and made them widely available for characterization and application experiments. The arc discharge technique, well known to produce the famed Buckminsterfullerene , thus played a role in the discoveries of both multi- and single-wall nanotubes, extending the run of serendipitous discoveries relating to fullerenes. The discovery of nanotubes remains a contentious issue. Many believe that Iijima's report in 1991 is of particular importance because it brought carbon nanotubes into the awareness of the scientific community as a whole.\nIn 2020, during archaeological excavation of Keezhadi in Tamil Nadu, India, ~2500-year-old pottery was discovered whose coatings appear to contain carbon nanotubes. The robust mechanical properties of the nanotubes are partially why the coatings have lasted for so many years, say the scientists.\nReferences.\n\"This article incorporates public domain text from National Institute of Environmental Health Sciences (NIEHS) as quoted.\"\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5321", "revid": "10951369", "url": "https://en.wikipedia.org/wiki?curid=5321", "title": "Czech Republic", "text": "Country in Central Europe\nThe Czech Republic, also known as Czechia, is a landlocked country in Central Europe. Historically known as Bohemia, it is bordered by Austria to the south, Germany to the west, Poland to the northeast, and Slovakia to the southeast. The Czech Republic has a hilly landscape that covers an area of with a mostly temperate continental and oceanic climate. The capital and largest city is Prague; other major cities and urban areas include Brno, Ostrava, Plze\u0148 and Liberec.\nThe Duchy of Bohemia was founded in the late 9th century under Great Moravia. It was formally recognized as an Imperial State of the Holy Roman Empire in 1002 and became a kingdom in 1198. Following the Battle of Moh\u00e1cs in 1526, all of the Crown lands of Bohemia were gradually integrated into the Habsburg monarchy. Nearly a hundred years later, the Protestant Bohemian Revolt led to the Thirty Years' War. After the Battle of White Mountain, the Habsburgs consolidated their rule. With the dissolution of the Holy Empire in 1806, the Crown lands became part of the Austrian Empire.\nIn the 19th century, the Czech lands became more industrialized, and in 1918 most of it became part of the First Czechoslovak Republic following the collapse of Austria-Hungary after World War I. Czechoslovakia was the only country in Central and Eastern Europe to remain a parliamentary democracy during the entirety of the interwar period. After the Munich Agreement in 1938, Nazi Germany systematically took control over the Czech lands. Czechoslovakia was restored in 1945 and three years later became an Eastern Bloc communist state following a coup d'\u00e9tat in 1948. Attempts to liberalize the government and economy were suppressed by a Soviet-led invasion of the country during the Prague Spring in 1968. In November 1989, the Velvet Revolution ended communist rule in the country and restored democracy. On 31 December 1992, Czechoslovakia was peacefully dissolved, with its constituent states becoming the independent states of the Czech Republic and Slovakia.\nThe Czech Republic is a unitary parliamentary republic and developed country with an advanced, high-income social market economy. It is a welfare state with a European social model, universal health care and free-tuition university education. It ranks 32nd in the Human Development Index. The Czech Republic is a member of the United Nations, NATO, the European Union, the OECD, the OSCE, the Council of Europe and the Visegr\u00e1d Group.\nName.\nThe traditional English name \"Bohemia\" derives from , which means \"home of the Boii\" (a Gallic tribe). The current English name comes from the Polish ethnonym associated with the area, which ultimately comes from the Czech word . The name comes from the Slavic tribe () and, according to legend, their leader \u010cech, who brought them to Bohemia, to settle on \u0158\u00edp Mountain. The etymology of the word can be traced back to the Proto-Slavic root , meaning \"member of the people; kinsman\", thus making it cognate to the Czech word (a person).\nThe country has been traditionally divided into three lands, namely Bohemia () in the west, Moravia () in the east, and Czech Silesia (; the smaller, south-eastern part of historical Silesia, most of which is located within modern Poland) in the northeast. Known as the \"lands of the Bohemian Crown\" since the 14th century, a number of other names for the country have been used, including \"Czech/Bohemian lands\", \"Bohemian Crown\", \"Czechia\" and the \"lands of the Crown of Saint Wenceslaus\". When the country regained its independence after the dissolution of the Austro-Hungarian empire in 1918, the new name of \"Czechoslovakia\" was coined to reflect the union of the Czech and Slovak nations within one country.\nAfter Czechoslovakia dissolved on the last day of 1992, was adopted as the Czech short name for the new state and the Ministry of Foreign Affairs of the Czech Republic recommended \"Czechia\" for the English-language equivalent. This form was not widely adopted at the time, leading to the long name \"Czech Republic\" being used in English in nearly all circumstances. The Czech government directed use of \"Czechia\" as the official English short name in 2016. The short name has been listed by the United Nations and is used by other organizations such as the European Union, NATO, the CIA, Google Maps, and the European Broadcasting Union. The American \"The Associated Press Stylebook\" stated in its entry on the country in 2022 \"Czechia, the Czech Republic. Both are acceptable. The shorter name Czechia is preferred by the Czech government. If using Czechia, clarify in the story that the country is more widely known in English as the Czech Republic.\"\nHistory.\nPrehistory.\nArchaeologists have found evidence of prehistoric human settlements in the area, dating back to the Paleolithic era.\nIn the classical era, as a result of the 3rd century BC Celtic migrations, Bohemia became associated with the Boii. The Boii founded an oppidum near the site of modern Prague. Later in the 1st century, the Germanic tribes of the Marcomanni and Quadi settled there.\nSlavs from the Black Sea\u2013Carpathian region settled in the area (their migration was pushed by an invasion of peoples from Siberia and Eastern Europe into their area: Huns, Avars, Bulgars and Magyars). In the sixth century, the Huns had moved westwards into Bohemia, Moravia, and some of present-day Austria and Germany.\nDuring the 7th century, the Frankish merchant Samo, supporting the Slavs fighting against nearby settled Avars, became the ruler of the first documented Slavic state in Central Europe, Samo's Empire. The principality of Great Moravia, controlled by Moymir dynasty, arose in the 8th century. It reached its zenith in the 9th (during the reign of Svatopluk I of Moravia), holding off the influence of the Franks. Great Moravia was Christianized, with a role being played by the Byzantine mission of Cyril and Methodius. They codified the Old Church Slavonic language, the first literary and liturgical language of the Slavs, and the Glagolitic alphabet.\nBohemia.\nThe Duchy of Bohemia emerged in the late 9th century when it was unified by the P\u0159emyslid dynasty. Bohemia was from 1002 until 1806 an Imperial State of the Holy Roman Empire.\nIn 1212, P\u0159emysl Ottokar I extracted the Golden Bull of Sicily from the emperor, confirming Ottokar and his descendants' royal status; the Duchy of Bohemia was raised to a Kingdom. German immigrants settled in the Bohemian periphery in the 13th century. The Mongols in the invasion of Europe carried their raids into Moravia but were defensively defeated at Olomouc.\nAfter a series of dynastic wars, the House of Luxembourg gained the Bohemian throne.\nEfforts for a reform of the church in Bohemia started already in the late 14th century. Jan Hus's followers seceded from some practices of the Roman Church and in the Hussite Wars (1419\u20131434) defeated five crusades organized against them by Sigismund. During the next two centuries, 90% of the population in Bohemia and Moravia were considered Hussites. The pacifist thinker Petr Chel\u010dick\u00fd inspired the movement of the Moravian Brethren (by the middle of the 15th century) that completely separated from the Roman Catholic Church.\nOn 21 December 1421, Jan \u017di\u017eka, a successful military commander and mercenary, led his group of forces in the Battle of Kutn\u00e1 Hora, resulting in a victory for the Hussites. He is honoured to this day as a national hero.\nAfter 1526 Bohemia came increasingly under Habsburg control as the Habsburgs became first the elected and then in 1627 the hereditary rulers of Bohemia. Between 1583 and 1611 Prague was the official seat of the Holy Roman Emperor Rudolf II and his court.\nThe Defenestration of Prague and subsequent revolt against the Habsburgs in 1618 marked the start of the Thirty Years' War. In 1620, the rebellion in Bohemia was crushed at the Battle of White Mountain and the ties between Bohemia and the Habsburgs' hereditary lands in Austria were strengthened. The leaders of the Bohemian Revolt were executed in 1621. The nobility and the middle class Protestants had to either convert to Catholicism or leave the country.\nThe following era of 1620 to the late 18th century became known as the \"Dark Age\". During the Thirty Years' War, the population of the Czech lands declined by a third through the expulsion of Czech Protestants as well as due to the war, disease and famine. The Habsburgs prohibited all Christian confessions other than Catholicism. The flowering of Baroque culture shows the ambiguity of this historical period.\nOttoman Turks and Tatars invaded Moravia in 1663. In 1679\u20131680 the Czech lands faced the Great Plague of Vienna and an uprising of serfs.\nThere were peasant uprisings influenced by famine. Serfdom was abolished between 1781 and 1848. Several battles of the Napoleonic Wars took place on the current territory of the Czech Republic.\nThe end of the Holy Roman Empire in 1806 led to degradation of the political status of Bohemia which lost its position of an electorate of the Holy Roman Empire as well as its own political representation in the Imperial Diet. Bohemian lands became part of the Austrian Empire. During the 18th and 19th century the Czech National Revival began its rise, with the purpose to revive Czech language, culture, and national identity. The Revolution of 1848 in Prague, striving for liberal reforms and autonomy of the Bohemian Crown within the Austrian Empire, was suppressed.\nIt seemed that some concessions would be made also to Bohemia, but in the end, the Emperor Franz Joseph I affected a compromise with Hungary only. The Austro-Hungarian Compromise of 1867 and the never realized coronation of Franz Joseph as King of Bohemia led to a disappointment of some Czech politicians. The Bohemian Crown lands became part of the so-called Cisleithania.\nThe Czech Social Democratic and progressive politicians started the fight for universal suffrage. The first elections under universal male suffrage were held in 1907.\nCzechoslovakia.\nIn 1918, during the collapse of the Habsburg monarchy at the end of World War I, the independent republic of Czechoslovakia, which joined the winning Allied powers, was created, with Tom\u00e1\u0161 Garrigue Masaryk in the lead. This new country incorporated the Bohemian Crown.\nThe First Czechoslovak Republic comprised only 27% of the population of the former Austria-Hungary, but nearly 80% of the industry, which enabled it to compete with Western industrial states. In 1929 compared to 1913, the gross domestic product increased by 52% and industrial production by 41%. In 1938 Czechoslovakia held 10th place in the world industrial production. Czechoslovakia was the only country in Central and Eastern Europe to remain a liberal democracy throughout the entire \ninterwar period. Although the First Czechoslovak Republic was a unitary state, it provided certain rights to its minorities, the largest being Germans (23.6% in 1921), Hungarians (5.6%) and Ukrainians (3.5%).\nWestern Czechoslovakia was occupied by Nazi Germany, which placed most of the region into the Protectorate of Bohemia and Moravia. The Protectorate was proclaimed part of the Third Reich, and the president and prime minister were subordinated to Nazi Germany's \"Reichsprotektor\". One Nazi concentration camp was located within the Czech territory at Terez\u00edn, north of Prague. The vast majority of the Protectorate's Jews were murdered in Nazi-run concentration camps. The Nazi \"Generalplan Ost\" called for the extermination, expulsion, Germanization or enslavement of most or all Czechs for the purpose of providing more living space for the German people. There was Czechoslovak resistance to Nazi occupation as well as reprisals against the Czechoslovaks for their anti-Nazi resistance. The German occupation ended on 9 May 1945, with the arrival of the Soviet and American armies and the Prague uprising. Most of Czechoslovakia's German-speakers were forcibly expelled from the country, first as a result of local acts of violence and then under the aegis of an \"organized transfer\" confirmed by the Soviet Union, the United States, and Great Britain at the Potsdam Conference.\nIn the 1946 elections, the Communist Party gained 38% of the votes and became the largest party in the Czechoslovak parliament, formed a coalition with other parties, and consolidated power. A coup d'\u00e9tat came in 1948 and a single-party government was formed. For the next 41 years, the Czechoslovak Communist state conformed to Eastern Bloc economic and political features. The Prague Spring political liberalization was stopped by the 1968 Warsaw Pact invasion of Czechoslovakia. Analysts believe that the invasion caused the communist movement to fracture, ultimately leading to the Revolutions of 1989.\nCzech Republic.\nIn November 1989, Czechoslovakia again became a liberal democracy through the Velvet Revolution. However, Slovak national aspirations strengthened (Hyphen War) and on 31 December 1992, the country peacefully split into the independent countries of the Czech Republic and Slovakia. Both countries went through economic reforms and privatizations, with the intention of creating a market economy, as they have been trying to do since 1990, when Czechs and Slovaks still shared the common state. This process was largely successful; in 2006 the Czech Republic was recognized by the World Bank as a \"developed country\", and in 2009 the Human Development Index ranked it as a nation of \"Very High Human Development\".\nFrom 1991, the Czech Republic, originally as part of Czechoslovakia and since 1993 in its own right, has been a member of the Visegr\u00e1d Group and from 1995, the OECD. The Czech Republic joined NATO on 12 March 1999 and the European Union on 1 May 2004. On 21 December 2007 the Czech Republic joined the Schengen Area.\nUntil 2017, either the centre-left Czech Social Democratic Party or the centre-right Civic Democratic Party led the governments of the Czech Republic. In October 2017, the populist movement ANO 2011, led by the country's second-richest man, Andrej Babi\u0161, won the elections with three times more votes than its closest rival, the Civic Democrats. In December 2017, Czech president Milo\u0161 Zeman appointed Andrej Babi\u0161 as the new prime minister.\nIn the 2021 elections, ANO 2011 was narrowly defeated and Petr Fiala became the new prime minister. He formed a government coalition of the alliance SPOLU (Civic Democratic Party, KDU-\u010cSL and TOP 09) and the alliance of Pirates and Mayors. In January 2023, retired general Petr Pavel won the presidential election, becoming new Czech president to succeed Milo\u0161 Zeman. Following the 2022 Russian invasion of Ukraine, the country took in half a million Ukrainian refugees, the largest number per capita in the world.\nGeography.\nThe Czech Republic lies mostly between latitudes 48\u00b0 and 51\u00b0 N and longitudes 12\u00b0 and 19\u00b0 E.\nBohemia, to the west, consists of a basin drained by the Elbe () and the Vltava rivers, surrounded by mostly low mountains, such as the Krkono\u0161e range of the Sudetes. The highest point in the country, Sn\u011b\u017eka at , is located here. Moravia, the eastern part of the country, is also hilly. It is drained mainly by the Morava River, but it also contains the source of the Oder River ().\nWater from the Czech Republic flows to three different seas: the North Sea, Baltic Sea, and Black Sea. The Czech Republic also leases the Moldauhafen, a lot in the middle of the Hamburg Docks, which was awarded to Czechoslovakia by Article 363 of the Treaty of Versailles, to allow the landlocked country a place where goods transported down river could be transferred to seagoing ships. The territory reverts to Germany in 2028.\nPhytogeographically, the Czech Republic belongs to the Central European province of the Circumboreal Region, within the Boreal Kingdom. According to the World Wide Fund for Nature, the territory of the Czech Republic can be subdivided into four ecoregions: the Western European broadleaf forests, Central European mixed forests, Pannonian mixed forests, and Carpathian montane conifer forests.\nThere are four national parks in the Czech Republic. The oldest is Krkono\u0161e National Park (Biosphere Reserve), and the others are \u0160umava National Park (Biosphere Reserve), Podyj\u00ed National Park, and Bohemian Switzerland.\nThe three historical lands of the Czech Republic (formerly some countries of the Bohemian Crown) correspond with the river basins of the Elbe and the Vltava basin for Bohemia, the Morava one for Moravia, and the Oder river basin for Czech Silesia (in terms of the Czech territory).\nClimate.\nThe Czech Republic has a temperate climate, situated in the transition zone between the oceanic and continental climate types, with warm summers and cold, cloudy and snowy winters. The temperature difference between summer and winter is due to the landlocked geographical position.\nTemperatures vary depending on the elevation. In general, at higher altitudes, the temperatures decrease and precipitation increases. The wettest area in the Czech Republic is found around B\u00edl\u00fd Potok in Jizera Mountains and the driest region is the Louny District to the northwest of Prague. Another factor is the distribution of the mountains.\nAt the highest peak of Sn\u011b\u017eka (), the average temperature is , whereas in the lowlands of the South Moravian Region, the average temperature is as high as . The country's capital, Prague, has a similar average temperature, although this is influenced by urban factors.\nThe coldest month is usually January, followed by February and December. During these months, there is snow in the mountains and sometimes in the cities and lowlands. During March, April, and May, the temperature usually increases, especially during April, when the temperature and weather tends to vary during the day. Spring is also characterized by higher water levels in the rivers, due to melting snow with occasional flooding.\nThe warmest month of the year is July, followed by August and June. On average, summer temperatures are about higher than during winter. Summer is also characterized by rain and storms.\nAutumn generally begins in September, which is still warm and dry. During October, temperatures usually fall below or and deciduous trees begin to shed their leaves. By the end of November, temperatures usually range around the freezing point.\nThe coldest temperature ever measured was in Litv\u00ednovice near \u010cesk\u00e9 Bud\u011bjovice in 1929, at and the hottest measured, was at in Dob\u0159ichovice in 2012.\nMost rain falls during the summer. Sporadic rainfall is throughout the year (in Prague, the average number of days per month experiencing at least of rain varies from 12 in September and October to 16 in November) but concentrated rainfall (days with more than per day) are more frequent in the months of May to August (average around two such days per month). Severe thunderstorms, producing damaging straight-line winds, hail, and occasional tornadoes occur, especially during the summer period.\nEnvironment.\nAs of 2020, the Czech Republic ranks as the 21st most environmentally conscious country in the world in Environmental Performance Index. It had a 2018 Forest Landscape Integrity Index mean score of 1.71/10, ranking it 160th globally out of 172 countries. The Czech Republic has four National Parks (\u0160umava National Park, Krkono\u0161e National Park, \u010cesk\u00e9 \u0160v\u00fdcarsko National Park, Podyj\u00ed National Park) and 25 Protected Landscape Areas.\nGovernment.\nThe Czech Republic is a pluralist multi-party parliamentary representative democracy. The Parliament (\"Parlament \u010cesk\u00e9 republiky\") is bicameral, with the Chamber of Deputies (, 200 members) and the Senate (, 81 members). The members of the Chamber of Deputies are elected for a four-year term by proportional representation, with a 5% election threshold. There are 14 voting districts, identical to the country's administrative regions. The Chamber of Deputies, the successor to the Czech National Council, has the powers and responsibilities of the now defunct federal parliament of the former Czechoslovakia. The members of the Senate are elected in single-seat constituencies by two-round runoff voting for a six-year term, with one-third elected every even year in the autumn. This arrangement is modeled on the U.S. Senate, but each constituency is roughly the same size and the voting system used is a two-round runoff.\nThe president is a formal head of state with limited and specific powers, who appoints the prime minister, as well the other members of the cabinet on a proposal by the prime minister. From 1993 until 2012, the President of the Czech Republic was selected by a joint session of the parliament for a five-year term, with no more than two consecutive terms (2x V\u00e1clav Havel, 2x V\u00e1clav Klaus). Since 2013, the president has been elected directly. Some commentators have argued that, with the introduction of direct election of the President, the Czech Republic has moved away from the parliamentary system and towards a semi-presidential one. The Government's exercise of executive power derives from the Constitution. The members of the government are the Prime Minister, Deputy prime ministers and other ministers. The Government is responsible to the Chamber of Deputies. The Prime Minister is the head of government and wields powers such as the right to set the agenda for most foreign and domestic policy and choose government ministers.\nLaw.\nThe Czech Republic is a unitary state, with a civil law system based on the continental type, rooted in Germanic legal culture. The basis of the legal system is the Constitution of the Czech Republic adopted in 1993. The Penal Code is effective from 2010. A new Civil code became effective in 2014. The court system includes district, county, and supreme courts and is divided into civil, criminal, and administrative branches. The Czech judiciary has a triumvirate of supreme courts. The Constitutional Court consists of 15 constitutional judges and oversees violations of the Constitution by either the legislature or by the government. The Supreme Court is formed of 67 judges and is the court of highest appeal for most legal cases heard in the Czech Republic. The Supreme Administrative Court decides on issues of procedural and administrative propriety. It also has jurisdiction over certain political matters, such as the formation and closure of political parties, jurisdictional boundaries between government entities, and the eligibility of persons to stand for public office. The Supreme Court and the Supreme Administrative Court are both based in Brno, as is the Supreme Public Prosecutor's Office.\nForeign relations.\nThe Czech Republic has ranked as one of the safest or most peaceful countries for the past few decades. It is a member of the United Nations, the European Union, NATO, OECD, Council of Europe and is an observer to the Organization of American States. The embassies of most countries with diplomatic relations with the Czech Republic are located in Prague, while consulates are located across the country.\nThe Czech passport is restricted by visas. According to the 2018 Henley &amp; Partners Visa Restrictions Index, Czech citizens have visa-free access to 173 countries, which ranks them 7th along with Malta and New Zealand. The World Tourism Organization ranks the Czech passport 24th. The US Visa Waiver Program applies to Czech nationals.\nThe Prime Minister and Minister of Foreign Affairs have primary roles in setting foreign policy, although the President also has influence and represents the country abroad. Membership in the European Union and NATO is central to the Czech Republic's foreign policy. The Office for Foreign Relations and Information (\u00daZSI) serves as the foreign intelligence agency responsible for espionage and foreign policy briefings, as well as protection of Czech Republic's embassies abroad.\nThe Czech Republic has ties with Slovakia, Poland and Hungary as a member of the Visegr\u00e1d Group, as well as with Germany, Israel, the United States and the European Union and its members. After 2020, relations with Asian democratic states, such as Taiwan, are being strengthened. On the contrary, the Czech Republic has long had bad relations with Russia, and from 2021 the Czech Republic appears on Russia's official list of enemy countries. The Czech Republic also has problematic relations with China.\nCzech officials have supported dissenters in Belarus, Moldova, Myanmar and Cuba.\nFamous Czech diplomats of the past included Jaroslav Lev of Ro\u017emit\u00e1l, Humprecht Jan Czernin, Count Philip Kinsky of Wchinitz and Tettau, Wenzel Anton, Prince of Kaunitz-Rietberg, Prince Karl Philipp Schwarzenberg, Alois Lexa von Aehrenthal, Ottokar Czernin, Edvard Bene\u0161, Jan Masaryk, Ji\u0159\u00ed H\u00e1jek, Ji\u0159\u00ed Dienstbier, Michael \u017dantovsk\u00fd, Petr Kol\u00e1\u0159, Alexandr Vondra, Prince Karel Schwarzenberg and Petr Pavel.\nMilitary.\nThe Czech armed forces consist of the Czech Land Forces, the Czech Air Force and of specialized support units. The armed forces are managed by the Ministry of Defence. The President of the Czech Republic is Commander-in-chief of the armed forces. In 2004 the army transformed itself into a fully professional organization and compulsory military service was abolished. The country has been a member of NATO since 12 March 1999. Defence spending is approximately 1.28% of the GDP (2021). The armed forces are charged with protecting the Czech Republic and its allies, promoting global security interests, and contributing to NATO.\nCurrently, as a member of NATO, the Czech military are participating in the Resolute Support and KFOR operations and have soldiers in Afghanistan, Mali, Bosnia and Herzegovina, Kosovo, Egypt, Israel and Somalia. The Czech Air Force also served in the Baltic states and Iceland. The main equipment of the Czech military includes JAS 39 Gripen multi-role fighters, Aero L-159 Alca combat aircraft, Mi-35 attack helicopters, armored vehicles (Pandur II, OT-64, OT-90, BVP-2) and tanks (T-72 and T-72M4CZ).\nThe most famous Czech, and therefore Czechoslovak, soldiers and military leaders of the past were Ottokar II of Bohemia, John of Bohemia, Jan \u017di\u017eka, Albrecht von Wallenstein, Karl Philipp, Prince of Schwarzenberg, Joseph Radetzky von Radetz, Josef \u0160nejd\u00e1rek, Heliodor P\u00edka, Ludv\u00edk Svoboda, Jan Kubi\u0161, Jozef Gab\u010d\u00edk, Franti\u0161ek Fajtl and Petr Pavel.\nHuman rights.\nHuman rights in the Czech Republic are guaranteed by the Charter of Fundamental Rights and Freedoms and international treaties on human rights. Nevertheless, there were cases of human rights violations such as discrimination against Roma children, for which the European Commission asked the Czech Republic to provide an explanation, or the illegal sterilization of Roma women, for which the government apologized.\nPrague is the seat of Radio Free Europe/Radio Liberty. Today, the station is based in Hagibor. At the beginning of the 1990s, V\u00e1clav Havel personally invited her to Czechoslovakia.\nPersons of the same sex can enter into a so-called registered partnership in the Czech Republic (see LGBT rights in the Czech Republic). \nThe best-known Czech activists and supporters of human rights include Berta von Suttner, born in Prague, who won the Nobel Peace Prize for her pacifist struggle, philosopher and the first Czechoslovak president Tom\u00e1\u0161 Garrigue Masaryk, student Jan Palach, who set himself on fire in 1969 in protest against the Soviet occupation, Karel Schwarzenberg, who was chairman of the International Helsinki Committee for Human Rights between 1984 and 1990, V\u00e1clav Havel, long-time dissident and later president, sociologist and dissident Ji\u0159ina \u0160iklov\u00e1 and \u0160imon P\u00e1nek, founder and director of the People in Need organization. \nAdministrative divisions.\nSince 2000, the Czech Republic has been divided into thirteen regions (Czech: \"kraje\", singular \"kraj\") and the capital city of Prague. Every region has its own elected regional assembly and a regional governor. In Prague, the assembly and presidential powers are executed by the city council and the mayor.\nThe older seventy-six districts (\"okresy\", singular \"okres\") including three \"statutory cities\" (without Prague, which had special status) lost most of their importance in 1999 in an administrative reform; they remain as territorial divisions and seats of various branches of state administration.\nThe smallest administrative units are \"obce\" (municipalities). As of 2021, the Czech Republic is divided into 6,254 municipalities. Cities and towns are also municipalities. The capital city of Prague is a region and municipality at the same time.\nEconomy.\nThe Czech Republic has a developed, high-income export-oriented social market economy based in services, manufacturing and innovation, that maintains a welfare state and the European social model. The Czech Republic participates in the European Single Market as a member of the European Union and is therefore a part of the economy of the European Union, but uses its own currency, the Czech koruna, instead of the euro. It has a per capita GDP rate that is 91% of the EU average and is a member of the OECD. Monetary policy is conducted by the Czech National Bank, whose independence is guaranteed by the Constitution. The Czech Republic ranks 12th in the UN inequality-adjusted human development and 24th in World Bank Human Capital Index. It was described by \"The Guardian\" as \"one of Europe's most flourishing economies\".\nAs of 2023[ [update]], the country's GDP per capita at purchasing power parity is $51,329 and $29,856 at nominal value. According to Allianz A.G., in 2018 the country was an MWC (mean wealth country), ranking 26th in net financial assets. The country experienced a 4.5% GDP growth in 2017. The 2016 unemployment rate was the lowest in the EU at 2.4%, and the 2016 poverty rate was the second lowest of OECD members. Czech Republic ranks 27th in the 2021 Index of Economic Freedom, 30th in the 2022 Global Innovation Index, down from 24th in the 2016,\n 29th in the Global Competitiveness Report,&lt;ref name=\"GCR 2018/19\"&gt;&lt;/ref&gt; and 25th in the Global Enabling Trade Report.\nThe Czech Republic has a diverse economy that ranks 7th in the 2016 Economic Complexity Index. The industrial sector accounts for 37.5% of the economy, while services account for 60% and agriculture for 2.5%. The largest trading partner for both export and import is Germany and the EU in general. Dividends worth CZK 270 billion were paid to the foreign owners of Czech companies in 2017, which has become a political issue. The country has been a member of the Schengen Area since 1 May 2004, having abolished border controls, completely opening its borders with all of its neighbors on 21 December 2007.\nIndustry.\nIn 2018[ [update]] the largest companies by revenue in the Czech Republic were: automobile manufacturer \u0160koda Auto, utility company \u010cEZ Group, conglomerate Agrofert, energy trading company EPH, oil processing company Unipetrol, electronics manufacturer Foxconn CZ and steel producer Moravia Steel. Other Czech transportation companies include: \u0160koda Transportation (tramways, trolleybuses, metro), Tatra (heavy trucks, the second oldest car maker in the world), Avia (medium trucks), Karosa and SOR Libchavy (buses), Aero Vodochody (military aircraft), Let Kunovice (civil aircraft), Zetor (tractors), Jawa Moto (motorcycles) and \u010cezeta (electric scooters).\n\u0160koda Transportation is the fourth largest tram producer in the world; nearly one third of all trams in the world come from Czech factories. The Czech Republic is also the world's largest vinyl records manufacturer, with GZ Media producing about 6 million pieces annually in Lod\u011bnice. \u010cesk\u00e1 zbrojovka is among the ten largest firearms producers in the world and five who produce automatic weapons.\nIn the food industry, Czech companies include Agrofert, Kofola and Ham\u00e9.\nEnergy.\nProduction of Czech electricity exceeds consumption by about 10 TWh per year, the excess being exported. Nuclear power presently provides about 30 percent of the total power needs, its share is projected to increase to 40 percent. In 2005, 65.4 percent of electricity was produced by steam and combustion power plants (mostly coal); 30 percent by nuclear plants; and 4.6 percent came from renewable sources, including hydropower. The largest Czech power resource is Temel\u00edn Nuclear Power Station, with another nuclear power plant in Dukovany.\nThe Czech Republic is reducing its dependence on highly polluting low-grade brown coal as a source of energy. Natural gas is purchased from Norwegian companies and as liquefied gas LNG from the Netherlands and Belgium. In the past, three-quarters of gas supplies came from Russia, but after the outbreak of the war in Ukraine, the government gradually stopped these supplies. Gas consumption (approx. 100 TWh in 2003\u20132005) is almost double electricity consumption. South Moravia has small oil and gas deposits.\nTransportation infrastructure.\nAs of 2020,[ [update]] the road network in the Czech Republic is long, out of which are motorways. The speed limit is 50\u00a0km/h within towns, 90\u00a0km/h outside of towns and 130\u00a0km/h on motorways.\nThe Czech Republic has one of the densest rail networks in the world. As of 2020,[ [update]] the country has of lines. Of that number, is electrified, are single-line tracks and are double and multiple-line tracks. The length of tracks is , out of which is electrified.\n\u010cesk\u00e9 dr\u00e1hy (the Czech Railways) is the main railway operator in the country, with about 180\u00a0million passengers carried yearly. Maximum speed is limited to 160\u00a0km/h.\nV\u00e1clav Havel Airport in Prague is the main international airport in the country. In 2019, it handled 17.8 million passengers. In total, the Czech Republic has 91 airports, six of which provide international air services. The public international airports are in Brno, Karlovy Vary, Mnichovo Hradi\u0161t\u011b, Mo\u0161nov (near Ostrava), Pardubice and Prague. The non-public international airports capable of handling airliners are in Kunovice and Vodochody.\nRussia, via pipelines through Ukraine and to a lesser extent, Norway, via pipelines through Germany, supply the Czech Republic with liquid and natural gas.\nCommunications and IT.\nThe Czech Republic ranks in the top 10 countries worldwide with the fastest average internet speed. By the beginning of 2008, there were over 800 mostly local WISPs, with about 350,000 subscribers in 2007. Plans based on either GPRS, EDGE, UMTS or CDMA2000 are being offered by all three mobile phone operators (T-Mobile, O2, Vodafone) and internet provider U:fon. Government-owned \u010cesk\u00fd Telecom slowed down broadband penetration. At the beginning of 2004, local-loop unbundling began and alternative operators started to offer ADSL and also SDSL. This and later privatization of \u010cesk\u00fd Telecom helped drive down prices.\nOn 1 July 2006, \u010cesk\u00fd Telecom was acquired by globalized company (Spain-owned) Telef\u00f3nica group and adopted the new name Telef\u00f3nica O2 Czech Republic. As of 2017[ [update]], VDSL and ADSL2+ are offered in variants, with download speeds of up to 50 Mbit/s and upload speeds of up to 5 Mbit/s. Cable internet is gaining more popularity with its higher download speeds ranging from 50 Mbit/s to 1 Gbit/s.\nTwo computer security companies, Avast and AVG, were founded in the Czech Republic. In 2016, Avast led by Pavel Baudi\u0161 bought rival AVG for US$1.3\u00a0billion, together at the time, these companies had a user base of about 400 million people and 40% of the consumer market outside of China. Avast is the leading provider of antivirus software, with a 20.5% market share.\nTourism.\nPrague is the fifth most visited city in Europe after London, Paris, Istanbul and Rome. In 2001, the total earnings from tourism reached 118\u00a0billion CZK, making up 5.5% of GNP and 9% of overall export earnings. The industry employs more than 110,000 people \u2013 over 1% of the population.\nGuidebooks and tourists reporting overcharging by taxi drivers and pickpocketing problems are mainly in Prague, though the situation has improved recently. Since 2005, Prague's mayor, Pavel B\u00e9m, has worked to improve this reputation by cracking down on petty crime and, aside from these problems, Prague is a \"safe\" city. The Czech Republic's crime rate is described by the United States State department as \"low\".\nOne of the tourist attractions in the Czech Republic is the Nether district V\u00edtkovice in Ostrava.\nThe Czech Republic boasts 16 UNESCO World Heritage Sites, 3 of them are transnational. As of 2021[ [update]], further 14 sites are on the tentative list.\nArchitectural heritage is an object of interest to visitors \u2013 it includes castles and ch\u00e2teaux from different historical epoques, namely Karl\u0161tejn Castle, \u010cesk\u00fd Krumlov and the Lednice\u2013Valtice Cultural Landscape. There are 12 cathedrals and 15 churches elevated to the rank of basilica by the Pope, calm monasteries.\nAway from the towns, areas such as Bohemian Paradise, Bohemian Forest and the Giant Mountains attract visitors seeking outdoor pursuits. There is a number of beer festivals.\nThe country is also known for its various museums. Puppetry and marionette exhibitions are with a number of puppet festivals throughout the country. Aquapalace Prague in \u010cestlice is the largest water park in the country.\nScience.\nThe Czech lands have a long and well-documented history of scientific innovation. Today, the Czech Republic has a highly sophisticated, developed, high-performing, innovation-oriented scientific community supported by the government, industry, and leading universities. Czech scientists are embedded members of the global scientific community. They contribute annually to multiple international academic journals and collaborate with their colleagues across boundaries and fields. The Czech Republic was ranked 24th in the Global Innovation Index in 2020 and 2021, up from 26th in 2019.\nHistorically, the Czech lands, especially Prague, have been the seat of scientific discovery going back to early modern times, including Tycho Brahe, Nicolaus Copernicus, and Johannes Kepler. In 1784 the scientific community was first formally organized under the charter of the Royal Czech Society of Sciences. Currently, this organization is known as the Czech Academy of Sciences. Similarly, the Czech lands have a well-established history of scientists, including Nobel laureates biochemists Gerty and Carl Ferdinand Cori, chemists Jaroslav Heyrovsk\u00fd and Otto Wichterle, physicists Ernst Mach and Peter Gr\u00fcnberg, physiologist Jan Evangelista Purkyn\u011b and chemist Anton\u00edn Hol\u00fd. Sigmund Freud, the founder of psychoanalysis, was born in P\u0159\u00edbor, Gregor Mendel, the founder of genetics, was born in Hyn\u010dice and spent most of his life in Brno, logician and mathematician Kurt G\u00f6del was born in Brno.\nHistorically, most scientific research was recorded in Latin, but from the 18th century onwards increasingly in German and later in Czech, archived in libraries supported and managed by religious groups and other denominations as evidenced by historical locations of international renown and heritage such as the Strahov Monastery and the Clementinum in Prague. Increasingly, Czech scientists publish their work and that of their history in English.\nThe current important scientific institution is the already mentioned Academy of Sciences of the Czech Republic, the CEITEC Institute in Brno or the HiLASE and Eli Beamlines centers with the most powerful laser in the world in Doln\u00ed B\u0159e\u017eany. Prague is the seat of the administrative center of the GSA Agency operating the European navigation system Galileo and the European Union Agency for the Space Programme.\nDemographics.\nThe total fertility rate (TFR) in 2020 was estimated at 1.71 children per woman, which is below the replacement rate of 2.1. The Czech Republic's population has an average age of 43.3 years. The life expectancy in 2021 was estimated to be 79.5 years (76.55 years male, 82.61 years female). About 77,000 people immigrate to the Czech Republic annually. Vietnamese immigrants began settling in the country during the Communist period, when they were invited as guest workers by the Czechoslovak government. In 2009, there were about 70,000 Vietnamese in the Czech Republic. Most decide to stay in the country permanently.\nAccording to results of the 2021 census, the majority of the inhabitants of the Czech Republic are Czechs (57.3%), followed by Moravians (3.4%), Slovaks (0.9%), Ukrainians (0.7%), Viets (0.3%), Poles (0.3%), Russians (0.2%), Silesians (0.1%) and Germans (0.1%). Another 4.0% declared combination of two nationalities (3.6% combination of Czech and other nationality). As the 'nationality' was an optional item, a number of people left this field blank (31.6%). According to some estimates, there are about 250,000 Romani people in the Czech Republic. The Polish minority resides mainly in the Trans-Olza region.\nThere were 658,564 foreigners residing in the country in 2021, according to the Czech Statistical Office, with the largest groups being Ukrainian (22%), Slovak (22%), Vietnamese (12%), Russian (7%) and German (4%). Most of the foreign population lives in Prague (37.3%) and Central Bohemia Region (13.2%).\nThe Jewish population of Bohemia and Moravia, 118,000 according to the 1930 census, was nearly annihilated by the Nazi Germans during the Holocaust. There were approximately 3,900 Jews in the Czech Republic in 2021. The former Czech prime minister, Jan Fischer, is of Jewish faith.\nNationality of residents, who answered the question in the Census 2021:\nLargest cities.\n&lt;templatestyles src=\"Template:Largest_cities/styles.css\" /&gt;\nReligion.\nAbout 75% to 79% of residents of the Czech Republic do not declare having any religion or faith in surveys, and the proportion of convinced atheists (30%) is the third highest in the world behind those of China (47%) and Japan (31%). The Czech people have been historically characterized as \"tolerant and even indifferent towards religion\". The religious identity of the country has changed drastically since the first half of the 20th century, when more than 90% of Czechs were Christians.\nChristianization in the 9th and 10th centuries introduced Catholicism. After the Bohemian Reformation, most Czechs became followers of Jan Hus, Petr Chel\u010dick\u00fd and other regional Protestant Reformers. Taborites and Utraquists were Hussite groups. Towards the end of the Hussite Wars, the Utraquists changed sides and allied with the Catholic Church. Following the joint Utraquist\u2014Catholic victory, Utraquism was accepted as a distinct form of Christianity to be practiced in Bohemia by the Catholic Church while all remaining Hussite groups were prohibited. After the Reformation, some Bohemians went with the teachings of Martin Luther, especially Sudeten Germans. In the wake of the Reformation, Utraquist Hussites took a renewed increasingly anti-Catholic stance, while some of the defeated Hussite factions were revived. After the Habsburgs regained control of Bohemia, the whole population was forcibly converted to Catholicism\u2014even the Utraquist Hussites. Going forward, Czechs have become more wary and pessimistic of religion as such. A history of resistance to the Catholic Church followed. It suffered a schism with the neo-Hussite Czechoslovak Hussite Church in 1920, lost the bulk of its adherents during the Communist era and continues to lose in the modern, ongoing secularization. Protestantism never recovered after the Counter-Reformation was introduced by the Austrian Habsburgs in 1620. Prior to the Holocaust, the Czech Republic had a sizable Jewish community of around 100,000. There are many historically important and culturally relevant Synagogues in the Czech Republic such as Europe's oldest active Synagogue, The Old New Synagogue and the second largest Synagogue in Europe, the Great Synagogue (Plze\u0148). The Holocaust decimated Czech Jewry and the Jewish population as of 2021 is 3,900.\nAccording to the 2011 census, 34% of the population stated they had no religion, 10.3% was Catholic, 0.8% was Protestant (0.5% Czech Brethren and 0.4% Hussite), and 9% followed other forms of religion both denominational or not (of which 863 people answered they are Pagan). 45% of the population did not answer the question about religion. From 1991 to 2001 and further to 2011 the adherence to Catholicism decreased from 39% to 27% and then to 10%; Protestantism similarly declined from 3.7% to 2% and then to 0.8%. The Muslim population is estimated to be 20,000 representing 0.2% of the population.\nThe proportion of religious believers varies significantly across the country, from 55% in Zl\u00edn Region to 16% in \u00dast\u00ed nad Labem Region.\nEducation and health care.\nEducation in the Czech Republic is compulsory for nine years and citizens have access to a free-tuition university education, while the average number of years of education is 13.1. Additionally, the Czech Republic has a \"relatively equal\" educational system in comparison with other countries in Europe. Founded in 1348, Charles University was the first university in Central Europe. Other major universities in the country are Masaryk University, Czech Technical University, Palack\u00fd University, Academy of Performing Arts and University of Economics.\nThe Programme for International Student Assessment, coordinated by the OECD, currently ranks the Czech education system as the 15th most successful in the world, higher than the OECD average. The UN Education Index ranks the Czech Republic 10th as of 2013[ [update]] (positioned behind Denmark and ahead of South Korea).\nHealth care in the Czech Republic is similar in quality to that of other developed nations. The Czech universal health care system is based on a compulsory insurance model, with fee-for-service care funded by mandatory employment-related insurance plans. According to the 2016 Euro health consumer index, a comparison of healthcare in Europe, the Czech healthcare is 13th, ranked behind Sweden and two positions ahead of the United Kingdom.\nCulture.\nArt.\nVenus of Doln\u00ed V\u011bstonice is the treasure of prehistoric art. Theodoric of Prague was a painter in the Gothic era who decorated the castle Karlstejn. In the Baroque era, there were Wenceslaus Hollar, Jan Kupeck\u00fd, Karel \u0160kr\u00e9ta, Anton Raphael Mengs or Petr Brandl, sculptors Matthias Braun and Ferdinand Brokoff. In the first half of the 19th century, Josef M\u00e1nes joined the romantic movement. In the second half of the 19th century had the main say the so-called \"National Theatre generation\": sculptor Josef V\u00e1clav Myslbek and painters Mikol\u00e1\u0161 Ale\u0161, V\u00e1clav Bro\u017e\u00edk, Vojt\u011bch Hynais or Julius Ma\u0159\u00e1k. At the end of the century came a wave of Art Nouveau. Alfons Mucha became the main representative. He is known for Art Nouveau posters and his cycle of 20 large canvases named the Slav Epic, which depicts the history of Czechs and other Slavs.\nAs of 2012[ [update]], the Slav Epic can be seen in the Veletr\u017en\u00ed Palace of the National Gallery in Prague, which manages the largest collection of art in the Czech Republic. Max \u0160vabinsk\u00fd was another Art nouveau painter. The 20th century brought an avant-garde revolution. In the Czech lands mainly expressionist and cubist: Josef \u010capek, Emil Filla, Bohumil Kubi\u0161ta, Jan Zrzav\u00fd. Surrealism emerged particularly in the work of Toyen, Josef \u0160\u00edma and Karel Teige. In the world, however, he pushed mainly Franti\u0161ek Kupka, a pioneer of abstract painting. As illustrators and cartoonists in the first half of the 20th century gained fame Josef Lada, Zden\u011bk Burian or Emil Orl\u00edk. Art photography has become a new field (Franti\u0161ek Drtikol, Josef Sudek, later Jan Saudek or Josef Koudelka).\nThe Czech Republic is known for its individually made, mouth-blown, and decorated Bohemian glass.\nArchitecture.\nThe earliest preserved stone buildings in Bohemia and Moravia date back to the time of the Christianization in the 9th and 10th centuries. Since the Middle Ages, the Czech lands have been using the same architectural styles as most of Western and Central Europe. The oldest still standing churches were built in the Romanesque style. During the 13th century, it was replaced by the Gothic style. In the 14th century, Emperor Charles IV invited architects from France and Germany, Matthias of Arras and Peter Parler, to his court in Prague. During the Middle Ages, some fortified castles were built by the king and aristocracy, as well as some monasteries.\nThe Renaissance style penetrated the Bohemian Crown in the late 15th century when the older Gothic style started to be mixed with Renaissance elements. An example of pure Renaissance architecture in Bohemia is the Queen Anne's Summer Palace, which was situated in the garden of Prague Castle. Evidence of the general reception of the Renaissance in Bohemia, involving an influx of Italian architects, can be found in spacious chateaus with arcade courtyards and geometrically arranged gardens. Emphasis was placed on comfort, and buildings that were built for entertainment purposes also appeared.\nIn the 17th century, the Baroque style spread throughout the Crown of Bohemia.\nIn the 18th century, Bohemia produced an architectural peculiarity \u2013 the \"Baroque Gothic style\", a synthesis of the Gothic and Baroque styles.\nDuring the 19th century stands the revival architectural styles. Some churches were restored to their presumed medieval appearance and there were constructed buildings in the Neo-Romanesque, Neo-Gothic and Neo-Renaissance styles. At the turn of the 19th and 20th centuries, the new art style appeared in the Czech lands \u2013 Art Nouveau.\nBohemia contributed an unusual style to the world's architectural heritage when Czech architects attempted to transpose the Cubism of painting and sculpture into architecture.\nBetween World Wars I and II, Functionalism, with its sober, progressive forms, took over as the main architectural style.\nAfter World War II and the Communist coup in 1948, art in Czechoslovakia became Soviet-influenced. The Czechoslovak avant-garde artistic movement is known as the \"Brussels style came up\" in the time of political liberalization of Czechoslovakia in the 1960s. Brutalism dominated in the 1970s and 1980s.\nThe Czech Republic is not shying away from the more modern trends of international architecture, an example is the Dancing House (Tan\u010d\u00edc\u00ed d\u016fm) in Prague, Golden Angel in Prague or Congress Centre in Zl\u00edn.\nInfluential Czech architects include Peter Parler, Benedikt Rejt, Jan Santini Aichel, Kilian Ignaz Dientzenhofer, Josef Fanta, Josef Hl\u00e1vka, Josef Go\u010d\u00e1r, Pavel Jan\u00e1k, Jan Kot\u011bra, V\u011bra Machoninov\u00e1, Karel Prager, Karel Hub\u00e1\u010dek, Jan Kaplick\u00fd, Eva Ji\u0159i\u010dn\u00e1 or Josef Pleskot.\nLiterature.\nThe literature from the area of today's Czech Republic was mostly written in Czech, but also in Latin and German or even Old Church Slavonic. Franz Kafka, although a competent user of Czech, wrote in his mother tongue, German. His included: (\"The Trial\" and \"The Castle\").\nIn the second half of the 13th century, the royal court in Prague became one of the centers of German Minnesang and courtly literature. The Czech German-language literature can be seen in the first half of the 20th century.\nBible translations played a role in the development of Czech literature. The oldest Czech translation of the Psalms originated in the late 13th century and the first complete Czech translation of the Bible was finished around 1360. The first complete printed Czech Bible was published in 1488. The first complete Czech Bible translation from the original languages was published between 1579 and 1593. The Codex Gigas from the 12th century is the largest extant medieval manuscript in the world.\nCzech-language literature can be divided into several periods: the Middle Ages; the Hussite period; the Renaissance humanism; the Baroque period; the Enlightenment and Czech reawakening in the first half of the 19th century, modern literature in the second half of the 19th century; the avant-garde of the interwar period; the years under Communism; and the Czech Republic.\nThe antiwar comedy novel \"The Good Soldier \u0160vejk\" is the most translated Czech book in history.\nThe international literary award the Franz Kafka Prize is awarded in the Czech Republic.\nThe Czech Republic has the densest network of libraries in Europe.\nCzech literature and culture played a role on at least two occasions when Czechs lived under oppression and political activity was suppressed. On both of these occasions, in the early 19th century and then again in the 1960s, the Czechs used their cultural and literary effort to strive for political freedom, establishing a confident, politically aware nation.\nMusic.\nThe musical tradition of the Czech lands arose from the first church hymns, whose first evidence is suggested at the break of the 10th and 11th centuries. Some pieces of Czech music include two chorales, which in their time performed the function of anthems: \"Lord, Have Mercy on Us\" and the hymn \"Saint Wenceslaus\" or \"Saint Wenceslaus Chorale\". The authorship of the anthem \"Lord, Have Mercy on Us\" is ascribed by some historians to Saint Adalbert of Prague (sv.Vojt\u011bch), bishop of Prague, living between 956 and 997.\nThe wealth of musical culture lies in the classical music tradition during all historical periods, especially in the Baroque, Classicism, Romantic, modern classical music and in the traditional folk music of Bohemia, Moravia and Silesia. Since the early era of artificial music, Czech musicians and composers have been influenced the folk music of the region and dance.\nCzech music can be considered to have been \"beneficial\" in both the European and worldwide context, several times co-determined or even determined a newly arriving era in musical art, above all of Classical era, as well as by original attitudes in Baroque, Romantic and modern classical music. Some Czech musical works are \"The Bartered Bride\", \"New World Symphony\", \"Sinfonietta\" and \"Jen\u016ffa\".\nA music festival in the country is Prague Spring International Music Festival of classical music, a permanent showcase for performing artists, symphony orchestras and chamber music ensembles of the world.\nTheatre.\nThe roots of Czech theatre can be found in the Middle Ages, especially in the cultural life of the Gothic period. In the 19th century, the theatre played a role in the national awakening movement and later, in the 20th century, it became a part of modern European theatre art. The original Czech cultural phenomenon came into being at the end of the 1950s. This project called Laterna magika, resulting in productions that combined theater, dance, and film in a poetic manner, considered the first multimedia art project in an international context.\nA drama is Karel \u010capek's play \"R.U.R.\", which introduced the word \"robot\".\nThe country has a tradition of puppet theater. In 2016, Czech and Slovak Puppetry was included on the UNESCO Intangible Cultural Heritage Lists.\nFilm.\nThe tradition of Czech cinematography started in the second half of the 1890s. Peaks of the production in the era of silent movies include the historical drama \"The Builder of the Temple\" and the social and erotic drama \"Erotikon\" directed by Gustav Machat\u00fd. The early Czech sound film era was productive, above all in mainstream genres, with the comedies of Martin Fri\u010d or Karel Lama\u010d. There were dramatic movies sought internationally.\nHerm\u00edna T\u00fdrlov\u00e1 was a prominent Czech animator, screenwriter, and film director. She was often called the mother of Czech animation. Over the course of her career, she produced over 60 animated children's short films using puppets and the technique of stop motion animation.\nBefore the German occupation, in 1933, filmmaker and animator Irena Dodalov\u00e1 established the first Czech animation studio \"IRE Film\" with her husband Karel Dodal.\nAfter the period of Nazi occupation and early communist official dramaturgy of socialist realism in movies at the turn of the 1940s and 1950s with fewer exceptions such as \"Krakatit\" or \"Men without wings\" (awarded by in 1946), an era of the Czech film began with animated films, performed in anglophone countries under the name \"The Fabulous World of Jules Verne\" from 1958, which combined acted drama with animation, and Ji\u0159\u00ed Trnka, the founder of the modern puppet film. This began a tradition of animated films (\"Mole\" etc.).\nIn the 1960s, the hallmark of Czechoslovak New Wave's films were improvised dialogues, black and absurd humor and the occupation of non-actors. Directors are trying to preserve natural atmosphere without refinement and artificial arrangement of scenes. A personality of the 1960s and the beginning of the 1970s with original manuscript and psychological impact is Franti\u0161ek Vl\u00e1\u010dil. Another international author is Jan \u0160vankmajer, a filmmaker and artist whose work spans several media. He is a self-labeled surrealist known for animations and features.\nThe Barrandov Studios in Prague are the largest film studios with film locations in the country. Filmmakers have come to Prague to shoot scenery no longer found in Berlin, Paris and Vienna. The city of Karlovy Vary was used as a location for the 2006 James Bond film Casino Royale.\nThe Czech Lion is the highest Czech award for film achievement. Karlovy Vary International Film Festival is one of the film festivals that have been given competitive status by the FIAPF. Other film festivals held in the country include Febiofest, Jihlava International Documentary Film Festival, One World Film Festival, Zl\u00edn Film Festival and Fresh Film Festival.\nMedia.\nCzech journalists and media enjoy a degree of freedom. There are restrictions against writing in support of Nazism, racism or violating Czech law. The Czech press was ranked as the 40th most free press in the World Freedom Index by Reporters Without Borders in 2021. Radio Free Europe/Radio Liberty has its headquarters in Prague.\nThe national public television service is Czech Television that operates the 24-hour news channel \u010cT24 and the news website ct24.cz. As of 2020, Czech Television is the most watched television, followed by private televisions TV Nova and Prima TV. However, TV Nova has the most watched main news program and prime time program. Other public services include the Czech Radio and the Czech News Agency.\nThe best-selling daily national newspapers in 2020/21 are Blesk (average 703,000 daily readers), Mlad\u00e1 fronta DNES (average 461,000 daily readers), Pr\u00e1vo (average 182,000 daily readers), Lidov\u00e9 noviny (average 163,000 daily readers) and Hospod\u00e1\u0159sk\u00e9 noviny (average 162,000 daily readers).\nMost Czechs (87%) read their news online, with Seznam.cz, iDNES.cz, Novinky.cz, iPrima.cz and Seznam Zpr\u00e1vy.cz being the most visited as of 2021.\nCuisine.\nCzech cuisine is marked by an emphasis on meat dishes with pork, beef, and chicken. Goose, duck, rabbit, and venison are served. Fish is less common, with the occasional exception of fresh trout and carp, which is served at Christmas.\nThere is a variety of local sausages, wurst, p\u00e2t\u00e9s, and smoked and cured meats. Czech desserts include a variety of whipped cream, chocolate, and fruit pastries and tarts, cr\u00eapes, creme desserts and cheese, poppy-seed-filled and other types of traditional cakes such as \"buchty\", \"kol\u00e1\u010de\" and \"\u0161tr\u00fadl\".\nCzech beer has a history extending more than a millennium; the earliest known brewery existed in 993. Today the Czech Republic has the highest beer consumption per capita in the world. The pilsner style beer (pils) originated in Plze\u0148, where the world's first blond lager Pilsner Urquell is still produced. It has served as the inspiration for more than two-thirds of the beer produced in the world today. The city of \u010cesk\u00e9 Bud\u011bjovice has similarly lent its name to its beer, known as Budweiser Budvar.\nThe South Moravian region has been producing wine since the Middle Ages; about 94% of vineyards in the Czech Republic are Moravian. Aside from beer, slivovitz and wine, the Czech Republic also produces two liquors, Fernet Stock and Becherovka. Kofola is a non-alcoholic domestic cola soft drink which competes with Coca-Cola and Pepsi.\nSport.\nThe two leading sports in the Czech Republic are football and ice hockey. The most watched sporting events are the Olympic tournament and World Championships of ice hockey. Other most popular sports include tennis, volleyball, floorball, golf, ball hockey, athletics, basketball and skiing.\nThe country has won 15 gold medals in the Summer Olympics and nine in the Winter Games. (See Olympic history.) The Czech ice hockey team won the gold medal at the 1998 Winter Olympics and has won twelve gold medals at the World Championships, including three straight from 1999 to 2001.\nThe \u0160koda Motorsport is engaged in competition racing since 1901 and has gained a number of titles with various vehicles around the world. MTX automobile company was formerly engaged in the manufacture of racing and formula cars since 1969.\nHiking is a popular sport. The word for 'tourist' in Czech, \"turista\", also means 'trekker' or 'hiker'. For hikers, thanks to the more than 120-year-old tradition, there is the Czech Hiking Markers System of trail blazing, that has been adopted by countries worldwide. There is a network of around 40,000\u00a0km of marked short- and long-distance trails crossing the whole country and all the Czech mountains.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nGeneral sources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt; "}
{"id": "5322", "revid": "19682140", "url": "https://en.wikipedia.org/wiki?curid=5322", "title": "Czechoslovakia", "text": "Country in Central Europe from 1918 to 1992\nCzechoslovakia (; Czech and , \"\u010cesko-Slovensko\") was a landlocked state in Central Europe, created in 1918, when it declared its independence from Austria-Hungary. In 1938, after the Munich Agreement, the Sudetenland became part of Nazi Germany, while the country lost further territories to Hungary and Poland. Between 1939 and 1945, the state ceased to exist, as Slovakia proclaimed its independence and the remaining territories in the east became part of Hungary, while in the remainder of the Czech Lands, the German Protectorate of Bohemia and Moravia was proclaimed. In 1939, after the outbreak of World War II, former Czechoslovak President Edvard Bene\u0161 formed a government-in-exile and sought recognition from the Allies.\nAfter World War II, the pre-1938 Czechoslovakia was reestablished, with the exception of Carpathian Ruthenia, which became part of the Ukrainian SSR (a republic of the Soviet Union). From 1948 to 1989, Czechoslovakia was part of the Eastern Bloc with a command economy. Its economic status was formalized in membership of Comecon from 1949 and its defense status in the Warsaw Pact of 1955. A period of political liberalization in 1968, the Prague Spring, ended violently when the Soviet Union, assisted by other Warsaw Pact countries, invaded Czechoslovakia. In 1989, as Marxist\u2013Leninist governments and communism were ending all over Central and Eastern Europe, Czechoslovaks peacefully deposed their communist government during the Velvet Revolution, which began on 17 November 1989 and ended 11 days later on 28 November when all of the top Communist leaders and Communist party itself resigned. On 31 December 1992, Czechoslovakia peacefully split into the two sovereign states of the Czech Republic and Slovakia as the result of national tensions of the Slovaks.\nCharacteristics.\nThe country was of generally irregular terrain. The western area was part of the north-central European uplands. The eastern region was composed of the northern reaches of the Carpathian Mountains and lands of the Danube River basin.\nThe weather is mild winters and mild summers. Influenced by the Atlantic Ocean from the west, the Baltic Sea from the north, and Mediterranean Sea from the south. There is no continental weather.\nHistory.\nOrigins.\nThe area was part of the Austro-Hungarian Empire until it collapsed at the end of World War I. The new state was founded by Tom\u00e1\u0161 Garrigue Masaryk, who served as its first president from 14\u00a0November 1918 to 14\u00a0December 1935. He was succeeded by his close ally Edvard Bene\u0161 (1884\u20131948).\nThe roots of Czech nationalism go back to the 19th century, when philologists and educators, influenced by Romanticism, promoted the Czech language and pride in the Czech people. Nationalism became a mass movement in the second half of the 19th century. Taking advantage of the limited opportunities for participation in political life under Austrian rule, Czech leaders such as historian Franti\u0161ek Palack\u00fd (1798\u20131876) founded various patriotic, self-help organizations which provided a chance for many of their compatriots to participate in communal life before independence. Palack\u00fd supported Austro-Slavism and worked for a reorganized federal Austrian Empire, which would protect the Slavic speaking peoples of Central Europe against Russian and German threats.\nAn advocate of democratic reform and Czech autonomy within Austria-Hungary, Masaryk was elected twice to the \"Reichsrat\" (Austrian Parliament), from 1891 to 1893 for the Young Czech Party, and from 1907 to 1914 for the Czech Realist Party, which he had founded in 1889 with Karel Kram\u00e1\u0159 and Josef Kaizl.\nDuring World War I a number of Czechs and Slovaks, the Czechoslovak Legions, fought with the Allies in France and Italy, while large numbers deserted to Russia in exchange for its support for the independence of Czechoslovakia from the Austrian Empire. With the outbreak of World War I, Masaryk began working for Czech independence in a union with Slovakia. With Edvard Bene\u0161 and Milan Rastislav \u0160tef\u00e1nik, Masaryk visited several Western countries and won support from influential publicists. The Czechoslovak National Council was the main organization that advanced the claims for a Czechoslovak state.\nFirst Czechoslovak Republic.\nFormation.\nThe Bohemian Kingdom ceased to exist in 1918 when it was incorporated into Czechoslovakia. Czechoslovakia was founded in October 1918, as one of the successor states of the Austro-Hungarian Empire at the end of World War I and as part of the Treaty of Saint-Germain-en-Laye. It consisted of the present day territories of Bohemia, Moravia, Slovakia and Carpathian Ruthenia. Its territory included some of the most industrialized regions of the former Austria-Hungary. \"The land consisted of modern day Czechia, Slovakia, and a region of Ukraine called Carpathian Ruthnia\nEthnicity.\nThe new country was a multi-ethnic state, with Czechs and Slovaks as \"constituent peoples\". The population consisted of Czechs (51%), Slovaks (16%), Germans (22%), Hungarians (5%) and Rusyns (4%). Many of the Germans, Hungarians, Ruthenians and Poles and some Slovaks, felt oppressed because the political elite did not generally allow political autonomy for minority ethnic groups. This policy led to unrest among the non-Czech population, particularly in German-speaking Sudetenland, which initially had proclaimed itself part of the Republic of German-Austria in accordance with the self-determination principle.\nThe state proclaimed the official ideology that there were no separate Czech and Slovak nations, but only one nation of Czechoslovaks (see Czechoslovakism), to the disagreement of Slovaks and other ethnic groups. Once a unified Czechoslovakia was restored after World War II (after the country had been divided during the war), the conflict between the Czechs and the Slovaks surfaced again. The governments of Czechoslovakia and other Central European nations deported ethnic Germans, reducing the presence of minorities in the nation. Most of the Jews had been killed during the war by the Nazis.\nInterwar period.\nDuring the period between the two world wars Czechoslovakia was a democratic state. The population was generally literate, and contained fewer alienated groups. The influence of these conditions was augmented by the political values of Czechoslovakia's leaders and the policies they adopted. Under Tomas Masaryk, Czech and Slovak politicians promoted progressive social and economic conditions that served to defuse discontent.\nForeign minister Bene\u0161 became the prime architect of the Czechoslovak-Romanian-Yugoslav alliance (the \"Little Entente\", 1921\u201338) directed against Hungarian attempts to reclaim lost areas. Bene\u0161 worked closely with France. Far more dangerous was the German element, which after 1933 became allied with the Nazis in Germany.\nCzech-Slovak relations came to be a central issue in Czechoslovak politics during the 1930s. The increasing feeling of inferiority among the Slovaks, who were hostile to the more numerous Czechs, weakened the country in the late 1930s. Slovakia became autonomous in the fall of 1938, and by mid-1939, Slovakia had become independent, with the First Slovak Republic set up as a satellite state of Nazi Germany and the far-right Slovak People\u2019s Party in power .\nAfter 1933, Czechoslovakia remained the only democracy in central and eastern Europe.\nMunich Agreement, and Two-Step German Occupation.\nIn September 1938, Adolf Hitler demanded control of the Sudetenland. On 29\u00a0September 1938, Britain and France ceded control in the Appeasement at the Munich Conference; France ignored the military alliance it had with Czechoslovakia. During October 1938, Nazi Germany occupied the Sudetenland border region, effectively crippling Czechoslovak defences.\nThe First Vienna Award assigned a strip of southern Slovakia and Carpathian Ruthenia to Hungary. Poland occupied Zaolzie, an area whose population was majority Polish, in October 1938.\nOn 14 March 1939, the remainder (\"rump\") of Czechoslovakia was dismembered by the proclamation of the Slovak State, the next day the rest of Carpathian Ruthenia was occupied and annexed by Hungary, while the following day the German Protectorate of Bohemia and Moravia was proclaimed.\nThe eventual goal of the German state under Nazi leadership was to eradicate Czech nationality through assimilation, deportation, and extermination of the Czech intelligentsia; the intellectual elites and middle class made up a considerable number of the 200,000 people who passed through concentration camps and the 250,000 who died during German occupation. Under Generalplan Ost, it was assumed that around 50% of Czechs would be fit for Germanization. The Czech intellectual elites were to be removed not only from Czech territories but from Europe completely. The authors of Generalplan Ost believed it would be best if they emigrated overseas, as even in Siberia they were considered a threat to German rule. Just like Jews, Poles, Serbs, and several other nations, Czechs were considered to be untermenschen by the Nazi state. In 1940, in a secret Nazi plan for the Germanization of the Protectorate of Bohemia and Moravia it was declared that those considered to be of racially Mongoloid origin and the Czech intelligentsia were not to be Germanized.\nThe deportation of Jews to concentration camps was organized under the direction of Reinhard Heydrich, and the fortress town of Terez\u00edn was made into a ghetto way station for Jewish families. On 4\u00a0June 1942 Heydrich died after being wounded by an assassin in Operation Anthropoid. Heydrich's successor, Colonel General Kurt Daluege, ordered mass arrests and executions and the destruction of the villages of Lidice and Le\u017e\u00e1ky. In 1943 the German war effort was accelerated. Under the authority of Karl Hermann Frank, German minister of state for Bohemia and Moravia, some 350,000 Czech laborers were dispatched to the Reich. Within the protectorate, all non-war-related industry was prohibited. Most of the Czech population obeyed quiescently up until the final months preceding the end of the war, while thousands were involved in the resistance movement.\nFor the Czechs of the Protectorate Bohemia and Moravia, German occupation was a period of brutal oppression. Czech losses resulting from political persecution and deaths in concentration camps totaled between 36,000 and 55,000. The Jewish populations of Bohemia and Moravia (118,000 according to the 1930 census) were virtually annihilated. Many Jews emigrated after 1939; more than 70,000 were killed; 8,000 survived at Terez\u00edn. Several thousand Jews managed to live in freedom or in hiding throughout the occupation.\nDespite the estimated 136,000 deaths at the hands of the Nazi regime, the population in the Reichsprotektorate saw a net increase during the war years of approximately 250,000 in line with an increased birth rate.\nOn 6 May 1945, the third US Army of General Patton entered Pilsen from the south west. On 9\u00a0May 1945, Soviet Red Army troops entered Prague.\nCommunist Czechoslovakia.\nAfter World War II, pre-war Czechoslovakia was re-established, with the exception of Subcarpathian Ruthenia, which was annexed by the Soviet Union and incorporated into the Ukrainian Soviet Socialist Republic. The Bene\u0161 decrees were promulgated concerning ethnic Germans (see Potsdam Agreement) and ethnic Hungarians. Under the decrees, citizenship was abrogated for people of German and Hungarian ethnic origin who had accepted German or Hungarian citizenship during the occupations. In 1948, this provision was cancelled for the Hungarians, but only partially for the Germans. The government then confiscated the property of the Germans and expelled about 90% of the ethnic German population, over 2\u00a0million people. Those who remained were collectively accused of supporting the Nazis after the Munich Agreement, as 97.32% of Sudeten Germans had voted for the NSDAP in the December 1938 elections. Almost every decree explicitly stated that the sanctions did not apply to antifascists. Some 250,000 Germans, many married to Czechs, some antifascists, and also those required for the post-war reconstruction of the country, remained in Czechoslovakia. The Bene\u0161 Decrees still cause controversy among nationalist groups in the Czech Republic, Germany, Austria and Hungary.\nFollowing the expulsion of the ethnic German population from Czechoslovakia, parts of the former Sudetenland, especially around Krnov and the surrounding villages of the Jesenik mountain region in northeastern Czechoslovakia, were settled in 1949 by Communist refugees from Northern Greece who had left their homeland as a result of the Greek Civil War. These Greeks made up a large proportion of the town and region's population until the late 1980s/early 1990s. Although defined as \"Greeks\", the Greek Communist community of Krnov and the Jeseniky region actually consisted of an ethnically diverse population, including Greek Macedonians, Macedonians, Vlachs, Pontic Greeks and Turkish speaking Urums or Caucasus Greeks.\nCarpathian Ruthenia (Podkarpatsk\u00e1 Rus) was occupied by (and in June 1945 formally ceded to) the Soviet Union. In the 1946 parliamentary election, the Communist Party of Czechoslovakia was the winner in the Czech lands, and the Democratic Party won in Slovakia. In February 1948 the Communists seized power. Although they would maintain the fiction of political pluralism through the existence of the National Front, except for a short period in the late 1960s (the Prague Spring) the country had no liberal democracy. Since citizens lacked significant electoral methods of registering protest against government policies, periodically there were street protests that became violent. For example, there were riots in the town of Plze\u0148 in 1953, reflecting economic discontent. Police and army units put down the rebellion, and hundreds were injured but no one was killed. While its economy remained more advanced than those of its neighbors in Eastern Europe, Czechoslovakia grew increasingly economically weak relative to Western Europe.\nThe currency reform of 1953 caused dissatisfaction among Czechoslovak laborers. To equalize the wage rate, Czechoslovaks had to turn in their old money for new at a decreased value. The banks also confiscated savings and bank deposits to control the amount of money in circulation. In the 1950s, Czechoslovakia experienced high economic growth (averaging 7% per year), which allowed for a substantial increase in wages and living standards, thus promoting the stability of the regime.\nIn 1968, when the reformer Alexander Dub\u010dek was appointed to the key post of First Secretary of the Czechoslovak Communist Party, there was a brief period of liberalization known as the Prague Spring. In response, after failing to persuade the Czechoslovak leaders to change course, five other members of the Warsaw Pact invaded. Soviet tanks rolled into Czechoslovakia on the night of 20\u201321\u00a0August 1968. Soviet Communist Party General Secretary Leonid Brezhnev viewed this intervention as vital for the preservation of the Soviet, socialist system and vowed to intervene in any state that sought to replace Marxism-Leninism with capitalism.\nIn the week after the invasion there was a spontaneous campaign of civil resistance against the occupation. This resistance involved a wide range of acts of non-cooperation and defiance: this was followed by a period in which the Czechoslovak Communist Party leadership, having been forced in Moscow to make concessions to the Soviet Union, gradually put the brakes on their earlier liberal policies.\nMeanwhile, one plank of the reform program had been carried out: in 1968\u201369, Czechoslovakia was turned into a federation of the Czech Socialist Republic and Slovak Socialist Republic. The theory was that under the federation, social and economic inequities between the Czech and Slovak halves of the state would be largely eliminated. A number of ministries, such as education, now became two formally equal bodies in the two formally equal republics. However, the centralized political control by the Czechoslovak Communist Party severely limited the effects of federalization.\nThe 1970s saw the rise of the dissident movement in Czechoslovakia, represented among others by V\u00e1clav Havel. The movement sought greater political participation and expression in the face of official disapproval, manifested in limitations on work activities, which went as far as a ban on professional employment, the refusal of higher education for the dissidents' children, police harassment and prison.\nDuring the 1980s, Czechoslovakia became one of the most tightly controlled Communist regimes in the Warsaw Pact in resistance to the mitigation of controls notified by Soviet president Mikhail Gorbachev.\nAfter 1989.\nIn 1989, the Velvet Revolution restored democracy. This occurred at around the same time as the fall of communism in Romania, Bulgaria, Hungary, East Germany and Poland.\nThe word \"socialist\" was removed from the country's full name on 29\u00a0March 1990 and replaced by \"federal\".\nPope John Paul II made a papal visit to Czechoslovakia on 21 April 1990, hailing it as a symbolic step of reviving Christianity in the newly-formed post-communist state.\nCzechoslovakia participated in the Gulf War with a small force of 200 troops under the command of the U.S.-led coalition.\nIn 1992, because of growing nationalist tensions in the government, Czechoslovakia was peacefully dissolved by parliament. On 31\u00a0December 1992 it formally separated into two independent countries, the Czech Republic and the Slovak Republic.\nGovernment and politics.\nAfter World War II, a political monopoly was held by the Communist Party of Czechoslovakia (KS\u010c). The leader of the KS\u010c was \"de facto\" the most powerful person in the country during this period. Gust\u00e1v Hus\u00e1k was elected first secretary of the KS\u010c in 1969 (changed to general secretary in 1971) and president of Czechoslovakia in 1975. Other parties and organizations existed but functioned in subordinate roles to the KS\u010c. All political parties, as well as numerous mass organizations, were grouped under umbrella of the National Front. Human rights activists and religious activists were severely repressed.\nConstitutional development.\nCzechoslovakia had the following constitutions during its history (1918\u20131992):\nForeign policy.\nInternational agreements and membership.\nIn the 1930s, the nation formed a military alliance with France, which collapsed in the Munich Agreement of 1938. After World War II, an active participant in Council for Mutual Economic Assistance (Comecon), Warsaw Pact, United Nations and its specialized agencies; signatory of conference on Security and Cooperation in Europe.\nEconomy.\nBefore World War II, the economy was about the fourth in all industrial countries in Europe. The state was based on strong economy, manufacturing cars (\u0160koda, Tatra), trams, aircraft (Aero, Avia), ships, ship engines (\u0160koda), cannons, shoes (Ba\u0165a), turbines, guns (Zbrojovka Brno). It was the industrial workshop for the Austro-Hungarian empire. The Slovak lands relied more heavily on agriculture than the Czech lands.\nAfter World War II, the economy was centrally planned, with command links controlled by the communist party, similarly to the Soviet Union. The large metallurgical industry was dependent on imports of iron and non-ferrous ores.\nResource base.\nAfter World War II, the country was short of energy, relying on imported crude oil and natural gas from the Soviet Union, domestic brown coal, and nuclear and hydroelectric energy. Energy constraints were a major factor in the 1980s.\nTransport and communications.\nSlightly after the foundation of Czechoslovakia in 1918, there was a lack of essential infrastructure in many areas \u2013 paved roads, railways, bridges, etc. Massive improvement in the following years enabled Czechoslovakia to develop its industry. Prague's civil airport in Ruzyn\u011b became one of the most modern terminals in the world when it was finished in 1937. Tom\u00e1\u0161 Ba\u0165a, a Czech entrepreneur and visionary, outlined his ideas in the publication \"Budujme st\u00e1t pro 40 milion\u016f lid\u00ed\", where he described the future motorway system. Construction of the first motorways in Czechoslovakia begun in 1939, nevertheless, they were stopped after German occupation during World War II.\nEducation.\nEducation was free at all levels and compulsory from ages 6 to 15. The vast majority of the population was literate. There was a highly developed system of apprenticeship training and vocational schools supplemented general secondary schools and institutions of higher education.\nReligion.\nIn 1991, 46% of the population were Roman Catholics, 5.3% were Evangelical Lutheran, 30% were Atheist, and other religions made up 17% of the country, but there were huge differences in religious practices between the two constituent republics; see Czech Republic and Slovakia.\nHealth, social welfare and housing.\nAfter World War II, free health care was available to all citizens. National health planning emphasized preventive medicine; factory and local health care centres supplemented hospitals and other inpatient institutions. There was a substantial improvement in rural health care during the 1960s and 1970s.\nMass media.\nDuring the era between the World Wars, Czechoslovak democracy and liberalism facilitated conditions for free publication. The most significant daily newspapers in these times were Lidov\u00e9 noviny, N\u00e1rodn\u00ed listy, \u010cesk\u00fd den\u00edk and \u010ceskoslovensk\u00e1 Republika.\nDuring Communist rule, the mass media in Czechoslovakia were controlled by the Communist Party. Private ownership of any publication or agency of the mass media was generally forbidden, although churches and other organizations published small periodicals and newspapers. Even with this information monopoly in the hands of organizations under KS\u010c control, all publications were reviewed by the government's Office for Press and Information.\nSports.\nThe Czechoslovakia national football team was a consistent performer on the international scene, with eight appearances in the FIFA World Cup Finals, finishing in second place in 1934 and 1962. The team also won the European Football Championship in 1976, came in third in 1980 and won the Olympic gold in 1980.\nWell-known football players such as Pavel Nedv\u011bd, Anton\u00edn Panenka, Milan Baro\u0161, Tom\u00e1\u0161 Rosick\u00fd, Vladim\u00edr \u0160micer or Petr \u010cech were all born in Czechoslovakia.\nThe International Olympic Committee code for Czechoslovakia is TCH, which is still used in historical listings of results.\nThe Czechoslovak national ice hockey team won many medals from the world championships and Olympic Games. Peter \u0160\u0165astn\u00fd, Jarom\u00edr J\u00e1gr, Dominik Ha\u0161ek, Peter Bondra, Petr Kl\u00edma, Mari\u00e1n G\u00e1bor\u00edk, Mari\u00e1n Hossa, Miroslav \u0160atan and Pavol Demitra all come from Czechoslovakia.\nEmil Z\u00e1topek, winner of four Olympic gold medals in athletics, is considered one of the top athletes in Czechoslovak history.\nV\u011bra \u010c\u00e1slavsk\u00e1 was an Olympic gold medallist in gymnastics, winning seven gold medals and four silver medals. She represented Czechoslovakia in three consecutive Olympics.\nSeveral accomplished professional tennis players including Jaroslav Drobn\u00fd, Ivan Lendl, Jan Kode\u0161, Miloslav Me\u010d\u00ed\u0159, Hana Mandl\u00edkov\u00e1, Martina Hingis, Martina Navratilova, Jana Novotna, Petra Kvitov\u00e1 and Daniela Hantuchov\u00e1 were born in Czechoslovakia.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nExternal links.\nMaps with Hungarian-language rubrics:"}
{"id": "5323", "revid": "46026720", "url": "https://en.wikipedia.org/wiki?curid=5323", "title": "Computer science", "text": "Study of computation\nComputer science is the study of computation, information, and automation. Computer science spans theoretical disciplines (such as algorithms, theory of computation, and information theory) to applied disciplines (including the design and implementation of hardware and software). Though more often considered an academic discipline, computer science is closely related to computer programming.\nAlgorithms and data structures are central to computer science.\nThe theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human\u2013computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.\nThe fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nHistory.\nThe earliest foundations of what would become computer science predate the invention of the modern digital computer. Machines for calculating fixed numerical tasks such as the abacus have existed since antiquity, aiding in computations such as multiplication and division. Algorithms for performing computations have existed since antiquity, even before the development of sophisticated computing equipment.\nWilhelm Schickard designed and constructed the first working mechanical calculator in 1623. In 1673, Gottfried Leibniz demonstrated a digital mechanical calculator, called the Stepped Reckoner. Leibniz may be considered the first computer scientist and information theorist, because of various reasons, including the fact that he documented the binary number system. In 1820, Thomas de Colmar launched the mechanical calculator industry when he invented his simplified arithmometer, the first calculating machine strong enough and reliable enough to be used daily in an office environment. Charles Babbage started the design of the first \"automatic mechanical calculator\", his Difference Engine, in 1822, which eventually gave him the idea of the first \"programmable mechanical calculator\", his Analytical Engine. He started developing this machine in 1834, and \"in less than two years, he had sketched out many of the salient features of the modern computer\". \"A crucial step was the adoption of a punched card system derived from the Jacquard loom\" making it infinitely programmable. In 1843, during the translation of a French article on the Analytical Engine, Ada Lovelace wrote, in one of the many notes she included, an algorithm to compute the Bernoulli numbers, which is considered to be the first published algorithm ever specifically tailored for implementation on a computer. Around 1885, Herman Hollerith invented the tabulator, which used punched cards to process statistical information; eventually his company became part of IBM. Following Babbage, although unaware of his earlier work, Percy Ludgate in 1909 published the 2nd of the only two designs for mechanical analytical engines in history. In 1913, the Spanish engineer Leonardo Torres Quevedo wrote his \"Essays on Automatics\", and designed, inspired by Babbage, a theoretical electromechanical calculating machine which was to be controlled by a read-only program. The paper also introduced the idea of floating-point arithmetic. In 1920, to demonstrate his concepts, Torres built an electromechanical calculator that could automatically perform all four arithmetic operations. In 1937, one hundred years after Babbage's impossible dream, Howard Aiken convinced IBM, which was making all kinds of punched card equipment and was also in the calculator business to develop his giant programmable calculator, the ASCC/Harvard Mark I, based on Babbage's Analytical Engine, which itself used cards and a central computing unit. When the machine was finished, some hailed it as \"Babbage's dream come true\".\nDuring the 1940s, with the development of new and more powerful computing machines such as the Atanasoff\u2013Berry computer and ENIAC, the term \"computer\" came to refer to the machines rather than their human predecessors. As it became clear that computers could be used for more than just mathematical calculations, the field of computer science broadened to study computation in general. In 1945, IBM founded the Watson Scientific Computing Laboratory at Columbia University in New York City. The renovated fraternity house on Manhattan's West Side was IBM's first laboratory devoted to pure science. The lab is the forerunner of IBM's Research Division, which today operates research facilities around the world. Ultimately, the close relationship between IBM and Columbia University was instrumental in the emergence of a new scientific discipline, with Columbia offering one of the first academic-credit courses in computer science in 1946. Computer science began to be established as a distinct academic discipline in the 1950s and early 1960s. The world's first computer science degree program, the Cambridge Diploma in Computer Science, began at the University of Cambridge Computer Laboratory in 1953. The first computer science department in the United States was formed at Purdue University in 1962. Since practical computers became available, many applications of computing have become distinct areas of study in their own rights.\nEtymology.\nAlthough first proposed in 1956, the term \"computer science\" appears in a 1959 article in \"Communications of the ACM\",\nin which Louis Fein argues for the creation of a \"Graduate School in Computer Sciences\" analogous to the creation of Harvard Business School in 1921. Louis justifies the name by arguing that, like management science, the subject is applied and interdisciplinary in nature, while having the characteristics typical of an academic discipline.\nHis efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Despite its name, a significant amount of computer science does not involve the study of computers themselves. Because of this, several alternative names have been proposed. Certain departments of major universities prefer the term \"computing science\", to emphasize precisely that difference. Danish scientist Peter Naur suggested the term \"datalogy\", to reflect the fact that the scientific discipline revolves around data and data treatment, while not necessarily involving computers. The first scientific institution to use the term was the Department of Datalogy at the University of Copenhagen, founded in 1969, with Peter Naur being the first professor in datalogy. The term is used mainly in the Scandinavian countries. An alternative term, also proposed by Naur, is data science; this is now used for a multi-disciplinary field of data analysis, including statistics and databases.\nIn the early days of computing, a number of terms for the practitioners of the field of computing were suggested in the \"Communications of the ACM\"\u2014\"turingineer\", \"turologist\", \"flow-charts-man\", \"applied meta-mathematician\", and \"applied epistemologist\". Three months later in the same journal, \"comptologist\" was suggested, followed next year by \"hypologist\". The term \"computics\" has also been suggested. In Europe, terms derived from contracted translations of the expression \"automatic information\" (e.g. \"informazione automatica\" in Italian) or \"information and mathematics\" are often used, e.g. \"informatique\" (French), \"Informatik\" (German), \"informatica\" (Italian, Dutch), \"inform\u00e1tica\" (Spanish, Portuguese), \"informatika\" (Slavic languages and Hungarian) or \"pliroforiki\" (\"\u03c0\u03bb\u03b7\u03c1\u03bf\u03c6\u03bf\u03c1\u03b9\u03ba\u03ae\", which means informatics) in Greek. Similar words have also been adopted in the UK (as in the School of Informatics, University of Edinburgh). \"In the U.S., however, informatics is linked with applied computing, or computing in the context of another domain.\"\nA folkloric quotation, often attributed to\u2014but almost certainly not first formulated by\u2014Edsger Dijkstra, states that \"computer science is no more about computers than astronomy is about telescopes.\" The design and deployment of computers and computer systems is generally considered the province of disciplines other than computer science. For example, the study of computer hardware is usually considered part of computer engineering, while the study of commercial computer systems and their deployment is often called information technology or information systems. However, there has been exchange of ideas between the various computer-related disciplines. Computer science research also often intersects other disciplines, such as cognitive science, linguistics, mathematics, physics, biology, Earth science, statistics, philosophy, and logic.\nComputer science is considered by some to have a much closer relationship with mathematics than many scientific disciplines, with some observers saying that computing is a mathematical science. Early computer science was strongly influenced by the work of mathematicians such as Kurt G\u00f6del, Alan Turing, John von Neumann, R\u00f3zsa P\u00e9ter and Alonzo Church and there continues to be a useful interchange of ideas between the two fields in areas such as mathematical logic, category theory, domain theory, and algebra.\nThe relationship between Computer Science and Software Engineering is a contentious issue, which is further muddied by disputes over what the term \"Software Engineering\" means, and how computer science is defined. David Parnas, taking a cue from the relationship between other engineering and science disciplines, has claimed that the principal focus of computer science is studying the properties of computation in general, while the principal focus of software engineering is the design of specific computations to achieve practical goals, making the two separate but complementary disciplines.\nThe academic, political, and funding aspects of computer science tend to depend on whether a department is formed with a mathematical emphasis or with an engineering emphasis. Computer science departments with a mathematics emphasis and with a numerical orientation consider alignment with computational science. Both types of departments tend to make efforts to bridge the field educationally if not across all research.\nPhilosophy.\nEpistemology of computer science.\nDespite the word \"science\" in its name, there is debate over whether or not computer science is a discipline of science, mathematics, or engineering. Allen Newell and Herbert A. Simon argued in 1975, &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Computer science is an empirical discipline. We would have called it an experimental science, but like astronomy, economics, and geology, some of its unique forms of observation and experience do not fit a narrow stereotype of the experimental method. Nonetheless, they are experiments. Each new machine that is built is an experiment. Actually constructing the machine poses a question to nature; and we listen for the answer by observing the machine in operation and analyzing it by all analytical and measurement means available. It has since been argued that computer science can be classified as an empirical science since it makes use of empirical testing to evaluate the correctness of programs, but a problem remains in defining the laws and theorems of computer science (if any exist) and defining the nature of experiments in computer science. Proponents of classifying computer science as an engineering discipline argue that the reliability of computational systems is investigated in the same way as bridges in civil engineering and airplanes in aerospace engineering. They also argue that while empirical sciences observe what presently exists, computer science observes what is possible to exist and while scientists discover laws from observation, no proper laws have been found in computer science and it is instead concerned with creating phenomena.\nProponents of classifying computer science as a mathematical discipline argue that computer programs are physical realizations of mathematical entities and programs can be deductively reasoned through mathematical formal methods. Computer scientists Edsger W. Dijkstra and Tony Hoare regard instructions for computer programs as mathematical sentences and interpret formal semantics for programming languages as mathematical axiomatic systems.\nParadigms of computer science.\nA number of computer scientists have argued for the distinction of three separate paradigms in computer science. Peter Wegner argued that those paradigms are science, technology, and mathematics. Peter Denning's working group argued that they are theory, abstraction (modeling), and design. Amnon H. Eden described them as the \"rationalist paradigm\" (which treats computer science as a branch of mathematics, which is prevalent in theoretical computer science, and mainly employs deductive reasoning), the \"technocratic paradigm\" (which might be found in engineering approaches, most prominently in software engineering), and the \"scientific paradigm\" (which approaches computer-related artifacts from the empirical perspective of natural sciences, identifiable in some branches of artificial intelligence).\nComputer science focuses on methods involved in design, specification, programming, verification, implementation and testing of human-made computing systems.\nFields.\nAs a discipline, computer science spans a range of topics from theoretical studies of algorithms and the limits of computation to the practical issues of implementing computing systems in hardware and software.\nCSAB, formerly called Computing Sciences Accreditation Board\u2014which is made up of representatives of the Association for Computing Machinery (ACM), and the IEEE Computer Society (IEEE CS)\u2014identifies four areas that it considers crucial to the discipline of computer science: \"theory of computation\", \"algorithms and data structures\", \"programming methodology and languages\", and \"computer elements and architecture\". In addition to these four areas, CSAB also identifies fields such as software engineering, artificial intelligence, computer networking and communication, database systems, parallel computation, distributed computation, human\u2013computer interaction, computer graphics, operating systems, and numerical and symbolic computation as being important areas of computer science.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Computer science is no more about computers than astronomy is about telescopes.\nTheoretical computer science.\n\"Theoretical Computer Science\" is mathematical and abstract in spirit, but it derives its motivation from the practical and everyday computation. Its aim is to understand the nature of computation and, as a consequence of this understanding, provide more efficient methodologies.\nTheory of computation.\nAccording to Peter Denning, the fundamental question underlying computer science is, \"What can be automated?\" Theory of computation is focused on answering fundamental questions about what can be computed and what amount of resources are required to perform those computations. In an effort to answer the first question, computability theory examines which computational problems are solvable on various theoretical models of computation. The second question is addressed by computational complexity theory, which studies the time and space costs associated with different approaches to solving a multitude of computational problems.\nThe famous P = NP? problem, one of the Millennium Prize Problems, is an open problem in the theory of computation.\nInformation and coding theory.\nInformation theory, closely related to probability and statistics, is related to the quantification of information. This was developed by Claude Shannon to find fundamental limits on signal processing operations such as compressing data and on reliably storing and communicating data.\nCoding theory is the study of the properties of codes (systems for converting information from one form to another) and their fitness for a specific application. Codes are used for data compression, cryptography, error detection and correction, and more recently also for network coding. Codes are studied for the purpose of designing efficient and reliable data transmission methods.\nData structures and algorithms.\nData structures and algorithms are the studies of commonly used computational methods and their computational efficiency.\nProgramming language theory and formal methods.\nProgramming language theory is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages and their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, and linguistics. It is an active research area, with numerous dedicated academic journals.\nFormal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design. They form an important theoretical underpinning for software engineering, especially where safety or security is involved. Formal methods are a useful adjunct to software testing since they help avoid errors and can also give a framework for testing. For industrial use, tool support is required. However, the high cost of using formal methods means that they are usually only used in the development of high-integrity and life-critical systems, where safety or security is of utmost importance. Formal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\nApplied computer science.\nComputer graphics and visualization.\nComputer graphics is the study of digital visual contents and involves the synthesis and manipulation of image data. The study is connected to many other fields in computer science, including computer vision, image processing, and computational geometry, and is heavily applied in the fields of special effects and video games.\nImage and sound processing.\nInformation can take the form of images, sound, video or other multimedia. Bits of information can be streamed via signals. Its processing is the central notion of informatics, the European view on computing, which studies information processing algorithms independently of the type of information carrier \u2013 whether it is electrical, mechanical or biological. This field plays important role in information theory, telecommunications, information engineering and has applications in medical image computing and speech synthesis, among others. \"What is the lower bound on the complexity of fast Fourier transform algorithms?\" is one of unsolved problems in theoretical computer science.\nComputational science, finance and engineering.\nScientific computing (or computational science) is the field of study concerned with constructing mathematical models and quantitative analysis techniques and using computers to analyze and solve scientific problems. A major usage of scientific computing is simulation of various processes, including computational fluid dynamics, physical, electrical, and electronic systems and circuits, as well as societies and social situations (notably war games) along with their habitats, among many others. Modern computers enable optimization of such designs as complete aircraft. Notable in electrical and electronic circuit design are SPICE, as well as software for physical realization of new (or modified) designs. The latter includes essential design software for integrated circuits.\nSocial computing and human\u2013computer interaction.\nSocial computing is an area that is concerned with the intersection of social behavior and computational systems. Human\u2013computer interaction research develops theories, principles, and guidelines for user interface designers.\nSoftware engineering.\nSoftware engineering is the study of designing, implementing, and modifying the software in order to ensure it is of high quality, affordable, maintainable, and fast to build. It is a systematic approach to software design, involving the application of engineering practices to software. Software engineering deals with the organizing and analyzing of software\u2014it does not just deal with the creation or manufacture of new software, but its internal arrangement and maintenance. For example software testing, systems engineering, technical debt and software development processes.\nArtificial intelligence.\nArtificial intelligence (AI) aims to or is required to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, learning, and communication found in humans and animals. From its origins in cybernetics and in the Dartmouth Conference (1956), artificial intelligence research has been necessarily cross-disciplinary, drawing on areas of expertise such as applied mathematics, symbolic logic, semiotics, electrical engineering, philosophy of mind, neurophysiology, and social intelligence. AI is associated in the popular mind with robotic development, but the main field of practical application has been as an embedded component in areas of software development, which require computational understanding. The starting point in the late 1940s was Alan Turing's question \"Can computers think?\", and the question remains effectively unanswered, although the Turing test is still used to assess computer output on the scale of human intelligence. But the automation of evaluative and predictive tasks has been increasingly successful as a substitute for human monitoring and intervention in domains of computer application involving complex real-world data.\nComputer systems.\nComputer architecture and organization.\nComputer architecture, or digital computer organization, is the conceptual design and fundamental operational structure of a computer system. It focuses largely on the way by which the central processing unit performs internally and accesses addresses in memory. Computer engineers study computational logic and design of computer hardware, from individual processor components, microcontrollers, personal computers to supercomputers and embedded systems. The term \"architecture\" in computer literature can be traced to the work of Lyle R. Johnson and Frederick P. Brooks, Jr., members of the Machine Organization department in IBM's main research center in 1959.\nConcurrent, parallel and distributed computing.\nConcurrency is a property of systems in which several computations are executing simultaneously, and potentially interacting with each other. A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi and the Parallel Random Access Machine model. When multiple computers are connected in a network while using concurrency, this is known as a distributed system. Computers within that distributed system have their own private memory, and information can be exchanged to achieve common goals.\nComputer networks.\nThis branch of computer science aims to manage networks between computers worldwide.\nComputer security and cryptography.\nComputer security is a branch of computer technology with the objective of protecting information from unauthorized access, disruption, or modification while maintaining the accessibility and usability of the system for its intended users.\nHistorical cryptography is the art of writing and deciphering secret messages. Modern cryptography is the scientific study of problems relating to distributed computations that can be attacked. Technologies studied in modern cryptography include symmetric and asymmetric encryption, digital signatures, cryptographic hash functions, key-agreement protocols, blockchain, zero-knowledge proofs, and garbled circuits.\nDatabases and data mining.\nA database is intended to organize, store, and retrieve large amounts of data easily. Digital databases are managed using database management systems to store, create, maintain, and search data, through database models and query languages. Data mining is a process of discovering patterns in large data sets.\nDiscoveries.\nThe philosopher of computing Bill Rapaport noted three \"Great Insights of Computer Science\":\n All the information about any computable problem can be represented using only 0 and 1 (or any other bistable pair that can flip-flop between two easily distinguishable states, such as \"on/off\", \"magnetized/de-magnetized\", \"high-voltage/low-voltage\", etc.).\n Every algorithm can be expressed in a language for a computer consisting of only five basic instructions:\n* move left one location;\n* move right one location;\n* read symbol at current location;\n* print 0 at current location;\n* print 1 at current location.\n Only three rules are needed to combine any set of basic instructions into more complex ones:\n*\"sequence\": first do this, then do that;\n* \" selection\": IF such-and-such is the case, THEN do this, ELSE do that;\n* \"repetition\": WHILE such-and-such is the case, DO this.\n Note that the three rules of Boehm's and Jacopini's insight can be further simplified with the use of goto (which means it is more elementary than structured programming).\nProgramming paradigms.\nProgramming languages can be used to accomplish different tasks in different ways. Common programming paradigms include:\nMany languages offer support for multiple paradigms, making the distinction more a matter of style than of technical capabilities.\nResearch.\nConferences are important events for computer science research. During these conferences, researchers from the public and private sectors present their recent work and meet. Unlike in most other academic fields, in computer science, the prestige of conference papers is greater than that of journal publications. One proposed explanation for this is the quick development of this relatively new field requires rapid review and distribution of results, a task better handled by conferences than by journals.\nEducation.\nComputer Science, known by its near synonyms, Computing, Computer Studies, has been taught in UK schools since the days of batch processing, mark sensitive cards and paper tape but usually to a select few students. In 1981, the BBC produced a micro-computer and classroom network and Computer Studies became common for GCE O level students (11\u201316-year-old), and Computer Science to A level students. Its importance was recognised, and it became a compulsory part of the National Curriculum, for Key Stage 3 &amp; 4. In September 2014 it became an entitlement for all pupils over the age of 4.\nIn the US, with 14,000 school districts deciding the curriculum, provision was fractured. According to a 2010 report by the Association for Computing Machinery (ACM) and Computer Science Teachers Association (CSTA), only 14 out of 50 states have adopted significant education standards for high school computer science. According to a 2021 report, only 51% of high schools in the US offer computer science.\nIsrael, New Zealand, and South Korea have included computer science in their national secondary education curricula, and several others are following.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5324", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=5324", "title": "Catalan", "text": "Catalan may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nCatalonia.\nFrom, or related to Catalonia:\nMathematics.\nMathematical concepts named after mathematician Eug\u00e8ne Catalan:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5326", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5326", "title": "Creationism", "text": "Belief that nature originated through supernatural acts\nCreationism is the religious belief that nature, and aspects such as the universe, Earth, life, and humans, originated with supernatural acts of divine creation. In its broadest sense, creationism includes a continuum of religious views, which vary in their acceptance or rejection of scientific explanations such as evolution that describe the origin and development of natural phenomena.\nThe term \"creationism\" most often refers to belief in special creation; the claim that the universe and lifeforms were created as they exist today by divine action, and that the only true explanations are those which are compatible with a Christian fundamentalist literal interpretation of the creation myth found in the Bible's Genesis creation narrative. Since the 1970s, the most common form of this has been Young Earth creationism which posits special creation of the universe and lifeforms within the last 10,000 years on the basis of flood geology, and promotes pseudoscientific creation science. From the 18th century onward, Old Earth creationism accepted geological time harmonized with Genesis through gap or day-age theory, while supporting anti-evolution. Modern old-Earth creationists support progressive creationism and continue to reject evolutionary explanations. Following political controversy, creation science was reformulated as intelligent design and neo-creationism.\nMainline Protestants and the Catholic Church reconcile modern science with their faith in Creation through forms of theistic evolution which hold that God purposefully created through the laws of nature, and accept evolution. Some groups call their belief evolutionary creationism. Less prominently, there are also members of the Islamic and Hindu faiths who are creationists. Use of the term \"creationist\" in this context dates back to Charles Darwin's unpublished 1842 sketch draft for what became \"On the Origin of Species\", and he used the term later in letters to colleagues. In 1873, Asa Gray published an article in \"The Nation\" saying a \"special creationist\" who held that species \"were supernaturally originated just as they are, by the very terms of his doctrine places them out of the reach of scientific explanation.\"\nBiblical basis.\nThe basis for many creationists' beliefs is a literal or quasi-literal interpretation of the Book of Genesis. The Genesis creation narratives (Genesis 1\u20132) describe how God brings the Universe into being in a series of creative acts over six days and places the first man and woman (Adam and Eve) in the Garden of Eden. This story is the basis of creationist cosmology and biology. The Genesis flood narrative (Genesis 6\u20139) tells how God destroys the world and all life through a great flood, saving representatives of each form of life by means of Noah's Ark. This forms the basis of creationist geology, better known as flood geology.\nRecent decades have seen attempts to de-link creationism from the Bible and recast it as science; these include creation science and intelligent design.\nTypes.\nTo counter the common misunderstanding that the creation\u2013evolution controversy was a simple dichotomy of views, with \"creationists\" set against \"evolutionists\", Eugenie Scott of the National Center for Science Education produced a diagram and description of a continuum of religious views as a spectrum ranging from extreme literal biblical creationism to materialist evolution, grouped under main headings. This was used in public presentations, then published in 1999 in \"Reports of the NCSE\". Other versions of a taxonomy of creationists were produced, and comparisons made between the different groupings. In 2009 Scott produced a revised continuum taking account of these issues, emphasizing that intelligent design creationism overlaps other types, and each type is a grouping of various beliefs and positions. The revised diagram is labelled to shows a spectrum relating to positions on the age of the Earth, and the part played by special creation as against evolution. This was published in the book \"Evolution Vs. Creationism: An Introduction\", and the NCSE website rewritten on the basis of the book version.\nThe main general types are listed below.\nYoung Earth creationism.\nYoung Earth creationists such as Ken Ham and Doug Phillips believe that God created the Earth within the last ten thousand years, with a literalist interpretation of the Genesis creation narrative, within the approximate time-frame of biblical genealogies. Most young Earth creationists believe that the universe has a similar age as the Earth. A few assign a much older age to the universe than to Earth. Young Earth creationism gives the universe an age consistent with the Ussher chronology and other young Earth time frames. Other young Earth creationists believe that the Earth and the universe were created with the appearance of age, so that the world appears to be much older than it is, and that this appearance is what gives the geological findings and other methods of dating the Earth and the universe their much longer timelines.\nThe Christian organizations Answers in Genesis (AiG), Institute for Creation Research (ICR) and the Creation Research Society (CRS) promote young Earth creationism in the United States. Carl Baugh's Creation Evidence Museum in Texas, United States AiG's Creation Museum and Ark Encounter in Kentucky, United States were opened to promote young Earth creationism. Creation Ministries International promotes young Earth views in Australia, Canada, South Africa, New Zealand, the United States, and the United Kingdom.\nAmong Roman Catholics, the Kolbe Center for the Study of Creation promotes similar ideas.\nOld Earth creationism.\nOld Earth creationism holds that the physical universe was created by God, but that the creation event described in the Book of Genesis is to be taken figuratively. This group generally believes that the age of the universe and the age of the Earth are as described by astronomers and geologists, but that details of modern evolutionary theory are questionable.\nOld Earth creationism itself comes in at least three types:\nGap creationism.\nGap creationism (also known as \"ruin-restoration creationism\", \"restoration creationism\", or \"the Gap Theory\") is a form of old Earth creationism that posits that the six-\"yom\" creation period, as described in the Book of Genesis, involved six literal 24-hour days, but that there was a gap of time between two distinct creations in the first and the second verses of Genesis, which the theory states explains many scientific observations, including the age of the Earth. Thus, the six days of creation (verse 3 onwards) start sometime after the Earth was \"without form and void.\" This allows an indefinite gap of time to be inserted after the original creation of the universe, but prior to the Genesis creation narrative, (when present biological species and humanity were created). Gap theorists can therefore agree with the scientific consensus regarding the age of the Earth and universe, while maintaining a literal interpretation of the biblical text.\nSome gap creationists expand the basic version of creationism by proposing a \"primordial creation\" of biological life within the \"gap\" of time. This is thought to be \"the world that then was\" mentioned in 2 Peter 3:3\u20136. Discoveries of fossils and archaeological ruins older than 10,000 years are generally ascribed to this \"world that then was,\" which may also be associated with Lucifer's rebellion.\nDay-age creationism.\nDay-age creationism, a type of old Earth creationism, is a metaphorical interpretation of the creation accounts in Genesis. It holds that the six days referred to in the Genesis account of creation are not ordinary 24-hour days, but are much longer periods (from thousands to billions of years). The Genesis account is then reconciled with the age of the Earth. Proponents of the day-age theory can be found among both theistic evolutionists, who accept the scientific consensus on evolution, and progressive creationists, who reject it. The theories are said to be built on the understanding that the Hebrew word \"yom\" is also used to refer to a time period, with a beginning and an end and not necessarily that of a 24-hour day.\nThe day-age theory attempts to reconcile the Genesis creation narrative and modern science by asserting that the creation \"days\" were not ordinary 24-hour days, but actually lasted for long periods of time (as day-age implies, the \"days\" each lasted an age). According to this view, the sequence and duration of the creation \"days\" may be paralleled to the scientific consensus for the age of the earth and the universe.\nProgressive creationism.\nProgressive creationism is the religious belief that God created new forms of life gradually over a period of hundreds of millions of years. As a form of old Earth creationism, it accepts mainstream geological and cosmological estimates for the age of the Earth, some tenets of biology such as microevolution as well as archaeology to make its case. In this view creation occurred in rapid bursts in which all \"kinds\" of plants and animals appear in stages lasting millions of years. The bursts are followed by periods of stasis or equilibrium to accommodate new arrivals. These bursts represent instances of God creating new types of organisms by divine intervention. As viewed from the archaeological record, progressive creationism holds that \"species do not gradually appear by the steady transformation of its ancestors; [but] appear all at once and \"fully formed.\"\nThe view rejects macroevolution, claiming it is biologically untenable and not supported by the fossil record, as well as rejects the concept of common descent from a last universal common ancestor. Thus the evidence for macroevolution is claimed to be false, but microevolution is accepted as a genetic parameter designed by the Creator into the fabric of genetics to allow for environmental adaptations and survival. Generally, it is viewed by proponents as a middle ground between literal creationism and evolution. Organizations such as Reasons To Believe, founded by Hugh Ross, promote this version of creationism.\nProgressive creationism can be held in conjunction with hermeneutic approaches to the Genesis creation narrative such as the day-age creationism or framework/metaphoric/poetic views.\nPhilosophic and scientific creationism.\nCreation science.\nCreation science, or initially scientific creationism, is a pseudoscience that emerged in the 1960s with proponents aiming to have young Earth creationist beliefs taught in school science classes as a counter to teaching of evolution. Common features of creation science argument include: creationist cosmologies which accommodate a universe on the order of thousands of years old, criticism of radiometric dating through a technical argument about radiohalos, explanations for the fossil record as a record of the Genesis flood narrative (see flood geology), and explanations for the present diversity as a result of pre-designed genetic variability and partially due to the rapid degradation of the perfect genomes God placed in \"created kinds\" or \"baramins\" due to mutations.\nNeo-creationism.\nNeo-creationism is a pseudoscientific movement which aims to restate creationism in terms more likely to be well received by the public, by policy makers, by educators and by the scientific community. It aims to re-frame the debate over the origins of life in non-religious terms and without appeals to scripture. This comes in response to the 1987 ruling by the United States Supreme Court in \"Edwards v. Aguillard\" that creationism is an inherently religious concept and that advocating it as correct or accurate in public-school curricula violates the Establishment Clause of the First Amendment.\nOne of the principal claims of neo-creationism propounds that ostensibly objective orthodox science, with a foundation in naturalism, is actually a dogmatically atheistic religion. Its proponents argue that the scientific method excludes certain explanations of phenomena, particularly where they point towards supernatural elements, thus effectively excluding religious insight from contributing to understanding the universe. This leads to an open and often hostile opposition to what neo-creationists term \"Darwinism\", which they generally mean to refer to evolution, but which they may extend to include such concepts as abiogenesis, stellar evolution and the Big Bang theory.\nUnlike their philosophical forebears, neo-creationists largely do not believe in many of the traditional cornerstones of creationism such as a young Earth, or in a dogmatically literal interpretation of the Bible.\nIntelligent design.\nIntelligent design (ID) is the pseudoscientific view that \"certain features of the universe and of living things are best explained by an intelligent cause, not an undirected process such as natural selection.\" All of its leading proponents are associated with the Discovery Institute, a think tank whose wedge strategy aims to replace the scientific method with \"a science consonant with Christian and theistic convictions\" which accepts supernatural explanations. It is widely accepted in the scientific and academic communities that intelligent design is a form of creationism, and is sometimes referred to as \"intelligent design creationism.\"\nID originated as a re-branding of creation science in an attempt to avoid a series of court decisions ruling out the teaching of creationism in American public schools, and the Discovery Institute has run a series of campaigns to change school curricula. In Australia, where curricula are under the control of state governments rather than local school boards, there was a public outcry when the notion of ID being taught in science classes was raised by the Federal Education Minister Brendan Nelson; the minister quickly conceded that the correct forum for ID, if it were to be taught, is in religious or philosophy classes.\nIn the US, teaching of intelligent design in public schools has been decisively ruled by a federal district court to be in violation of the Establishment Clause of the First Amendment to the United States Constitution. In Kitzmiller v. Dover, the court found that intelligent design is not science and \"cannot uncouple itself from its creationist, and thus religious, antecedents,\" and hence cannot be taught as an alternative to evolution in public school science classrooms under the jurisdiction of that court. This sets a persuasive precedent, based on previous US Supreme Court decisions in \"Edwards v. Aguillard\" and \"Epperson v. Arkansas\" (1968), and by the application of the Lemon test, that creates a legal hurdle to teaching intelligent design in public school districts in other federal court jurisdictions.\nGeocentrism.\nIn astronomy, the geocentric model (also known as geocentrism, or the Ptolemaic system), is a description of the cosmos where Earth is at the orbital center of all celestial bodies. This model served as the predominant cosmological system in many ancient civilizations such as ancient Greece. As such, they assumed that the Sun, Moon, stars, and naked eye planets circled Earth, including the noteworthy systems of Aristotle (see Aristotelian physics) and Ptolemy.\nArticles arguing that geocentrism was the biblical perspective appeared in some early creation science newsletters associated with the Creation Research Society pointing to some passages in the Bible, which, when taken literally, indicate that the daily apparent motions of the Sun and the Moon are due to their actual motions around the Earth rather than due to the rotation of the Earth about its axis. For example, where the Sun and Moon are said to stop in the sky, and where the world is described as immobile. Contemporary advocates for such religious beliefs include Robert Sungenis, co-author of the self-published \"Galileo Was Wrong: The Church Was Right\" (2006). These people subscribe to the view that a plain reading of the Bible contains an accurate account of the manner in which the universe was created and requires a geocentric worldview. \nMost contemporary creationist organizations reject such perspectives.\nOmphalos hypothesis.\nThe Omphalos hypothesis is one attempt to reconcile the scientific evidence that the universe is billions of years old with a literal interpretation of the Genesis creation narrative, which implies that the Earth is only a few thousand years old. It is based on the religious belief that the universe was created by a divine being, within the past six to ten thousand years (in keeping with flood geology), and that the presence of objective, verifiable evidence that the universe is older than approximately ten millennia is due to the creator introducing false evidence that makes the universe appear significantly older.\nThe idea was named after the title of an 1857 book, \"Omphalos\" by Philip Henry Gosse, in which Gosse argued that in order for the world to be functional God must have created the Earth with mountains and canyons, trees with growth rings, Adam and Eve with fully grown hair, fingernails, and navels (\u1f40\u03bc\u03c6\u03b1\u03bb\u03cc\u03c2 \"omphalos\" is Greek for \"navel\"), and all living creatures with fully formed evolutionary features, etc..., and that, therefore, \"no\" empirical evidence about the age of the Earth or universe can be taken as reliable.\nVarious supporters of Young Earth creationism have given different explanations for their belief that the universe is filled with false evidence of the universe's age, including a belief that some things needed to be created at a certain age for the ecosystems to function, or their belief that the creator was deliberately planting deceptive evidence. The idea has seen some revival in the 20th century by some modern creationists, who have extended the argument to address the \"starlight problem\". The idea has been criticised as Last Thursdayism, and on the grounds that it requires a deliberately deceptive creator.\nTheistic evolution.\nTheistic evolution, or evolutionary creation, is a belief that \"the personal God of the Bible created the universe and life through evolutionary processes.\" According to the American Scientific Affiliation:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A theory of theistic evolution (TE)\u00a0\u2013 also called evolutionary creation\u00a0\u2013 proposes that God's method of creation was to cleverly design a universe in which everything would naturally evolve. Usually the \"evolution\" in \"theistic evolution\" means Total Evolution\u00a0\u2013 astronomical evolution (to form galaxies, solar systems...) and geological evolution (to form the earth's geology) plus chemical evolution (to form the first life) and biological evolution (for the development of life)\u00a0\u2013 but it can refer only to biological evolution.\nThrough the 19th century the term \"creationism\" most commonly referred to direct creation of individual souls, in contrast to traducianism. Following the publication of \"Vestiges of the Natural History of Creation\", there was interest in ideas of Creation by divine law. In particular, the liberal theologian Baden Powell argued that this illustrated the Creator's power better than the idea of miraculous creation, which he thought ridiculous. When \"On the Origin of Species\" was published, the cleric Charles Kingsley wrote of evolution as \"just as noble a conception of Deity.\" Darwin's view at the time was of God creating life through the laws of nature, and the book makes several references to \"creation,\" though he later regretted using the term rather than calling it an unknown process. In America, Asa Gray argued that evolution is the secondary effect, or \"modus operandi\", of the first cause, design, and published a pamphlet defending the book in theistic terms, \"Natural Selection not inconsistent with Natural Theology\". Theistic evolution, also called, evolutionary creation, became a popular compromise, and St. George Jackson Mivart was among those accepting evolution but attacking Darwin's naturalistic mechanism. Eventually it was realised that supernatural intervention could not be a scientific explanation, and naturalistic mechanisms such as neo-Lamarckism were favoured as being more compatible with purpose than natural selection.\nSome theists took the general view that, instead of faith being in opposition to biological evolution, some or all classical religious teachings about Christian God and creation are compatible with some or all of modern scientific theory, including specifically evolution; it is also known as \"evolutionary creation.\" In \"Evolution versus Creationism\", Eugenie Scott and Niles Eldredge state that it is in fact a type of evolution.\nIt generally views evolution as a tool used by God, who is both the first cause and immanent sustainer/upholder of the universe; it is therefore well accepted by people of strong theistic (as opposed to deistic) convictions. Theistic evolution can synthesize with the day-age creationist interpretation of the Genesis creation narrative; however most adherents consider that the first chapters of the Book of Genesis should not be interpreted as a \"literal\" description, but rather as a literary framework or allegory.\nFrom a theistic viewpoint, the underlying laws of nature were designed by God for a purpose, and are so self-sufficient that the complexity of the entire physical universe evolved from fundamental particles in processes such as stellar evolution, life forms developed in biological evolution, and in the same way the origin of life by natural causes has resulted from these laws.\nIn one form or another, theistic evolution is the view of creation taught at the majority of mainline Protestant seminaries. For Roman Catholics, human evolution is not a matter of religious teaching, and must stand or fall on its own scientific merits. Evolution and the Roman Catholic Church are not in conflict. The Catechism of the Catholic Church comments positively on the theory of evolution, which is neither precluded nor required by the sources of faith, stating that scientific studies \"have splendidly enriched our knowledge of the age and dimensions of the cosmos, the development of life-forms and the appearance of man.\" Roman Catholic schools teach evolution without controversy on the basis that scientific knowledge does not extend beyond the physical, and scientific truth and religious truth cannot be in conflict. Theistic evolution can be described as \"creationism\" in holding that divine intervention brought about the origin of life or that divine laws govern formation of species, though many creationists (in the strict sense) would deny that the position is creationism at all. In the creation\u2013evolution controversy, its proponents generally take the \"evolutionist\" side. This sentiment was expressed by Fr. George Coyne, (the Vatican's chief astronomer between 1978 and 2006):...in America, creationism has come to mean some fundamentalistic, literal, scientific interpretation of Genesis. Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in a belief that everything depends upon God, or better, all is a gift from God.\nWhile supporting the methodological naturalism inherent in modern science, the proponents of theistic evolution reject the implication taken by some atheists that this gives credence to ontological materialism. In fact, many modern philosophers of science, including atheists, refer to the long-standing convention in the scientific method that observable events in nature should be explained by natural causes, with the distinction that it does not assume the actual existence or non-existence of the supernatural. \nReligious views.\nThere are also non-Christian forms of creationism, notably Islamic creationism and Hindu creationism.\nBah\u00e1\u02bc\u00ed Faith.\nIn the creation myth taught by Bah\u00e1'u'll\u00e1h, the Bah\u00e1\u02bc\u00ed Faith founder, the universe has \"neither beginning nor ending,\" and that the component elements of the material world have always existed and will always exist. With regard to evolution and the origin of human beings, `Abdu'l-Bah\u00e1 gave extensive comments on the subject when he addressed western audiences in the beginning of the 20th century. Transcripts of these comments can be found in \"Some Answered Questions\", \"Paris Talks\" and \"The Promulgation of Universal Peace\". `Abdu'l-Bah\u00e1 described the human species as having evolved from a primitive form to modern man, but that the capacity to form human intelligence was always in existence.\nBuddhism.\nBuddhism denies a creator deity and posits that mundane deities such as Mahabrahma are sometimes misperceived to be a creator. While Buddhism includes belief in divine beings called devas, it holds that they are mortal, limited in their power, and that none of them are creators of the universe. In the Sa\u1e43yutta Nik\u0101ya, the Buddha also states that the cycle of rebirths stretches back hundreds of thousands of eons, without discernible beginning.\nMajor Buddhist Indian philosophers such as Nagarjuna, Vasubandhu, Dharmakirti and Buddhaghosa, consistently critiqued Creator God views put forth by Hindu thinkers.\nChristianity.\nAs of 2006[ [update]], most Christians around the world accepted evolution as the most likely explanation for the origins of species, and did not take a literal view of the Genesis creation narrative. The United States is an exception where belief in religious fundamentalism is much more likely to affect attitudes towards evolution than it is for believers elsewhere. Political partisanship affecting religious belief may be a factor because political partisanship in the US is highly correlated with fundamentalist thinking, unlike in Europe.\nMost contemporary Christian leaders and scholars from mainstream churches, such as Anglicans and Lutherans, consider that there is no conflict between the spiritual meaning of creation and the science of evolution. According to the former archbishop of Canterbury, Rowan Williams, \"...for most of the history of Christianity, and I think this is fair enough, most of the history of the Christianity there's been an awareness that a belief that everything depends on the creative act of God, is quite compatible with a degree of uncertainty or latitude about how precisely that unfolds in creative time.\"\nLeaders of the Anglican and Roman Catholic churches have made statements in favor of evolutionary theory, as have scholars such as the physicist John Polkinghorne, who argues that evolution is one of the principles through which God created living beings. Earlier supporters of evolutionary theory include Frederick Temple, Asa Gray and Charles Kingsley who were enthusiastic supporters of Darwin's theories upon their publication, and the French Jesuit priest and geologist Pierre Teilhard de Chardin saw evolution as confirmation of his Christian beliefs, despite condemnation from Church authorities for his more speculative theories. Another example is that of Liberal theology, not providing any creation models, but instead focusing on the symbolism in beliefs of the time of authoring Genesis and the cultural environment.\nMany Christians and Jews had been considering the idea of the creation history as an allegory (instead of historical) long before the development of Darwin's theory of evolution. For example, Philo, whose works were taken up by early Church writers, wrote that it would be a mistake to think that creation happened in six days, or in any set amount of time. Augustine of the late fourth century who was also a former neoplatonist argued that everything in the universe was created by God at the same moment in time (and not in six days as a literal reading of the Book of Genesis would seem to require); It appears that both Philo and Augustine felt uncomfortable with the idea of a seven-day creation because it detracted from the notion of God's omnipotence. In 1950, Pope Pius XII stated limited support for the idea in his encyclical \"Humani generis\". In 1996, Pope John Paul II stated that \"new knowledge has led to the recognition of the theory of evolution as more than a hypothesis,\" but, referring to previous papal writings, he concluded that \"if the human body takes its origin from pre-existent living matter, the spiritual soul is immediately created by God.\"\nIn the US, Evangelical Christians have continued to believe in a literal Genesis. Members of evangelical Protestant (70%), Mormon (76%) and Jehovah's Witnesses (90%) denominations are the most likely to reject the evolutionary interpretation of the origins of life.\nJehovah's Witnesses adhere to a combination of gap creationism and day-age creationism, asserting that scientific evidence about the age of the universe is compatible with the Bible, but that the 'days' after Genesis 1:1 were each thousands of years in length.\nThe historic Christian literal interpretation of creation requires the harmonization of the two creation stories, Genesis 1:1\u20132:3 and Genesis 2:4\u201325, for there to be a consistent interpretation. They sometimes seek to ensure that their belief is taught in science classes, mainly in American schools. Opponents reject the claim that the literalistic biblical view meets the criteria required to be considered scientific. Many religious groups teach that God created the Cosmos. From the days of the early Christian Church Fathers there were allegorical interpretations of the Book of Genesis as well as literal aspects.\nChristian Science, a system of thought and practice derived from the writings of Mary Baker Eddy, interprets the Book of Genesis figuratively rather than literally. It holds that the material world is an illusion, and consequently not created by God: the only real creation is the spiritual realm, of which the material world is a distorted version. Christian Scientists regard the story of the creation in the Book of Genesis as having symbolic rather than literal meaning. According to Christian Science, both creationism and evolution are false from an absolute or \"spiritual\" point of view, as they both proceed from a (false) belief in the reality of a material universe. However, Christian Scientists do not oppose the teaching of evolution in schools, nor do they demand that alternative accounts be taught: they believe that both material science and literalist theology are concerned with the illusory, mortal and material, rather than the real, immortal and spiritual. With regard to material theories of creation, Eddy showed a preference for Darwin's theory of evolution over others.\nHinduism.\nHindu creationists claim that species of plants and animals are material forms adopted by pure consciousness which live an endless cycle of births and rebirths. Ronald Numbers says that: \"Hindu Creationists have insisted on the antiquity of humans, who they believe appeared fully formed as long, perhaps, as trillions of years ago.\" Hindu creationism is a form of old Earth creationism, according to Hindu creationists the universe may even be older than billions of years. These views are based on the Vedas, the creation myths of which depict an extreme antiquity of the universe and history of the Earth.\nIn Hindu cosmology, time cyclically repeats general events of creation and destruction, with many \"first man\", each known as Manu, the progenitor of mankind. Each Manu successively reigns over a 306.72 million year period known as a \"manvantara\", each ending with the destruction of mankind followed by a \"sandhya\" (period of non-activity) before the next \"manvantara\". 120.53 million years have elapsed in the current \"manvantara\" (current mankind) according to calculations on Hindu units of time. The universe is cyclically created at the start and destroyed at the end of a \"kalpa\" (day of Brahma), lasting for 4.32 billion years, which is followed by a \"pralaya\" (period of dissolution) of equal length. 1.97 billion years have elapsed in the current \"kalpa\" (current universe).The universal elements or building blocks (unmanifest matter) exists for a period known as a \"maha-kalpa\", lasting for 311.04 trillion years, which is followed by a \"maha-pralaya\" (period of great dissolution) of equal length. 155.52 trillion years have elapsed in the current \"maha-kalpa\".\nIslam.\nIslamic creationism is the belief that the universe (including humanity) was directly created by God as explained in the Quran. It usually views the Book of Genesis as a corrupted version of God's message. The creation myths in the Quran are vaguer and allow for a wider range of interpretations similar to those in other Abrahamic religions.\nIslam also has its own school of theistic evolutionism, which holds that mainstream scientific analysis of the origin of the universe is supported by the Quran. Some Muslims believe in evolutionary creation, especially among liberal movements within Islam.\nWriting for \"The Boston Globe\", Drake Bennett noted: \"Without a Book of Genesis to account for ... Muslim creationists have little interest in proving that the age of the Earth is measured in the thousands rather than the billions of years, nor do they show much interest in the problem of the dinosaurs. And the idea that animals might evolve into other animals also tends to be less controversial, in part because there are passages of the Koran that seem to support it. But the issue of whether human beings are the product of evolution is just as fraught among Muslims.\" Khalid Anees, president of the Islamic Society of Britain, states that Muslims do not agree that one species can develop from another.\nSince the 1980s, Turkey has been a site of strong advocacy for creationism, supported by American adherents.\nThere are several verses in the Qur'an which some modern writers have interpreted as being compatible with the expansion of the universe, Big Bang and Big Crunch theories:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Do not the Unbelievers see that the heavens and the earth were joined together (as one unit of creation), before we clove them asunder? We made from water every living thing. Will they not then believe?\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Moreover He comprehended in His design the sky, and it had been (as) smoke: He said to it and to the earth: 'Come ye together, willingly or unwillingly.' They said: 'We do come (together), in willing obedience.'\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;With power and skill did We construct the Firmament: for it is We Who create the vastness of space.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Day that We roll up the heavens like a scroll rolled up for books (completed),- even as We produced the first creation, so shall We produce a new one: a promise We have undertaken: truly shall We fulfil it.\nAhmadiyya.\nThe Ahmadiyya movement actively promotes evolutionary theory. Ahmadis interpret scripture from the Qur'an to support the concept of macroevolution and give precedence to scientific theories. Furthermore, unlike orthodox Muslims, Ahmadis believe that humans have gradually evolved from different species. Ahmadis regard Adam as being the first Prophet of God\u00a0\u2013 as opposed to him being the first man on Earth. Rather than wholly adopting the theory of natural selection, Ahmadis promote the idea of a \"guided evolution,\" viewing each stage of the evolutionary process as having been selectively woven by God. Mirza Tahir Ahmad, Fourth Caliph of the Ahmadiyya Muslim Community has stated in his magnum opus \"Revelation, Rationality, Knowledge &amp; Truth\" (1998) that evolution did occur but only through God being the One who brings it about. It does not occur itself, according to the Ahmadiyya Muslim Community.\nJudaism.\nFor Orthodox Jews who seek to reconcile discrepancies between science and the creation myths in the Bible, the notion that science and the Bible should even be reconciled through traditional scientific means is questioned. To these groups, science is as true as the Torah and if there seems to be a problem, epistemological limits are to blame for apparently irreconcilable points. They point to discrepancies between what is expected and what actually is to demonstrate that things are not always as they appear. They note that even the root word for \"world\" in the Hebrew language\u2014\u05e2\u05d5\u05dc\u05dd (Olam)\u2014means hidden\u2014\u05e0\u05e2\u05dc\u05dd (Neh-Eh-Lahm). Just as they know from the Torah that God created man and trees and the light on its way from the stars in their observed state, so too can they know that the world was created in its over the six days of Creation that reflects progression to its currently-observed state, with the understanding that physical ways to verify this may eventually be identified. This knowledge has been advanced by Rabbi Dovid Gottlieb, former philosophy professor at Johns Hopkins University. Also, relatively old Kabbalistic sources from well before the scientifically apparent age of the universe was first determined are in close concord with modern scientific estimates of the age of the universe, according to Rabbi Aryeh Kaplan, and based on Sefer Temunah, an early kabbalistic work attributed to the first-century Tanna Nehunya ben HaKanah. Many kabbalists accepted the teachings of the Sefer HaTemunah, including the medieval Jewish scholar Nahmanides, his close student Isaac ben Samuel of Acre, and David ben Solomon ibn Abi Zimra. Other parallels are derived, among other sources, from Nahmanides, who expounds that there was a Neanderthal-like species with which Adam mated (he did this long before Neanderthals had even been discovered scientifically). Reform Judaism does not take the Torah as a literal text, but rather as a symbolic or open-ended work.\nSome contemporary writers such as Rabbi Gedalyah Nadel have sought to reconcile the discrepancy between the account in the Torah, and scientific findings by arguing that each day referred to in the Bible was not 24 hours, but billions of years long. Others claim that the Earth was created a few thousand years ago, but was deliberately made to look as if it was five billion years old, e.g. by being created with ready made fossils. The best known exponent of this approach being Rabbi Menachem Mendel Schneerson Others state that although the world was physically created in six 24-hour days, the Torah accounts can be interpreted to mean that there was a period of billions of years before the six days of creation.\nPrevalence.\nMost vocal literalist creationists are from the US, and strict creationist views are much less common in other developed countries. According to a study published in \"Science\", a survey of the US, Turkey, Japan and Europe showed that public acceptance of evolution is most prevalent in Iceland, Denmark and Sweden at 80% of the population. There seems to be no significant correlation between believing in evolution and understanding evolutionary science.\nAustralia.\nA 2009 Nielsen poll showed that 23% of Australians believe \"the biblical account of human origins,\" 42% believe in a \"wholly scientific\" explanation for the origins of life, while 32% believe in an evolutionary process \"guided by God\".\nA 2013 survey conducted by Auspoll and the Australian Academy of Science found that 80% of Australians believe in evolution (70% believe it is currently occurring, 10% believe in evolution but do not think it is currently occurring), 12% were not sure and 9% stated they do not believe in evolution.\nBrazil.\nA 2011 Ipsos survey found that 47% of responders in Brazil identified themselves as \"creationists and believe that human beings were in fact created by a spiritual force such as the God they believe in and do not believe that the origin of man came from evolving from other species such as apes\".\nIn 2004, IBOPE conducted a poll in Brazil that asked questions about creationism and the teaching of creationism in schools. When asked if creationism should be taught in schools, 89% of people said that creationism should be taught in schools. When asked if the teaching of creationism should replace the teaching of evolution in schools, 75% of people said that the teaching of creationism should replace the teaching of evolution in schools.\nCanada.\nA 2012 survey, by Angus Reid Public Opinion revealed that 61 percent of Canadians believe in evolution. The poll asked \"Where did human beings come from\u00a0\u2013 did we start as singular cells millions of year ago and evolve into our present form, or did God create us in his image 10,000 years ago?\"\nIn 2019, a Research Co. poll asked people in Canada if creationism \"should be part of the school curriculum in their province\". 38% of Canadians said that creationism should be part of the school curriculum, 39% of Canadians said that it should not be part of the school curriculum, and 23% of Canadians were undecided.\nIn 2023, a Research Co. poll found that 21% of Canadians \"believe God created human beings in their present form within the last 10,000 years\". The poll also found that \"More than two-in-five Canadians (43%) think creationism should be part of the school curriculum in their province.\"\nEurope.\nIn Europe, literalist creationism is more widely rejected, though regular opinion polls are not available. Most people accept that evolution is the most widely accepted scientific theory as taught in most schools. In countries with a Roman Catholic majority, papal acceptance of evolutionary creationism as worthy of study has essentially ended debate on the matter for many people.\nIn the UK, a 2006 poll on the \"origin and development of life\", asked participants to choose between three different perspectives on the origin of life: 22% chose creationism, 17% opted for intelligent design, 48% selected evolutionary theory, and the rest did not know. A subsequent 2010 YouGov poll on the correct explanation for the origin of humans found that 9% opted for creationism, 12% intelligent design, 65% evolutionary theory and 13% didn't know. The former Archbishop of Canterbury Rowan Williams, head of the worldwide Anglican Communion, views the idea of teaching creationism in schools as a mistake. In 2009, an Ipsos Mori survey in the United Kingdom found that 54% of Britons agreed with the view: \"Evolutionary theories should be taught in science lessons in schools together with other possible perspectives, such as intelligent design and creationism.\"\nIn Italy, Education Minister Letizia Moratti wanted to retire evolution from the secondary school level; after one week of massive protests, she reversed her opinion.\nThere continues to be scattered and possibly mounting efforts on the part of religious groups throughout Europe to introduce creationism into public education. In response, the Parliamentary Assembly of the Council of Europe has released a draft report titled \"The dangers of creationism in education\" on June 8, 2007, reinforced by a further proposal of banning it in schools dated October 4, 2007.\nSerbia suspended the teaching of evolution for one week in September 2004, under education minister Ljiljana \u010coli\u0107, only allowing schools to reintroduce evolution into the curriculum if they also taught creationism. \"After a deluge of protest from scientists, teachers and opposition parties\" says the BBC report, \u010coli\u0107's deputy made the statement, \"I have come here to confirm Charles Darwin is still alive\" and announced that the decision was reversed. \u010coli\u0107 resigned after the government said that she had caused \"problems that had started to reflect on the work of the entire government.\"\nPoland saw a major controversy over creationism in 2006, when the Deputy Education Minister, Miros\u0142aw Orzechowski, denounced evolution as \"one of many lies\" taught in Polish schools. His superior, Minister of Education Roman Giertych, has stated that the theory of evolution would continue to be taught in Polish schools, \"as long as most scientists in our country say that it is the right theory.\" Giertych's father, Member of the European Parliament Maciej Giertych, has opposed the teaching of evolution and has claimed that dinosaurs and humans co-existed.\nA June 2015 - July 2016 Pew poll of Eastern European countries found that 56% of people from Armenia say that humans and other living things have \"Existed in present state since the beginning of time\". Armenia is followed by 52% from Bosnia, 42% from Moldova, 37% from Lithuania, 34% from Georgia and Ukraine, 33% from Croatia and Romania, 31% from Bulgaria, 29% from Greece and Serbia, 26% from Russia, 25% from Latvia, 23% from Belarus and Poland, 21% from Estonia and Hungary, and 16% from the Czech Republic.\nSouth Africa.\nA 2011 Ipsos survey found that 56% of responders in South Africa identified themselves as \"creationists and believe that human beings were in fact created by a spiritual force such as the God they believe in and do not believe that the origin of man came from evolving from other species such as apes\".\nSouth Korea.\nIn 2009, an EBS survey in South Korea found that 63% of people believed that creation and evolution should both be taught in schools simultaneously.\nUnited States.\nA 2017 poll by Pew Research found that 62% of Americans believe humans have evolved over time and 34% of Americans believe humans and other living things have existed in their present form since the beginning of time. A 2019 Gallup creationism survey found that 40% of adults in the United States inclined to the view that \"God created humans in their present form at one time within the last 10,000 years\" when asked for their views on the origin and development of human beings.\nAccording to a 2014 Gallup poll, about 42% of Americans believe that \"God created human beings pretty much in their present form at one time within the last 10,000 years or so.\" Another 31% believe that \"human beings have developed over millions of years from less advanced forms of life, but God guided this process,\"and 19% believe that \"human beings have developed over millions of years from less advanced forms of life, but God had no part in this process.\"\nBelief in creationism is inversely correlated to education; of those with postgraduate degrees, 74% accept evolution. In 1987, \"Newsweek\" reported: \"By one count there are some 700 scientists with respectable academic credentials (out of a total of 480,000 U.S. earth and life scientists) who give credence to creation-science, the general theory that complex life forms did not evolve but appeared 'abruptly.'\"\nA 2000 poll for People for the American Way found 70% of the US public felt that evolution was compatible with a belief in God.\nAccording to a study published in \"Science\", between 1985 and 2005 the number of adult North Americans who accept evolution declined from 45% to 40%, the number of adults who reject evolution declined from 48% to 39% and the number of people who were unsure increased from 7% to 21%. Besides the US the study also compared data from 32 European countries, Turkey, and Japan. The only country where acceptance of evolution was lower than in the US was Turkey (25%).\nAccording to a 2011 Fox News poll, 45% of Americans believe in creationism, down from 50% in a similar poll in 1999. 21% believe in 'the theory of evolution as outlined by Darwin and other scientists' (up from 15% in 1999), and 27% answered that both are true (up from 26% in 1999).\nIn September 2012, educator and television personality Bill Nye spoke with the Associated Press and aired his fears about acceptance of creationism, believing that teaching children that creationism is the only true answer without letting them understand the way science works will prevent any future innovation in the world of science. In February 2014, Nye defended evolution in the classroom in a debate with creationist Ken Ham on the topic of whether creation is a viable model of origins in today's modern, scientific era.\nEducation controversies.\nIn the US, creationism has become centered in the political controversy over creation and evolution in public education, and whether teaching creationism in science classes conflicts with the separation of church and state. Currently, the controversy comes in the form of whether advocates of the intelligent design movement who wish to \"Teach the Controversy\" in science classes have conflated science with religion.\nPeople for the American Way polled 1500 North Americans about the teaching of evolution and creationism in November and December 1999. They found that most North Americans were not familiar with creationism, and most North Americans had heard of evolution, but many did not fully understand the basics of the theory. The main findings were:\nIn such political contexts, creationists argue that their particular religiously based origin belief is superior to those of other belief systems, in particular those made through secular or scientific rationale. Political creationists are opposed by many individuals and organizations who have made detailed critiques and given testimony in various court cases that the alternatives to scientific reasoning offered by creationists are opposed by the consensus of the scientific community.\nCriticism.\nChristian criticism.\nMost Christians disagree with the teaching of creationism as an alternative to evolution in schools. Several religious organizations, among them the Catholic Church, hold that their faith does not conflict with the scientific consensus regarding evolution. The Clergy Letter Project, which has collected more than 13,000 signatures, is an \"endeavor designed to demonstrate that religion and science can be compatible.\"\nIn his 2002 article \"Intelligent Design as a Theological Problem,\" George Murphy argues against the view that life on Earth, in all its forms, is direct evidence of God's act of creation (Murphy quotes Phillip E. Johnson's claim that he is speaking \"of a God who acted openly and left his fingerprints on all the evidence.\"). Murphy argues that this view of God is incompatible with the Christian understanding of God as \"the one revealed in the cross and resurrection of Christ.\" The basis of this theology is Isaiah 45:15, \"Verily thou art a God that hidest thyself, O God of Israel, the Saviour.\"\nMurphy observes that the execution of a Jewish carpenter by Roman authorities is in and of itself an ordinary event and did not require divine action. On the contrary, for the crucifixion to occur, God had to limit or \"empty\" himself. It was for this reason that Paul the Apostle wrote, in Philippians 2:5-8:\nLet this mind be in you, which was also in Christ Jesus: Who, being in the form of God, thought it not robbery to be equal with God: But made himself of no reputation, and took upon him the form of a servant, and was made in the likeness of men: And being found in fashion as a man, he humbled himself, and became obedient unto death, even the death of the cross.\nMurphy concludes that,Just as the Son of God limited himself by taking human form and dying on a cross, God limits divine action in the world to be in accord with rational laws which God has chosen. This enables us to understand the world on its own terms, but it also means that natural processes hide God from scientific observation.For Murphy, a theology of the cross requires that Christians accept a \"methodological\" naturalism, meaning that one cannot invoke God to explain natural phenomena, while recognizing that such acceptance does not require one to accept a \"metaphysical\" naturalism, which proposes that nature is all that there is.\nThe Jesuit priest George Coyne has stated that it is \"unfortunate that, especially here in America, creationism has come to mean...some literal interpretation of Genesis.\" He argues that \"...Judaic-Christian faith is radically creationist, but in a totally different sense. It is rooted in belief that everything depends on God, or better, all is a gift from God.\"\nTeaching of creationism.\nOther Christians have expressed qualms about teaching creationism. In March 2006, then Archbishop of Canterbury Rowan Williams, the leader of the world's Anglicans, stated his discomfort about teaching creationism, saying that creationism was \"a kind of category mistake, as if the Bible were a theory like other theories.\" He also said: \"My worry is creationism can end up reducing the doctrine of creation rather than enhancing it.\" The views of the Episcopal Church\u00a0\u2013 a major American-based branch of the Anglican Communion\u00a0\u2013 on teaching creationism resemble those of Williams.\nThe National Science Teachers Association is opposed to teaching creationism as a science, as is the Association for Science Teacher Education, the National Association of Biology Teachers, the American Anthropological Association, the American Geosciences Institute, the Geological Society of America, the American Geophysical Union, and numerous other professional teaching and scientific societies.\nIn April 2010, the American Academy of Religion issued \"Guidelines for Teaching About Religion in K\u201012 Public Schools in the United States\", which included guidance that creation science or intelligent design should not be taught in science classes, as \"Creation science and intelligent design represent worldviews that fall outside of the realm of science that is defined as (and limited to) a method of inquiry based on gathering observable and measurable evidence subject to specific principles of reasoning.\" However, they, as well as other \"worldviews that focus on speculation regarding the origins of life represent another important and relevant form of human inquiry that is appropriately studied in literature or social sciences courses. Such study, however, must include a diversity of worldviews representing a variety of religious and philosophical perspectives and must avoid privileging one view as more legitimate than others.\"\nRandy Moore and Sehoya Cotner, from the biology program at the University of Minnesota, reflect on the relevance of teaching creationism in the article \"The Creationist Down the Hall: Does It Matter When Teachers Teach Creationism?\", in which they write: \"Despite decades of science education reform, numerous legal decisions declaring the teaching of creationism in public-school science classes to be unconstitutional, overwhelming evidence supporting evolution, and the many denunciations of creationism as nonscientific by professional scientific societies, creationism remains popular throughout the United States.\"\nScientific criticism.\nScience is a system of knowledge based on observation, empirical evidence, and the development of theories that yield testable explanations and predictions of natural phenomena. By contrast, creationism is often based on literal interpretations of the narratives of particular religious texts. Creationist beliefs involve purported forces that lie outside of nature, such as supernatural intervention, and often do not allow predictions at all. Therefore, these can neither be confirmed nor disproved by scientists. However, many creationist beliefs can be framed as testable predictions about phenomena such as the age of the Earth, its geological history and the origins, distributions and relationships of living organisms found on it. Early science incorporated elements of these beliefs, but as science developed these beliefs were gradually falsified and were replaced with understandings based on accumulated and reproducible evidence that often allows the accurate prediction of future results.\nSome scientists, such as Stephen Jay Gould, consider science and religion to be two compatible and complementary fields, with authorities in distinct areas of human experience, so-called non-overlapping magisteria. This view is also held by many theologians, who believe that ultimate origins and meaning are addressed by religion, but favor verifiable scientific explanations of natural phenomena over those of creationist beliefs. Other scientists, such as Richard Dawkins, reject the non-overlapping magisteria and argue that, in disproving literal interpretations of creationists, the scientific method also undermines religious texts as a source of truth. Irrespective of this diversity in viewpoints, since creationist beliefs are not supported by empirical evidence, the scientific consensus is that any attempt to teach creationism as science should be rejected.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nWorks cited.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5329", "revid": "1160905980", "url": "https://en.wikipedia.org/wiki?curid=5329", "title": "History of Chad", "text": "Historical development of Chad\nChad (; ), officially the Republic of Chad, is a landlocked country in Central Africa. It borders Libya to the north, Sudan to the east, the Central African Republic to the south, Cameroon and Nigeria to the southwest, and Niger to the west. Due to its distance from the sea and its largely desert climate, the country is sometimes referred to as the \"Dead Heart of Africa\".\nPrehistory.\nThe territory now known as Chad possesses some of the richest archaeological sites in Africa. A hominid skull was found by Michel Brunet, that is more than 7 million years old, the oldest discovered anywhere in the world; it has been given the name Sahelanthropus tchadensis. In 1996 Michel Brunet had unearthed a hominid jaw which he named Australopithecus bahrelghazali, and unofficially dubbed Abel. It was dated using Beryllium based Radiometric dating as living circa. 3.6 million years ago.\nDuring the 7th millennium BC, the northern half of Chad was part of a broad expanse of land, stretching from the Indus River in the east to the Atlantic Ocean in the west, in which ecological conditions favored early human settlement. Rock art of the \"Round Head\" style, found in the Ennedi region, has been dated to before the 7th millennium BC and, because of the tools with which the rocks were carved and the scenes they depict, may represent the oldest evidence in the Sahara of Neolithic industries. Many of the pottery-making and Neolithic activities in Ennedi date back further than any of those of the Nile Valley to the east.\nIn the prehistoric period, Chad was much wetter than it is today, as evidenced by large game animals depicted in rock paintings in the Tibesti and Borkou regions.\nRecent linguistic research suggests that all of Africa's major language groupings south of the Sahara Desert (except Khoisan, which is not considered a valid genetic grouping anyway), i.e. the Afro-Asiatic, Nilo-Saharan and Niger\u2013Congo phyla, originated in prehistoric times in a narrow band between Lake Chad and the Nile Valley. The origins of Chad's peoples, however, remain unclear. Several of the proven archaeological sites have been only partially studied, and other sites of great potential have yet to be mapped.\nEra of Empires (AD 900\u20131900).\nAt the end of the 1st millennium AD, the formation of states began across central Chad in the sahelian zone between the desert and the savanna. For almost the next 1,000 years, these states, their relations with each other, and their effects on the peoples who lived in stateless societies along their peripheries dominated Chad's political history. Recent research suggests that indigenous Africans founded of these states, not migrating Arabic-speaking groups, as was believed previously. Nonetheless, immigrants, Arabic-speaking or otherwise, played a significant role, along with Islam, in the formation and early evolution of these states.\nMost states began as kingdoms, in which the king was considered divine and endowed with temporal and spiritual powers. All states were militaristic (or they did not survive long), but none was able to expand far into southern Chad, where forests and the tsetse fly complicated the use of cavalry. Control over the trans-Saharan trade routes that passed through the region formed the economic basis of these kingdoms. Although many states rose and fell, the most important and durable of the empires were Kanem\u2013Bornu, Baguirmi, and Ouaddai, according to most written sources (mainly court chronicles and writings of Arab traders and travelers).Chad - ERA OF EMPIRES, A.D. 900\u20131900\nKanem\u2013Bornu.\nThe Kanem Empire originated in the 9th century AD to the northeast of Lake Chad. Historians agree that the leaders of the new state were ancestors of the Kanembu people. Toward the end of the 11th century the Sayfawa king (or \"mai\", the title of the Sayfawa rulers) Hummay, converted to Islam. In the following century the Sayfawa rulers expanded southward into Kanem, where was to rise their first capital, Njimi. Kanem's expansion peaked during the long and energetic reign of Mai Dunama Dabbalemi (c. 1221\u20131259).\nBy the end of the 14th century, internal struggles and external attacks had torn Kanem apart. Finally, around 1396 the Bulala invaders forced \"Mai\" Umar Idrismi to abandon Njimi and move the Kanembu people to Bornu on the western edge of Lake Chad. Over time, the intermarriage of the Kanembu and Bornu peoples created a new people and language, the Kanuri, and founded a new capital, Ngazargamu.\nKanem\u2013Bornu peaked during the reign of the outstanding statesman \"Mai\" Idris Aluma (c. 1571\u20131603). Aluma is remembered for his military skills, administrative reforms, and Islamic piety. The administrative reforms and military brilliance of Aluma sustained the empire until the mid-17th century, when its power began to fade. By the early 19th century, Kanem\u2013Bornu was clearly an empire in decline, and in 1808 Fulani warriors conquered Ngazargamu. Bornu survived, but the Sayfawa dynasty ended in 1846 and the Empire itself fell in 1893.\nBaguirmi and Ouaddai.\nThe Kingdom of Baguirmi, located southeast of Kanem-Bornu, was founded in the late 15th or early 16th century, and adopted Islam in the reign of Abdullah IV (1568-98). Baguirmi was in a tributary relationship with Kanem\u2013Bornu at various points in the 17th and 18th centuries, then to Ouaddai in the 19th century. In 1893, Baguirmi sultan Abd ar Rahman Gwaranga surrendered the territory to France, and it became a French protectorate.\nThe Ouaddai Kingdom, west of Kanem\u2013Bornu, was established in the early 16th century by Tunjur rulers. In the 1630s, Abd al Karim invaded and established an Islamic sultanate. Among its most impactful rulers for the next three centuries were Muhammad Sabun, who controlled a new trade route to the north and established a currency during the early 19th century, and Muhammad Sharif, whose military campaigns in the mid 19th century fended off an assimilation attempt from Darfur, conquered Baguirmi, and successfully resisted French colonization. However, Ouaddai lost its independence to France after a war from 1909 to 1912.\nColonialism (1900\u20131940).\nThe French first invaded Chad in 1891, establishing their authority through military expeditions primarily against the Muslim kingdoms. The decisive colonial battle for Chad was fought on April 22, 1900 at Battle of Kouss\u00e9ri between forces of French Major Am\u00e9d\u00e9e-Fran\u00e7ois Lamy and forces of the Sudanese warlord Rabih az-Zubayr. Both leaders were killed in the battle.\nIn 1905, administrative responsibility for Chad was placed under a governor-general stationed at Brazzaville, capital of French Equatorial Africa (FEA). Chad did not have a separate colonial status until 1920, when it was placed under a lieutenant-governor stationed in Fort-Lamy (today N'Djamena).\nTwo fundamental themes dominated Chad's colonial experience with the French: an absence of policies designed to unify the territory and an exceptionally slow pace of modernization. In the French scale of priorities, the colony of Chad ranked near the bottom, and the French came to perceive Chad primarily as a source of raw cotton and untrained labour to be used in the more productive colonies to the south.\nThroughout the colonial period, large areas of Chad were never governed effectively: in the huge BET Prefecture, the handful of French military administrators usually left the people alone, and in central Chad, French rule was only slightly more substantive. Truly speaking, France managed to govern effectively only the south.\nDecolonization (1940\u20131960).\nDuring World War II, Chad was the first French colony to rejoin the Allies (August 26, 1940), after the defeat of France by Germany. Under the administration of F\u00e9lix \u00c9bou\u00e9, France's first black colonial governor, a military column, commanded by Colonel Philippe Leclerc de Hauteclocque, and including two battalions of Sara troops, moved north from N'Djamena (then Fort Lamy) to engage Axis forces in Libya, where, in partnership with the British Army's Long Range Desert Group, they captured Kufra. On 21 January 1942, N'Djamena was bombed by a German aircraft.\nAfter the war ended, local parties started to develop in Chad. The first to be born was the radical Chadian Progressive Party (PPT) in February 1947, initially headed by Panamanian born Gabriel Lisette, but from 1959 headed by Fran\u00e7ois Tombalbaye. The more conservative Chadian Democratic Union (UDT) was founded in November 1947 and represented French commercial interests and a bloc of traditional leaders composed primarily of Muslim and Ouadda\u00efan nobility. The confrontation between the PPT and UDT was more than simply ideological; it represented different regional identities, with the PPT representing the Christian and animist south and the UDT the Islamic north.\nThe PPT won the May 1957 pre-independence elections thanks to a greatly expanded franchise, and Lisette led the government of the Territorial Assembly until he lost a confidence vote on 11 February 1959. After a referendum on territorial autonomy on 28 September 1958, French Equatorial Africa was dissolved, and its four constituent states \u2013 Gabon, Congo (Brazzaville), the Central African Republic, and Chad became autonomous members of the French Community from 28 November 1958. Following Lisette's fall in February 1959 the opposition leaders Gontchome Sahoulba and Ahmed Koulamallah could not form a stable government, so the PPT was again asked to form an administration - which it did under the leadership of Fran\u00e7ois Tombalbaye on 26 March 1959. On 12 July 1960 France agreed to Chad becoming fully independent. On 11 August 1960, Chad became an independent country and Fran\u00e7ois Tombalbaye became its first president.\nThe Tombalbaye era (1960\u20131975).\nOne of the most prominent aspects of Tombalbaye's rule to prove itself was his authoritarianism and distrust of democracy. Already in January 1962 he banned all political parties except his own PPT, and started immediately concentrating all power in his own hands. His treatment of opponents, real or imagined, was extremely harsh, filling the prisons with thousands of political prisoners.\nWhat was even worse was his constant discrimination against the central and northern regions of Chad, where the southern Chadian administrators came to be perceived as arrogant and incompetent. This resentment at last exploded in a tax revolt on September 2, 1965 in the Gu\u00e9ra Prefecture, causing 500 deaths. The year after saw the birth in Sudan of the National Liberation Front of Chad (FROLINAT), created to militarily oust Tombalbaye and the Southern dominance. It was the start of a bloody civil war.\nTombalbaye resorted to calling in French troops; while moderately successful, they were not fully able to quell the insurgency. Proving more fortunate was his choice to break with the French and seek friendly ties with Libyan Brotherly Leader Gaddafi, taking away the rebels' principal source of supplies.\nBut while he had reported some success against the rebels, Tombalbaye started behaving more and more irrationally and brutally, continuously eroding his consensus among the southern elites, which dominated all key positions in the army, the civil service and the ruling party. As a consequence on April 13, 1975, several units of N'Djamena's gendarmerie killed Tombalbaye during a coup.\nMilitary rule (1975\u20131978).\nThe coup d'\u00e9tat that terminated Tombalbaye's government received an enthusiastic response in N'Djamena. The southerner General F\u00e9lix Malloum emerged early as the chairman of the new \"junta\".\nThe new military leaders were unable to retain for long the popularity that they had gained through their overthrow of Tombalbaye. Malloum proved himself unable to cope with the FROLINAT and at the end decided his only chance was in coopting some of the rebels: in 1978 he allied himself with the insurgent leader Hiss\u00e8ne Habr\u00e9, who entered the government as prime minister.\nCivil war (1979-1982).\nInternal dissent within the government led Prime Minister Habr\u00e9 to send his forces against Malloum's national army in the capital in February 1979. Malloum was ousted from the presidency, but the resulting civil war amongst the 11 emergent factions was so widespread that it rendered the central government largely irrelevant. At that point, other African governments decided to intervene.\nA series of four international conferences held first under Nigerian and then Organization of African Unity (OAU) sponsorship attempted to bring the Chadian factions together. At the fourth conference, held in Lagos, Nigeria, in August 1979, the Lagos Accord was signed. This accord established a transitional government pending national elections. In November 1979, the Transitional Government of National Unity (GUNT) was created with a mandate to govern for 18 months. Goukouni Oueddei, a northerner, was named president; Colonel Kamougu\u00e9, a southerner, Vice President; and Habr\u00e9, Minister of Defense. This coalition proved fragile; in January 1980, fighting broke out again between Goukouni's and Habr\u00e9's forces. With assistance from Libya, Goukouni regained control of the capital and other urban centers by year's end. However, Goukouni's January 1981 statement that Chad and Libya had agreed to work for the realization of complete unity between the two countries generated intense international pressure and Goukouni's subsequent call for the complete withdrawal of external forces.\nThe Habr\u00e9 era (1982\u20131990).\nLibya's partial withdrawal to the Aozou Strip in northern Chad cleared the way for Habr\u00e9's forces to enter N\u2019Djamena in June. French troops and an OAU peacekeeping force of 3,500 Nigerian, Senegalese, and Zairian troops (partially funded by the United States) remained neutral during the conflict.\nHabr\u00e9 continued to face armed opposition on various fronts, and was brutal in his repression of suspected opponents, massacring and torturing many during his rule. In the summer of 1983, GUNT forces launched an offensive against government positions in northern and eastern Chad with heavy Libyan support. In response to Libya's direct intervention, French and Zairian forces intervened to defend Habr\u00e9, pushing Libyan and rebel forces north of the 16th parallel. In September 1984, the French and the Libyan governments announced an agreement for the mutual withdrawal of their forces from Chad. By the end of the year, all French and Zairian troops were withdrawn. Libya did not honor the withdrawal accord, and its forces continued to occupy the northern third of Chad.\nRebel commando groups (Codos) in southern Chad were broken up by government massacres in 1984. In 1985 Habr\u00e9 briefly reconciled with some of his opponents, including the Democratic Front of Chad (FDT) and the Coordinating Action Committee of the Democratic Revolutionary Council. Goukouni also began to rally toward Habr\u00e9, and with his support Habr\u00e9 successfully expelled Libyan forces from most of Chadian territory. A cease-fire between Chad and Libya held from 1987 to 1988, and negotiations over the next several years led to the 1994 International Court of Justice decision granting Chad sovereignty over the Aouzou strip, effectively ending Libyan occupation.\nThe Idriss D\u00e9by era (1990\u20132021).\nRise to power.\nHowever, rivalry between Hadjerai, Zaghawa and Gorane groups within the government grew in the late 1980s. In April 1989, Idriss D\u00e9by, one of Habr\u00e9's leading generals and a Zaghawa, defected and fled to Darfur in Sudan, from which he mounted a Zaghawa-supported series of attacks on Habr\u00e9 (a Gorane). In December 1990, with Libyan assistance and no opposition from French troops stationed in Chad, D\u00e9by's forces successfully marched on N\u2019Djamena. After 3 months of provisional government, D\u00e9by's Patriotic Salvation Movement (MPS) approved a national charter on February 28, 1991, with D\u00e9by as president.\nDuring the next two years, D\u00e9by faced at least two coup attempts. Government forces clashed violently with rebel forces, including the Movement for Democracy and Development, MDD, National Revival Committee for Peace and Democracy (CSNPD), Chadian National Front (FNT) and the Western Armed Forces (FAO), near Lake Chad and in southern regions of the country. Earlier French demands for the country to hold a National Conference resulted in the gathering of 750 delegates representing political parties (which were legalized in 1992), the government, trade unions and the army to discuss the creation of a pluralist democratic regime.\nHowever, unrest continued, sparked in part by large-scale killings of civilians in southern Chad. The CSNPD, led by Kette Moise and other southern groups entered into a peace agreement with government forces in 1994, which later broke down. Two new groups, the Armed Forces for a Federal Republic (FARF) led by former Kette ally Laokein Barde and the Democratic Front for Renewal (FDR), and a reformulated MDD clashed with government forces from 1994 to 1995.\nMultiparty elections.\nTalks with political opponents in early 1996 did not go well, but D\u00e9by announced his intent to hold presidential elections in June. D\u00e9by won the country's first multi-party presidential elections with support in the second round from opposition leader Kebzabo, defeating General Kamougue (leader of the 1975 coup against Tombalbaye). D\u00e9by's MPS party won 63 of 125 seats in the January 1997 legislative elections. International observers noted numerous serious irregularities in presidential and legislative election proceedings.\nBy mid-1997 the government signed peace deals with FARF and the MDD leadership and succeeded in cutting off the groups from their rear bases in the Central African Republic and Cameroon. Agreements also were struck with rebels from the National Front of Chad (FNT) and Movement for Social Justice and Democracy in October 1997. However, peace was short-lived, as FARF rebels clashed with government soldiers, finally surrendering to government forces in May 1998. Barde was killed in the fighting, as were hundreds of other southerners, most civilians.\nSince October 1998, Chadian Movement for Justice and Democracy (MDJT) rebels, led by Youssuf Togoimi until his death in September 2002, have skirmished with government troops in the Tibesti region, resulting in hundreds of civilian, government, and rebel casualties, but little ground won or lost. No active armed opposition has emerged in other parts of Chad, although Kette Moise, following senior postings at the Ministry of Interior, mounted a smallscale local operation near Moundou which was quickly and violently suppressed by government forces in late 2000.\nD\u00e9by, in the mid-1990s, gradually restored basic functions of government and entered into agreements with the World Bank and IMF to carry out substantial economic reforms. Oil exploitation in the southern Doba region began in June 2000, with World Bank Board approval to finance a small portion of a project, the Chad-Cameroon Petroleum Development Project, aimed at transport of Chadian crude through a 1000-km buried pipeline through Cameroon to the Gulf of Guinea. The project established unique mechanisms for World Bank, private sector, government, and civil society collaboration to guarantee that future oil revenues benefit local populations and result in poverty alleviation. Success of the project depended on multiple monitoring efforts to ensure that all parties keep their commitments. These \"unique\" mechanisms for monitoring and revenue management have faced intense criticism from the beginning. Debt relief was accorded to Chad in May 2001.\nD\u00e9by won a flawed 63% first-round victory in May 2001 presidential elections after legislative elections were postponed until spring 2002. Having accused the government of fraud, six opposition leaders were arrested (twice) and one opposition party activist was killed following the announcement of election results. However, despite claims of government corruption, favoritism of Zaghawas, and abuses by the security forces, opposition party and labor union calls for general strikes and more active demonstrations against the government have been unsuccessful. Despite movement toward democratic reform, power remains in the hands of a northern ethnic oligarchy.\nIn 2003, Chad began receiving refugees from the Darfur region of western Sudan. More than 200,000 refugees fled the fighting between two rebel groups and government-supported militias known as Janjaweed. A number of border incidents led to the Chadian-Sudanese War.\nOil producing and military improvement.\nChad become an oil producer in 2003. In order to avoid resource curse and corruption, elaborate plans sponsored by World Bank were made. This plan ensured transparency in payments, as well as that 80% of money from oil exports would be spent on five priority development sectors, two most important of these being: education and healthcare. However money started getting diverted towards the military even before the civil war broke out. In 2006 when the civil war escalated, Chad abandoned previous economic plans sponsored by World Bank and added \"national security\" as priority development sector, money from this sector was used to improve the military. During the civil war, more than 600 million dollars were used to buy fighter jets, attack helicopters, and armored personnel carriers. \nChad earned between 10 and 11 billion dollars from oil production, and estimated 4 billion dollars were invested in the army.\nWar in the East.\nThe war started on December 23, 2005, when the government of Chad declared a state of war with Sudan and called for the citizens of Chad to mobilize themselves against the \"common enemy,\" which the Chadian government sees as the Rally for Democracy and Liberty (RDL) militants, Chadian rebels, backed by the Sudanese government, and Sudanese militiamen. Militants have attacked villages and towns in eastern Chad, stealing cattle, murdering citizens, and burning houses. Over 200,000 refugees from the Darfur region of northwestern Sudan currently claim asylum in eastern Chad. Chadian president Idriss D\u00e9by accuses Sudanese President Omar Hasan Ahmad al-Bashir of trying to \"destabilize our country, to drive our people into misery, to create disorder and export the war from Darfur to Chad.\"\nAn attack on the Chadian town of Adre near the Sudanese border led to the deaths of either one hundred rebels, as every news source other than CNN has reported, or three hundred rebels. The Sudanese government was blamed for the attack, which was the second in the region in three days, but Sudanese foreign ministry spokesman Jamal Mohammed Ibrahim denies any Sudanese involvement, \"We are not for any escalation with Chad. We technically deny involvement in Chadian internal affairs.\" This attack was the final straw that led to the declaration of war by Chad and the alleged deployment of the Chadian airforce into Sudanese airspace, which the Chadian government denies.\nAn attack on N'Djamena was defeated on April 13, 2006 in the Battle of N'Djamena. The President on national radio stated that the situation was under control, but residents, diplomats and journalists reportedly heard shots of weapons fire.\nOn November 25, 2006, rebels captured the eastern town of Abeche, capital of the Ouadda\u00ef Region and center for humanitarian aid to the Darfur region in Sudan. On the same day, a separate rebel group Rally of Democratic Forces had captured Biltine. On November 26, 2006, the Chadian government claimed to have recaptured both towns, although rebels still claimed control of Biltine. Government buildings and humanitarian aid offices in Abeche were said to have been looted. The Chadian government denied a warning issued by the French Embassy in N'Djamena that a group of rebels was making its way through the Batha Prefecture in central Chad. Chad insists that both rebel groups are supported by the Sudanese government.\nInternational orphanage scandal.\nNearly 100 children at the center of an international scandal that left them stranded at an orphanage in remote eastern Chad returned home after nearly five months March 14, 2008. The 97 children were taken from their homes in October 2007 by a then-obscure French charity, Zo\u00e9's Ark, which claimed they were orphans from Sudan's war-torn Darfur region.\nRebel attack on Ndjamena.\nOn Friday, February 1, 2008, rebels, an opposition alliance of leaders Mahamat Nouri, a former defense minister, and Timane Erdimi, a nephew of Idriss D\u00e9by who was his chief of staff, attacked the Chadian capital of Ndjamena - even surrounding the Presidential Palace. But Idris Deby with government troops fought back. French forces flew in ammunition for Chadian government troops but took no active part in the fighting. UN has said that up to 20,000 people left the region, taking refuge in nearby Cameroon and Nigeria. Hundreds of people were killed, mostly civilians. The rebels accuse Deby of corruption and embezzling millions in oil revenue. While many Chadians may share that assessment, the uprising appears to be a power struggle within the elite that has long controlled Chad. The French government believes that the opposition has regrouped east of the capital. D\u00e9by has blamed Sudan for the current unrest in Chad.\nRegional interventionism.\nDuring the D\u00e9by era, Chad intervened in conflicts in Mali, Central African Republic, Niger and Nigeria.\nIn 2013, Chad sent 2000 men from its military to help France in Operation Serval during the Mali War. Later in the same year Chad sent 850 troops to Central African Republic to help peacekeeping operation MISCA, those troops withdrew in April 2014 after allegations of human rights violations.\nDuring the Boko Haram insurgency, Chad multiple times sent troops to assist the fight against Boko Haram in Niger and Nigeria.\nIn August 2018, rebel fighters of the Military Command Council for the Salvation of the Republic (CCMSR) attacked government forces in northern Chad. Chad experienced threats from jihadists fleeing the Libyan conflict. Chad had been an ally of the West in the fight against Islamist militants in West Africa.\nIn January 2019, after 47 years, Chad restored diplomatic relations with Israel. It was announced during a visit to N\u2019Djamena by Israeli Prime Minister Benjamin Netanyahu.\nAfter Idriss D\u00e9by (2021\u2013present).\nIn April 2021, Chad's army announced that President Idriss D\u00e9by had died of his injuries following clashes with rebels in the north of the country. Idriss Deby ruled the country for more than 30 years since 1990. It was also announced that a military council led by D\u00e9by's son, Mahamat Idriss D\u00e9by a 37-year-old four star general, will govern for the next 18 months.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5330", "revid": "45310583", "url": "https://en.wikipedia.org/wiki?curid=5330", "title": "Geography of Chad", "text": "African country\nChad is one of the 47 landlocked countries in the world and is located in North Central Africa, measuring , nearly twice the size of France and slightly more than three times the size of California. Most of its ethnically and linguistically diverse population lives in the south, with densities ranging from 54 persons per square kilometer in the Logone River basin to 0.1 persons in the northern B.E.T. (Borkou-Ennedi-Tibesti) desert region, which itself is larger than France. The capital city of N'Djam\u00e9na, situated at the confluence of the Chari and Logone Rivers, is cosmopolitan in nature, with a current population in excess of 700,000 people.\nChad has four climatic zones. The northernmost Saharan zone averages less than of rainfall annually. The sparse human population is largely nomadic, with some livestock, mostly small ruminants and camels. The central Sahelian zone receives between rainfall and has vegetation ranging from grass/shrub steppe to thorny, open savanna. The southern zone, often referred to as the Sudan zone, receives between , with woodland savanna and deciduous forests for vegetation. Rainfall in the Guinea zone, located in Chad's southwestern tip, ranges between .\nThe country's topography is generally flat, with the elevation gradually rising as one moves north and east away from Lake Chad. The highest point in Chad is Emi Koussi, a mountain that rises in the northern Tibesti Mountains. The Ennedi Plateau and the Ouadda\u00ef highlands in the east complete the image of a gradually sloping basin, which descends towards Lake Chad. There are also central highlands in the Guera region rising to .\nLake Chad is the second largest lake in west Africa and is one of the most important wetlands on the continent. Home to 120 species of fish and at least that many species of birds, the lake has shrunk dramatically in the last four decades due to increased water usage from an expanding population and low rainfall. Bordered by Chad, Niger, Nigeria, and Cameroon, Lake Chad currently covers only 1350 square kilometers, down from 25,000 square kilometers in 1963. The Chari and Logone Rivers, both of which originate in the Central African Republic and flow northward, provide most of the surface water entering Lake Chad. Chad is also next to Niger.\nGeographical placement.\nLocated in north-central Africa, Chad stretches for about 1,800 kilometers from its northernmost point to its southern boundary. Except in the far northwest and south, where its borders converge, Chad's average width is about 800 kilometers. Its area of 1,284,000 square kilometers is roughly equal to the combined areas of Idaho, Wyoming, Utah, Nevada, and Arizona. Chad's neighbors include Libya to the north, Niger and Nigeria to the west, Sudan to the east, Central African Republic to the south, and Cameroon to the southwest.\nChad exhibits two striking geographical characteristics. First, the country is landlocked. N'Djamena, the capital, is located more than 1,100 kilometers northeast of the Atlantic Ocean; Ab\u00e9ch\u00e9, a major city in the east, lies 2,650 kilometers from the Red Sea; and Faya-Largeau, a much smaller but strategically important center in the north, is in the middle of the Sahara Desert, 1,550 kilometers from the Mediterranean Sea. These vast distances from the sea have had a profound impact on Chad's historical and contemporary development.\nThe second noteworthy characteristic is that the country borders on very different parts of the African continent: North Africa, with its Islamic culture and economic orientation toward the Mediterranean Basin; and West Africa, with its diverse religions and cultures and its history of highly developed states and regional economies.\nChad also borders Northeast Africa, oriented toward the Nile Valley and the Red Sea region - and Central or Equatorial Africa, some of whose people have retained classical African religions while others have adopted Christianity, and whose economies were part of the great Congo River system. Although much of Chad's distinctiveness comes from this diversity of influences, since independence the diversity has also been an obstacle to the creation of a national identity.\nLand.\nAlthough Chadian society is economically, socially, and culturally fragmented, the country's geography is unified by the Lake Chad Basin. Once a huge inland sea (the Pale-Chadian Sea) whose only remnant is shallow Lake Chad, this vast depression extends west into Nigeria and Niger. The larger, northern portion of the basin is bounded within Chad by the Tibesti Mountains in the northwest, the Ennedi Plateau in the northeast, the Ouadda\u00ef Highlands in the east along the border with Sudan, the Gu\u00e9ra Massif in central Chad, and the Mandara Mountains along Chad's southwestern border with Cameroon. The smaller, southern part of the basin falls almost exclusively in Chad. It is delimited in the north by the Gu\u00e9ra Massif, in the south by highlands 250 kilometers south of the border with Central African Republic, and in the southwest by the Mandara Mountains.\nLake Chad, located in the southwestern part of the basin at an altitude of 282 meters, surprisingly does not mark the basin's lowest point; instead, this is found in the Bodele and Djourab regions in the north-central and northeastern parts of the country, respectively. This oddity arises because the great stationary dunes (ergs) of the Kanem region create a dam, preventing lake waters from flowing to the basin's lowest point. At various times in the past, and as late as the 1870s, the Bahr el Ghazal Depression, which extends from the northeastern part of the lake to the Djourab, acted as an overflow canal; since independence, climatic conditions have made overflows impossible.\nNorth and northeast of Lake Chad, the basin extends for more than 800 kilometers, passing through regions characterized by great rolling dunes separated by very deep depressions. Although vegetation holds the dunes in place in the Kanem region, farther north they are bare and have a fluid, rippling character. From its low point in the Djourab, the basin then rises to the plateaus and peaks of the Tibesti Mountains in the north. The summit of this formation\u2014as well as the highest point in the Sahara Desert\u2014is Emi Koussi, a dormant volcano that reaches 3,414 meters above sea level.\nThe basin's northeastern limit is the Ennedi Plateau, whose limestone bed rises in steps etched by erosion. East of the lake, the basin rises gradually to the Ouadda\u00ef Highlands, which mark Chad's eastern border and also divide the Chad and Nile watersheds. These highland areas are part of the East Saharan montane xeric woodlands ecoregion.\nSoutheast of Lake Chad, the regular contours of the terrain are broken by the Gu\u00e9ra Massif, which divides the basin into its northern and southern parts. South of the lake lie the floodplains of the Chari and Logone rivers, much of which are inundated during the rainy season. Farther south, the basin floor slopes upward, forming a series of low sand and clay plateaus, called koros, which eventually climb to 615 meters above sea level. South of the Chadian border, the koros divide the Lake Chad Basin from the Ubangi-Zaire river system.\nWater systems.\nPermanent streams do not exist in northern or central Chad. Following infrequent rains in the Ennedi Plateau and Ouadda\u00ef Highlands, water may flow through depressions called enneris and wadis. Often the result of flash floods, such streams usually dry out within a few days as the remaining puddles seep into the sandy clay soil. The most important of these streams is the Batha, which in the rainy season carries water west from the Ouadda\u00ef Highlands and the Gu\u00e9ra Massif to Lake Fitri.\nChad's major rivers are the Chari and the Logone and their tributaries, which flow from the southeast into Lake Chad. Both river systems rise in the highlands of Central African Republic and Cameroon, regions that receive more than 1,250 millimeters of rainfall annually. Fed by rivers of Central African Republic, as well as by the Bahr Salamat, Bahr Aouk, and Bahr Sara rivers of southeastern Chad, the Chari River is about 1,200 kilometers long. From its origins near the city of Sarh, the middle course of the Chari makes its way through swampy terrain; the lower Chari is joined by the Logone River near N'Djamena. The Chari's volume varies greatly, from 17 cubic meters per second during the dry season to 340 cubic meters per second during the wettest part of the year.\nThe Logone River is formed by tributaries flowing from Cameroon and Central African Republic. Both shorter and smaller in volume than the Chari, it flows northeast for 960 kilometers; its volume ranges from five to eighty-five cubic meters per second. At N'Djamena the Logone empties into the Chari, and the combined rivers flow together for thirty kilometers through a large delta and into Lake Chad. At the end of the rainy season in the fall, the river overflows its banks and creates a huge floodplain in the delta.\nThe seventh largest lake in the world (and the fourth largest in Africa), Lake Chad is located in the sahelian zone, a region just south of the Sahara Desert. The Chari River contributes 95 percent of Lake Chad's water, an average annual volume of 40 billion cubic meters, 95% of which is lost to evaporation. The size of the lake is determined by rains in the southern highlands bordering the basin and by temperatures in the Sahel. Fluctuations in both cause the lake to change dramatically in size, from 9,800 square kilometers in the dry season to 25,500 at the end of the rainy season.\nLake Chad also changes greatly in size from one year to another. In 1870 its maximum area was 28,000 square kilometers. The measurement dropped to 12,700 in 1908. In the 1940s and 1950s, the lake remained small, but it grew again to 26,000 square kilometers in 1963. The droughts of the late 1960s, early 1970s, and mid-1980s caused Lake Chad to shrink once again, however. The only other lakes of importance in Chad are Lake Fitri, in Batha Prefecture, and Lake Iro, in the marshy southeast.\nClimate.\nThe Lake Chad Basin embraces a great range of tropical climates from north to south, although most of these climates tend to be dry. Apart from the far north, most regions are characterized by a cycle of alternating rainy and dry seasons. In any given year, the duration of each season is determined largely by the positions of two great air masses\u2014a maritime mass over the Atlantic Ocean to the southwest and a much drier continental mass.\nDuring the rainy season, winds from the southwest push the moister maritime system north over the African continent where it meets and slips under the continental mass along a front called the \"intertropical convergence zone\". At the height of the rainy season, the front may reach as far as Kanem Prefecture. By the middle of the dry season, the intertropical convergence zone moves south of Chad, taking the rain with it. This weather system contributes to the formation of three major regions of climate and vegetation.\nSaharan region.\nThe Saharan region covers roughly the northern half of the country, including Borkou-Ennedi-Tibesti Prefecture along with the northern parts of Kanem, Batha, and Biltine prefectures. Much of this area receives only traces of rain during the entire year; at Faya-Largeau, for example, annual rainfall averages less than , and there are nearly 3800 hours of sunshine. Scattered small oases and occasional wells provide water for a few date palms or small plots of millet and garden crops.\nIn much of the north, the average daily maximum temperature is about during January, the coolest month of the year, and about during May, the hottest month. On occasion, strong winds from the northeast produce violent sandstorms. In northern Biltine Prefecture, a region called the Mortcha plays a major role in animal husbandry. Dry for nine months of the year, it receives or more of rain, mostly during July and August.\nA carpet of green springs from the desert during this brief wet season, attracting herders from throughout the region who come to pasture their cattle and camels. Because very few wells and springs have water throughout the year, the herders leave with the end of the rains, turning over the land to the antelopes, gazelles, and ostriches that can survive with little groundwater. Northern Chad averages over 3500 hours of sunlight per year, the south somewhat less.\nSahelian region.\nThe semiarid sahelian zone, or Sahel, forms a belt about wide that runs from Lac and Chari-Baguirmi prefectures eastward through Gu\u00e9ra, Ouadda\u00ef, and northern Salamat prefectures to the Sudanese frontier. The climate in this transition zone between the desert and the southern sudanian zone is divided into a rainy season (from June to September) and a dry period (from October to May).\nIn the northern Sahel, thorny shrubs and acacia trees grow wild, while date palms, cereals, and garden crops are raised in scattered oases. Outside these settlements, nomads tend their flocks during the rainy season, moving southward as forage and surface water disappear with the onset of the dry part of the year. The central Sahel is characterized by drought-resistant grasses and small woods. Rainfall is more abundant there than in the Saharan region. For example, N'Djamena records a maximum annual average rainfall of , while Ouadda\u00ef Prefecture receives just a bit less.\nDuring the hot season, in April and May, maximum temperatures frequently rise above . In the southern part of the Sahel, rainfall is sufficient to permit crop production on unirrigated land, and millet and sorghum are grown. Agriculture is also common in the marshlands east of Lake Chad and near swamps or wells. Many farmers in the region combine subsistence agriculture with the raising of cattle, sheep, goats, and poultry.\nSudanian region.\nThe humid \"sudanian\" zone includes the Sahel, the southern prefectures of Mayo-Kebbi, Tandjil\u00e9, Logone Occidental, Logone Oriental, Moyen-Chari, and southern Salamat. Between April and October, the rainy season brings between of precipitation. Temperatures are high throughout the year. Daytime readings in Moundou, the major city in the southwest, range from in the middle of the cool season in January to about in the hot months of March, April, and May.\nThe sudanian region is predominantly East Sudanian savanna, or plains covered with a mixture of tropical or subtropical grasses and woodlands. The growth is lush during the rainy season but turns brown and dormant during the five-month dry season between November and March. Over a large part of the region, however, natural vegetation has yielded to agriculture.\n2010 drought.\nOn 22 June, the temperature reached in Faya, breaking a record set in 1961 at the same location. Similar temperature rises were also reported in Niger, which began to enter a famine situation.\nOn 26 July the heat reached near-record levels over Chad and Niger.\nArea.\nArea:\n&lt;br&gt;\"total:\"\n1.284 million km2\n&lt;br&gt;\"land:\"\n1,259,200\u00a0km2\n&lt;br&gt;\"water:\"\n24,800\u00a0km2\nArea - comparative:\n&lt;br&gt;Canada: smaller than the Northwest Territories\n&lt;br&gt;US: slightly more than three times the size of California\nBoundaries.\nLand boundaries:\n&lt;br&gt;\"total:\"\n6,406\u00a0km\n&lt;br&gt;\"border countries:\"\nCameroon 1,116\u00a0km, Central African Republic 1,556\u00a0km, Libya 1,050\u00a0km, Niger 1,196\u00a0km, Nigeria 85\u00a0km, Sudan 1,403\u00a0km\nCoastline:\n0\u00a0km (landlocked)\nMaritime claims:\nnone (landlocked)\nElevation extremes:\n&lt;br&gt;\"lowest point:\"\nBod\u00e9l\u00e9 Depression 160 m\n&lt;br&gt;\"highest point:\"\nEmi Koussi 3,415 m\nLand use and resources.\nNatural resources:\npetroleum, uranium, natron, kaolin, fish (Chari River, Logone River), gold, limestone, sand and gravel, salt\nLand use:\n&lt;br&gt;\"arable land:\"\n3.89%\n&lt;br&gt;\"permanent crops:\"\n0.03%\n&lt;br&gt;\"other:\"\n96.08% (2012)\nIrrigated land:\n302.7\u00a0km2 (2003)\nTotal renewable water resources:\n43\u00a0km3 (2011)\nFreshwater withdrawal (domestic/industrial/agricultural):\n&lt;br&gt;\"total:\"\n0.88\u00a0km3/yr (12%/12%/76%)\n&lt;br&gt;\"per capita:\"\n84.81 m3/yr (2005)\nEnvironmental issues.\nNatural hazards:\nhot, dry, dusty, Harmattan winds occur in north; periodic droughts; locust plagues\nEnvironment - current issues:\ninadequate supplies of potable water; improper waste disposal in rural areas contributes to soil and water pollution; desertification\nExtreme points.\nThis is a list of the extreme points of Chad, the points that are farther north, south, east or west than any other location.\n\"*Note: technically Chad does not have an easternmost point, the easternmost section of the border being formed by the 24\u00b0 of longitude\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5331", "revid": "1155780206", "url": "https://en.wikipedia.org/wiki?curid=5331", "title": "Demographics of Chad", "text": "The people of Chad speak more than 100 different languages and divide themselves into many ethnic groups. However, language and ethnicity are not the same. Moreover, neither element can be tied to a particular physical type.\nAlthough the possession of a common language shows that its speakers have lived together and have a common history, peoples also change languages. This is particularly so in Chad, where the openness of the terrain, marginal rainfall, frequent drought and famine, and low population densities have encouraged physical and linguistic mobility. Slave raids among non-Muslim peoples, internal slave trade, and exports of captives northward from the ninth to the twentieth centuries also have resulted in language changes.\nAnthropologists view ethnicity as being more than genetics. Like language, ethnicity implies a shared heritage, partly economic, where people of the same ethnic group may share a livelihood, and partly social, taking the form of shared ways of doing things and organizing relations among individuals and groups. Ethnicity also involves a cultural component made up of shared values and a common worldview. Like language, ethnicity is not immutable. Shared ways of doing things change over time and alter a group's perception of its own identity.\nNot only do the social aspects of ethnic identity change but the biological composition (or gene pool) also may change over time. Although most ethnic groups emphasize intermarriage, people are often proscribed from seeking partners among close relatives\u2014a prohibition that promotes biological variation. In all groups, the departure of some individuals or groups and the integration of others also changes the biological component.\nThe Chadian government has avoided official recognition of ethnicity. With the exception of a few surveys conducted shortly after independence, little data were available on this important aspect of Chadian society. Nonetheless, ethnic identity was a significant component of life in Chad.\nThe peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa.\nChad's languages fall into ten major groups, each of which belongs to either the\nNilo-Saharan, Afro-Asiatic, or Niger\u2013Congo language family. These represent three of the four major language families in Africa; only the Khoisan languages of southern Africa are not represented. The presence of such different languages suggests that the Lake Chad Basin may have been an important point of dispersal in ancient times.\nPopulation.\nAccording to the total population was in , compared to only 2 429 000 in 1950. The proportion of children below the age of 15 in 2010 was 45.4%, 51.7% was between 15 and 65 years of age, while 2.9% was 65 years or the country is projected to have a population of 34 millions peoples in 2050 and 61 millions peoples in 2100\nPopulation by Sex and Age Group (Census 20.V.2009):\nVital statistics.\nRegistration of vital events is in Chad not complete. The Population Departement of the United Nations prepared the following estimates.\nSource: UN DESA, World Population Prospects, 2022\nFertility and births.\nTotal Fertility Rate (TFR) (Wanted Fertility Rate) and Crude Birth Rate (CBR):\nFertility data as of 2014-2015 (DHS Program):\nReligions.\nThe separation of religion from social structure in Chad represents a false dichotomy, for they are perceived as two sides of the same coin. Three religious traditions coexist in Chad- classical African religions, Islam, and Christianity. None is monolithic. The first tradition includes a variety of ancestor and/or place-oriented religions whose expression is highly specific. Islam, although characterized by an orthodox set of beliefs and observances, also is expressed in diverse ways. Christianity arrived in Chad much more recently with the arrival of Europeans. Its followers are divided into Roman Catholics and Protestants (including several denominations); as with Chadian Islam, Chadian Christianity retains aspects of pre-Christian religious belief.\nThe number of followers of each tradition in Chad is unknown. Estimates made in 1962 suggested that 35 percent of Chadians practiced classical African religions, 55 percent were Muslims, and 10 percent were Christians. In the 1970s and 1980s, this distribution undoubtedly changed. Observers report that Islam has spread among the Hajerai and among other non-Muslim populations of the Saharan and sahelian zones. However, the proportion of Muslims may have fallen because the birthrate among the followers of traditional religions and Christians in southern Chad is thought to be higher than that among Muslims. In addition, the upheavals since the mid-1970s have resulted in the departure of some missionaries; whether or not Chadian Christians have been numerous enough and organized enough to have attracted more converts since that time is unknown.\nOther demographic statistics.\nDemographic statistics according to the World Population Review in 2022.\nThe following demographic statistics are from the CIA World Factbook.\n17,963,211 (2022 est.)\n15,833,116 (July 2018 est.)\n12,075,985 (2017 est.)\nReligions.\nMuslim 52.1%, Protestant 23.9%, Roman Catholic 20%, animist 0.3%, other Christian 0.2%, none 2.8%, unspecified 0.7% (2014-15 est.)\n\"0-14 years:\" 47.43% (male 4,050,505/female 3,954,413)\n\"15-24 years:\" 19.77% (male 1,676,495/female 1,660,417)\n\"25-54 years:\" 27.14% (male 2,208,181/female 2,371,490)\n\"55-64 years:\" 3.24% (male 239,634/female 306,477)\n\"65 years and over:\" 2.43% (2020 est.) (male 176,658/female 233,087)\n\"0-14 years:\" 48.12% (male 3,856,001 /female 3,763,622)\n\"15-24 years:\" 19.27% (male 1,532,687 /female 1,518,940)\n\"25-54 years:\" 26.95% (male 2,044,795 /female 2,222,751)\n\"55-64 years:\" 3.25% (male 228,930 /female 286,379)\n\"65 years and over:\" 2.39% (male 164,257 /female 214,754) (2018 est.)\ntotal: 16.1 years. Country comparison to the world: 223rd\nmale: 15.6 years\nfemale: 16.5 years (2020 est.)\ntotal: 15.8 years. Country comparison to the world: 226th\nmale: 15.3 years \nfemale: 16.3 years (2018 est.)\nTotal: 17.8 years\nMale: 16.8 years\nFemale: 18.8 years (2017 est.)\n3.09% (2022 est.) Country comparison to the world: 10th\n3.23% (2018 est.) Country comparison to the world: 5th\n40.45 births/1,000 population (2022 est.) Country comparison to the world: 6th\n43 births/1,000 population (2018 est.) Country comparison to the world: 4th\n9.45 deaths/1,000 population (2022 est.) Country comparison to the world: 49th\n10.5 deaths/1,000 population (2018 est.) Country comparison to the world: 26th\n-0.13 migrant(s)/1,000 population (2022 est.) Country comparison to the world: 105th\n-3.2 migrant(s)/1,000 population (2017 est.) Country comparison to the world: 176th\n5.46 children born/woman (2022 est.) Country comparison to the world: 5th\n5.9 children born/woman (2018 est.) Country comparison to the world: 4th\n18.1 years (2014/15 est.)\nnote: median age at first birth among women 25-49\ntotal dependency ratio: 100.2 (2015 est.)\nyouth dependency ratio: 95.2 (2015 est.)\nelderly dependency ratio: 4.9 (2015 est.)\npotential support ratio: 20.3 (2015 est.)\n8.1% (2019)\n5.7% (2014/15)\n\"urban population:\" 24.1% of total population (2022)\n\"rate of urbanization:\" 4.1% annual rate of change (2020-25 est.)\n\"urban population:\" 23.1% of total population (2018)\n\"rate of urbanization:\" 3.88% annual rate of change (2015-20 est.)\nAt birth: 1.04 male(s)/female\nUnder 15 years: 1.01 male(s)/female\n15\u201364 years: 0.92 male(s)/female\n65 years and over: 0.66 male(s)/female\nTotal population: 0.96 male(s)/female (2006 est.)\ntotal population: 59.15 years. Country comparison to the world: 222nd\nmale: 57.32 years\nfemale: 61.06 years (2022 est.)\ntotal population: 57.5 years (2018 est.) Country comparison to the world: 214th\nmale: 55.7 years (2018 est.)\nfemale: 59.3 years (2018 est.)\nTotal population: 50.6 years\nMale: 49.4 years\nFemale: 51.9 years (2017 est.)\nAdult prevalence rate: 1.3% (2017 est.)\nPeople living with HIV/AIDS: 110,000(2017 est.)\nDeaths: 3,100 (2017 est.)\n28.8% (2015)\ndegree of risk: very high (2020)\nfood or waterborne diseases: bacterial and protozoal diarrhea, hepatitis A and E, and typhoid fever\nvectorborne diseases: malaria and dengue fever\nwater contact diseases: schistosomiasis\nanimal contact diseases: rabies\nrespiratory diseases: meningococcal meningitis\nMajor infectious diseases.\nnote: on 21 March 2022, the US Centers for Disease Control and Prevention (CDC) issued a Travel Alert for polio in Africa; Chad is currently considered a high risk to travelers for circulating vaccine-derived polioviruses (cVDPV); vaccine-derived poliovirus (VDPV) is a strain of the weakened poliovirus that was initially included in oral polio vaccine (OPV) and that has changed over time and behaves more like the wild or naturally occurring virus; this means it can be spread more easily to people who are unvaccinated against polio and who come in contact with the stool or respiratory secretions, such as from a sneeze, of an \u201cinfected\u201d person who received oral polio vaccine; the CDC recommends that before any international travel, anyone unvaccinated, incompletely vaccinated, or with an unknown polio vaccination status should complete the routine polio vaccine series; before travel to any high-risk destination, CDC recommends that adults who previously completed the full, routine polio vaccine series receive a single, lifetime booster dose of polio vaccine\nwomen married by age 15: 24.2% (2019)\nwomen married by age 18: 60.6% (2019)\nmen married by age 18: 8.1% (2019 est.)\nNoun: Chadian(s)\nAdjective: Chadian\nEthnic groups.\nThe peoples of Chad carry significant ancestry from Eastern, Central, Western, and Northern Africa. \n200 distinct groups\nAbout 5,000 French citizens live in Chad.\nArabic (official), French (official), Sara (in south), more than 120 different languages and dialects\nDefinition: age 15 and over can read and write French or Arabic\ntotal population: 22.3% (2016 est.)\nmale: 31.3% (2016 est.)\nfemale: 14% (2016 est.)\ntotal: 7 years\nmale: 9 years\nfemale: 6 years (2015)\ntotal: 8 years (2014)\nmale: 9 years (2014)\nfemale: 6 years (2014)\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nAttribution:"}
{"id": "5332", "revid": "38004052", "url": "https://en.wikipedia.org/wiki?curid=5332", "title": "Politics of Chad", "text": "The Politics of Chad take place in a framework of a presidential republic, whereby the President of Chad is both head of state and head of government. Executive power is exercised by the government. Legislative power is vested in both the government and parliament. Chad is one of the most corrupt countries in the world.\nIn May 2013, security forces in Chad foiled a coup against the President Idriss Deby that had been in preparation for several months. In April 2021, President D\u00e9by was injured by the rebel group Front Pour l'Alternance et La Concorde au Tchad (FACT). He succumbed to his injuries on April 20, 2021. His presidency was taken by his family member Mahamat D\u00e9by in April of 2021. This resulted in both the National Assembly and Chadian Government being dissolved and replaced with a Transitional Military Council.\nThe National Transitional Council will oversee the transition to democracy.\nExecutive branch.\nChad's executive branch is headed by the President and dominates the Chadian political system. Following the military overthrow of Hiss\u00e8ne Habr\u00e9 in December 1990, Idriss D\u00e9by won the presidential elections in 1996 and 2001. The constitutional basis for the government is the 1996 constitution, under which the president was limited to two terms of office until D\u00e9by had that provision repealed in 2005. The president has the power to appoint the Council of State (or cabinet), and exercises considerable influence over appointments of judges, generals, provincial officials and heads of Chad's parastatal firms. In cases of grave and immediate threat, the president, in consultation with the National Assembly President and Council of State, may declare a state of emergency. Most of the key advisors for former president D\u00e9by were members of the Zaghawa clan, although some southern and opposition personalities were represented in his government.\nLegislative branch.\nAccording to the 1996 constitution, the National Assembly deputies are elected by universal suffrage for 4-year terms. The Assembly holds regular sessions twice a year, starting in March and October, and can hold special sessions as necessary and called by the prime minister. Deputies elect a president of the National Assembly every 2 years. Assembly deputies or members of the executive branch may introduce legislation; once passed by the Assembly, the president must take action to either sign or reject the law within 15 days. The National Assembly must approve the prime minister's plan of government and may force the prime minister to resign through a majority vote of no-confidence. However, if the National Assembly rejects the executive branch's program twice in one year, the president may disband the Assembly and call for new legislative elections. In practice, the president exercises considerable influence over the National Assembly through the MPS party structure.\nJudicial branch.\nDespite the constitution's guarantee of judicial independence from the executive branch, the president names most key judicial officials. The Supreme Court is made up of a chief justice, named by the president, and 15 councilors chosen by the president and National Assembly; appointments are for life. The Constitutional Council, with nine judges elected to 9-year terms, has the power to review all legislation, treaties and international agreements prior to their adoption. The constitution recognizes customary and traditional law in locales where it is recognized and to the extent it does not interfere with public order or constitutional guarantees of equality for all citizens.\nInternational organization participation.\nACCT, \nACP, \nAfDB, \nAU, \nBDEAC, \nCEMAC, \nFAO, \nFZ, \nG-77, \nIBRD, \nICAO, \nICCt, \nICFTU, \nICRM, \nIDA, \nIDB, \nIFAD, \nIFC, \nIFRCS, \nILO, \nIMF, \nInterpol, \nIOC, \nITU, \nMIGA, \nNAM, \nOIC, \nONUB, \nOPCW, \nUN, \nUNCTAD, \nUNESCO, \nUNIDO, \nUNOCI, \nUPU, \nWCL, \nWHO, \nWIPO, \nWMO, \nWToO, \nWTrO\n2021 government shakeup.\nOn 20 April 2021, following the death of longtime Chad President Idriss D\u00e9by, the Military of Chad released a statement confirming that both the Government of Chad and the nation's National Assembly had been dissolved and that a Transitional Military Council led by D\u00e9by's son Mahamat would lead the nation for at least 18 months.\nFollowing protests on 14 May 2022, the authorities in Chad detained several members of civil society organizations. The protests were organized in N\u2019Djamena, and other cities across the country by Chadian civil society organizations, united under the coalition Wakit Tamma.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5333", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=5333", "title": "Economy of Chad", "text": "National economy\nThe economy of Chad suffers from the landlocked country's geographic remoteness, drought, lack of infrastructure, and political turmoil. About 85% of the population depends on agriculture, including the herding of livestock. Of Africa's Francophone countries, Chad benefited least from the 50% devaluation of their currencies in January 1994. Financial aid from the World Bank, the African Development Bank, and other sources is directed largely at the improvement of agriculture, especially livestock production. Because of lack of financing, the development of oil fields near Doba, originally due to finish in 2000, was delayed until 2003. It was finally developed and is now operated by ExxonMobil. In terms of gross domestic product, Chad ranks 143rd globally with $11.051 billion dollars as of 2018.\nAgriculture.\nChad produced in 2018:\nIn addition to smaller productions of other agricultural products. \nMacro-economic trend.\nThe following table shows the main economic indicators in 1980\u20132017.\nOther statistics.\nGDP:\npurchasing power parity \u2013 $28.62 billion (2017 est.)\nGDP \u2013 real growth rate:\n-3.1% (2017 est.)\nGDP \u2013 per capita:\n$2,300 (2017 est.)\nGross national saving: \n15.5% of GDP (2017 est.)\nGDP \u2013 composition by sector:\n\"agriculture:\"\n52.3% (2017 est.) \n\"industry:\"\n14.7% (2017 est.) \n\"services:\"\n33.1% (2017 est.)\nPopulation below poverty line::\n46.7% (2011 est.)\nDistribution of family income \u2013 Gini index:\n43.3 (2011 est.)\nInflation rate (consumer prices):\n-0.9% (2017 est.)\nLabor force:\n5.654 million (2017 est.)\nLabor force \u2013 by occupation:\nagriculture 80%, industry and services 20% (2006 est.)\nBudget:\n\"revenues:\"\n1.337 billion (2017 est.) \n\"expenditures:\"\n1.481 billion (2017 est.)\nBudget surplus (+) or deficit (-):\n-1.5% (of GDP) (2017 est.)\nPublic debt: \n52.5% of GDP (2017 est.) \nIndustries:\noil, cotton textiles, brewing, natron (sodium carbonate), soap, cigarettes, construction materials\nIndustrial production growth rate:\n-4% (2017 est.)\nelectrification: total population: 4% (2013)\nelectrification: urban areas: 14% (2013)\nelectrification: rural areas: 1% (2013)\nElectricity \u2013 production:\n224.3 million kWh (2016 est.)\nElectricity \u2013 production by source:\n&lt;br&gt;\"fossil fuel:\"\n98%\n&lt;br&gt;\"hydro:\"\n0%\n&lt;br&gt;\"nuclear:\"\n0%\n&lt;br&gt;\"other renewable:\"\n3% (2017)\nElectricity \u2013 consumption:\n208.6 million kWh (2016 est.)\nElectricity \u2013 exports:\n0 kWh (2016 est.)\nElectricity \u2013 imports:\n0 kWh (2016 est.)\nAgriculture \u2013 products:\ncotton, sorghum, millet, peanuts, sesame, corn, rice, potatoes, onions, cassava (manioc, tapioca), cattle, sheep, goats, camels\nExports:\n$2.464 billion (2017 est.)\nExports \u2013 commodities:\noil, livestock, cotton, sesame, gum arabic, shea butter\nExports \u2013 partners:\nUS 38.7%, China 16.6%, Netherlands 15.7%, UAE 12.2%, India 6.3% (2017)\nImports:\n$2.16 billion (2017 est.)\nImports \u2013 commodities:\nmachinery and transportation equipment, industrial goods, foodstuffs, textiles\nImports \u2013 partners:\nChina 19.9%, Cameroon 17.2%, France 17%, US 5.4%, India 4.9%, Senegal 4.5% (2017)\nDebt \u2013 external:\n$1.724 billion (31 December 2017 est.)\nReserves of foreign exchange and gold:\n$22.9 million (31 December 2017 est.)\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5334", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=5334", "title": "Telecommunications in Chad", "text": "Overview of telecommunications service in Chad\nTelecommunications in Chad include radio, television, fixed and mobile telephones, and the Internet.\nRadio and television.\nRadio stations:\nRadios:\n1.7 million (1997).\nTelevision stations:\nTelevision sets:\n10,000 (1997).\nRadio is the most important medium of mass communication. State-run Radiodiffusion Nationale Tchadienne operates national and regional radio stations. Around a dozen private radio stations are on the air, despite high licensing fees, some run by religious or other non-profit groups. The BBC World Service (FM 90.6) and Radio France Internationale (RFI) broadcast in the capital, N'Djamena. The only television station, Tele Tchad, is state-owned.\nState control of many broadcasting outlets allows few dissenting views. Journalists are harassed and attacked. On rare occasions journalists are warned in writing by the High Council for Communication to produce more \"responsible\" journalism or face fines. Some journalists and publishers practice self-censorship. On 10 October 2012, the High Council on Communications issued a formal warning to La Voix du Paysan, claiming that the station's live broadcast on 30 September incited the public to \"insurrection against the government.\" The station had broadcast a sermon by a bishop who criticized the government for allegedly failing to use oil wealth to benefit the region.\nTelephones.\nCalling code: +235\nInternational call prefix: 00\nMain lines:\nMobile cellular:\nTelephone system: inadequate system of radiotelephone communication stations with high costs and low telephone density; fixed-line connections for less than 1 per 100 persons coupled with mobile-cellular subscribership base of only about 35 per 100 persons (2011).\nSatellite earth stations: 1 Intelsat (Atlantic Ocean) (2011).\nInternet.\nTop-level domain: .td\nInternet users:\nFixed broadband: 18,000 subscriptions, 132nd in the world; 0.2% of the population, 161st in the world (2012).\nWireless broadband: Unknown (2012).\nInternet hosts:\nIPv4: 4,096 addresses allocated, less than 0.05% of the world total, 0.4 addresses per 1000 people (2012).\nInternet censorship and surveillance.\nThere are no government restrictions on access to the Internet or credible reports that the government monitors e-mail or Internet chat rooms.\nThe constitution provides for freedom of opinion, expression, and press, but the government does not always respect these rights. Private individuals are generally free to criticize the government without reprisal, but reporters and publishers risk harassment from authorities when publishing critical articles. The 2010 media law abolished prison sentences for defamation and insult, but prohibits \"inciting racial, ethnic, or religious hatred,\" which is punishable by one to two years in prison and a fine of one to three million CFA francs ($2,000 to $6,000).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5335", "revid": "27015025", "url": "https://en.wikipedia.org/wiki?curid=5335", "title": "Transport in Chad", "text": "Transport infrastructure within Chad is generally poor, especially in the north and east of the country. River transport is limited to the south-west corner. As of 2011 Chad had no railways though two lines are planned - from the capital to the Sudanese and Cameroonian borders during the wet season, especially in the southern half of the country. In the north, roads are merely tracks across the desert and land mines continue to present a danger. Draft animals (horses, donkeys and camels) remain important in much of the country.\nFuel supplies can be erratic, even in the south-west of the country, and are expensive. Elsewhere they are practically non-existent.\nRailways.\nAs of 2011 Chad had no railways. Two lines were planned to Sudan and Cameroon from the capital, with construction expected to start in 2012. \nNo operative lines were listed as of 2019.\nIn 2021, an ADB study was funded for that rail link from Cameroon to Chad.\nHighways.\nAs at 2018 Chad had a total of 44,000\u00a0km of roads of which approximately 260\u00a0km are paved. Some, but not all of the roads in the capital N'Djamena are paved. Outside of N'Djamena there is one paved road which runs from Massakory in the north, through N'Djamena and then south, through the cities of Gu\u00e9lengdeng, Bongor, K\u00e9lo and Moundou, with a short spur leading in the direction of Kousseri, Cameroon, near N'Djamena. Expansion of the road towards Cameroon through Pala and L\u00e9r\u00e9 is reportedly in the preparatory stages.\nWaterways.\nAs at 2012, Chari and Logone Rivers were navigable only in wet season (2002). Both flow northwards, from the south of Chad, into Lake Chad.\nPipelines.\nSince 2003, a 1,070\u00a0km pipeline has been used to export crude oil from the oil fields around Doba to offshore oil-loading facilities on Cameroon's Atlantic coast at Kribi.\nThe CIA World Factbook however cites only 582\u00a0km of pipeline in Chad itself as at 2013.\nSeaports and harbors.\nNone (landlocked).\nChad's main routes to the sea are:\nIn colonial times, the main access was by road to Bangui, in the Central African Republic, then by river boat to Brazzaville, and onwards by rail from Brazzaville to Pointe Noire, on Congo's Atlantic coast. This route is now little used.\nThere is also a route across Sudan, to the Red Sea, but very little trade goes this way.\nLinks with Niger, north of Lake Chad, are practically nonexistent; it is easier to reach Niger via Cameroon and Nigeria. \nAirports.\nAs of 2012[ [update]] Chad had an estimated 58 airports, only 9 of which had paved runways. In 2015, scheduled airlines in Chad carried approximately 28,332 passengers.\nAirports with paved runways.\nStatistics on airports with paved runways as of 2017:\nList of airports with paved runways:\nAirports - with unpaved runways.\nStatistics on airports with unpaved runways as of 2013:\nAirline.\nSAGA Airline of Chad - see http://www.airsaga.com\nMinistry of Transport.\nThe Ministry is represented at the regional level by the Regional Delegations, which have jurisdiction over a part of the National Territory as defined by Decree No. 003 / PCE / CTPT / 91. Their organization and responsibilities are defined by Order No. 006 / MTPT / SE / DG / 92.\nThe Regional Delegations are:\n*The Regional Delegation of the Center covering the regions of Batha, Gu\u00e9ra and Salamat with headquarters in Mongo;\n*The Regional Delegation of the Center-Ouest covering the regions of Chari Baguirmi and Hatier Lamis with headquarters Massakory;\n*The North-West Regional Delegation covering the Kanem and Lake regions with headquarters in Mao;\n*The Western Regional Delegation covering the areas of Mayo-East Kebbi, Mayo-West Kebbi and Tandjile with headquarters in Bongor;\n*The Eastern Regional Delegation covering the regions of Wadi Fira and Ouaddai with headquarters in Ab\u00e9ch\u00e9;\n*The South-East Regional Delegation covering the Mandoul and Moyen Chari regions with headquarters in Sarh;\n*The Southwest Regional Delegation covering the regions of Logone Occidental and Logone Orientai with headquarters in Moundou;\n*The Northern Regional Delegation covering the BET region with headquarters in Faya.\nEach Regional Delegation is organized into regional services, namely: the Regional Roads Service, the Regional Transport Service, the Civilian Buildings Regional Service and, as needed, other regional services may be established in one or more Delegations .\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5336", "revid": "37702766", "url": "https://en.wikipedia.org/wiki?curid=5336", "title": "Chad National Army", "text": "Combined military forces of Chad\nThe Chad National Army (; , ANT) consists of the five Defence and Security Forces listed in Article 185 of the Chadian Constitution that came into effect on 4 May 2018. These are the National Army ((including Ground Forces, and Air Force), the National Gendarmerie), the National Police, the National and Nomadic Guard (GNNT) and the Judicial Police. Article 188 of the Constitution specifies that National Defence is the responsibility of the Army, Gendarmerie and GNNT, whilst the maintenance of public order and security is the responsibility of the Police, Gendarmerie and GNNT.\nHistory.\nFrom independence through the period of the presidency of F\u00e9lix Malloum (1975\u201379), the official national army was known as the Chadian Armed Forces (Forces Arm\u00e9es Tchadiennes\u2014FAT). Composed mainly of soldiers from southern Chad, FAT had its roots in the army recruited by France and had military traditions dating back to World War I. FAT lost its status as the legal state army when Malloum's civil and military administration disintegrated in 1979. Although it remained a distinct military body for several years, FAT was eventually reduced to the status of a regional army representing the south.\nAfter Habr\u00e9 consolidated his authority and assumed the presidency in 1982, his victorious army, the Armed Forces of the North (Forces Arm\u00e9es du Nord\u2014FAN), became the nucleus of a new national army. The force was officially constituted in January 1983, when the various pro-Habr\u00e9 contingents were merged and renamed the Chadian National Armed Forces (Forces Arm\u00e9es Nationales Tchadiennes\u2014FANT).\nThe Military of Chad was dominated by members of Toubou, Zaghawa, Kanembou, Hadjerai, and Massa ethnic groups during the presidency of Hiss\u00e8ne Habr\u00e9. Later Chadian president Idriss D\u00e9by revolted and fled to the Sudan, taking with him many Zaghawa and Hadjerai soldiers in 1989.\nChad's armed forces numbered about 36,000 at the end of the Habr\u00e9 regime, but swelled to an estimated 50,000 in the early days of D\u00e9by's rule. With French support, a reorganization of the armed forces was initiated early in 1991 with the goal of reducing its numbers and making its ethnic composition reflective of the country as a whole. Neither of these goals was achieved, and the military is still dominated by the Zaghawa.\nIn 2004, the government discovered that many of the soldiers it was paying did not exist and that there were only about 19,000 soldiers in the army, as opposed to the 24,000 that had been previously believed. Government crackdowns against the practice are thought to have been a factor in a failed military mutiny in May 2004.\nRenewed conflict, in which the Chadian military is involved, came in the form of a civil war against Sudanese-backed rebels. Chad successfully managed to repel many rebel movements, albeit with some losses (see Battle of N'Djamena (2008)). The army used its artillery systems and tanks, but well-equipped insurgents probably managed to destroy over 20 of Chad's 60 T-55 tanks, and probably shot down a Mi-24 Hind gunship, which bombed enemy positions near the border with Sudan. In November 2006 Libya supplied Chad with four Aermacchi SF.260W light attack planes. They were used to strike enemy positions by the Chadian Air Force, but one was shot down by rebels. During the 2008 battle of N'Djamena, gunships and tanks were put to good use, pushing armed militia forces back from the Presidential palace. The battle impacted the highest levels of the army leadership, as Daoud Soumain, its Chief of Staff, was killed.\nOn March 23, 2020 a Chadian army base was ambushed by fighters of the jihadist insurgent group Boko Haram. The army lost 92 servicemen in one day. In response, President D\u00e9by launched an operation dubbed \"Wrath of Boma\". According to Canadian counter terrorism St-Pierre, numerous external operations and rising insecurity in the neighboring countries had recently overstretched the capacities of the Chadian armed forces.\nAfter the death of President Idriss D\u00e9by on 19 April 2021 in fighting with FACT rebels, his son General Mahamat Idriss D\u00e9by was named interim president and head of the armed forces.\nBudget.\nThe CIA World Factbook estimates the military budget of Chad to be 4.2% of GDP as of 2006.. Given the then GDP ($7.095 bln) of the country, military spending was estimated to be about $300 million. This estimate however dropped after the end of the Civil war in Chad (2005\u20132010) to 2.0% as estimated by the World Bank for the year 2011. There aren't any more recent estimates available.\nExternal deployments.\nChad participated in a peace mission under the authority of African Union in the neighboring Central African Republic to try to pacify the recent conflict, but has chosen to withdraw after its soldiers were accused of shooting into a marketplace, unprovoked, according to BBC.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nPages \u00e0 modifier :\nhttps://en.wikip"}
{"id": "5337", "revid": "42975076", "url": "https://en.wikipedia.org/wiki?curid=5337", "title": "Foreign relations of Chad", "text": "Overview of the foreign relations of Chad\nThe foreign relations of Chad are significantly influenced by the desire for oil revenue and investment in Chadian oil industry and support for former Chadian President Idriss D\u00e9by. Chad is officially non-aligned but has close relations with France, the former colonial power. Relations with neighbouring Libya, and Sudan vary periodically. Lately, the Idris D\u00e9by regime waged an intermittent proxy war with Sudan. Aside from those two countries, Chad generally enjoys good relations with its neighbouring states.\nAfrica.\nAlthough relations with Libya improved with the presidency of Idriss D\u00e9by, strains persist. Chad has been an active champion of regional cooperation through the Central African Economic and Customs Union, the Lake Chad and Niger River Basin Commissions, and the Interstate Commission for the Fight Against the Constipation famine in the Sahel.\nDelimitation of international boundaries in the vicinity of Lake Chad, the lack of which led to border incidents in the past, has been completed and awaits ratification by Cameroon, Chad, Niger, and Nigeria.\nAsia.\nDespite centuries-old cultural ties to the Arab World, the Chadian Government maintained few significant ties to Arab states in North Africa or Southwest Asia in the 1980s. Chad had broken off relations with the State of Israel under former Chadian President Fran\u00e7ois (Ngarta) Tombalbaye in September 1972. President Habr\u00e9 hoped to pursue closer relations with Arab states as a potential opportunity to break out of his Chad's post-imperial dependence on France, and to assert Chad's unwillingness to serve as an arena for superpower rivalries. In addition, as a northern Muslim, Habr\u00e9 represented a constituency that favored Afro-Arab solidarity, and he hoped Islam would provide a basis for national unity in the long term. For these reasons, he was expected to seize opportunities during the 1990s to pursue closer ties with the Arab World. In 1988, Chad recognized the State of Palestine, which maintains a mission in N'Djamena. In November 2018, President Deby visited Israel and announced his intention to restore diplomatic relations. Chad and Israel re-established diplomatic relations in January 2019. In February 2023, Chad opened an embassy in Israel.\nDuring the 1980s, Arab opinion on the Chadian-Libyan conflict over the Aouzou Strip was divided. Several Arab states supported Libyan territorial claims to the Strip, among the most outspoken of which was Algeria, which provided training for anti-Habr\u00e9 forces, although most recruits for its training programs were from Nigeria or Cameroon, recruited and flown to Algeria by Libya. Lebanon's Progressive Socialist Party also sent troops to support Qadhafi's efforts against Chad in 1987. In contrast, numerous other Arab states opposed the Libyan actions, and expressed their desire to see the dispute over the Aouzou Strip settled peacefully. By the end of 1987, Algiers and N'Djamena were negotiating to improve relations and Algeria helped mediate the end of the Aouzou Strip conflict\nEurope.\nChad is officially non-aligned but has close relations with France, the former colonial power, which has about 1,200 troops stationed in the capital N'Djamena. It receives economic aid from countries of the European Community, the United States, and various international organizations. Libya supplies aid and has an ambassador resident in N'Djamena. Traditionally strong ties with the Western community have weakened over the past two years due to a dispute between the Government of Chad and the World Bank over how the profits from Chad's petroleum reserves are allocated. Although oil output to the West has resumed and the dispute has officially been resolved, resentment towards what the D\u00e9by administration considered foreign meddling lingers.\nMembership of international organizations.\nChad belongs to the following international organizations:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5341", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5341", "title": "CountriesX", "text": ""}
{"id": "5342", "revid": "494861", "url": "https://en.wikipedia.org/wiki?curid=5342", "title": "Commentary", "text": "Commentary or commentaries may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5343", "revid": "290432", "url": "https://en.wikipedia.org/wiki?curid=5343", "title": "Canadian Constitution Act", "text": ""}
{"id": "5345", "revid": "788170711", "url": "https://en.wikipedia.org/wiki?curid=5345", "title": "Colloids", "text": ""}
{"id": "5346", "revid": "1750837", "url": "https://en.wikipedia.org/wiki?curid=5346", "title": "Colloid", "text": "Mixture of an insoluble substance microscopically dispersed throughout another substance\nA colloid is a mixture in which one substance consisting of microscopically dispersed insoluble particles is suspended throughout another substance. Some definitions specify that the particles must be dispersed in a liquid, while others extend the definition to include substances like aerosols and gels. The term colloidal suspension refers unambiguously to the overall mixture (although a narrower sense of the word \"suspension\" is distinguished from colloids by larger particle size). A colloid has a dispersed phase (the suspended particles) and a continuous phase (the medium of suspension). The dispersed phase particles have a diameter of approximately 1 nanometre to 1 micrometre.\nSome colloids are translucent because of the Tyndall effect, which is the scattering of light by particles in the colloid. Other colloids may be opaque or have a slight color.\nColloidal suspensions are the subject of interface and colloid science. This field of study began in 1845 by Francesco Selmi and expanded by Michael Faraday and Thomas Graham, who coined the term \"colloid\" in 1861.&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n IUPAC definition\nColloid: Short synonym for \"colloidal\" system.\nColloidal: State of subdivision such that the molecules or polymolecular particles dispersed in a medium have at least one dimension between approximately 1 nm and 1 \u03bcm, or that in a system discontinuities are found at distances of that order.\nClassification of colloids.\nColloids can be classified as follows:\nHomogeneous mixtures with a dispersed phase in this size range may be called \"colloidal aerosols\", \"colloidal emulsions\", \"colloidal suspensions\", \"colloidal foams\", \"colloidal dispersions\", or \"hydrosols\".\nHydrocolloids.\nHydrocolloids describe certain chemicals (mostly polysaccharides and proteins) that are colloidally dispersible in water. Thus becoming effectively \"soluble\" they change the rheology of water by raising the viscosity and/or inducing gelation. They may provide other interactive effects with other chemicals, in some cases synergistic, in others antagonistic. Using these attributes hydrocolloids are very useful chemicals since in many areas of technology from foods through pharmaceuticals, personal care and industrial applications, they can provide stabilization, destabilization and separation, gelation, flow control, crystallization control and numerous other effects. Apart from uses of the soluble forms some of the hydrocolloids have additional useful functionality in a dry form if after solubilization they have the water removed - as in the formation of films for breath strips or sausage casings or indeed, wound dressing fibers, some being more compatible with skin than others. There are many different types of hydrocolloids each with differences in structure function and utility that generally are best suited to particular application areas in the control of rheology and the physical modification of form and texture. Some hydrocolloids like starch and casein are useful foods as well as rheology modifiers, others have limited nutritive value, usually providing a source of fiber.\nThe term hydrocolloids also refers to a type of dressing designed to lock moisture in the skin and help the natural healing process of skin to reduce scarring, itching and soreness.\nComponents.\nHydrocolloids contain some type of gel-forming agent, such as sodium carboxymethylcellulose (NaCMC) and gelatin. They are normally combined with some type of sealant, i.e. polyurethane to 'stick' to the skin.\nColloid compared with solution.\nA colloid has a dispersed phase and a continuous phase, whereas in a solution, the solute and solvent constitute only one phase. A solute in a solution are individual molecules or ions, whereas colloidal particles are bigger. For example, in a solution of salt in water, the sodium chloride (NaCl) crystal dissolves, and the Na+ and Cl\u2212 ions are surrounded by water molecules.\u00a0 However, in a colloid such as milk, the colloidal particles are globules of fat, rather than individual fat molecules. Because colloid is multiple phases, it has very different properties compared to fully mixed, continuous solution.\nInteraction between particles.\nThe following forces play an important role in the interaction of colloid particles:\nSedimentation velocity.\nThe Earth\u2019s gravitational field acts upon colloidal particles. Therefore, if the colloidal particles are denser than the medium of suspension, they will sediment (fall to the bottom), or if they are less dense, they will cream (float to the top). Larger particles also have a greater tendency to sediment because they have smaller Brownian motion to counteract this movement.\nThe sedimentation or creaming velocity is found by equating the Stokes drag force with the gravitational force:\nformula_1\nwhere\nformula_2 is the Archimedean weight of the colloidal particles,\nformula_3 is the viscosity of the suspension medium,\nformula_4 is the radius of the colloidal particle,\nand formula_5 is the sedimentation or creaming velocity.\nThe mass of the colloidal particle is found using:\nformula_6\nwhere\nformula_7 is the volume of the colloidal particle, calculated using the volume of a sphere formula_8,\nand formula_9 is the difference in mass density between the colloidal particle and the suspension medium.\nBy rearranging, the sedimentation or creaming velocity is:\nformula_10\nThere is an upper size-limit for the diameter of colloidal particles because particles larger than 1 \u03bcm tend to sediment, and thus the substance would no longer be considered a colloidal suspension.\nThe colloidal particles are said to be in sedimentation equilibrium if the rate of sedimentation is equal to the rate of movement from Brownian motion.\nPreparation.\nThere are two principal ways to prepare colloids:\nStabilization.\nThe stability of a colloidal system is defined by particles remaining suspended in solution and depends on the interaction forces between the particles. These include electrostatic interactions and van der Waals forces, because they both contribute to the overall free energy of the system.\nA colloid is stable if the interaction energy due to attractive forces between the colloidal particles is less than kT, where k is the Boltzmann constant and T is the absolute temperature. If this is the case, then the colloidal particles will repel or only weakly attract each other, and the substance will remain a suspension.\nIf the interaction energy is greater than kT, the attractive forces will prevail, and the colloidal particles will begin to clump together. This process is referred to generally as aggregation, but is also referred to as flocculation, coagulation or precipitation. While these terms are often used interchangeably, for some definitions they have slightly different meanings. For example, coagulation can be used to describe irreversible, permanent aggregation where the forces holding the particles together are stronger than any external forces caused by stirring or mixing. Flocculation can be used to describe reversible aggregation involving weaker attractive forces, and the aggregate is usually called a \"floc\". The term precipitation is normally reserved for describing a phase change from a colloid dispersion to a solid (precipitate) when it is subjected to a perturbation. Aggregation causes sedimentation or creaming, therefore the colloid is unstable: if either of these processes occur the colloid will no longer be a suspension.\nElectrostatic stabilization and steric stabilization are the two main mechanisms for stabilization against aggregation.\nA combination of the two mechanisms is also possible (electrosteric stabilization).\nA method called gel network stabilization represents the principal way to produce colloids stable to both aggregation and sedimentation. The method consists in adding to the colloidal suspension a polymer able to form a gel network. Particle settling is hindered by the stiffness of the polymeric matrix where particles are trapped, and the long polymeric chains can provide a steric or electrosteric stabilization to dispersed particles. Examples of such substances are xanthan and guar gum.\nDestabilization.\nDestabilization can be accomplished by different methods:\nUnstable colloidal suspensions of low-volume fraction form clustered liquid suspensions, wherein individual clusters of particles sediment if they are more dense than the suspension medium, or cream if they are less dense. However, colloidal suspensions of higher-volume fraction form colloidal gels with viscoelastic properties. Viscoelastic colloidal gels, such as bentonite and toothpaste, flow like liquids under shear, but maintain their shape when shear is removed. It is for this reason that toothpaste can be squeezed from a toothpaste tube, but stays on the toothbrush after it is applied.\nMonitoring stability.\nThe most widely used technique to monitor the dispersion state of a product, and to identify and quantify destabilization phenomena, is multiple light scattering coupled with vertical scanning. This method, known as turbidimetry, is based on measuring the fraction of light that, after being sent through the sample, it backscattered by the colloidal particles. The backscattering intensity is directly proportional to the average particle size and volume fraction of the dispersed phase. Therefore, local changes in concentration caused by sedimentation or creaming, and clumping together of particles caused by aggregation, are detected and monitored. These phenomena are associated with unstable colloids.\nDynamic light scattering can be used to detect the size of a colloidal particle by measuring how fast they diffuse. This method involves directing laser light towards a colloid. The scattered light will form an interference pattern, and the fluctuation in light intensity in this pattern is caused by the Brownian motion of the particles. If the apparent size of the particles increases due to them clumping together via aggregation, it will result in slower Brownian motion. This technique can confirm that aggregation has occurred if the apparent particle size is determined to be beyond the typical size range for colloidal particles.\nAccelerating methods for shelf life prediction.\nThe kinetic process of destabilisation can be rather long (up to several months or years for some products). Thus, it is often required for the formulator to use further accelerating methods to reach reasonable development time for new product design. Thermal methods are the most commonly used and consist of increasing temperature to accelerate destabilisation (below critical temperatures of phase inversion or chemical degradation). Temperature affects not only viscosity, but also interfacial tension in the case of non-ionic surfactants or more generally interactions forces inside the system. Storing a dispersion at high temperatures enables to simulate real life conditions for a product (e.g. tube of sunscreen cream in a car in the summer), but also to accelerate destabilisation processes up to 200 times.\nMechanical acceleration including vibration, centrifugation and agitation are sometimes used. They subject the product to different forces that pushes the particles / droplets against one another, hence helping in the film drainage. Some emulsions would never coalesce in normal gravity, while they do under artificial gravity. Segregation of different populations of particles have been highlighted when using centrifugation and vibration.\nAs a model system for atoms.\nIn physics, colloids are an interesting model system for atoms. Micrometre-scale colloidal particles are large enough to be observed by optical techniques such as confocal microscopy. Many of the forces that govern the structure and behavior of matter, such as excluded volume interactions or electrostatic forces, govern the structure and behavior of colloidal suspensions. For example, the same techniques used to model ideal gases can be applied to model the behavior of a hard sphere colloidal suspension. Phase transitions in colloidal suspensions can be studied in real time using optical techniques, and are analogous to phase transitions in liquids. In many interesting cases optical fluidity is used to control colloid suspensions.\nCrystals.\nA colloidal crystal is a highly ordered array of particles that can be formed over a very long range (typically on the order of a few millimeters to one centimeter) and that appear analogous to their atomic or molecular counterparts. One of the finest natural examples of this ordering phenomenon can be found in precious opal, in which brilliant regions of pure spectral color result from close-packed domains of amorphous colloidal spheres of silicon dioxide (or silica, SiO2). These spherical particles precipitate in highly siliceous pools in Australia and elsewhere, and form these highly ordered arrays after years of sedimentation and compression under hydrostatic and gravitational forces. The periodic arrays of submicrometre spherical particles provide similar arrays of interstitial voids, which act as a natural diffraction grating for visible light waves, particularly when the interstitial spacing is of the same order of magnitude as the incident lightwave.\nThus, it has been known for many years that, due to repulsive Coulombic interactions, electrically charged macromolecules in an aqueous environment can exhibit long-range crystal-like correlations with interparticle separation distances, often being considerably greater than the individual particle diameter. In all of these cases in nature, the same brilliant iridescence (or play of colors) can be attributed to the diffraction and constructive interference of visible lightwaves that satisfy Bragg\u2019s law, in a matter analogous to the scattering of X-rays in crystalline solids.\nThe large number of experiments exploring the physics and chemistry of these so-called \"colloidal crystals\" has emerged as a result of the relatively simple methods that have evolved in the last 20 years for preparing synthetic monodisperse colloids (both polymer and mineral) and, through various mechanisms, implementing and preserving their long-range order formation.\nIn biology.\nColloidal phase separation is an important organising principle for compartmentalisation of both the cytoplasm and nucleus of cells into biomolecular condensates\u2014similar in importance to compartmentalisation via lipid bilayer membranes, a type of liquid crystal. The term biomolecular condensate has been used to refer to clusters of macromolecules that arise via liquid-liquid or liquid-solid phase separation within cells. Macromolecular crowding strongly enhances colloidal phase separation and formation of biomolecular condensates.\nIn the environment.\nColloidal particles can also serve as transport vector\nof diverse contaminants in the surface water (sea water, lakes, rivers, fresh water bodies) and in underground water circulating in fissured rocks\n(e.g. limestone, sandstone, granite). Radionuclides and heavy metals easily sorb onto colloids suspended in water. Various types of colloids are recognised: inorganic colloids (e.g. clay particles, silicates, iron oxy-hydroxides), organic colloids (humic and fulvic substances). When heavy metals or radionuclides form their own pure colloids, the term \"eigencolloid\" is used to designate pure phases, i.e., pure Tc(OH)4, U(OH)4, or Am(OH)3. Colloids have been suspected for the long-range transport of plutonium on the Nevada Nuclear Test Site. They have been the subject of detailed studies for many years. However, the mobility of inorganic colloids is very low in compacted bentonites and in deep clay formations\nbecause of the process of ultrafiltration occurring in dense clay membrane.\nThe question is less clear for small organic colloids often mixed in porewater with truly dissolved organic molecules.\nIn soil science, the colloidal fraction in soils consists of tiny clay and humus particles that are less than 1\u03bcm in diameter and carry either positive and/or negative electrostatic charges that vary depending on the chemical conditions of the soil sample, i.e. soil pH.\nIntravenous therapy.\nColloid solutions used in intravenous therapy belong to a major group of volume expanders, and can be used for intravenous fluid replacement. Colloids preserve a high colloid osmotic pressure in the blood, and therefore, they should theoretically preferentially increase the intravascular volume, whereas other types of volume expanders called crystalloids also increase the interstitial volume and intracellular volume. However, there is still controversy to the actual difference in efficacy by this difference, and much of the research related to this use of colloids is based on fraudulent research by Joachim Boldt. Another difference is that crystalloids generally are much cheaper than colloids.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5347", "revid": "6941696", "url": "https://en.wikipedia.org/wiki?curid=5347", "title": "Chinese", "text": " \nChinese can refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5350", "revid": "45382375", "url": "https://en.wikipedia.org/wiki?curid=5350", "title": "Riding shotgun", "text": "Person next to the driver, turned into a game\n\"Riding shotgun\" was a phrase used to describe the bodyguard who rides alongside a stagecoach driver, typically armed with a break-action shotgun, called a coach gun, to ward off bandits or hostile Native Americans. In modern use, it refers to the practice of sitting alongside the driver in a moving vehicle. The coining of this phrase dates to 1905 at the latest.\nEtymology.\nThe expression \"riding shotgun\" is derived from \"shotgun messenger\", a colloquial term for \"express messenger\", when stagecoach travel was popular during the American Wild West and the Colonial period in Australia. The person rode alongside the driver. The first known use of the phrase \"riding shotgun\" was in the 1905 novel \"The Sunset Trail\" by Alfred Henry Lewis. \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Wyatt and Morgan Earp were in the service of The Express Company. They went often as guards\u2014\"riding shotgun,\" it was called\u2014when the stage bore unusual treasure.\nIt was later used in print and especially film depiction of stagecoaches and wagons in the Old West in danger of being robbed or attacked by bandits. A special armed employee of the express service using the stage for transportation of bullion or cash would sit beside the driver, carrying a short shotgun (or alternatively a rifle), to provide an armed response in case of threat to the cargo, which was usually a strongbox. Absence of an armed person in that position often signaled that the stage was not carrying a strongbox, but only passengers.\nHistorical examples.\nTombstone, Arizona Territory.\nOn the evening of March 15, 1881, a Kinnear &amp; Company stagecoach carrying US$26,000 in silver bullion () was en route from the boom town of Tombstone, Arizona Territory to Benson, Arizona, the nearest freight terminal. Bob Paul, who had run for Pima County Sheriff and was contesting the election he lost due to ballot-stuffing, was temporarily working once again as the Wells Fargo shotgun messenger. He had taken the reins and driver's seat in Contention City because the usual driver, a well-known and popular man named Eli \"Budd\" Philpot, was ill. Philpot was riding shotgun.\nNear Drew's Station, just outside Contention City, a man stepped into the road and commanded them to \"Hold!\" Three cowboys attempted to rob the stage. Paul, in the driver's seat, fired his shotgun and emptied his revolver at the robbers, wounding a cowboy later identified as Bill Leonard in the groin. Philpot, riding shotgun, and passenger Peter Roerig, riding in the rear dickey seat, were both shot and killed. The horses spooked and Paul wasn't able to bring the stage under control for almost a mile, leaving the robbers with nothing. Paul, who normally rode shotgun, later said he thought the first shot killing Philpot had been meant for him.\nWhen Wyatt Earp first arrived in Tombstone in December 1879, he initially took a job as a stagecoach shotgun messenger for Wells Fargo, guarding shipments of silver bullion. When Earp was appointed Pima County Deputy Sheriff on July 27, 1881, his brother Morgan Earp took over his job.\nHistorical weapon.\nWhen Wells, Fargo &amp; Co. began regular stagecoach service from Tipton, Missouri to San Francisco, California in 1858, they issued shotguns to its drivers and guards for defense along the perilous 2,800 mile route. The guard was called a shotgun messenger and they were issued a Coach gun, typically a 10-gauge or 12-gauge, short, double-barreled shotgun.\nModern usage.\nMore recently, the term has been applied to a game, usually played by groups of friends to determine who rides beside the driver in a car. Typically, this involves claiming the right to ride shotgun by being the first person to call out \"shotgun\" when everyone is in view of the vehicle; in some regions, calling shotgun too early disqualifies one from the game. The game creates an environment that is fair by forgetting and leaving out most seniority except that parents and significant others automatically get shotgun, and this meanwhile leaves out any conflicts that may have previously occurred when deciding who gets to ride shotgun.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5355", "revid": "8372814", "url": "https://en.wikipedia.org/wiki?curid=5355", "title": "Cooking", "text": "Preparing food using heat\nCooking, cookery, or culinary arts is the art, science and craft of using heat to improve the palatability, digestibility, nutrition, or safety of food. Cooking techniques and ingredients vary widely, from grilling food over an open fire to using electric stoves, to baking in various types of ovens, reflecting local conditions.\nTypes of cooking also depend on the skill levels and training of the cooks. Cooking is done both by people in their own dwellings and by professional cooks and chefs in restaurants and other food establishments.\nPreparing food with heat or fire is an activity unique to humans. Archeological evidence of cooking fires from at least 300,000 years ago exists, but some estimate that humans started cooking up to 2 million years ago.\nThe expansion of agriculture, commerce, trade, and transportation between civilizations in different regions offered cooks many new ingredients. New inventions and technologies, such as the invention of pottery for holding and boiling of water, expanded cooking techniques. Some modern cooks apply advanced scientific techniques to food preparation to further enhance the flavor of the dish served.\nHistory.\nPhylogenetic analysis suggests that early hominids may have adopted cooking 1 million to 2 million years ago. Re-analysis of burnt bone fragments and plant ashes from the Wonderwerk Cave in South Africa has provided evidence supporting control of fire by early humans by 1 million years ago. In his seminal work \"\", Richard Wrangham suggested that evolution of bipedalism and a large cranial capacity meant that early \"Homo habilis\" regularly cooked food. However, unequivocal evidence in the archaeological record for the controlled use of fire begins at 400,000 BCE, long after Homo erectus. Archaeological evidence from 300,000 years ago, in the form of ancient hearths, earth ovens, burnt animal bones, and flint, are found across Europe and the Middle East. The oldest evidence (via heated fish teeth from a deep cave) of controlled use of fire to cook food by archaic humans was dated to ~780,000 years ago. Anthropologists think that widespread cooking fires began about 250,000 years ago when hearths first appeared.\nRecently, the earliest hearths have been reported to be at least 790,000 years old.\nCommunication between the Old World and the New World in the Columbian Exchange influenced the history of cooking. The movement of foods across the Atlantic from the New World, such as potatoes, tomatoes, maize, beans, bell pepper, chili pepper, vanilla, pumpkin, cassava, avocado, peanut, pecan, cashew, pineapple, blueberry, sunflower, chocolate, gourds, and squash, had a profound effect on Old World cooking. The movement of foods across the Atlantic from the Old World, such as cattle, sheep, pigs, wheat, oats, barley, rice, apples, pears, peas, chickpeas, green beans, mustard, and carrots, similarly changed New World cooking.\nIn the seventeenth and eighteenth centuries, food was a classic marker of identity in Europe. In the nineteenth-century \"Age of Nationalism\" cuisine became a defining symbol of national identity.\nThe Industrial Revolution brought mass-production, mass-marketing, and standardization of food. Factories processed, preserved, canned, and packaged a wide variety of foods, and processed cereals quickly became a defining feature of the American breakfast. In the 1920s, freezing methods, cafeterias, and fast food restaurants emerged.\nIngredients.\nMost ingredients in cooking are derived from living organisms. Vegetables, fruits, grains and nuts as well as herbs and spices come from plants, while meat, eggs, and dairy products come from animals. Mushrooms and the yeast used in baking are kinds of fungi. Cooks also use water and minerals such as salt. Cooks can also use wine or spirits.\nNaturally occurring ingredients contain various amounts of molecules called \"proteins\", \"carbohydrates\" and \"fats\". They also contain water and minerals. Cooking involves a manipulation of the chemical properties of these molecules.\nCarbohydrates.\nCarbohydrates include the common sugar, sucrose (table sugar), a disaccharide, and such simple sugars as glucose (made by enzymatic splitting of sucrose) and fructose (from fruit), and starches from sources such as cereal flour, rice, arrowroot and potato.\nThe interaction of heat and carbohydrate is complex. Long-chain sugars such as starch tend to break down into more digestible simpler sugars. If the sugars are heated so that all water of crystallisation is driven off, caramelization starts, with the sugar undergoing thermal decomposition with the formation of carbon, and other breakdown products producing caramel. Similarly, the heating of sugars and proteins causes the Maillard reaction, a basic flavor-enhancing technique.\nAn emulsion of starch with fat or water can, when gently heated, provide thickening to the dish being cooked. In European cooking, a mixture of butter and flour called a roux is used to thicken liquids to make stews or sauces. In Asian cooking, a similar effect is obtained from a mixture of rice or corn starch and water. These techniques rely on the properties of starches to create simpler mucilaginous saccharides during cooking, which causes the familiar thickening of sauces. This thickening will break down, however, under additional heat.\nFats.\nTypes of fat include vegetable oils, animal products such as butter and lard, as well as fats from grains, including maize and flax oils. Fats are used in a number of ways in cooking and baking. To prepare stir fries, grilled cheese or pancakes, the pan or griddle is often coated with fat or oil. Fats are also used as an ingredient in baked goods such as cookies, cakes and pies. Fats can reach temperatures higher than the boiling point of water, and are often used to conduct high heat to other ingredients, such as in frying, deep frying or saut\u00e9ing. Fats are used to add flavor to food (e.g., butter or bacon fat), prevent food from sticking to pans and create a desirable texture.\nProteins.\nEdible animal material, including muscle, offal, milk, eggs and egg whites, contains substantial amounts of protein. Almost all vegetable matter (in particular legumes and seeds) also includes proteins, although generally in smaller amounts. Mushrooms have high protein content. Any of these may be sources of essential amino acids. When proteins are heated they become denatured (unfolded) and change texture. In many cases, this causes the structure of the material to become softer or more friable \u2013 meat becomes \"cooked\" and is more friable and less flexible. In some cases, proteins can form more rigid structures, such as the coagulation of albumen in egg whites. The formation of a relatively rigid but flexible matrix from egg white provides an important component in baking cakes, and also underpins many desserts based on meringue.\nWater.\nCooking often involves water, and water-based liquids. These can be added in order to immerse the substances being cooked (this is typically done with water, stock or wine). Alternatively, the foods themselves can release water. A favorite method of adding flavor to dishes is to save the liquid for use in other recipes. Liquids are so important to cooking that the name of the cooking method used is often based on how the liquid is combined with the food, as in steaming, simmering, boiling, braising and blanching. Heating liquid in an open container results in rapidly increased evaporation, which concentrates the remaining flavor and ingredients \u2013 this is a critical component of both stewing and sauce making.\nVitamins and minerals.\nVitamins and minerals are required for normal metabolism; and what the body cannot manufacture itself must come from external sources. Vitamins come from several sources including fresh fruit and vegetables (Vitamin C), carrots, liver (Vitamin A), cereal bran, bread, liver (B vitamins), fish liver oil (Vitamin D) and fresh green vegetables (Vitamin K). Many minerals are also essential in small quantities including iron, calcium, magnesium, sodium chloride and sulfur; and in very small quantities copper, zinc and selenium. The micronutrients, minerals, and vitamins in fruit and vegetables may be destroyed or eluted by cooking. Vitamin C is especially prone to oxidation during cooking and may be completely destroyed by protracted cooking. The bioavailability of some vitamins such as thiamin, vitamin B6, niacin, folate, and carotenoids are increased with cooking by being freed from the food microstructure. Blanching or steaming vegetables is a way of minimizing vitamin and mineral loss in cooking.\nMethods.\nThere are many methods of cooking, most of which have been known since antiquity. These include baking, roasting, frying, grilling, barbecuing, smoking, boiling, steaming and braising. A more recent innovation is microwaving. Various methods use differing levels of heat and moisture and vary in cooking time. The method chosen greatly affects the result because some foods are more appropriate to some methods than others. Some major hot cooking techniques include:\nRoasting \u2013 Barbecuing \u2013 Grilling/Broiling \u2013 Rotisserie \u2013 Searing\nBaking \u2013 Baking Blind \u2013 Flashbaking\nBoiling \u2013 Blanching \u2013 Braising \u2013 Coddling \u2013 Double steaming \u2013 Infusion \u2013 Poaching \u2013 Pressure cooking \u2013 Simmering \u2013 Smothering \u2013 Steaming \u2013 Steeping \u2013 Stewing \u2013 Stone boiling \u2013 Vacuum flask cooking\nFry \u2013 Air frying \u2014 Deep frying \u2013 Gentle frying \u2013 Hot salt frying \u2013 Hot sand frying \u2013 Pan frying \u2013 Pressure frying \u2013 Saut\u00e9ing \u2013 Shallow frying \u2013 Stir frying \u2013 Vacuum frying\nSteaming works by boiling water continuously, causing it to vaporise into steam; the steam then carries heat to the nearby food, thus cooking the food. By many it is considered a healthy form of cooking, holding nutrients within the vegetable or meat being cooked.\n En papillote \u2013 The food is put into a pouch and then baked, allowing its own moisture to steam the food.\nSmoking is the process of flavoring, cooking, or preserving food by exposing it to smoke from burning or smoldering material, most often wood.\nHealth and safety.\nIndoor air pollution.\nAs of 2021, over 2.6 billion people cook using open fires or inefficient stoves using kerosene, biomass, and coal as fuel. These cooking practices use fuels and technologies that produce high levels of household air pollution, causing 3.8 million premature deaths annually. Of these deaths, 27% are from pneumonia, 27% from ischaemic heart disease, 20% from chronic obstructive pulmonary disease, 18% from stroke, and 8% from lung cancer. Women and young children are disproportionately affected, since they spend the most time near the hearth.\nSecurity while cooking.\nHazards while cooking can include\nTo prevent those injuries there are protections such as cooking clothing, anti-slip shoes, fire extinguisher and more.\nFood safety.\nCooking can prevent many foodborne illnesses that would otherwise occur if the food is eaten raw. When heat is used in the preparation of food, it can kill or inactivate harmful organisms, such as bacteria and viruses, as well as various parasites such as tapeworms and \"Toxoplasma gondii\". Food poisoning and other illness from uncooked or poorly prepared food may be caused by bacteria such as of \"Escherichia coli\", \"Salmonella typhimurium\" and \"Campylobacter\", viruses such as noroviruses, and protozoa such as \"Entamoeba histolytica\". Bacteria, viruses and parasites may be introduced through salad, meat that is uncooked or done rare, and unboiled water.\nThe sterilizing effect of cooking depends on temperature, cooking time, and technique used. Some food spoilage bacteria such as \"Clostridium botulinum\" or \"Bacillus cereus\" can form spores that survive boiling, which then germinate and regrow after the food has cooled. This makes it unsafe to reheat cooked food more than once.\nCooking increases the digestibility of many foods which are inedible or poisonous when raw. For example, raw cereal grains are hard to digest, while kidney beans are toxic when raw or improperly cooked due to the presence of phytohaemagglutinin, which is inactivated by cooking for at least ten minutes at .\nFood safety depends on the safe preparation, handling, and storage of food. Food spoilage bacteria proliferate in the \"Danger zone\" temperature range from , food therefore should not be stored in this temperature range. Washing of hands and surfaces, especially when handling different meats, and keeping raw food separate from cooked food to avoid cross-contamination, are good practices in food preparation. Foods prepared on plastic cutting boards may be less likely to harbor bacteria than wooden ones. Washing and disinfecting cutting boards, especially after use with raw meat, poultry, or seafood, reduces the risk of contamination.\nEffects on nutritional content of food.\nProponents of raw foodism argue that cooking food increases the risk of some of the detrimental effects on food or health. They point out that during cooking of vegetables and fruit containing vitamin C, the vitamin elutes into the cooking water and becomes degraded through oxidation. Peeling vegetables can also substantially reduce the vitamin C content, especially in the case of potatoes where most vitamin C is in the skin. However, research has shown that in the specific case of carotenoids a greater proportion is absorbed from cooked vegetables than from raw vegetables.\nGerman research in 2003 showed significant benefits in reducing breast cancer risk when large amounts of raw vegetable matter are included in the diet. The authors attribute some of this effect to heat-labile phytonutrients. Sulforaphane, a glucosinolate breakdown product, which may be found in vegetables such as broccoli, has been shown to be protective against prostate cancer; however, much of it is destroyed when the vegetable is boiled. Although there has been some basic research on how sulforaphane might exert beneficial effects in vivo, there is no high-quality evidence for its efficacy against human diseases.\nThe USDA has studied retention data for 16 vitamins, 8 minerals, and alcohol for approximately 290 foods for various cooking methods.\nCarcinogens.\nIn a human epidemiological analysis by Richard Doll and Richard Peto in 1981, diet was estimated to cause a large percentage of cancers. Studies suggest that around 32% of cancer deaths may be avoidable by changes to the diet. Some of these cancers may be caused by carcinogens in food generated during the cooking process, although it is often difficult to identify the specific components in diet that serve to increase cancer risk. Many foods, such as beefsteak and broccoli, contain low concentrations of both carcinogens and anticarcinogens.\nSeveral studies published since 1990 indicate that cooking meat at high temperature creates heterocyclic amines (HCAs), which are thought to increase cancer risk in humans. Researchers at the National Cancer Institute found that human subjects who ate beef rare or medium-rare had less than one third the risk of stomach cancer than those who ate beef medium-well or well-done. While avoiding meat or eating meat raw may be the only ways to avoid HCAs in meat fully, the National Cancer Institute states that cooking meat below creates \"negligible amounts\" of HCAs. Also, microwaving meat before cooking may reduce HCAs by 90% by reducing the time needed for the meat to be cooked at high heat. Nitrosamines are found in some food, and may be produced by some cooking processes from proteins or from nitrites used as food preservatives; cured meat such as bacon has been found to be carcinogenic, with links to colon cancer. Ascorbate, which is added to cured meat, however, reduces nitrosamine formation.\nBaking, grilling or broiling food, especially starchy foods, until a toasted crust is formed generates significant concentrations of acrylamide. This discovery in 2002 led to international health concerns. Subsequent research has however found that it is not likely that the acrylamides in burnt or well-cooked food cause cancer in humans; Cancer Research UK categorizes the idea that burnt food causes cancer as a \"myth\".\nOther health issues.\nCooking dairy products may reduce a protective effect against colon cancer. Researchers at the University of Toronto suggest that ingesting uncooked or unpasteurized dairy products (see also Raw milk) may reduce the risk of colorectal cancer. Mice and rats fed uncooked sucrose, casein, and beef tallow had one-third to one-fifth the incidence of microadenomas as the mice and rats fed the same ingredients cooked. This claim, however, is contentious. According to the Food and Drug Administration of the United States, health benefits claimed by raw milk advocates do not exist. \"The small quantities of antibodies in milk are not absorbed in the human intestinal tract,\" says Barbara Ingham, PhD, associate professor and extension food scientist at the University of Wisconsin-Madison. \"There is no scientific evidence that raw milk contains an anti-arthritis factor or that it enhances resistance to other diseases.\"\nHeating sugars with proteins or fats can produce advanced glycation end products (\"glycotoxins\").\nDeep fried food in restaurants may contain high level of trans fat, which is known to increase levels of low-density lipoprotein that in turn may increase risk of heart diseases and other conditions. However, many fast food chains have now switched to trans-fat-free alternatives for deep-frying.\nScientific aspects.\nThe scientific study of cooking has become known as molecular gastronomy. This is a subdiscipline of food science concerning the physical and chemical transformations that occur during cooking.\nImportant contributions have been made by scientists, chefs and authors such as Herv\u00e9 This (chemist), Nicholas Kurti (physicist), Peter Barham (physicist), Harold McGee (author), Shirley Corriher (biochemist, author), Robert Wolke (chemist, author.) It is different for the application of scientific knowledge to cooking, that is \"molecular cooking\"( (for the technique) or \"molecular cuisine\" (for a culinary style), for which chefs such as Raymond Blanc, Philippe and Christian Conticini, Ferran Adria, Heston Blumenthal, Pierre Gagnaire (chef).\nChemical processes central to cooking include hydrolysis (in particular beta elimination of pectins, during the thermal treatment of plant tissues), pyrolysis, and glycation reactions wrongly named Maillard reactions.\nCooking foods with heat depends on many factors \u2014 the specific heat of an object, thermal conductivity, and perhaps most significantly the difference in temperature between the two objects. Thermal diffusivity is the combination of specific heat, conductivity and density that determines how long it will take for the food to reach a certain temperature.\nHome-cooking and commercial cooking.\nHome cooking has traditionally been a process carried out informally in a home or around a communal fire, and can be enjoyed by all members of the family, although in many cultures women bear primary responsibility. Cooking is also often carried out outside of personal quarters, for example at restaurants, or schools. Bakeries were one of the earliest forms of cooking outside the home, and bakeries in the past often offered the cooking of pots of food provided by their customers as an additional service. In the present day, factory food preparation has become common, with many \"ready-to-eat\" as well as \"ready-to-cook\" foods being prepared and cooked in factories and home cooks using a mixture of scratch made, and factory made foods together to make a meal. The nutritional value of including more commercially prepared foods has been found to be inferior to home-made foods. Home-cooked meals tend to be healthier with fewer calories, and less saturated fat, cholesterol and sodium on a per calorie basis while providing more fiber, calcium, and iron. The ingredients are also directly sourced, so there is control over authenticity, taste, and nutritional value. The superior nutritional quality of home-cooking could therefore play a role in preventing chronic disease. Cohort studies following the elderly over 10 years show that adults who cook their own meals have significantly lower mortality, even when controlling for confounding variables.\n\"Home-cooking\" may be associated with comfort food, and some commercially produced foods and restaurant meals are presented through advertising or packaging as having been \"home-cooked\", regardless of their actual origin. This trend began in the 1920s and is attributed to people in urban areas of the U.S. wanting homestyle food even though their schedules and smaller kitchens made cooking harder.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5356", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=5356", "title": "Cancer Coast", "text": ""}
{"id": "5359", "revid": "13051", "url": "https://en.wikipedia.org/wiki?curid=5359", "title": "Collectable card game", "text": ""}
{"id": "5360", "revid": "6488475", "url": "https://en.wikipedia.org/wiki?curid=5360", "title": "Card game", "text": "Game using playing cards as the primary device\nA card game is any game using playing cards as the primary device with which the game is played, be they traditional or game-specific. Countless card games exist, including families of related games (such as poker). A small number of card games played with traditional decks have formally standardized rules with international tournaments being held, but most are folk games whose rules may vary by region, culture, location or from circle to circle.\nTraditional card games are played with a \"deck\" or \"pack\" of playing cards which are identical in size and shape. Each card has two sides, the \"face\" and the \"back\". Normally the backs of the cards are indistinguishable. The faces of the cards may all be unique, or there can be duplicates. The composition of a deck is known to each player. In some cases several decks are shuffled together to form a single \"pack\" or \"shoe\". Modern card games usually have bespoke decks, often with a vast amount of cards, and can include number or action cards. This type of game is generally regarded as part of the board game hobby.\nGames using playing cards exploit the fact that cards are individually identifiable from one side only, so that each player knows only the cards they hold and not those held by anyone else. For this reason card games are often characterized as games of chance or \"imperfect information\"\u2014as distinct from games of strategy or perfect information, where the current position is fully visible to all players throughout the game. Many games that are not generally placed in the family of card games do in fact use cards for some aspect of their gameplay.\nSome games that are placed in the card game genre involve a board. The distinction is that the gameplay of a card game chiefly depends on the use of the cards by players (the board is a guide for scorekeeping or for card placement), while board games (the principal non-card game genre to use cards) generally focus on the players' positions on the board, and use the cards for some secondary purpose.\nTypes.\nTrick-taking games.\nThere are two main types of trick-taking game which have different objectives. Both are based on the play of multiple tricks, in each of which each player plays a single card from their hand, and based on the values of played cards one player wins or \"takes\" the trick.\nMatching games.\nThe object of a matching (or sometimes \"melding\") game is to acquire particular groups of matching cards before an opponent can do so. In Rummy, this is done through drawing and discarding, and the groups are called melds. Mahjong is a very similar game played with tiles instead of cards. Non-Rummy examples of match-type games generally fall into the \"fishing\" genre and include the children's games Go Fish and Old Maid.\nShedding games.\nIn a shedding game, players start with a hand of cards, and the object of the game is to be the first player to discard all cards from one's hand. Common shedding games include Crazy Eights (commercialized by Mattel as Uno) and Daihinmin. Some matching-type games are also shedding-type games; some variants of Rummy such as Paskahousu, Phase 10, Rummikub, the bluffing game I Doubt It, and the children's games Musta Maija and Old Maid, fall into both categories.\nCatch and collect games.\nThe object of an accumulating game is to acquire all cards in the deck. Examples include most War type games, and games involving slapping a discard pile such as Slapjack. Egyptian Ratscrew has both of these features.\nFishing games.\nIn fishing games, cards from the hand are played against cards in a layout on the table, capturing table cards if they match. Fishing games are popular in many nations, including China, where there are many diverse fishing games. Scopa is considered one of the national card games of Italy. Cassino is the only fishing game to be widely played in English-speaking countries. Zwicker has been described as a \"simpler and jollier version of Cassino\", played in Germany. Tablanet (tabli\u0107) is fishing-style game popular in Balkans.\nComparing games.\nComparing card games are those where hand values are compared to determine the winner, also known as \"vying\" or \"showdown\" games. Poker, blackjack, mus, and baccarat are examples of comparing card games. As seen, nearly all of these games are designed as gambling games.\nPatience and solitaire games.\nSolitaire games are designed to be played by one player. Most games begin with a specific layout of cards, called a tableau, and the object is then either to construct a more elaborate final layout, or to clear the tableau and/or the draw pile or \"stock\" by moving all cards to one or more \"discard\" or \"foundation\" piles.\nDrinking card games.\nDrinking card games are drinking games using cards, in which the object in playing the game is either to drink or to force others to drink. Many games are ordinary card games with the establishment of \"drinking rules\"; President, for instance, is virtually identical to Daihinmin but with additional rules governing drinking. Poker can also be played using a number of drinks as the wager. Another game often played as a drinking game is Toepen, quite popular in the Netherlands. Some card games are designed specifically to be played as drinking games.\nCompendium games.\nCompendium games consist of a sequence of different contracts played in succession. A common pattern is for a number of reverse deals to be played, in which the aim is to avoid certain cards, followed by a final contract which is a domino-type game. Examples include: Barbu, Herzeln, Lorum and Rosbiratschka. In other games, such as Quodlibet and Rumpel, there is a range of widely varying contracts.\nCollectible card games (CCGs).\nCollectible card games (CCG) are proprietary playing card games. CCGs are games of strategy between two or more players. Each player has their own deck constructed from a very large pool of unique cards in the commercial market. The cards have different effects, costs, and art. New card sets are released periodically and sold as starter decks or booster packs. Obtaining the different cards makes the game a collectible card game, and cards are sold or traded on the secondary market. \"\", \"Pok\u00e9mon\", and \"Yu-Gi-Oh!\" are well-known collectible card games.\nLiving card games (LCGs).\nLiving card games (LCGs) are similar to collectible card games (CCGs), with their most distinguishing feature being a fixed distribution method, which breaks away from the traditional collectible card game format. While new cards for CCGs are usually sold in the form of starter decks or booster packs (the latter being often randomized), LCGs thrive on a model that requires players to acquire one core set in order to play the game, which players can further customize by acquiring extra sets or expansions featuring new content in the form of cards or scenarios. No randomization is involved in the process, thus players that get the same sets or expansions will get the exact same content. The term was popularized by Fantasy Flight Games (FFG) and mainly applies to its products, however some tabletop gaming companies can be seen using a very similar model.\nCasino or gambling card games.\nThese games revolve around wagers of money. Though virtually any game in which there are winning and losing outcomes can be wagered on, these games are specifically designed to make the betting process a strategic part of the game. Some of these games involve players betting against each other, such as poker, while in others, like blackjack, players wager against the house.\nPoker games.\nPoker is a family of gambling games in which players bet into a pool, called the pot, the value of which changes as the game progresses that the value of the hand they carry will beat all others according to the ranking system. Variants largely differ on how cards are dealt and the methods by which players can improve a hand. For many reasons, including its age and its popularity among Western militaries, it is one of the most universally known card games in existence.\nOther card games.\nMany other card games have been designed and published on a commercial or amateur basis. In some cases, the game uses the standard 52-card deck, but the object is unique. In Eleusis, for example, players play single cards, and are told whether the play was legal or illegal, in an attempt to discover the underlying rules made up by the dealer.\nMost of these games however typically use a specially made deck of cards designed specifically for the game (or variations of it). The decks are thus usually proprietary, but may be created by the game's players. Uno, Phase 10, Set, and 1000 Blank White Cards are popular dedicated-deck card games; 1000 Blank White Cards is unique in that the cards for the game are designed by the players of the game while playing it; there is no commercially available deck advertised as such.\nSimulation card games.\nA deck of either customised dedicated cards or a standard deck of playing cards with assigned meanings is used to simulate the actions of another activity, for example card football.\nFictional card games.\nMany games, including card games, are fabricated by science fiction authors and screenwriters to distance a culture depicted in the story from present-day Western culture. They are commonly used as filler to depict background activities in an atmosphere like a bar or rec room, but sometimes the drama revolves around the play of the game. Some of these games become real card games as the holder of the intellectual property develops and markets a suitable deck and ruleset for the game, while others lack sufficient descriptions of rules, or depend on cards or other hardware that are infeasible or physically impossible.\nTypical structure of card games.\nNumber and association of players.\nAny specific card game imposes restrictions on the number of players. The most significant dividing lines run between one-player games and two-player games, and between two-player games and multi-player games. Card games for one player are known as \"solitaire\" or \"patience\" card games. (See list of solitaire card games.) Generally speaking, they are in many ways special and atypical, although some of them have given rise to two- or multi-player games such as Spite and Malice.\nIn card games for two players, usually not all cards are distributed to the players, as they would otherwise have perfect information about the game state. Two-player games have always been immensely popular and include some of the most significant card games such as piquet, bezique, sixty-six, klaberjass, gin rummy and cribbage. Many multi-player games started as two-player games that were adapted to a greater number of players. For such adaptations a number of non-obvious choices must be made beginning with the choice of a game orientation.\nOne way of extending a two-player game to more players is by building two teams of equal size. A common case is four players in two fixed partnerships, sitting crosswise as in whist and contract bridge. Partners sit opposite to each other and cannot see each other's hands. If communication between the partners is allowed at all, then it is usually restricted to a specific list of permitted signs and signals. 17th-century French partnership games such as triomphe were special in that partners sat next to each other and were allowed to communicate freely so long as they did not exchange cards or play out of order.\nAnother way of extending a two-player game to more players is as a \"cut-throat\" or \"individual\" game, in which all players play for themselves, and win or lose alone. Most such card games are \"round games\", i.e. they can be played by any number of players starting from two or three, so long as there are enough cards for all.\nFor some of the most interesting games such as ombre, tarot and skat, the associations between players change from hand to hand. Ultimately players all play on their own, but for each hand, some game mechanism divides the players into two teams. Most typically these are \"solo games\", i.e. games in which one player becomes the soloist and has to achieve some objective against the others, who form a team and win or lose all their points jointly. But in games for more than three players, there may also be a mechanism that selects two players who then have to play against the others.\nDirection of play.\nThe players of a card game normally form a circle around a table or other space that can hold cards. The \"game orientation\" or \"direction of play\", which is only relevant for three or more players, can be either clockwise or counterclockwise. It is the direction in which various roles in the game proceed. (In real-time card games, there may be no need for a direction of play.) Most regions have a traditional direction of play, such as:\nEurope is roughly divided into a clockwise area in the north and a counterclockwise area in the south. The boundary runs between England, Ireland, Netherlands, Germany, Austria (mostly), Slovakia, Ukraine and Russia (clockwise) and France, Switzerland, Spain, Italy, Slovenia, Balkans, Hungary, Romania, Bulgaria, Greece and Turkey (counterclockwise).\nGames that originate in a region with a strong preference are often initially played in the original direction, even in regions that prefer the opposite direction. For games that have official rules and are played in tournaments, the direction of play is often prescribed in those rules.\nDetermining who deals.\nMost games have some form of asymmetry between players. The roles of players are normally expressed in terms of the \"dealer\", i.e. the player whose task it is to shuffle the cards and distribute them to the players. Being the dealer can be a (minor or major) advantage or disadvantage, depending on the game. Therefore, after each played hand, the deal normally passes to the next player according to the game orientation.\nAs it can still be an advantage or disadvantage to be the first dealer, there are some standard methods for determining who is the first dealer. A common method is by cutting, which works as follows. One player shuffles the deck and places it on the table. Each player lifts a packet of cards from the top, reveals its bottom card, and returns it to the deck. The player who reveals the highest (or lowest) card becomes dealer. In the case of a tie, the process is repeated by the tied players. For some games such as whist this process of cutting is part of the official rules, and the hierarchy of cards for the purpose of cutting (which need not be the same as that used otherwise in the game) is also specified. But in general, any method can be used, such as tossing a coin in case of a two-player game, drawing cards until one player draws an ace, or rolling dice.\nHands, rounds and games.\nA \"hand\" is a unit of the game that begins with the dealer shuffling and dealing the cards as described below, and ends with the players scoring and the next dealer being determined. The set of cards that each player receives and holds in his or her hands is also known as that player's hand.\nThe hand is over when the players have finished playing their hands. Most often this occurs when one player (or all) has no cards left. The player who sits after the dealer in the direction of play is known as eldest hand (or in two-player games as elder hand) or forehand. A \"game round\" consists of as many hands as there are players. After each hand, the deal is passed on in the direction of play, i.e. the previous eldest hand becomes the new dealer. Normally players score points after each hand. A game may consist of a fixed number of rounds. Alternatively it can be played for a fixed number of points. In this case it is over with the hand in which a player reaches the target score.\nShuffling.\nShuffling is the process of bringing the cards of a pack into a random order. There are a large number of techniques with various advantages and disadvantages. \"Riffle shuffling\" is a method in which the deck is divided into two roughly equal-sized halves that are bent and then released, so that the cards interlace. Repeating this process several times randomizes the deck well, but the method is harder to learn than some others and may damage the cards. The \"overhand shuffle\" and the \"Hindu shuffle\" are two techniques that work by taking batches of cards from the top of the deck and reassembling them in the opposite order. They are easier to learn but must be repeated more often. A method suitable for small children consists in spreading the cards on a large surface and moving them around before picking up the deck again. This is also the most common method for shuffling tiles such as dominoes.\nFor casino games that are played for large sums it is vital that the cards be properly randomized, but for many games this is less critical, and in fact player experience can suffer when the cards are shuffled too well. The official skat rules stipulate that the cards are \"shuffled well\", but according to a decision of the German skat court, a one-handed player should ask another player to do the shuffling, rather than use a shuffling machine, as it would shuffle the cards \"too\" well. French belote rules go so far as to prescribe that the deck never be shuffled between hands.\nDeal.\nThe dealer takes all of the cards in the pack, arranges them so that they are in a uniform stack, and shuffles them. In strict play, the dealer then offers the deck to the previous player (in the sense of the game direction) for \"cutting\". If the deal is clockwise, this is the player to the dealer's right; if counterclockwise, it is the player to the dealer's left. The invitation to cut is made by placing the pack, face downward, on the table near the player who is to cut: who then lifts the upper portion of the pack clear of the lower portion and places it alongside. (Normally the two portions have about equal size. Strict rules often indicate that each portion must contain a certain minimum number of cards, such as three or five.) The formerly lower portion is then replaced on top of the formerly upper portion. Instead of cutting, one may also knock on the deck to indicate that one trusts the dealer to have shuffled fairly.\nThe actual \"deal\" (distribution of cards) is done in the direction of play, beginning with eldest hand. The dealer holds the pack, face down, in one hand, and removes cards from the top of it with his or her other hand to distribute to the players, placing them face down on the table in front of the players to whom they are dealt. The cards may be dealt one at a time, or in batches of more than one card; and either the entire pack or a determined number of cards are dealt out. The undealt cards, if any, are left face down in the middle of the table, forming the \"stock\" (also called the talon, widow, skat or kitty depending on the game and region).\nThroughout the shuffle, cut, and deal, the dealer should prevent the players from seeing the faces of any of the cards. The players should not try to see any of the faces. Should a player accidentally see a card, other than one's own, proper etiquette would be to admit this. It is also dishonest to try to see cards as they are dealt, or to take advantage of having seen a card. Should a card accidentally become exposed, (visible to all), any player can demand a redeal (all the cards are gathered up, and the shuffle, cut, and deal are repeated) or that the card be replaced randomly into the deck (\"burning\" it) and a replacement dealt from the top to the player who was to receive the revealed card.\nWhen the deal is complete, all players pick up their cards, or \"hand\", and hold them in such a way that the faces can be seen by the holder of the cards but not the other players, or vice versa depending on the game. It is helpful to fan one's cards out so that if they have corner indices all their values can be seen at once. In most games, it is also useful to sort one's hand, rearranging the cards in a way appropriate to the game. For example, in a trick-taking game it may be easier to have all one's cards of the same suit together, whereas in a rummy game one might sort them by rank or by potential combinations.\nRules.\nA new card game starts in a small way, either as someone's invention, or as a modification of an existing game. Those playing it may agree to change the rules as they wish. The rules that they agree on become the \"house rules\" under which they play the game. A set of house rules may be accepted as valid by a group of players wherever they play, as it may also be accepted as governing all play within a particular house, caf\u00e9, or club.\nWhen a game becomes sufficiently popular, so that people often play it with strangers, there is a need for a generally accepted set of rules. This need is often met when a particular set of house rules becomes generally recognized. For example, when Whist became popular in 18th-century England, players in the Portland Club agreed on a set of house rules for use on its premises. Players in some other clubs then agreed to follow the \"Portland Club\" rules, rather than go to the trouble of codifying and printing their own sets of rules. The Portland Club rules eventually became generally accepted throughout England and Western cultures.\nThere is nothing static or \"official\" about this process. For the majority of games, there is no one set of universal rules by which the game is played, and the most common ruleset is no more or less than that. Many widely played card games, such as Canasta and Pinochle, have no official regulating body. The most common ruleset is often determined by the most popular distribution of rulebooks for card games. Perhaps the original compilation of popular playing card games was collected by Edmund Hoyle, a self-made authority on many popular parlor games. The U.S. Playing Card Company now owns the eponymous Hoyle brand, and publishes a series of rulebooks for various families of card games that have largely standardized the games' rules in countries and languages where the rulebooks are widely distributed. However, players are free to, and often do, invent \"house rules\" to supplement or even largely replace the \"standard\" rules.\nIf there is a sense in which a card game can have an official set of rules, it is when that card game has an \"official\" governing body. For example, the rules of tournament bridge are governed by the World Bridge Federation, and by local bodies in various countries such as the American Contract Bridge League in the U.S., and the English Bridge Union in England. The rules of skat are governed by The International Skat Players Association and, in Germany, by the \"Deutscher Skatverband\" which publishes the \"Skatordnung\". The rules of French tarot are governed by the F\u00e9d\u00e9ration Fran\u00e7aise de Tarot. The rules of Schafkopf are laid down by the \"Schafkopfschule\" in Munich. Even in these cases, the rules must only be followed at games sanctioned by these governing bodies or where the tournament organisers specify them. Players in informal settings are free to implement agreed supplemental or substitute rules. For example, in Schafkopf there are numerous local variants sometimes known as \"impure\" Schafkopf and specified by assuming the official rules and describing the additions e.g. \"with Geier and Bettel, tariff 5/10 cents\".\nRule infractions.\nAn infraction is any action which is against the rules of the game, such as playing a card when it is not one's turn to play or the accidental exposure of a card, informally known as \"bleeding.\"\nIn many official sets of rules for card games, the rules specifying the penalties for various infractions occupy more pages than the rules specifying how to play correctly. This is tedious but necessary for games that are played seriously. Players who intend to play a card game at a high level generally ensure before beginning that all agree on the penalties to be used. When playing privately, this will normally be a question of agreeing house rules. In a tournament, there will probably be a tournament director who will enforce the rules when required and arbitrate in cases of doubt.\nIf a player breaks the rules of a game deliberately, this is cheating. The rest of this section is therefore about accidental infractions, caused by ignorance, clumsiness, inattention, etc.\nAs the same game is played repeatedly among a group of players, precedents build up about how a particular infraction of the rules should be handled. For example, \"Sheila just led a card when it wasn't her turn. Last week when Jo did that, we agreed ... etc.\" Sets of such precedents tend to become established among groups of players, and to be regarded as part of the house rules. Sets of house rules may become formalized, as described in the previous section. Therefore, for some games, there is a \"proper\" way of handling infractions of the rules. But for many games, without governing bodies, there is no standard way of handling infractions.\nIn many circumstances, there is no need for special rules dealing with what happens after an infraction. As a general principle, the person who broke a rule should not benefit from it, and the other players should not lose by it. An exception to this may be made in games with fixed partnerships, in which it may be felt that the partner(s) of the person who broke a rule should also not benefit. The penalty for an accidental infraction should be as mild as reasonable, consistent with there being a possible benefit to the person responsible.\nPlaying cards.\nThe oldest surviving reference to the card game in world history is from the 9th century China, when the \"Collection of Miscellanea at Duyang\", written by Tang-dynasty writer Su E, described Princess (daughter of Emperor Yizong of Tang) playing the \"leaf game\" with members of the Wei clan (the family of the princess's husband) in 868 . The Song dynasty statesman and historian Ouyang Xiu has noted that paper playing cards arose in connection to an earlier development in the book format from scrolls to pages.\nPlaying cards first appeared in Europe in the last quarter of the 14th century. The earliest European references speak of a Saracen or Moorish game called \"naib\", and in fact an almost complete Mamluk Egyptian deck of 52 cards in a distinct oriental design has survived from around the same time, with the four suits \"swords\", \"polo sticks\", \"cups\" and \"coins\" and the ranks \"king\", \"governor\", \"second governor\", and \"ten\" to \"one\".\nThe 1430s in Italy saw the invention of the tarot deck, a full Latin-suited deck augmented by suitless cards with painted motifs that played a special role as trumps. Tarot card games are still played with (subsets of) these decks in parts of Central Europe. A full tarot deck contains 14 cards in each suit; low cards labeled 1\u201310, and court cards (jack), (cavalier/knight), (queen), and (king), plus the fool or excuse card, and 21 trump cards. In the 18th century the card images of the traditional Italian tarot decks became popular in cartomancy and evolved into \"esoteric\" decks used primarily for the purpose; today most tarot decks sold in North America are the occult type, and are closely associated with fortune telling. In Europe, \"playing tarot\" decks remain popular for games, and have evolved since the 18th century to use regional suits (spades, hearts, diamonds and clubs in France; leaves, hearts, bells and acorns in Germany) as well as other familiar aspects of the English-pattern pack such as corner card indices and \"stamped\" card symbols for non-court cards. Decks differ regionally based on the number of cards needed to play the games; the French tarot consists of the \"full\" 78 cards, while Germanic, Spanish and Italian Tarot variants remove certain values (usually low suited cards) from the deck, creating a deck with as few as 32 cards.\nThe French suits were introduced around 1480 and, in France, mostly replaced the earlier Latin suits of \"swords\", \"clubs\", \"cups\" and \"coins\". (which are still common in Spanish- and Portuguese-speaking countries as well as in some northern regions of Italy) The suit symbols, being very simple and single-color, could be stamped onto the playing cards to create a deck, thus only requiring special full-color card art for the court cards. This drastically simplifies the production of a deck of cards versus the traditional Italian deck, which used unique full-color art for each card in the deck. The French suits became popular in English playing cards in the 16th century (despite historic animosity between France and England), and from there were introduced to British colonies including North America. The rise of Western culture has led to the near-universal popularity and availability of French-suited playing cards even in areas with their own regional card art.\nIn Japan, a distinct 48-card hanafuda deck is popular. It is derived from 16th-century Portuguese decks, after undergoing a long evolution driven by laws enacted by the Tokugawa shogunate attempting to ban the use of playing cards\nThe best-known deck internationally is the English pattern of the 52-card French deck, also called the International or Anglo-American pattern, used for such games as poker and contract bridge. It contains one card for each unique combination of thirteen \"ranks\" and the four French \"suits\" \"spades\", \"hearts\", \"diamonds\", and \"clubs\". The ranks (from highest to lowest in bridge and poker) are \"ace\", \"king\", \"queen\", \"jack\" (or \"knave\"), and the numbers from \"ten\" down to \"two\" (or \"deuce\"). The trump cards and \"knight\" cards from the French playing tarot are not included.\nOriginally the term \"knave\" was more common than \"jack\"; the card had been called a jack as part of the terminology of All-Fours since the 17th century, but the word was considered vulgar. (Note the exclamation by Estella in Charles Dickens's novel \"Great Expectations\": \"He calls the knaves, Jacks, this boy!\") However, because the card abbreviation for knave (\"Kn\") was so close to that of the king, it was very easy to confuse them, especially after suits and rankings were moved to the corners of the card in order to enable people to fan them in one hand and still see all the values. (The earliest known deck to place suits and rankings in the corner of the card is from 1693, but these cards did not become common until after 1864 when Hart reintroduced them along with the knave-to-jack change.) However, books of card games published in the third quarter of the 19th century evidently still referred to the \"knave\", and the term with this definition is still recognized in the United Kingdom.\nIn the 17th century, a French, five-trick, gambling game called B\u00eate became popular and spread to Germany, where it was called La Bete and England where it was named Beast. It was a derivative of Triomphe and was the first card game in history to introduce the concept of bidding.\nChinese handmade mother-of-pearl gaming counters were used in scoring and bidding of card games in the West during the approximate period of 1700\u20131840. The gaming counters would bear an engraving such as a coat of arms or a monogram to identify a family or individual. Many of the gaming counters also depict Chinese scenes, flowers or animals. Queen Charlotte is one prominent British individual who is known to have played with the Chinese gaming counters. Card games such as Ombre, Quadrille and Pope Joan were popular at the time and required counters for scoring. The production of counters declined after Whist, with its different scoring method, became the most popular card game in the West.\nBased on the association of card games and gambling, Pope Benedict XIV banned card games on October 17, 1750.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5361", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5361", "title": "Cross-stitch", "text": "Form of counted-thread embroidery.\nCross-stitch is a form of sewing and a popular form of counted-thread embroidery in which X-shaped stitches in a tiled, raster-like pattern are used to form a picture. The stitcher counts the threads on a piece of evenweave fabric (such as linen) in each direction so that the stitches are of uniform size and appearance. This form of cross-stitch is also called counted cross-stitch in order to distinguish it from other forms of cross-stitch. Sometimes cross-stitch is done on designs printed on the fabric (stamped cross-stitch); the stitcher simply stitches over the printed pattern. Cross-stitch is often executed on easily countable fabric called aida cloth whose weave creates a plainly visible grid of squares with holes for the needle at each corner.\nFabrics used in cross-stitch include linen, aida cloth, and mixed-content fabrics called 'evenweave' such as jobelan. All cross-stitch fabrics are technically \"evenweave\" as the term refers to the fact that the fabric is woven to make sure that there are the same number of threads per inch in both the warp and the weft (i.e. vertically and horizontally). Fabrics are categorized by threads per inch (referred to as 'count'), which can range from 11 to 40 count.\nCounted cross-stitch projects are worked from a gridded pattern called a chart and can be used on any count fabric; the count of the fabric and the number of threads per stitch determine the size of the finished stitching. For example, if a given design is stitched on a 28 count cross-stitch fabric with each cross worked over two threads, the finished stitching size is the same as it would be on a 14 count aida fabric with each cross worked over one square. These methods are referred to as \"2 over 2\" (2 embroidery threads used to stitch over 2 fabric threads) and \"1 over 1\" (1 embroidery thread used to stitch over 1 fabric thread or square), respectively. There are different methods of stitching a pattern, including the cross-country method where one colour is stitched at a time, or the parking method where one block of fabric is stitched at a time and the end of the thread is \"parked\" at the next point the same colour occurs in the pattern.\nHistory.\nCross-stitch is the oldest form of embroidery and can be found all over the world since the middle ages. Many folk museums show examples of clothing decorated with cross-stitch, especially from continental Europe and Asia.\nThe cross-stitch sampler is called that because it was generally stitched by a young girl to learn how to stitch and to record alphabet and other patterns to be used in her household sewing. These samples of her stitching could be referred back to over the years. Often, motifs and initials were stitched on household items to identify their owner, or simply to decorate the otherwise-plain cloth. The earliest known cross stitch sampler made in the United States is currently housed at Pilgrim Hall in Plymouth, Massachusetts. The sampler was created by Loara Standish, daughter of Captain Myles Standish and pioneer of the Leviathan stitch, circa 1653.\nTraditionally, cross-stitch was used to embellish items like household linens, tablecloths, dishcloths, and doilies (only a small portion of which would actually be embroidered, such as a border). Although there are many cross-stitchers who still employ it in this fashion, it is now increasingly popular to work the pattern on pieces of fabric and hang them on the wall for decoration. Cross-stitch is also often used to make greeting cards, pillow tops, or as inserts for box tops, coasters and trivets.\nMulticoloured, shaded, painting-like patterns as we know them today are a fairly modern development, deriving from similar shaded patterns of Berlin wool work of the mid-nineteenth century. Besides designs created expressly for cross-stitch, there are software programs that convert a photograph or a fine art image into a chart suitable for stitching. One example of this is in the cross-stitched reproduction of the Sistine Chapel charted and stitched by Joanna Lopianowski-Roberts.\nThere are many cross-stitching \"guilds\" and groups across the United States and Europe which offer classes, collaborate on large projects, stitch for charity, and provide other ways for local cross-stitchers to get to know one another. Individually owned local needlework shops (LNS) often have stitching nights at their shops, or host weekend stitching retreats.\nToday, cotton floss is the most common embroidery thread. It is a thread made of mercerized cotton, composed of six strands that are only loosely twisted together and easily separable. While there are other manufacturers, the two most-commonly used (and oldest) brands are DMC and Anchor, both of which have been manufacturing embroidery floss since the 1800s.\nOther materials used are pearl (or perle) cotton, Danish flower thread, silk and Rayon. Different wool threads, metallic threads or other novelty threads are also used, sometimes for the whole work, but often for accents and embellishments. Hand-dyed cross-stitch floss is created just as the name implies\u2014it is dyed by hand. Because of this, there are variations in the amount of color throughout the thread. Some variations can be subtle, while some can be a huge contrast. Some also have more than one color per thread.\nCross-stitch is widely used in traditional Palestinian dressmaking.\nRelated stitches and forms of embroidery.\nThe cross-stitch can be executed partially such as in quarter-, half-, and three-quarter-stitches. A single straight stitch, done in the form of backstitching, is often used as an outline, to add detail or definition.\nThere are many stitches which are related structurally to cross-stitch. The best known are Italian cross-stitch (as seen in Assisi embroidery), long-armed cross-stitch, and Montenegrin stitch. Italian cross-stitch and Montenegrin stitch are reversible, meaning the work looks the same on both sides. These styles have a slightly different look than ordinary cross-stitch. These more difficult stitches are rarely used in mainstream embroidery, but they are still used to recreate historical pieces of embroidery or by the creative and adventurous stitcher. The double cross-stitch, also known as a Leviathan stitch or Smyrna cross-stitch, combines a cross-stitch with an upright cross-stitch.\nBerlin wool work and similar petit point stitchery resembles the heavily shaded, opulent styles of cross-stitch, and sometimes also used charted patterns on paper.\nCross-stitch is often combined with other popular forms of embroidery, such as Hardanger embroidery or blackwork embroidery. Cross-stitch may also be combined with other work, such as canvaswork or drawn thread work. Beadwork and other embellishments such as paillettes, charms, small buttons and specialty threads of various kinds may also be used. Cross stitch can often used in needlepoint.\nRecent trends for cross stitch.\nCross-stitch has become increasingly popular with the younger generation of Europe in recent years. Retailers such as John Lewis experienced a 17% rise in sales of haberdashery products between 2009 and 2010. Hobbycraft, a chain of stores selling craft supplies, also enjoyed an 11% increase in sales over the year to February 22, 2009.\nKnitting and cross-stitching have become more popular hobbies for a younger market, in contrast to its traditional reputation as a hobby for retirees. Sewing and craft groups such as Stitch and Bitch London have resurrected the idea of the traditional craft club. At Clothes Show Live 2010 there was a new area called \"Sknitch\" promoting modern sewing, knitting and embroidery.\nIn a departure from the traditional designs associated with cross-stitch, there is a current trend for more postmodern or tongue-in-cheek designs featuring retro images or contemporary sayings. It is linked to a concept known as 'subversive cross-stitch', which involves more risque designs, often fusing the traditional sampler style with sayings designed to shock or be incongruous with the old-fashioned image of cross-stitch.\nStitching designs on other materials can be accomplished by using waste canvas. This is a temporary gridded canvas similar to regular canvas used for embroidery that is held together by a water-soluble glue, which is removed after completion of stitch design. Other crafters have taken to cross-stitching on all manner of gridded objects as well including old kitchen strainers or chain-link fences.\nTraditionally, it is believed that cross stitch is a woman's craft. But lately there are men who are also addicted to this. \nCross-stitch and feminism.\nIn the 21st century, an emphasis on feminist design has emerged within cross-stitch communities. Some cross-stitchers have commented on the way that the practice of embroidery makes them feel connected to the women who practised it before them. There is a push for all embroidery, including cross-stitch, to be respected as a significant art form.\nCross-stitch and computers.\nThe development of computer technology has also affected such a seemingly conservative craft as cross-stitch. With the help of computer visualization algorithms, it is now possible to create embroidery designs using a photograph or any other picture. Visualisation uses a drawing on a graphical grid, representing colors and / or symbols, which gives the user an indication of the possible use of colors, the position of those colors, and the type of stitch used, such as full cross or quarter stitch.\nFlosstube.\nAn increasingly popular activity for cross-stitchers is to watch and make YouTube videos detailing their hobby. Flosstubers, as they are known, typically cover WIPs (Works in Progress), FOs (Finished Objects), and Haul (new patterns, thread, and fabric, as well as cross-stitching accessories, such as needle minders). Other accessories include but are not limited to: Floss organizers, thread conditioner, pin cushions, AIDA cloth or plastic canvas, and embroidery needles.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5362", "revid": "39368017", "url": "https://en.wikipedia.org/wiki?curid=5362", "title": "Casino game", "text": "Types of casino games\nGames available in most casinos are commonly called casino games. In a casino game, the players gamble cash or casino chips on various possible random outcomes or combinations of outcomes. Casino games are also available in online casinos, where permitted by law. Casino games can also be played outside casino for entertainment purposes like in parties or in school competitions, some on machines that simulate gambling.\nCategories.\nThere are three general categories of casino games: gaming machines, table games, and random number games. Gaming machines, such as slot machines and pachinko, are usually played by one player at a time and do not require the involvement of casino employees to play. Tables games, such as blackjack or craps, involve one or more players who are competing against the house (the casino itself) rather than each other. Table games are usually conducted by casino employees known as croupiers or dealers. Random number games are based upon the selection of random numbers, either from a computerized random number generator or from other gaming equipment. Random number games may be played at a table or through the purchase of paper tickets or cards, such as keno or bingo.\nSome casino games combine multiple of the above aspects; for example, roulette is a table game conducted by a dealer, which involves random numbers. Casinos may also offer other types of gaming, such as hosting poker games or tournaments, where players compete against each other.\nCommon casino games.\nGames commonly found at casinos include table games, gaming machines and random numbers games.\nTable games.\nIn the United States, 'table game' is the term used for games of chance such as blackjack, craps, roulette and baccarat that are played against the casino and operated by one or more live croupiers, as opposed to those played on a mechanical device like a slot machine or against other players instead of the casino, such as standard poker.\nTable games are popularly played in casinos and involve some form of legal gambling, but also are played privately by varying house rules. The term has significance in that some jurisdictions permit casinos to have only slots and no table games. In some states, this law has resulted in casinos employing electronic table games, such as roulette, blackjack, and craps.\nTable games found in casinos include:\nGaming machines.\nGaming machines found in casinos include:\nRandom numbers games.\nRandom numbers games found in casinos include:\nHouse advantage.\nCasino games typically provide a predictable long-term advantage to the casino, or \"house\", while offering the players the possibility of a short-term gain that in some cases can be large. Some casino games have a skill element, where the players' decisions have an impact on the results. Players possessing sufficient skills to eliminate the inherent long-term disadvantage (the \"house edge\" or vigorish) in a casino game are referred to as advantage players.\nThe players' disadvantage is a result of the casino not paying winning wagers according to the game's \"true odds\", which are the payouts that would be expected considering the odds of a wager either winning or losing. For example, if a game is played by wagering on the number that would result from the roll of one die, true odds would be 6 times the amount wagered since there is a 1 in 6 chance of any single number appearing, assuming that the player gets the original amount wagered back. However, the casino may only pay 4 times the amount wagered for a winning wager.\nThe house edge or vigorish is defined as the casino profit expressed as the percentage of the player's original bet. (In games such as blackjack or Spanish 21, the final bet may be several times the original bet, if the player double and splits.)\nIn American roulette, there are two \"zeroes\" (0, 00) and 36 non-zero numbers (18 red and 18 black). This leads to a higher house edge compared to European roulette. The chances of a player, who bets 1 unit on red, winning are 18/38 and his chances of losing 1 unit are 20/38. The player's expected value is EV = (18/38 \u00d7 1) + (20/38 \u00d7 (\u22121)) = 18/38 \u2212 20/38 = \u22122/38 = \u22125.26%. Therefore, the house edge is 5.26%. After 10 spins, betting 1 unit per spin, the average house profit will be 10 \u00d7 1 \u00d7 5.26% = 0.53 units. European roulette wheels have only one \"zero\" and therefore the house advantage (ignoring the en prison rule) is equal to 1/37 = 2.7%.\nThe house edge of casino games varies greatly with the game, with some games having an edge as low as 0.3%. Keno can have house edges up to 25%, slot machines having up to\u00a015%.\nThe calculation of the roulette house edge is a trivial exercise; for other games, this is not usually the case. Combinatorial analysis and/or computer simulation is necessary to complete the task.\nIn games that have a skill element, such as blackjack or Spanish 21, the house edge is defined as the house advantage from optimal play (without the use of advanced techniques such as card counting), on the first hand of the shoe (the container that holds the cards). The set of the optimal plays for all possible hands is known as \"basic strategy\" and is highly dependent on the specific rules and even the number of decks used.\nTraditionally, the majority of casinos have refused to reveal the house edge information for their slots games and due to the unknown number of symbols and weightings of the reels, in most cases, it is much more difficult to calculate the house edge than in other casino games. However, due to some online properties revealing this information and some independent research conducted by Michael Shackleford in the offline sector, this pattern is slowly changing.\nIn games where players are not competing against the house, such as poker, the casino usually earns money via a commission, known as a \"rake\".\nStandard deviation.\nThe luck factor in a casino game is quantified using standard deviations (SD). The standard deviation of a simple game like roulette can be calculated using the binomial distribution. In the binomial distribution, SD = formula_1, where \"n\" = number of rounds played, \"p\" = probability of winning, and \"q\" = probability of losing. The binomial distribution assumes a result of 1 unit for a win, and 0 units for a loss, rather than \u22121 units for a loss, which doubles the range of possible outcomes. Furthermore, if we flat bet at 10 units per round instead of 1 unit, the range of possible outcomes increases 10 fold.\n SD (roulette, even-money bet) = 2\"b\" formula_1, where \"b\" = flat bet per round, \"n\" = number of rounds, \"p\" = 18/38, and \"q\" = 20/38.\nFor example, after 10 rounds at 1 unit per round, the standard deviation will be 2 \u00d7 1 \u00d7 formula_3 = 3.16 units. After 10 rounds, the expected loss will be 10 \u00d7 1 \u00d7 5.26% = 0.53. As you can see, standard deviation is many times the magnitude of the expected loss.\nThe standard deviation for pai gow poker is the lowest out of all common casino games. Many casino games, particularly slot machines, have extremely high standard deviations. The bigger size of the potential payouts, the more the standard deviation may increase.\nAs the number of rounds increases, eventually, the expected loss will exceed the standard deviation, many times over. From the formula, we can see the standard deviation is proportional to the square root of the number of rounds played, while the expected loss is proportional to the number of rounds played. As the number of rounds increases, the expected loss increases at a much faster rate. This is why it is impossible for a gambler to win in the long term. It is the high ratio of short-term standard deviation to expected loss that fools gamblers into thinking that they can win.\nIt is important for a casino to know both the house edge and variance for all of their games. The house edge tells them what kind of profit they will make as a percentage of turnover, and the variance tells them how much they need in the way of cash reserves. The mathematicians and computer programmers that do this kind of work are called gaming mathematicians and gaming analysts. Casinos do not have in-house expertise in this field, so they outsource their requirements to experts in the gaming analysis field.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5363", "revid": "7611264", "url": "https://en.wikipedia.org/wiki?curid=5363", "title": "Video game", "text": "Electronic game with user interface and visual feedback\nA video game is an electronic game that involves interaction with a user interface or input device (such as a joystick, controller, keyboard, or motion sensing device) to generate visual feedback from a display device, most commonly shown in a video format on a television set, computer monitor, flat-panel display or touchscreen on handheld devices, or a virtual reality headset. Most modern video games are audiovisual, with audio complement delivered through speakers or headphones, and sometimes also with other types of sensory feedback (e.g., haptic technology that provides tactile sensations), and some video games also allow microphone and webcam inputs for in-game chatting and livestreaming.\nVideo games are typically categorized according to their hardware platform, which traditionally includes arcade video games, console games, and computer (PC) games; the latter also encompasses LAN games, online games, and browser games. More recently, the video game industry has expanded onto mobile gaming through mobile devices (such as smartphones and tablet computers), virtual and augmented reality systems, and remote cloud gaming. Video games are also classified into a wide range of genres based on their style of gameplay and target audience.\nThe first video game prototypes in the 1950s and 1960s were simple extensions of electronic games using video-like output from large, room-sized mainframe computers. The first consumer video game was the arcade video game \"Computer Space\" in 1971. In 1972 came the iconic hit game \"Pong\" and the first home console, the Magnavox Odyssey. The industry grew quickly during the \"golden age\" of arcade video games from the late 1970s to early 1980s but suffered from the crash of the North American video game market in 1983 due to loss of publishing control and saturation of the market. Following the crash, the industry matured, was dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a similar crash in the future, many of which continue to be followed. In the 2000s, the core industry centered on \"AAA\" games, leaving little room for riskier experimental games. Coupled with the availability of the Internet and digital distribution, this gave room for independent video game development (or \"indie games\") to gain prominence into the 2010s. Since then, the commercial importance of the video game industry has been increasing. The emerging Asian markets and proliferation of smartphone games in particular are altering player demographics towards casual gaming and increasing monetization by incorporating games as a service.\nToday, video game development requires numerous interdisciplinary skills, vision, teamwork, and liaisons between different parties, including developers, publishers, distributors, retailers, hardware manufacturers, and other marketers, to successfully bring a game to its consumers. As of 2020[ [update]], the global video game market had estimated annual revenues of US$ across hardware, software, and services, which is three times the size of the global music industry and four times that of the film industry in 2019, making it a formidable heavyweight across the modern entertainment industry. The video game market is also a major influence behind the electronics industry, where personal computer component, console, and peripheral sales, as well as consumer demands for better game performance, have been powerful driving factors for hardware design and innovation.\nOrigins.\nEarly video games use interactive electronic devices with various display formats. The earliest example is from 1947\u2014a \"cathode-ray tube amusement device\" was filed for a patent on 25 January 1947, by Thomas T. Goldsmith Jr. and Estle Ray Mann, and issued on 14 December 1948, as U.S. Patent 2455992. Inspired by radar display technology, it consists of an analog device allowing a user to control the parabolic arc of a dot on the screen to simulate a missile being fired at targets, which are paper drawings fixed to the screen. Other early examples include Christopher Strachey's draughts game, the Nimrod computer at the 1951 Festival of Britain; \"OXO\", a tic-tac-toe computer game by Alexander S. Douglas for the EDSAC in 1952; \"Tennis for Two\", an electronic interactive game engineered by William Higinbotham in 1958; and \"Spacewar!\", written by Massachusetts Institute of Technology students Martin Graetz, Steve Russell, and Wayne Wiitanen's on a DEC PDP-1 computer in 1961. Each game has different means of display: NIMROD has a panel of lights to play the game of Nim, OXO has a graphical display to play tic-tac-toe, \"Tennis for Two\" has an oscilloscope to display a side view of a tennis court, and \"Spacewar!\" has the DEC PDP-1's vector display to have two spaceships battle each other.\nThese preliminary inventions paved the way for the origins of video games today. Ralph H. Baer, while working at Sanders Associates in 1966, devised a control system to play a rudimentary game of table tennis on a television screen. With the company's approval, Baer built the prototype \"Brown Box\". Sanders patented Baer's inventions and licensed them to Magnavox, which commercialized it as the first home video game console, the Magnavox Odyssey, released in 1972. Separately, Nolan Bushnell and Ted Dabney, inspired by seeing \"Spacewar!\" running at Stanford University, devised a similar version running in a smaller coin-operated arcade cabinet using a less expensive computer. This was released as \"Computer Space\", the first arcade video game, in 1971. Bushnell and Dabney went on to form Atari, Inc., and with Allan Alcorn, created their second arcade game in 1972, the hit ping pong-style \"Pong\", which was directly inspired by the table tennis game on the Odyssey. Sanders and Magnavox sued Atari for infringement of Baer's patents, but Atari settled out of court, paying for perpetual rights to the patents. Following their agreement, Atari made a home version of \"Pong\", which was released by Christmas 1975. The success of the Odyssey and \"Pong\", both as an arcade game and home machine, launched the video game industry. Both Baer and Bushnell have been titled \"Father of Video Games\" for their contributions.\nTerminology.\nThe term \"video game\" was developed to distinguish this class of electronic games that were played on some type of video display rather than on a teletype printer, audio speaker or similar device. This also distinguished from many handheld electronic games like \"Merlin\" which commonly used LED lights for indicators but did not use these in combination for imaging purposes.\n\"Computer game\" may also be used as a descriptor, as all these types of games essentially require the use of a computer processor, and in some cases, it is used interchangeably with \"video game\". Particularly in the United Kingdom and Western Europe, this is common due to the historic relevance of domestically produced microcomputers. Other terms used include digital game, for example by the Australian Bureau of Statistics. However, the term \"computer game\" can also be used to more specifically refer to games played primarily on personal computers or other type of flexible hardware systems (also known as a PC game), as a way distinguish them from console games, arcade games or mobile games. Other terms such as \"television game\" or \"telegame\" had been used in the 1970s and early 1980s, particularly for the home gaming consoles that rely on connection to a television set. In Japan, where consoles like the Odyssey were first imported and then made within the country by the large television manufacturers such as Toshiba and Sharp Corporation, such games are known as \"TV games\", or \"TV geemu\" or \"terebi geemu\". \"Electronic game\" may also be used to refer to video games, but this also incorporates devices like early handheld electronic games that lack any video output. and the term \"TV game\" is still commonly used into the 21st century.\nThe first appearance of the term \"video game\" emerged around 1973. The Oxford English Dictionary cited a November 10, 1973 \"BusinessWeek\" article as the first printed use of the term. Though Bushnell believed the term came from a vending magazine review of \"Computer Space\" in 1971, a review of the major vending magazines \"Vending Times\" and \"Cashbox\" showed that the term came much earlier, appearing first around March 1973 in these magazines in mass usage including by the arcade game manufacturers. As analyzed by video game historian Keith Smith, the sudden appearance suggested that the term had been proposed and readily adopted by those involved. This appeared to trace to Ed Adlum, who ran \"Cashbox\"'s coin-operated section until 1972 and then later founded \"RePlay Magazine\", covering the coin-op amusement field, in 1975. In a September 1982 issue of \"RePlay\", Adlum is credited with first naming these games as \"video games\": \"RePlay's Eddie Adlum worked at 'Cash Box' when 'TV games' first came out. The personalities in those days were Bushnell, his sales manager Pat Karns and a handful of other 'TV game' manufacturers like Henry Leyser and the McEwan brothers. It seemed awkward to call their products 'TV games', so borrowing a word from \"Billboard\"'s description of movie jukeboxes, Adlum started to refer to this new breed of amusement machine as 'video games.' The phrase stuck.\" Adlum explained in 1985 that up until the early 1970s, amusement arcades typically had non-video arcade games such as pinball machines and electro-mechanical games. With the arrival of video games in arcades during the early 1970s, there was initially some confusion in the arcade industry over what term should be used to describe the new games. He \"wrestled with descriptions of this type of game,\" alternating between \"TV game\" and \"television game\" but \"finally woke up one day\" and said, \"what the hell... video game!\"\nFor many years, the traveling Videotopia exhibit served as the closest representation of such a vital resource. In addition to collecting home video game consoles, the Electronics Conservancy organization set out to locate and restore 400 antique arcade cabinets after realizing that the majority of these games had been destroyed and feared the loss of their historical significance. Video games have significantly began to be seen in the real-world as a purpose to present history in a way of understanding the methodology and terms that are being compared. Researchers have looked at how historical representations affect how the public perceives the past, and digital humanists encourage historians to use video games as primary materials. Video games, considering their past and age, have over time progressed as what a video game really means. Whether played through a monitor, TV, or a hand-held device, there are many ways that video games are being displayed for users to enjoy. People have drawn comparisons between flow-state-engaged video gamers and pupils in conventional school settings. In traditional, teacher-led classrooms, students have little say in what they learn, are passive consumers of the information selected by teachers, are required to follow the pace and skill level of the group (group teaching), and receive brief, imprecise, normative feedback on their work. Video games, as they continue to develop into better graphic definition and genre's, create new terminology when something unknown tends to become known. Yearly, consoles are being created to compete against other brands with similar functioning features that tends to lead the consumer into which they'd like to purchase. Now, companies have moved towards games only the specific console can play to grasp the consumer into purchasing their product compared to when video games first began, there was little to no variety. In 1989, a console war begun with Nintendo, one of the biggest in gaming was up against target, Sega with their brand new Master System which, failed to compete, allowing the Nintendo Emulator System to be one of the most consumed product in the world. More technology continued to be created, as the computer began to be used in people's houses for more than just office and daily use. Games began being implemented into computers and have progressively grown since then with coded robots to play against you. Early games like tic-tac-toe, solitaire, and Tennis for Two were great ways to bring new gaming to another system rather than one specifically meant for gaming.\nDefinition.\nWhile many games readily fall into a clear, well-understood definition of video games, new genres and innovations in game development have raised the question of what are the essential factors of a video game that separate the medium from other forms of entertainment.\nThe introduction of interactive films in the 1980s with games like \"Dragon's Lair\", featured games with full motion video played off a form of media but only limited user interaction. This had required a means to distinguish these games from more traditional board games that happen to also use external media, such as the \"Clue VCR Mystery Game\" which required players to watch VCR clips between turns. To distinguish between these two, video games are considered to require some interactivity that affects the visual display.\nMost video games tend to feature some type of victory or winning conditions, such as a scoring mechanism or a final boss fight. The introduction of walking simulators (adventure games that allow for exploration but lack any objectives) like \"Gone Home\", and empathy games (video games that tend to focus on emotion) like \"That Dragon, Cancer\" brought the idea of games that did not have any such type of winning condition and raising the question of whether these were actually games. These are still commonly justified as video games as they provide a game world that the player can interact with by some means.\nThe lack of any industry definition for a video game by 2021 was an issue during the case \"Epic Games v. Apple\" which dealt with video games offered on Apple's iOS App Store. Among concerns raised were games like \"Fortnite Creative\" and \"Roblox\" which created metaverses of interactive experiences, and whether the larger game and the individual experiences themselves were games or not in relation to fees that Apple charged for the App Store. Judge Yvonne Gonzalez Rogers, recognizing that there was yet an industry standard definition for a video game, established for her ruling that \"At a bare minimum, videogames appear to require some level of interactivity or involvement between the player and the medium\" compared to passive entertainment like film, music, and television, and \"videogames are also generally graphically rendered or animated, as opposed to being recorded live or via motion capture as in films or television\". Rogers still concluded that what is a video game \"appears highly eclectic and diverse\".\nVideo game terminology.\nThe gameplay experience varies radically between video games, but many common elements exist. Most games will launch into a title screen and give the player a chance to review options such as the number of players before starting a game. Most games are divided into levels which the player must work the avatar through, scoring points, collecting power-ups to boost the avatar's innate attributes, all while either using special attacks to defeat enemies or moves to avoid them. This information is relayed to the player through a type of on-screen user interface such as a heads-up display atop the rendering of the game itself. Taking damage will deplete their avatar's health, and if that falls to zero or if the avatar otherwise falls into an impossible-to-escape location, the player will lose one of their lives. Should they lose all their lives without gaining an extra life or \"1-UP\", then the player will reach the \"game over\" screen. Many levels as well as the game's finale end with a type of boss character the player must defeat to continue on. In some games, intermediate points between levels will offer save points where the player can create a saved game on storage media to restart the game should they lose all their lives or need to stop the game and restart at a later time. These also may be in the form of a passage that can be written down and reentered at the title screen.\nProduct flaws include software bugs which can manifest as glitches which may be exploited by the player; this is often the foundation of speedrunning a video game. These bugs, along with cheat codes, Easter eggs, and other hidden secrets that were intentionally added to the game can also be exploited. On some consoles, cheat cartridges allow players to execute these cheat codes, and user-developed trainers allow similar bypassing for computer software games. Both of which might make the game easier, give the player additional power-ups, or change the appearance of the game.\nComponents.\nTo distinguish from electronic games, a video game is generally considered to require a platform, the hardware which contains computing elements, to process player interaction from some type of input device and displays the results to a video output display.\nPlatform.\nVideo games require a platform, a specific combination of electronic components or computer hardware and associated software, to operate. The term system is also commonly used. Games are typically designed to be played on one or a limited number of platforms, and exclusivity to a platform is used as a competitive edge in the video game market. However, games may be developed for alternative platforms than intended, which are described as ports or conversions. These also may be remasters - where most of the original game's source code is reused and art assets, models, and game levels are updated for modern systems \u2013 and remakes, where in addition to asset improvements, significant reworking of the original game and possibly from scratch is performed.\nThe list below is not exhaustive and excludes other electronic devices capable of playing video games such as PDAs and graphing calculators.\nMost computer games are PC games, referring to those that involve a player interacting with a personal computer (PC) connected to a video monitor. Personal computers are not dedicated game platforms, so there may be differences running the same game on different hardware. Also, the openness allows some features to developers like reduced software cost, increased flexibility, increased innovation, emulation, creation of modifications or mods, open hosting for online gaming (in which a person plays a video game with people who are in a different household) and others. A gaming computer is a PC or laptop intended specifically for gaming, typically using high-performance, high-cost components. In additional to personal computer gaming, there also exist games that work on mainframe computers and other similarly shared systems, with users logging in remotely to use the computer.\nA console game is played on a home console, a specialized electronic device that connects to a common television set or composite video monitor. Home consoles are specifically designed to play games using a dedicated hardware environment, giving developers a concrete hardware target for development and assurances of what features will be available, simplifying development compared to PC game development. Usually consoles only run games developed for it, or games from other platform made by the same company, but never games developed by its direct competitor, even if the same game is available on different platforms. It often comes with a specific game controller. Major console platforms include Xbox, PlayStation and Nintendo.\nA handheld game console is a small, self-contained electronic device that is portable and can be held in a user's hands. It features the console, a small screen, speakers and buttons, joystick or other game controllers in a single unit. Like consoles, handhelds are dedicated platforms, and share almost the same characteristics. Handheld hardware usually is less powerful than PC or console hardware. Some handheld games from the late 1970s and early 1980s could only play one game. In the 1990s and 2000s, a number of handheld games used cartridges, which enabled them to be used to play many different games. The handheld console has waned in the 2010s as mobile device gaming has become a more dominant factor.\n An arcade video game generally refers to a game played on an even more specialized type of electronic device that is typically designed to play only one game and is encased in a special, large coin-operated cabinet which has one built-in console, controllers (joystick, buttons, etc.), a CRT screen, and audio amplifier and speakers. Arcade games often have brightly painted logos and images relating to the theme of the game. While most arcade games are housed in a vertical cabinet, which the user typically stands in front of to play, some arcade games use a tabletop approach, in which the display screen is housed in a table-style cabinet with a see-through table top. With table-top games, the users typically sit to play. In the 1990s and 2000s, some arcade games offered players a choice of multiple games. In the 1980s, video arcades were businesses in which game players could use a number of arcade video games. In the 2010s, there are far fewer video arcades, but some movie theaters and family entertainment centers still have them.\nA browser game takes advantages of standardizations of technologies for the functionality of web browsers across multiple devices providing a cross-platform environment. These games may be identified based on the website that they appear, such as with Miniclip games. Others are named based on the programming platform used to develop them, such as Java and Flash games.\nWith the introduction of smartphones and tablet computers standardized on the iOS and Android operating systems, mobile gaming has become a significant platform. These games may use unique features of mobile devices that are not necessary present on other platforms, such as accelerometers, global positing information and camera devices to support augmented reality gameplay.\nCloud gaming requires a minimal hardware device, such as a basic computer, console, laptop, mobile phone or even a dedicated hardware device connected to a display with good Internet connectivity that connects to hardware systems by the cloud gaming provider. The game is computed and rendered on the remote hardware, using a number of predictive methods to reduce the network latency between player input and output on their display device. For example, the Xbox Cloud Gaming and PlayStation Now platforms use dedicated custom server blade hardware in cloud computing centers.\n Virtual reality (VR) games generally require players to use a special head-mounted unit that provides stereoscopic screens and motion tracking to immerse a player within virtual environment that responds to their head movements. Some VR systems include control units for the player's hands as to provide a direct way to interact with the virtual world. VR systems generally require a separate computer, console, or other processing device that couples with the head-mounted unit.\nAn emulator enables games from a console or otherwise different system to be run in a type of virtual machine on a modern system, simulating the hardware of the original and allows old games to be played. While emulators themselves have been found to be legal in United States case law, the act of obtaining the game software that one does not already own may violate copyrights. However, there are some official releases of emulated software from game manufacturers, such as Nintendo with its Virtual Console or Nintendo Switch Online offerings.\nBackward compatibility is similar in nature to emulation in that older games can be played on newer platforms, but typically directly though hardware and build-in software within the platform. For example, the PlayStation 2 is capable of playing original PlayStation games simply by inserting the original game media into the newer console, while Nintendo's Wii could play GameCube titles as well in the same manner.\nGame media.\nEarly arcade games, home consoles, and handheld games were dedicated hardware units with the game's logic built into the electronic componentry of the hardware. Since then, most video game platforms are considered programmable, having means to read and play multiple games distributed on different types of media or formats. Physical formats include ROM cartridges, magnetic storage including magnetic-tape data storage and floppy discs, optical media formats including CD-ROM and DVDs, and flash memory cards. Furthermore digital distribution over the Internet or other communication methods as well as cloud gaming alleviate the need for any physical media. In some cases, the media serves as the direct read-only memory for the game, or it may be the form of installation media that is used to write the main assets to the player's platform's local storage for faster loading periods and later updates.\nGames can be extended with new content and software patches through either expansion packs which are typically available as physical media, or as downloadable content nominally available via digital distribution. These can be offered freely or can be used to monetize a game following its initial release. Several games offer players the ability to create user-generated content to share with others to play. Other games, mostly those on personal computers, can be extended with user-created modifications or mods that alter or add onto the game; these often are unofficial and were developed by players from reverse engineering of the game, but other games provide official support for modding the game.\nInput device.\nVideo game can use several types of input devices to translate human actions to a game. Most common are the use of game controllers like gamepads and joysticks for most consoles, and as accessories for personal computer systems along keyboard and mouse controls. Common controls on the most recent controllers include face buttons, shoulder triggers, analog sticks, and directional pads (\"d-pads\"). Consoles typically include standard controllers which are shipped or bundled with the console itself, while peripheral controllers are available as a separate purchase from the console manufacturer or third-party vendors. Similar control sets are built into handheld consoles and onto arcade cabinets. Newer technology improvements have incorporated additional technology into the controller or the game platform, such as touchscreens and motion detection sensors that give more options for how the player interacts with the game. Specialized controllers may be used for certain genres of games, including racing wheels, light guns and dance pads. Digital cameras and motion detection can capture movements of the player as input into the game, which can, in some cases, effectively eliminate the control, and on other systems such as virtual reality, are used to enhance immersion into the game.\nDisplay and output.\nBy definition, all video games are intended to output graphics to an external video display, such as cathode-ray tube televisions, newer liquid-crystal display (LCD) televisions and built-in screens, projectors or computer monitors, depending on the type of platform the game is played on. Features such as color depth, refresh rate, frame rate, and screen resolution are a combination of the limitations of the game platform and display device and the program efficiency of the game itself. The game's output can range from fixed displays using LED or LCD elements, text-based games, two-dimensional and three-dimensional graphics, and augmented reality displays.\nThe game's graphics are often accompanied by sound produced by internal speakers on the game platform or external speakers attached to the platform, as directed by the game's programming. This often will include sound effects tied to the player's actions to provide audio feedback, as well as background music for the game.\nSome platforms support additional feedback mechanics to the player that a game can take advantage of. This is most commonly haptic technology built into the game controller, such as causing the controller to shake in the player's hands to simulate a shaking earthquake occurring in game.\nClassifications.\nVideo games are frequently classified by a number of factors related to how one plays them.\nGenre.\nA video game, like most other forms of media, may be categorized into genres. However, unlike film or television which use visual or narrative elements, video games are generally categorized into genres based on their gameplay interaction, since this is the primary means which one interacts with a video game. The narrative setting does not impact gameplay; a shooter game is still a shooter game, regardless of whether it takes place in a fantasy world or in outer space. An exception is the horror game genre, used for games that are based on narrative elements of horror fiction, the supernatural, and psychological horror.\nGenre names are normally self-describing in terms of the type of gameplay, such as action game, role playing game, or shoot 'em up, though some genres have derivations from influential works that have defined that genre, such as roguelikes from \"Rogue\", Grand Theft Auto clones from \"Grand Theft Auto III\", and battle royale games from the film \"Battle Royale\". The names may shift over time as players, developers and the media come up with new terms; for example, first-person shooters were originally called \"Doom clones\" based on the 1993 game. A hierarchy of game genres exist, with top-level genres like \"shooter game\" and \"action game\" that broadly capture the game's main gameplay style, and several subgenres of specific implementation, such as within the shooter game first-person shooter and third-person shooter. Some cross-genre types also exist that fall until multiple top-level genres such as action-adventure game.\nMode.\nA video game's mode describes how many players can use the game at the same type. This is primarily distinguished by single-player video games and multiplayer video games. Within the latter category, multiplayer games can be played in a variety of ways, including locally at the same device, on separate devices connected through a local network such as LAN parties, or online via separate Internet connections. Most multiplayer games are based on competitive gameplay, but many offer cooperative and team-based options as well as asymmetric gameplay. Online games use server structures that can also enable massively multiplayer online games (MMOs) to support hundreds of players at the same time.\nA small number of video games are zero-player games, in which the player has very limited interaction with the game itself. These are most commonly simulation games where the player may establish a starting state and then let the game proceed on its own, watching the results as a passive observer, such as with many computerized simulations of Conway's Game of Life.\nTypes.\nMost video games are intended for entertainment purposes. Different game types include:\n Casual games are designed for ease of accessibility, simple to understand gameplay and quick to grasp rule sets, and aimed at mass market audience. They frequently support the ability to jump in and out of play on demand, such as during commuting or lunch breaks. Numerous browser and mobile games fall into the casual game area, and casual games often are from genres with low intensity game elements such as match three, hidden object, time management, and puzzle games. Causal games frequently use social-network game mechanics, where players can enlist the help of friends on their social media networks for extra turns or moves each day. Popular casual games include \"Tetris\" and \"Candy Crush Saga\". More recent, starting in the late 2010s, are hyper-casual games which use even more simplistic rules for short but infinitely replayable games, such as \"Flappy Bird\".\n This is as opposed to core games or hardcore games, which are designed for committed players. Their game mechanics are challenging and playing them is complex.\n Education software has been used in homes and classrooms to help teach children and students, and video games have been similarly adapted for these reasons, all designed to provide a form of interactivity and entertainment tied to game design elements. There are a variety of differences in their designs and how they educate the user. These are broadly split between edutainment games that tend to focus on the entertainment value and rote learning but are unlikely to engage in critical thinking, and educational video games that are geared towards problem solving through motivation and positive reinforcement while downplaying the entertainment value. Examples of educational games include \"The Oregon Trail\" and the \"Carmen Sandiego\" series. Further, games not initially developed for educational purposes have found their way into the classroom after release, such as that feature open worlds or virtual sandboxes like \"Minecraft\", or offer critical thinking skills through puzzle video games like \"SpaceChem\".\n Further extending from educational games, serious games are those where the entertainment factor may be augmented, overshadowed, or even eliminated by other purposes for the game. Game design is used to reinforce the non-entertainment purpose of the game, such as using video game technology for the game's interactive world, or gamification for reinforcement training. Educational games are a form of serious games, but other types of games include fitness games that incorporate significant physical exercise to help keep the player fit (such as \"Wii Fit\"), simulator games that resemble fight simulators to pilot aircraft (such as \"Microsoft Flight Simulator\"), advergames that are built around the advertising of a product (such as \"Pepsiman\"), and newsgames aimed at conveying a specific advocacy message (such as \"NarcoGuerra\").\nAlthough video games have been considered an art form on their own, games may be developed to try to purposely communicate a story or message, using the medium as a work of art. These art or arthouse games are designed to generate emotion and empathy from the player by challenging societal norms and offering critique through the interactivity of the video game medium. They may not have any type of win condition and are designed to let the player explore through the game world and scenarios. Most art games are indie games in nature, designed based on personal experiences or stories through a single developer or small team. Examples of art games include \"Passage\", \"Flower\", and \"That Dragon, Cancer\".\nContent rating.\nVideo games can be subject to national and international content rating requirements. Like with film content ratings, video game ratings typing identify the target age group that the national or regional ratings board believes is appropriate for the player, ranging from all-ages, to a teenager-or-older, to mature, to the infrequent adult-only games. Most content review is based on the level of violence, both in the type of violence and how graphic it may be represented, and sexual content, but other themes such as drug and alcohol use and gambling that can influence children may also be identified. A primary identifier based on a minimum age is used by nearly all systems, along with additional descriptors to identify specific content that players and parents should be aware of.\nThe regulations vary from country to country but generally are voluntary systems upheld by vendor practices, with penalty and fines issued by the ratings body on the video game publisher for misuse of the ratings. Among the major content rating systems include:\nAdditionally, the major content system provides have worked to create the International Age Rating Coalition (IARC), a means to streamline and align the content ratings system between different region, so that a publisher would only need to complete the content ratings review for one provider, and use the IARC transition to affirm the content rating for all other regions.\nCertain nations have even more restrictive rules related to political or ideological content. Within Germany, until 2018, the Unterhaltungssoftware Selbstkontrolle (\"Entertainment Software Self-Regulation\") would refuse to classify, and thus allow sale, of any game depicting Nazi imagery, and thus often requiring developers to replace such imagery with fictional ones. This ruling was relaxed in 2018 to allow for such imagery for \"social adequacy\" purposes that applied to other works of art. China's video game segment is mostly isolated from the rest of the world due to the government's censorship, and all games published there must adhere to strict government review, disallowing content such as smearing the image of the Chinese Communist Party. Foreign games published in China often require modification by developers and publishers to meet these requirements.\nDevelopment.\nVideo game development and authorship, much like any other form of entertainment, is frequently a cross-disciplinary field. Video game developers, as employees within this industry are commonly referred, primarily include programmers and graphic designers. Over the years this has expanded to include almost every type of skill that one might see prevalent in the creation of any movie or television program, including sound designers, musicians, and other technicians; as well as skills that are specific to video games, such as the game designer. All of these are managed by producers.\nIn the early days of the industry, it was more common for a single person to manage all of the roles needed to create a video game. As platforms have become more complex and powerful in the type of material they can present, larger teams have been needed to generate all of the art, programming, cinematography, and more. This is not to say that the age of the \"one-man shop\" is gone, as this is still sometimes found in the casual gaming and handheld markets, where smaller games are prevalent due to technical limitations such as limited RAM or lack of dedicated 3D graphics rendering capabilities on the target platform (e.g., some PDAs).\nVideo games are programmed like any other piece of computer software. Prior to the mid-1970s, arcade and home consoles were programmed by assembling discrete electro-mechanical components on circuit boards, which limited games to relatively simple logic. By 1975, low-cost microprocessors were available at volume to be used for video game hardware, which allowed game developers to program more detailed games, widening the scope of what was possible. Ongoing improvements in computer hardware technology has expanded what has become possible to create in video games, coupled with convergence of common hardware between console, computer, and arcade platforms to simplify the development process. Today, game developers have a number of commercial and open source tools available for use to make games, often which are across multiple platforms to support portability, or may still opt to create their own for more specialized features and direct control of the game. Today, many games are built around a game engine that handles the bulk of the game's logic, gameplay, and rendering. These engines can be augmented with specialized engines for specific features, such as a physics engine that simulates the physics of objects in real-time. A variety of middleware exists to help developers to access other features, such as for playback of videos within games, network-oriented code for games that communicate via online services, matchmaking for online games, and similar features. These features can be used from a developers' programming language of choice, or they may opt to also use game development kits that minimize the amount of direct programming they have to do but can also limit the amount of customization they can add into a game. Like all software, video games usually undergo quality testing before release to assure there are no bugs or glitches in the product, though frequently developers will release patches and updates.\nWith the growth of the size of development teams in the industry, the problem of cost has increased. Development studios need the best talent, while publishers reduce costs to maintain profitability on their investment. Typically, a video game console development team ranges from 5 to 50 people, and some exceed 100. In May 2009, \"Assassin's Creed II\" was reported to have a development staff of 450. The growth of team size combined with greater pressure to get completed projects into the market to begin recouping production costs has led to a greater occurrence of missed deadlines, rushed games and the release of unfinished products.\nWhile amateur and hobbyist game programming had existed since the late 1970s with the introduction of home computers, a newer trend since the mid-2000s is indie game development. Indie games are made by small teams outside any direct publisher control, their games being smaller in scope than those from the larger \"AAA\" game studios, and are often experiment in gameplay and art style. Indie game development are aided by larger availability of digital distribution, including the newer mobile gaming marker, and readily-available and low-cost development tools for these platforms.\nGame theory and studies.\nAlthough departments of computer science have been studying the technical aspects of video games for years, theories that examine games as an artistic medium are a relatively recent development in the humanities. The two most visible schools in this emerging field are ludology and narratology. Narrativists approach video games in the context of what Janet Murray calls \"Cyberdrama\". That is to say, their major concern is with video games as a storytelling medium, one that arises out of interactive fiction. Murray puts video games in the context of the Holodeck, a fictional piece of technology from \"Star Trek\", arguing for the video game as a medium in which the player is allowed to become another person, and to act out in another world. This image of video games received early widespread popular support, and forms the basis of films such as \"Tron\", \"eXistenZ\" and \"The Last Starfighter\".\nLudologists break sharply and radically from this idea. They argue that a video game is first and foremost a game, which must be understood in terms of its rules, interface, and the concept of play that it deploys. Espen J. Aarseth argues that, although games certainly have plots, characters, and aspects of traditional narratives, these aspects are incidental to gameplay. For example, Aarseth is critical of the widespread attention that narrativists have given to the heroine of the game \"Tomb Raider\", saying that \"the dimensions of Lara Croft's body, already analyzed to death by film theorists, are irrelevant to me as a player, because a different-looking body would not make me play differently... When I play, I don't even see her body, but see through it and past it.\" Simply put, ludologists reject traditional theories of art because they claim that the artistic and socially relevant qualities of a video game are primarily determined by the underlying set of rules, demands, and expectations imposed on the player.\nWhile many games rely on emergent principles, video games commonly present simulated story worlds where emergent behavior occurs within the context of the game. The term \"emergent narrative\" has been used to describe how, in a simulated environment, storyline can be created simply by \"what happens to the player.\" However, emergent behavior is not limited to sophisticated games. In general, any place where event-driven instructions occur for AI in a game, emergent behavior will exist. For instance, take a racing game in which cars are programmed to avoid crashing, and they encounter an obstacle in the track: the cars might then maneuver to avoid the obstacle causing the cars behind them to slow or maneuver to accommodate the cars in front of them and the obstacle. The programmer never wrote code to specifically create a traffic jam, yet one now exists in the game.\nIntellectual property for video games.\nMost commonly, video games are protected by copyright, though both patents and trademarks have been used as well.\nThough local copyright regulations vary to the degree of protection, video games qualify as copyrighted visual-audio works, and enjoy cross-country protection under the Berne Convention. This typically only applies to the underlying code, as well as to the artistic aspects of the game such as its writing, art assets, and music. Gameplay itself is generally not considered copyrightable; in the United States among other countries, video games are considered to fall into the idea\u2013expression distinction in that it is how the game is presented and expressed to the player that can be copyrighted, but not the underlying principles of the game.\nBecause gameplay is normally ineligible for copyright, gameplay ideas in popular games are often replicated and built upon in other games. At times, this repurposing of gameplay can be seen as beneficial and a fundamental part of how the industry has grown by building on the ideas of others. For example \"Doom\" (1993) and \"Grand Theft Auto III\" (2001) introduced gameplay that created popular new game genres, the first-person shooter and the \"Grand Theft Auto\" clone, respectively, in the few years after their release. However, at times and more frequently at the onset of the industry, developers would intentionally create video game clones of successful games and game hardware with few changes, which led to the flooded arcade and dedicated home console market around 1978. Cloning is also a major issue with countries that do not have strong intellectual property protection laws, such as within China. The lax oversight by China's government and the difficulty for foreign companies to take Chinese entities to court had enabled China to support a large grey market of cloned hardware and software systems. The industry remains challenged to distinguish between creating new games based on refinements of past successful games to create a new type of gameplay, and intentionally creating a clone of a game that may simply swap out art assets.\nIndustry.\nHistory.\nThe early history of the video game industry, following the first game hardware releases and through 1983, had little structure. Video games quickly took off during the golden age of arcade video games from the late 1970s to early 1980s, but the newfound industry was mainly composed of game developers with little business experience. This led to numerous companies forming simply to create clones of popular games to try to capitalize on the market. Due to loss of publishing control and oversaturation of the market, the North American home video game market crashed in 1983, dropping from revenues of around $ in 1983 to $ by 1985. Many of the North American companies created in the prior years closed down. Japan's growing game industry was briefly shocked by this crash but had sufficient longevity to withstand the short-term effects, and Nintendo helped to revitalize the industry with the release of the Nintendo Entertainment System in North America in 1985. Along with it, Nintendo established a number of core industrial practices to prevent unlicensed game development and control game distribution on their platform, methods that continue to be used by console manufacturers today.\nThe industry remained more conservative following the 1983 crash, forming around the concept of publisher-developer dichotomies, and by the 2000s, leading to the industry centralizing around low-risk, triple-A games and studios with large development budgets of at least $ or more. The advent of the Internet brought digital distribution as a viable means to distribute games, and contributed to the growth of more riskier, experimental independent game development as an alternative to triple-A games in the late 2000s and which has continued to grow as a significant portion of the video game industry.\nIndustry roles.\nVideo games have a large network effect that draw on many different sectors that tie into the larger video game industry. While video game developers are a significant portion of the industry, other key participants in the market include:\nMajor regional markets.\nThe industry itself grew out from both the United States and Japan in the 1970s and 1980s before having a larger worldwide contribution. Today, the video game industry is predominantly led by major companies in North America (primarily the United States and Canada), Europe, and southeast Asia including Japan, South Korea, and China. Hardware production remains an area dominated by Asian companies either directly involved in hardware design or part of the production process, but digital distribution and indie game development of the late 2000s has allowed game developers to flourish nearly anywhere and diversify the field.\nGame sales.\nAccording to the market research firm Newzoo, the global video game industry drew estimated revenues of over $ in 2020. Mobile games accounted for the bulk of this, with a 48% share of the market, followed by console games at 28% and personal computer games at 23%.\nSales of different types of games vary widely between countries due to local preferences. Japanese consumers tend to purchase much more handheld games than console games and especially PC games, with a strong preference for games catering to local tastes. Another key difference is that, though having declined in the West, arcade games remain an important sector of the Japanese gaming industry. In South Korea, computer games are generally preferred over console games, especially MMORPG games and real-time strategy games. Computer games are also popular in China.\nEffects on society.\nCulture.\nVideo game culture is a worldwide new media subculture formed around video games and game playing. As computer and video games have increased in popularity over time, they have had a significant influence on popular culture. Video game culture has also evolved over time hand in hand with internet culture as well as the increasing popularity of mobile games. Many people who play video games identify as gamers, which can mean anything from someone who enjoys games to someone who is passionate about it. As video games become more social with multiplayer and online capability, gamers find themselves in growing social networks. Gaming can both be entertainment as well as competition, as a new trend known as electronic sports is becoming more widely accepted. In the 2010s, video games and discussions of video game trends and topics can be seen in social media, politics, television, film and music. The COVID-19 pandemic during 2020\u20132021 gave further visibility to video games as a pastime to enjoy with friends and family online as a means of social distancing.\nSince the mid-2000s there has been debate whether video games qualify as art, primarily as the form's interactivity interfered with the artistic intent of the work and that they are designed for commercial appeal. A significant debate on the matter came after film critic Roger Ebert published an essay \"Video Games can never be art\", which challenged the industry to prove him and other critics wrong. The view that video games were an art form was cemented in 2011 when the U.S. Supreme Court ruled in the landmark case \"Brown v. Entertainment Merchants Association\" that video games were a protected form of speech with artistic merit. Since then, video game developers have come to use the form more for artistic expression, including the development of art games, and the cultural heritage of video games as works of arts, beyond their technical capabilities, have been part of major museum exhibits, including \"The Art of Video Games\" at the Smithsonian American Art Museum and toured at other museums from 2012 to 2016.\nVideo games will inspire sequels and other video games within the same franchise, but also have influenced works outside of the video game medium. Numerous television shows (both animated and live-action), films, comics and novels have been created based on existing video game franchises. Because video games are an interactive medium there has been trouble in converting them to these passive forms of media, and typically such works have been critically panned or treated as children's media. For example, until 2019, no video game film had ever been received a \"Fresh\" rating on Rotten Tomatoes, but the releases of \"Detective Pikachu\" (2019) and \"Sonic the Hedgehog\" (2020), both receiving \"Fresh\" ratings, shows signs of the film industry having found an approach to adapt video games for the large screen. That said, some early video game-based films have been highly successful at the box office, such as 1995's \"Mortal Kombat\" and 2001's \"\".\nMore recently since the 2000s, there has also become a larger appreciation of video game music, which ranges from chiptunes composed for limited sound-output devices on early computers and consoles, to fully-scored compositions for most modern games. Such music has frequently served as a platform for covers and remixes, and concerts featuring video game soundtracks performed by bands or orchestras, such as \"Video Games Live\", have also become popular. Video games also frequently incorporate licensed music, particularly in the area of rhythm games, furthering the depth of which video games and music can work together.\nFurther, video games can serve as a virtual environment under full control of a producer to create new works. With the capability to render 3D actors and settings in real-time, a new type of work machinima (short for \"machine cinema\") grew out from using video game engines to craft narratives. As video game engines gain higher fidelity, they have also become part of the tools used in more traditional filmmaking. Unreal Engine has been used as a backbone by Industrial Light &amp; Magic for their StageCraft technology for shows like \"The Mandalorian\".\nSeparately, video games are also frequently used as part of the promotion and marketing for other media, such as for films, anime, and comics. However, these licensed games in the 1990s and 2000s often had a reputation for poor quality, developed without any input from the intellectual property rights owners, and several of them are considered among lists of games with notably negative reception, such as \"Superman 64\". More recently, with these licensed games being developed by triple-A studios or through studios directly connected to the licensed property owner, there has been a significant improvement in the quality of these games, with an early trendsetting example of \"\".\nBeneficial uses.\nBesides their entertainment value, appropriately-designed video games have been seen to provide value in education across several ages and comprehension levels. Learning principles found in video games have been identified as possible techniques with which to reform the U.S. education system. It has been noticed that gamers adopt an attitude while playing that is of such high concentration, they do not realize they are learning, and that if the same attitude could be adopted at school, education would enjoy significant benefits. Students are found to be \"learning by doing\" while playing video games while fostering creative thinking.\nVideo games are also believed to be beneficial to the mind and body. It has been shown that action video game players have better hand\u2013eye coordination and visuo-motor skills, such as their resistance to distraction, their sensitivity to information in the peripheral vision and their ability to count briefly presented objects, than nonplayers. Researchers found that such enhanced abilities could be acquired by training with action games, involving challenges that switch attention between different locations, but not with games requiring concentration on single objects. A 2018 systematic review found evidence that video gaming training had positive effects on cognitive and emotional skills in the adult population, especially with young adults. A 2019 systematic review also added support for the claim that video games are beneficial to the brain, although the beneficial effects of video gaming on the brain differed by video games types.\nOrganisers of video gaming events, such as the organisers of the \"D-Lux\" video game festival in Dumfries, Scotland, have emphasised the positive aspects video games can have on mental health. Organisers, mental health workers and mental health nurses at the event emphasised the relationships and friendships that can be built around video games and how playing games can help people learn about others as a precursor to discussing the person's mental health. A study in 2020 from Oxford University also suggested that playing video games can be a benefit to a person's mental health. The report of 3,274 gamers, all over the age of 18, focused on the games and and used actual play-time data. The report found that those that played more games tended to report greater \"wellbeing\". Also in 2020, computer science professor Regan Mandryk of the University of Saskatchewan said her research also showed that video games can have health benefits such as reducing stress and improving mental health. The university's research studied all age groups \u2013 \"from pre-literate children through to older adults living in long term care homes\" \u2013 with a main focus on 18 to 55-year-olds.\nA study of gamers attitudes towards gaming which was reported about in 2018 found that millennials use video games as a key strategy for coping with stress. In the study of 1,000 gamers, 55% said that it \"helps them to unwind and relieve stress ... and half said they see the value in gaming as a method of escapism to help them deal with daily work pressures\".\nControversies.\nVideo games have had controversy since the 1970s. Video games have emerged as one of the primary playthings used by young people all over the world. Parents and children's advocates have raised concerns that violent video games can influence young players into performing those violent acts in real life, and events such as the Columbine High School massacre in 1999 in which the perpetrators specifically alluded to using video games to plot out their attack, raised further fears. Medical experts and mental health professionals have also raised concerned that video games may be addictive, and the World Health Organization has included \"gaming disorder\" in the 11th revision of its International Statistical Classification of Diseases. Other health experts, including the American Psychiatric Association, have stated that there is insufficient evidence that video games can create violent tendencies or lead to addictive behavior, though agree that video games typically use a compulsion loop in their core design that can create dopamine that can help reinforce the desire to continue to play through that compulsion loop and potentially lead into violent or addictive behavior. Even with case law establishing that video games qualify as a protected art form, there has been pressure on the video game industry to keep their products in check to avoid over-excessive violence particularly for games aimed at younger children. The potential addictive behavior around games, coupled with increased used of post-sale monetization of video games, has also raised concern among parents, advocates, and government officials about gambling tendencies that may come from video games, such as controversy around the use of loot boxes in many high-profile games.\nNumerous other controversies around video games and its industry have arisen over the years, among the more notable incidents include the 1993 United States Congressional hearings on violent games like \"Mortal Kombat\" which lead to the formation of the ESRB ratings system, numerous legal actions taken by attorney Jack Thompson over violent games such as \"Grand Theft Auto III\" and \"Manhunt\" from 2003 to 2007, the outrage over the \"No Russian\" level from \"\" in 2009 which allowed the player to shoot a number of innocent non-player characters at an airport, and the Gamergate harassment campaign in 2014 that highlighted misogamy from a portion of the player demographic. The industry as a whole has also dealt with issues related to gender, racial, and LGBTQ+ discrimination and mischaracterization of these minority groups in video games. A further issue in the industry is related to working conditions, as development studios and publishers frequently use \"crunch time\", required extended working hours, in the weeks and months ahead of a game's release to assure on-time delivery.\nCollecting and preservation.\nPlayers of video games often maintain collections of games. More recently there has been interest in retrogaming, focusing on games from the first decades. Games in retail packaging in good shape have become collectors items for the early days of the industry, with some rare publications having gone for over US$ as of 2020[ [update]]. Separately, there is also concern about the preservation of video games, as both game media and the hardware to play them degrade over time. Further, many of the game developers and publishers from the first decades no longer exist, so records of their games have disappeared. Archivists and preservations have worked within the scope of copyright law to save these games as part of the cultural history of the industry.\nThere are many video game museums around the world, including the National Videogame Museum in Frisco, Texas, which serves as the largest museum wholly dedicated to the display and preservation of the industry's most important artifacts. Europe hosts video game museums such as the Computer Games Museum in Berlin and the Museum of Soviet Arcade Machines in Moscow and Saint-Petersburg. The Museum of Art and Digital Entertainment in Oakland, California is a dedicated video game museum focusing on playable exhibits of console and computer games. The Video Game Museum of Rome is also dedicated to preserving video games and their history. The International Center for the History of Electronic Games at The Strong in Rochester, New York contains one of the largest collections of electronic games and game-related historical materials in the world, including a exhibit which allows guests to play their way through the history of video games. The Smithsonian Institution in Washington, DC has three video games on permanent display: \"Pac-Man\", \"Dragon's Lair\", and \"Pong\".\nThe Museum of Modern Art has added a total of 20 video games and one video game console to its permanent Architecture and Design Collection since 2012. In 2012, the Smithsonian American Art Museum ran an exhibition on \"The Art of Video Games\". However, the reviews of the exhibit were mixed, including questioning whether video games belong in an art museum.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5365", "revid": "42316941", "url": "https://en.wikipedia.org/wiki?curid=5365", "title": "Christianity/Fish", "text": ""}
{"id": "5367", "revid": "2510232", "url": "https://en.wikipedia.org/wiki?curid=5367", "title": "Cambrian", "text": "First period of the Paleozoic Era, 539\u2013485 million years ago\nThe Cambrian Period ( ; sometimes symbolized \ua792) is the first geological period of the Paleozoic Era, and of the Phanerozoic Eon. The Cambrian lasted 53.4 million years from the end of the preceding Ediacaran Period 538.8 million years ago (mya) to the beginning of the Ordovician Period mya. Its subdivisions, and its base, are somewhat in flux. The period was established as \"Cambrian series\" by Adam Sedgwick, who named it after Cambria, the Latin name for 'Cymru' (Wales), where Britain's Cambrian rocks are best exposed. Sedgwick identified the layer as part of his task, along with Roderick Murchison, to subdivide the large \"Transition Series\", although the two geologists disagreed for a while on the appropriate categorization. The Cambrian is unique in its unusually high proportion of sedimentary deposits, sites of exceptional preservation where \"soft\" parts of organisms are preserved as well as their more resistant shells. As a result, our understanding of the Cambrian biology surpasses that of some later periods.\nThe Cambrian marked a profound change in life on Earth: prior to the Cambrian, the majority of living organisms on the whole were small, unicellular and simple (Ediacaran fauna and earlier Tonian Huainan biota being notable exceptions). Complex, multicellular organisms gradually became more common in the millions of years immediately preceding the Cambrian, but it was not until this period that mineralized \u2013 hence readily fossilized \u2013 organisms became common. The rapid diversification of lifeforms in the Cambrian, known as the Cambrian explosion, produced the first representatives of all modern animal phyla. Phylogenetic analysis has supported the view that before the Cambrian radiation, in the Cryogenian or Tonian, animals (metazoans) evolved monophyletically from a single common ancestor: flagellated colonial protists similar to modern choanoflagellates.\nAlthough diverse life forms prospered in the oceans, the land is thought to have been comparatively barren \u2013 with nothing more complex than a microbial soil crust and a few molluscs and arthropods (albeit not terrestrial) that emerged to browse on the microbial biofilm. By the end of the Cambrian, myriapods, arachnids, and hexapods started adapting to the land, along with the first plants. Most of the continents were probably dry and rocky due to a lack of vegetation. Shallow seas flanked the margins of several continents created during the breakup of the supercontinent Pannotia. The seas were relatively warm, and polar ice was absent for much of the period.\nStratigraphy.\nThe Cambrian Period followed the Ediacaran Period and was followed by the Ordovician Period.\nThe base of the Cambrian lies atop a complex assemblage of trace fossils known as the \"Treptichnus pedum\" assemblage.\nThe use of \"Treptichnus pedum\", a reference ichnofossil to mark the lower boundary of the Cambrian, is problematic because very similar trace fossils belonging to the Treptichnids group are found well below \"T. pedum\" in Namibia, Spain and Newfoundland, and possibly in the western US. The stratigraphic range of \"T. pedum\" overlaps the range of the Ediacaran fossils in Namibia, and probably in Spain.\nSubdivisions.\nThe Cambrian is divided into four epochs (series) and ten ages (stages). Currently only three series and six stages are named and have a GSSP (an internationally agreed-upon stratigraphic reference point).\nBecause the international stratigraphic subdivision is not yet complete, many local subdivisions are still widely used. In some of these subdivisions the Cambrian is divided into three epochs with locally differing names\u00a0\u2013 the Early Cambrian (Caerfai or Waucoban, {{}}: Error! the period you specified has not been recognised! This could be because you mis-spelt it, or because it is missing from or . mya), Middle Cambrian (St Davids or Albertan, {{}}: Error! the period you specified has not been recognised! This could be because you mis-spelt it, or because it is missing from or . mya) and Late Cambrian ({{}}: Error! the period you specified has not been recognised! This could be because you mis-spelt it, or because it is missing from or . mya; also known as Merioneth or Croixan). \nTrilobite zones allow biostratigraphic correlation in the Cambrian. Rocks of these epochs are referred to as belonging to the Lower, Middle, or Upper Cambrian.\nEach of the local series is divided into several stages. The Cambrian is divided into several regional faunal stages of which the Russian-Kazakhian system is most used in international parlance:\nDating the Cambrian.\nThe International Commission on Stratigraphy lists the Cambrian Period as beginning at \u00a0million years ago and ending at \u00a0million years ago.\nThe lower boundary of the Cambrian was originally held to represent the first appearance of complex life, represented by trilobites. The recognition of small shelly fossils before the first trilobites, and Ediacara biota substantially earlier, led to calls for a more precisely defined base to the Cambrian Period.\nDespite the long recognition of its distinction from younger Ordovician rocks and older Precambrian rocks, it was not until 1994 that the Cambrian system/period was internationally ratified. After decades of careful consideration, a continuous sedimentary sequence at Fortune Head, Newfoundland was settled upon as a formal base of the Cambrian Period, which was to be correlated worldwide by the earliest appearance of \"Treptichnus pedum\". Discovery of this fossil a few metres below the GSSP led to the refinement of this statement, and it is the \"T. pedum\" ichnofossil assemblage that is now formally used to correlate the base of the Cambrian.\nThis formal designation allowed radiometric dates to be obtained from samples across the globe that corresponded to the base of the Cambrian. Early dates of \u00a0million years ago quickly gained favour, though the methods used to obtain this number are now considered to be unsuitable and inaccurate. A more precise date using modern radiometric dating yield a date of \u00a0\u00b1\u00a00.2\u00a0million years ago. The ash horizon in Oman from which this date was recovered corresponds to a marked fall in the abundance of carbon-13 that correlates to equivalent excursions elsewhere in the world, and to the disappearance of distinctive Ediacaran fossils (\"Namacalathus\", \"Cloudina\"). Nevertheless, there are arguments that the dated horizon in Oman does not correspond to the Ediacaran-Cambrian boundary, but represents a facies change from marine to evaporite-dominated strata \u2013 which would mean that dates from other sections, ranging from 544 or 542 Ma, are more suitable.\nPaleogeography.\nPlate reconstructions suggest a global supercontinent, Pannotia, was in the process of breaking up early in the Cambrian, with Laurentia (North America), Baltica, and Siberia having separated from the main supercontinent of Gondwana to form isolated land masses. Most continental land was clustered in the Southern Hemisphere at this time, but was drifting north. Large, high-velocity rotational movement of Gondwana appears to have occurred in the Early Cambrian.\nWith a lack of sea ice\u00a0\u2013 the great glaciers of the Marinoan Snowball Earth were long melted\u00a0\u2013 the sea level was high, which led to large areas of the continents being flooded in warm, shallow seas ideal for sea life. The sea levels fluctuated somewhat, suggesting there were \"ice ages\", associated with pulses of expansion and contraction of a south polar ice cap.\nIn Baltoscandia a Lower Cambrian transgression transformed large swathes of the Sub-Cambrian peneplain into an epicontinental sea.\nClimate.\nGlaciers likely existed during the earliest Cambrian at high and possibly even at middle palaeolatitudes, possibly due to the ancient continent of Gondwana covering the South Pole and cutting off polar ocean currents. Middle Terreneuvian deposits, corresponding to the boundary between the Fortunian and Stage 2, show evidence of glaciation. However, other authors believe these very early, pretrilobitic glacial deposits may not even be of Cambrian age at all but instead date back to the Neoproterozoic, an era characterised by numerous severe icehouse periods. The beginning of Stage 3 was relatively cool, with the period between 521 and 517 Ma being known as the Cambrian Arthropod Radiation Cool Event (CARCE). The Earth was generally very warm during Stage 4; its climate was comparable to the hot greenhouse of the Late Cretaceous and Early Palaeogene, as evidenced by a maximum in continental weathering rates over the last 900 million years and the presence of tropical, lateritic palaeosols at high palaeolatitudes during this time. The Archaecyathid Extinction Warm Event (AEWE), lasting from 511 to 510.5 Ma, was particularly warm. Another warm event, the Redlichiid-Olenid Extinction Warm Event, occurred at the beginning of the Wuliuan. It became even warmer towards the end of the period, and sea levels rose dramatically. This warming trend continued into the Early Ordovician, the start of which was characterised by an extremely hot global climate.\nFlora.\nThe Cambrian flora was little different from the Ediacaran. The principal taxa were the marine macroalgae \"Fuxianospira\", \"Sinocylindra\", and \"Marpolia\". No calcareous macroalgae are known from the period.\nNo land plant (embryophyte) fossils are known from the Cambrian. However, biofilms and microbial mats were well developed on Cambrian tidal flats and beaches 500 mya, and microbes forming microbial Earth ecosystems, comparable with modern soil crust of desert regions, contributing to soil formation. Although molecular clock estimates suggest terrestrial plants may have first emerged during the Middle or Late Cambrian, the consequent large-scale removal of the greenhouse gas CO2 from the atmosphere through sequestration did not begin until the Ordovician.\nOceanic life.\nThe Cambrian explosion was a period of rapid multicellular growth. Most animal life during the Cambrian was aquatic. Trilobites were once assumed to be the dominant life form at that time, but this has proven to be incorrect. Arthropods were by far the most dominant animals in the ocean, but trilobites were only a minor part of the total arthropod diversity. What made them so apparently abundant was their heavy armor reinforced by calcium carbonate (CaCO3), which fossilized far more easily than the fragile chitinous exoskeletons of other arthropods, leaving numerous preserved remains.\nThe period marked a steep change in the diversity and composition of Earth's biosphere. The Ediacaran biota suffered a mass extinction at the start of the Cambrian Period, which corresponded with an increase in the abundance and complexity of burrowing behaviour. This behaviour had a profound and irreversible effect on the substrate which transformed the seabed ecosystems. Before the Cambrian, the sea floor was covered by microbial mats. By the end of the Cambrian, burrowing animals had destroyed the mats in many areas through bioturbation. As a consequence, many of those organisms that were dependent on the mats became extinct, while the other species adapted to the changed environment that now offered new ecological niches. Around the same time there was a seemingly rapid appearance of representatives of all the mineralized phyla, including the Bryozoa, which were once thought to have only appeared in the Lower Ordovician. However, many of those phyla were represented only by stem-group forms; and since mineralized phyla generally have a benthic origin, they may not be a good proxy for (more abundant) non-mineralized phyla.\nWhile the early Cambrian showed such diversification that it has been named the Cambrian Explosion, this changed later in the period, when there occurred a sharp drop in biodiversity. About 515 million years ago, the number of species going extinct exceeded the number of new species appearing. Five million years later, the number of genera had dropped from an earlier peak of about 600 to just 450. Also, the speciation rate in many groups was reduced to between a fifth and a third of previous levels. 500 million years ago, oxygen levels fell dramatically in the oceans, leading to hypoxia, while the level of poisonous hydrogen sulfide simultaneously increased, causing another extinction. The later half of Cambrian was surprisingly barren and showed evidence of several rapid extinction events; the stromatolites which had been replaced by reef building sponges known as Archaeocyatha, returned once more as the archaeocyathids became extinct. This declining trend did not change until the Great Ordovician Biodiversification Event.\nSome Cambrian organisms ventured onto land, producing the trace fossils \"Protichnites\" and \"Climactichnites\". Fossil evidence suggests that euthycarcinoids, an extinct group of arthropods, produced at least some of the \"Protichnites\". Fossils of the track-maker of \"Climactichnites\" have not been found; however, fossil trackways and resting traces suggest a large, slug-like mollusc.\nIn contrast to later periods, the Cambrian fauna was somewhat restricted; free-floating organisms were rare, with the majority living on or close to the sea floor; and mineralizing animals were rarer than in future periods, in part due to the unfavourable ocean chemistry.\nMany modes of preservation are unique to the Cambrian, and some preserve soft body parts, resulting in an abundance of . These include Sirius Passet, the Sinsk Algal Lens, the Maotianshan Shales, the Emu Bay Shale, and the Burgess Shale.\nSymbol.\nThe United States Federal Geographic Data Committee uses a \"barred capital C\" \u27e8\ua792\u27e9 character to represent the Cambrian Period.\nThe Unicode character is .\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5369", "revid": "3810835", "url": "https://en.wikipedia.org/wiki?curid=5369", "title": "Contraception", "text": ""}
{"id": "5370", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5370", "title": "Theory of categories", "text": "In ontology, the highest kinds or genera of entities\nIn ontology, the theory of categories concerns itself with the \"categories of being\": the highest \"genera\" or \"kinds of entities\" according to Amie Thomasson. To investigate the categories of being, or simply categories, is to determine the most fundamental and the broadest classes of entities. A distinction between such categories, in making the categories or applying them, is called an ontological distinction. Various systems of categories have been proposed, they often include categories for substances, properties, relations, states of affairs or events. A representative question within the theory of categories might articulate itself, for example, in a query like, \"Are universals prior to particulars?\"\nEarly development.\nThe process of abstraction required to discover the number and names of the categories of being has been undertaken by many philosophers since Aristotle and involves the careful inspection of each concept to ensure that there is no higher category or categories under which that concept could be subsumed. The scholars of the twelfth and thirteenth centuries developed Aristotle's ideas. For example, Gilbert of Poitiers divides Aristotle's ten categories into two sets, primary and secondary, according to whether they inhere in the subject or not:\nFurthermore, following Porphyry\u2019s likening of the classificatory hierarchy to a tree, they concluded that the major classes could be subdivided to form subclasses, for example, Substance could be divided into Genus and Species, and Quality could be subdivided into Property and Accident, depending on whether the property was necessary or contingent. An alternative line of development was taken by Plotinus in the second century who by a process of abstraction reduced Aristotle's list of ten categories to five: Substance, Relation, Quantity, Motion and Quality. Plotinus further suggested that the latter three categories of his list, namely Quantity, Motion and Quality correspond to three different kinds of relation and that these three categories could therefore be subsumed under the category of Relation. This was to lead to the supposition that there were only two categories at the top of the hierarchical tree, namely Substance and Relation. Many supposed that relations only exist in the mind. Substance and Relation, then, are closely commutative with Mind and Matter--this is expressed most clearly in the dualism of Ren\u00e9 Descartes.\nAristotle.\nOne of Aristotle\u2019s early interests lay in the classification of the natural world, how for example the genus \"animal\" could be first divided into \"two-footed animal\" and then into \"wingless, two-footed animal\". He realised that the distinctions were being made according to the qualities the animal possesses, the quantity of its parts and the kind of motion that it exhibits. To fully complete the proposition \"this animal is\u00a0...\" Aristotle stated in his work on the Categories that there were ten kinds of predicate where\u00a0...\n\"...\u00a0each signifies either substance or quantity or quality or relation or where or when or being-in-a-position or having or acting or being acted upon\".\nHe realised that predicates could be simple or complex. The simple kinds consist of a subject and a predicate linked together by the \"categorical\" or inherent type of relation. For Aristotle the more complex kinds were limited to propositions where the predicate is compounded of two of the above categories for example \"this is a horse running\". More complex kinds of proposition were only discovered after Aristotle by the Stoic, Chrysippus, who developed the \"hypothetical\" and \"disjunctive\" types of syllogism and these were terms which were to be developed through the Middle Ages and were to reappear in Kant's system of categories.\n\"Category\" came into use with Aristotle's essay \"Categories\", in which he discussed univocal and equivocal terms, predication, and ten categories:\nPlotinus.\nPlotinus in writing his \"Enneads\" around AD 250 recorded that \"philosophy at a very early age investigated the number and character of the existents\u00a0... some found ten, others less\u00a0... to some the genera were the first principles, to others only a generic classification of existents\". He realised that some categories were reducible to others saying \"why are not Beauty, Goodness and the virtues, Knowledge and Intelligence included among the primary genera?\" He concluded that such transcendental categories and even the categories of Aristotle were in some way posterior to the three Eleatic categories first recorded in Plato's dialogue \"Parmenides\" and which comprised the following three coupled terms: \nPlotinus called these \"the hearth of reality\" deriving from them not only the three categories of Quantity, Motion and Quality but also what came to be known as \"the three moments of the Neoplatonic world process\":\nPlotinus likened the three to the centre, the radii and the circumference of a circle, and clearly thought that the principles underlying the categories were the first principles of creation. \"From a single root all being multiplies\". Similar ideas were to be introduced into Early Christian thought by, for example, Gregory of Nazianzus who summed it up saying \"Therefore, Unity, having from all eternity arrived by motion at duality, came to rest in trinity\".\nModern development.\nKant and Hegel accused the Aristotelian table of categories of being 'rhapsodic', derived arbitrarily and in bulk from experience, without any systematic necessity.\nThe early modern dualism, which has been described above, of Mind and Matter or Subject and Relation, as reflected in the writings of Descartes underwent a substantial revision in the late 18th century. The first objections to this stance were formulated in the eighteenth century by Immanuel Kant who realised that we can say nothing about Substance except through the relation of the subject to other things. \nFor example: In the sentence \"This is a house\" the substantive subject \"house\" only gains meaning in relation to human use patterns or to other similar houses. The category of Substance disappears from Kant's tables, and under the heading of Relation, Kant lists \"inter alia\" the three relationship types of Disjunction, Causality and Inherence. The three older concepts of Quantity, Motion and Quality, as Peirce discovered, could be subsumed under these three broader headings in that Quantity relates to the subject through the relation of Disjunction; Motion relates to the subject through the relation of Causality; and Quality relates to the subject through the relation of Inherence. Sets of three continued to play an important part in the nineteenth century development of the categories, most notably in G.W.F. Hegel's extensive tabulation of categories, and in C.S. Peirce's categories set out in his work on the logic of relations. One of Peirce's contributions was to call the three primary categories Firstness, Secondness and Thirdness which both emphasises their general nature, and avoids the confusion of having the same name for both the category itself and for a concept within that category.\nIn a separate development, and building on the notion of primary and secondary categories introduced by the Scholastics, Kant introduced the idea that secondary or \"derivative\" categories could be derived from the primary categories through the combination of one primary category with another. This would result in the formation of three secondary categories: the first, \"Community\" was an example that Kant gave of such a derivative category; the second, \"Modality\", introduced by Kant, was a term which Hegel, in developing Kant's dialectical method, showed could also be seen as a derivative category; and the third, \"Spirit\" or \"Will\" were terms that Hegel and Schopenhauer were developing separately for use in their own systems. Karl Jaspers in the twentieth century, in his development of existential categories, brought the three together, allowing for differences in terminology, as Substantiality, Communication and Will. This pattern of three primary and three secondary categories was used most notably in the nineteenth century by Peter Mark Roget to form the six headings of his Thesaurus of English Words and Phrases. The headings used were the three objective categories of Abstract Relation, Space (including Motion) and Matter and the three subjective categories of Intellect, Feeling and Volition, and he found that under these six headings all the words of the English language, and hence any possible predicate, could be assembled.\nKant.\nIn the \"Critique of Pure Reason\" (1781), Immanuel Kant argued that the categories are part of our own mental structure and consist of a set of \"a priori\" concepts through which we interpret the world around us. These concepts correspond to twelve logical functions of the understanding which we use to make judgements and there are therefore two tables given in the \"Critique\", one of the Judgements and a corresponding one for the Categories. To give an example, the logical function behind our reasoning from ground to consequence (based on the Hypothetical relation) underlies our understanding of the world in terms of cause and effect (the Causal relation). In each table the number twelve arises from, firstly, an initial division into two: the Mathematical and the Dynamical; a second division of each of these headings into a further two: Quantity and Quality, and Relation and Modality respectively; and, thirdly, each of these then divides into a further three subheadings as follows.\nCriticism of Kant's system followed, firstly, by Arthur Schopenhauer, who amongst other things was unhappy with the term \"Community\", and declared that the tables \"do open violence to truth, treating it as nature was treated by old-fashioned gardeners\", and secondly, by W.T.Stace who in his book \"The Philosophy of Hegel\" suggested that in order to make Kant's structure completely symmetrical a third category would need to be added to the Mathematical and the Dynamical. This, he said, Hegel was to do with his category of concept.\nHegel.\nG.W.F. Hegel in his \"Science of Logic\" (1812) attempted to provide a more comprehensive system of categories than Kant and developed a structure that was almost entirely triadic. So important were the categories to Hegel that he claimed the first principle of the world, which he called the \"absolute\", is \"a system of categories\u00a0... the categories must be the reason of which the world is a consequent\".\nUsing his own logical method of sublation, later called the Hegelian dialectic, reasoning from the abstract through the negative to the concrete, he arrived at a hierarchy of some 270 categories, as explained by W.\u00a0T.\u00a0Stace. The three very highest categories were \"logic\", \"nature\" and \"spirit\". The three highest categories of \"logic\", however, he called \"being\", \"essence\", and \"notion\" which he explained as follows:\nSchopenhauer's category that corresponded with \"notion\" was that of \"idea\", which in his \"Four-Fold Root of Sufficient Reason\" he complemented with the category of the \"will\". The title of his major work was \"The World as Will and Idea\". The two other complementary categories, reflecting one of Hegel's initial divisions, were those of Being and Becoming. At around the same time, Goethe was developing his colour theories in the of 1810, and introduced similar principles of combination and complementation, symbolising, for Goethe, \"the primordial relations which belong both to nature and vision\". Hegel in his \"Science of Logic\" accordingly asks us to see his system not as a tree but as a circle.\nTwentieth-century development.\nIn the twentieth century the primacy of the division between the subjective and the objective, or between mind and matter, was disputed by, among others, Bertrand Russell and Gilbert Ryle. Philosophy began to move away from the metaphysics of categorisation towards the linguistic problem of trying to differentiate between, and define, the words being used. Ludwig Wittgenstein\u2019s conclusion was that there were no clear definitions which we can give to words and categories but only a \"halo\" or \"corona\" of related meanings radiating around each term. Gilbert Ryle thought the problem could be seen in terms of dealing with \"a galaxy of ideas\" rather than a single idea, and suggested that category mistakes are made when a concept (e.g. \"university\"), understood as falling under one category (e.g. abstract idea), is used as though it falls under another (e.g. physical object). With regard to the visual analogies being used, Peirce and Lewis, just like Plotinus earlier, likened the terms of propositions to points, and the relations between the terms to lines. Peirce, taking this further, talked of univalent, bivalent and trivalent relations linking predicates to their subject and it is just the number and types of relation linking subject and predicate that determine the category into which a predicate might fall. Primary categories contain concepts where there is one dominant kind of relation to the subject. Secondary categories contain concepts where there are two dominant kinds of relation. Examples of the latter were given by Heidegger in his two propositions \"the house is on the creek\" where the two dominant relations are spatial location (Disjunction) and cultural association (Inherence), and \"the house is eighteenth century\" where the two relations are temporal location (Causality) and cultural quality (Inherence). A third example may be inferred from Kant in the proposition \"the house is impressive or sublime\" where the two relations are spatial or mathematical disposition (Disjunction) and dynamic or motive power (Causality). Both Peirce and Wittgenstein introduced the analogy of colour theory in order to illustrate the shades of meanings of words. Primary categories, like primary colours, are analytical representing the furthest we can go in terms of analysis and abstraction and include Quantity, Motion and Quality. Secondary categories, like secondary colours, are synthetic and include concepts such as Substance, Community and Spirit. \nApart from these, the categorial scheme of Alfred North Whitehead and his Process Philosophy, alongside Nicolai Hartmann and his Critical Realism, remain one of the most detailed and advanced systems in categorial research in metaphysics.\nPeirce.\nCharles Sanders Peirce, who had read Kant and Hegel closely, and who also had some knowledge of Aristotle, proposed a system of merely three phenomenological categories: Firstness, Secondness, and Thirdness, which he repeatedly invoked in his subsequent writings. Like Hegel, C.S. Peirce attempted to develop a system of categories from a single indisputable principle, in Peirce's case the notion that in the first instance he could only be aware of his own ideas. \n \"It seems that the true categories of consciousness are first, feeling\u00a0... second, a sense of resistance\u00a0... and third, synthetic consciousness, or thought\".\n Elsewhere he called the three primary categories: Quality, Reaction and Meaning, and even Firstness, Secondness and Thirdness, saying, \"perhaps it is not right to call these categories conceptions, they are so intangible that they are rather tones or tints upon conceptions\": \nAlthough Peirce's three categories correspond to the three concepts of relation given in Kant's tables, the sequence is now reversed and follows that given by Hegel, and indeed before Hegel of the three moments of the world-process given by Plotinus. Later, Peirce gave a mathematical reason for there being three categories in that although monadic, dyadic and triadic nodes are irreducible, every node of a higher valency is reducible to a \"compound of triadic relations\". Ferdinand de Saussure, who was developing \"semiology\" in France just as Peirce was developing \"semiotics\" in the US, likened each term of a proposition to \"the centre of a constellation, the point where other coordinate terms, the sum of which is indefinite, converge\".\nOthers.\nEdmund Husserl (1962, 2000) wrote extensively about categorial systems as part of his phenomenology.\nFor Gilbert Ryle (1949), a category (in particular a \"category mistake\") is an important semantic concept, but one having only loose affinities to an ontological category.\nContemporary systems of categories have been proposed by John G. Bennett (The Dramatic Universe, 4 vols., 1956\u201365), Wilfrid Sellars (1974), Reinhardt Grossmann (1983, 1992), Johansson (1989), Hoffman and Rosenkrantz (1994), Roderick Chisholm (1996), Barry Smith (ontologist) (2003), and Jonathan Lowe (2006).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5371", "revid": "33625371", "url": "https://en.wikipedia.org/wiki?curid=5371", "title": "Concrete", "text": "Composite construction material\nConcrete is a composite material composed of aggregate bonded together with a fluid cement that cures over time. Concrete is the second-most-used substance in the world after water, and is the most widely used building material. Its usage worldwide, ton for ton, is twice that of steel, wood, plastics, and aluminium combined.\nWhen aggregate is mixed with dry Portland cement and water, the mixture forms a fluid slurry that is easily poured and molded into shape. The cement reacts with the water through a process called concrete hydration that hardens it over several hours to form a hard matrix that binds the materials together into a durable stone-like material that has many uses. This time allows concrete to not only be cast in forms, but also to have a variety of tooled processes preformed. The hydration process is exothermic, which means ambient temperature plays a significant role in how long it takes concrete to set. Often, additives (such as pozzolans or superplasticizers) are included in the mixture to improve the physical properties of the wet mix, delay or accelerate the curing time, or otherwise change the finished material. Most concrete is poured with reinforcing materials (such as steel rebar) embedded to provide tensile strength, yielding reinforced concrete.\nIn the past, lime based cement binders, such as lime putty, were often used but sometimes with other hydraulic cements, (water resistant) such as a calcium aluminate cement or with Portland cement to form Portland cement concrete (named for its visual resemblance to Portland stone). Many other non-cementitious types of concrete exist with other methods of binding aggregate together, including asphalt concrete with a bitumen binder, which is frequently used for road surfaces, and polymer concretes that use polymers as a binder. Concrete is distinct from mortar. Whereas concrete is itself a building material, mortar is a bonding agent that typically holds bricks, tiles and other masonry units together. Grout is another material associated with concrete and cement. It does not contain coarse aggregates and is usually either pourable or thixotropic, and is used to fill gaps between masonry components or coarse aggregate which has already been put in place. Some methods of concrete manufacture and repair involve pumping grout into the gaps to make up a solid mass \"in situ\".\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nEtymology.\nThe word concrete comes from the Latin word \"\"concretus\" (meaning compact or condensed), the perfect passive participle of \"concrescere\", from \"con\"-\" (together) and \"crescere\" (to grow).\nHistory.\nAncient times.\nMayan concrete at the ruins of Uxmal (850-925 A.D.) is referenced in \"Incidents of Travel in the Yucat\u00e1n\" by John L. Stephens. \"The roof is flat and had been covered with cement\". \"The floors were cement, in some places hard, but, by long exposure, broken, and now crumbling under the feet.\" \"But throughout the wall was solid, and consisting of large stones imbedded in mortar, almost as hard as rock.\"\nSmall-scale production of concrete-like materials was pioneered by the Nabatean traders who occupied and controlled a series of oases and developed a small empire in the regions of southern Syria and northern Jordan from the 4th century BC. They discovered the advantages of hydraulic lime, with some self-cementing properties, by 700 BC. They built kilns to supply mortar for the construction of rubble masonry houses, concrete floors, and underground waterproof cisterns. They kept the cisterns secret as these enabled the Nabataeans to thrive in the desert. Some of these structures survive to this day.\nClassical era.\nIn the Ancient Egyptian and later Roman eras, builders discovered that adding volcanic ash to the mix allowed it to set underwater.\nConcrete floors were found in the royal palace of Tiryns, Greece, which dates roughly to 1400-1200 BC. Lime mortars were used in Greece, Crete, and Cyprus in 800 BC. The Assyrian Jerwan Aqueduct (688 BC) made use of waterproof concrete. Concrete was used for construction in many ancient structures.\nThe Romans used concrete extensively from 300 BC to 476 AD. During the Roman Empire, Roman concrete (or \"opus caementicium\") was made from quicklime, pozzolana and an aggregate of pumice. Its widespread use in many Roman structures, a key event in the history of architecture termed the Roman architectural revolution, freed Roman construction from the restrictions of stone and brick materials. It enabled revolutionary new designs in terms of both structural complexity and dimension. The Colosseum in Rome was built largely of concrete, and the Pantheon has the world's largest unreinforced concrete dome.\nConcrete, as the Romans knew it, was a new and revolutionary material. Laid in the shape of arches, vaults and domes, it quickly hardened into a rigid mass, free from many of the internal thrusts and strains that troubled the builders of similar structures in stone or brick.\nModern tests show that \"opus caementicium\" had as much compressive strength as modern Portland-cement concrete (ca. ). However, due to the absence of reinforcement, its tensile strength was far lower than modern reinforced concrete, and its mode of application also differed:\nModern structural concrete differs from Roman concrete in two important details. First, its mix consistency is fluid and homogeneous, allowing it to be poured into forms rather than requiring hand-layering together with the placement of aggregate, which, in Roman practice, often consisted of rubble. Second, integral reinforcing steel gives modern concrete assemblies great strength in tension, whereas Roman concrete could depend only upon the strength of the concrete bonding to resist tension.\nThe long-term durability of Roman concrete structures has been found to be due to its use of pyroclastic (volcanic) rock and ash, whereby the crystallization of str\u00e4tlingite (a specific and complex calcium aluminosilicate hydrate) and the coalescence of this and similar calcium\u2013aluminium-silicate\u2013hydrate cementing binders helped give the concrete a greater degree of fracture resistance even in seismically active environments. Roman concrete is significantly more resistant to erosion by seawater than modern concrete; it used pyroclastic materials which react with seawater to form Al-tobermorite crystals over time.\nThe widespread use of concrete in many Roman structures ensured that many survive to the present day. The Baths of Caracalla in Rome are just one example. Many Roman aqueducts and bridges, such as the magnificent Pont du Gard in southern France, have masonry cladding on a concrete core, as does the dome of the Pantheon.\nMiddle Ages.\nAfter the Roman Empire, the use of burned lime and pozzolana was greatly reduced. Low kiln temperatures in the burning of lime, lack of pozzolana, and poor mixing all contributed to a decline in the quality of concrete and mortar. From the 11th century, the increased use of stone in church and castle construction led to an increased demand for mortar. Quality began to improve in the 12th century through better grinding and sieving. Medieval lime mortars and concretes were non-hydraulic and were used for binding masonry, \"hearting\" (binding rubble masonry cores) and foundations. Bartholomaeus Anglicus in his \"De proprietatibus rerum\" (1240) describes the making of mortar. In an English translation from 1397, it reads \"lyme ... is a stone brent; by medlynge thereof with sonde and water sement is made\". From the 14th century, the quality of mortar was again excellent, but only from the 17th century was pozzolana commonly added.\nThe \"Canal du Midi\" was built using concrete in 1670.\nIndustrial era.\nPerhaps the greatest step forward in the modern use of concrete was Smeaton's Tower, built by British engineer John Smeaton in Devon, England, between 1756 and 1759. This third Eddystone Lighthouse pioneered the use of hydraulic lime in concrete, using pebbles and powdered brick as aggregate.\nA method for producing Portland cement was developed in England and patented by Joseph Aspdin in 1824. Aspdin chose the name for its similarity to Portland stone, which was quarried on the Isle of Portland in Dorset, England. His son William continued developments into the 1840s, earning him recognition for the development of \"modern\" Portland cement.\nReinforced concrete was invented in 1849 by Joseph Monier. and the first reinforced concrete house was built by Fran\u00e7ois Coignet in 1853.\nThe first concrete reinforced bridge was designed and built by Joseph Monier in 1875.\nComposition.\nConcrete is an artificial composite material, comprising a matrix of cementitious binder (typically Portland cement paste or asphalt) and a dispersed phase or \"filler\" of aggregate (typically a rocky material, loose stones, and sand). The binder \"glues\" the filler together to form a synthetic conglomerate. Many types of concrete are available, determined by the formulations of binders and the types of aggregate used to suit the application of the engineered material. These variables determine strength and density, as well as chemical and thermal resistance of the finished product.\nAggregates consist of large chunks of material in a concrete mix, generally a coarse gravel or crushed rocks such as limestone, or granite, along with finer materials such as sand.\nCement paste, most commonly made of Portland cement, is the most prevalent kind of concrete binder. For cementitious binders, water is mixed with the dry cement powder and aggregate, which produces a semi-liquid slurry (paste) that can be shaped, typically by pouring it into a form. The concrete solidifies and hardens through a chemical process called hydration. The water reacts with the cement, which bonds the other components together, creating a robust, stone-like material. Other cementitious materials, such as fly ash and slag cement, are sometimes added\u2014either pre-blended with the cement or directly as a concrete component\u2014and become a part of the binder for the aggregate. Fly ash and slag can enhance some properties of concrete such as fresh properties and durability. Alternatively, other materials can also be used as a concrete binder: the most prevalent substitute is asphalt, which is used as the binder in asphalt concrete.\nAdmixtures are added to modify the cure rate or properties of the material. Mineral admixtures use recycled materials as concrete ingredients. Conspicuous materials include fly ash, a by-product of coal-fired power plants; ground granulated blast furnace slag, a by-product of steelmaking; and silica fume, a by-product of industrial electric arc furnaces.\nStructures employing Portland cement concrete usually include steel reinforcement because this type of concrete can be formulated with high compressive strength, but always has lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension, typically steel rebar.\nThe \"mix design\" depends on the type of structure being built, how the concrete is mixed and delivered, and how it is placed to form the structure.\nCement.\nPortland cement is the most common type of cement in general usage. It is a basic ingredient of concrete, mortar, and many plasters. British masonry worker Joseph Aspdin patented Portland cement in 1824. It was named because of the similarity of its color to Portland limestone, quarried from the English Isle of Portland and used extensively in London architecture. It consists of a mixture of calcium silicates (alite, belite), aluminates and ferrites\u2014compounds which combine calcium, silicon, aluminium and iron in forms which will react with water. Portland cement and similar materials are made by heating limestone (a source of calcium) with clay or shale (a source of silicon, aluminium and iron) and grinding this product (called \"clinker\") with a source of sulfate (most commonly gypsum).\nIn modern cement kilns, many advanced features are used to lower the fuel consumption per ton of clinker produced. Cement kilns are extremely large, complex, and inherently dusty industrial installations, and have emissions which must be controlled. Of the various ingredients used to produce a given quantity of concrete, the cement is the most energetically expensive. Even complex and efficient kilns require 3.3 to 3.6 gigajoules of energy to produce a ton of clinker and then grind it into cement. Many kilns can be fueled with difficult-to-dispose-of wastes, the most common being used tires. The extremely high temperatures and long periods of time at those temperatures allows cement kilns to efficiently and completely burn even difficult-to-use fuels.\nWater.\nCombining water with a cementitious material forms a cement paste by the process of hydration. The cement paste glues the aggregate together, fills voids within it, and makes it flow more freely.\nAs stated by Abrams' law, a lower water-to-cement ratio yields a stronger, more durable concrete, whereas more water gives a freer-flowing concrete with a higher slump. Impure water used to make concrete can cause problems when setting or in causing premature failure of the structure.\nPortland cement consists of five major compounds of calcium silicates and aluminates ranging from 5 to 50% in weight, which all undergo hydration to contribute to final material's strength. Thus, the hydration of cement involves many reactions, often occurring at the same time. As the reactions proceed, the products of the cement hydration process gradually bond together the individual sand and gravel particles and other components of the concrete to form a solid mass.\nCement chemist notation: C3S + H \u2192 C-S-H + CH + heat\nStandard notation: Ca3SiO5 + H2O \u2192 (CaO)\u00b7(SiO2)\u00b7(H2O)(gel) + Ca(OH)2\nBalanced: 2Ca3SiO5 + 7H2O \u2192 3(CaO)\u00b72(SiO2)\u00b74(H2O)(gel) + 3Ca(OH)2 (approximately; the exact ratios of the CaO, SiO2 and H2O in C-S-H can vary)\nHydration of tricalcium silicate.\nDue to the nature of the chemical bonds created in these reactions and the final characteristics of the particles formed, the process of cement hydration is considered irreversible, which makes methods of cement recycling prohibitive.\nAggregates.\nFine and coarse aggregates make up the bulk of a concrete mixture. Sand, natural gravel, and crushed stone are used mainly for this purpose. Recycled aggregates (from construction, demolition, and excavation waste) are increasingly used as partial replacements for natural aggregates, while a number of manufactured aggregates, including air-cooled blast furnace slag and bottom ash are also permitted.\nThe size distribution of the aggregate determines how much binder is required. Aggregate with a very even size distribution has the biggest gaps whereas adding aggregate with smaller particles tends to fill these gaps. The binder must fill the gaps between the aggregate as well as paste the surfaces of the aggregate together, and is typically the most expensive component. Thus, variation in sizes of the aggregate reduces the cost of concrete. The aggregate is nearly always stronger than the binder, so its use does not negatively affect the strength of the concrete.\nRedistribution of aggregates after compaction often creates non-homogeneity due to the influence of vibration. This can lead to strength gradients.\nDecorative stones such as quartzite, small river stones or crushed glass are sometimes added to the surface of concrete for a decorative \"exposed aggregate\" finish, popular among landscape designers.\nAdmixtures.\nAdmixtures are materials in the form of powder or fluids that are added to the concrete to give it certain characteristics not obtainable with plain concrete mixes. Admixtures are defined as additions \"made as the concrete mix is being prepared\". The most common admixtures are retarders and accelerators. In normal use, admixture dosages are less than 5% by mass of cement and are added to the concrete at the time of batching/mixing. (See below.) The common types of admixtures are as follows:\nMineral admixtures and blended cements.\nInorganic materials that have pozzolanic or latent hydraulic properties, these very fine-grained materials are added to the concrete mix to improve the properties of concrete (mineral admixtures), or as a replacement for Portland cement (blended cements). Products which incorporate limestone, fly ash, blast furnace slag, and other useful materials with pozzolanic properties into the mix, are being tested and used. These developments are ever growing in relevance to minimize the impacts caused by cement use, notorious for being one of the largest producers (at about 5 to 10%) of global greenhouse gas emissions. The use of alternative materials also is capable of lowering costs, improving concrete properties, and recycling wastes, the latest being relevant for circular economy aspects of the construction industry, whose demand is ever growing with greater impacts on raw material extraction, waste generation and landfill practices.\nProduction.\nConcrete production is the process of mixing together the various ingredients\u2014water, aggregate, cement, and any additives\u2014to produce concrete. Concrete production is time-sensitive. Once the ingredients are mixed, workers must put the concrete in place before it hardens. In modern usage, most concrete production takes place in a large type of industrial facility called a concrete plant, or often a batch plant. The usual method of placement is casting in formwork, which holds the mix in shape until it has set enough to hold its shape unaided.\nIn general usage, concrete plants come in two main types, ready mix plants and central mix plants. A ready-mix plant mixes all the ingredients except water, while a central mix plant mixes all the ingredients including water. A central-mix plant offers more accurate control of the concrete quality through better measurements of the amount of water added, but must be placed closer to the work site where the concrete will be used, since hydration begins at the plant.\nA concrete plant consists of large storage hoppers for various reactive ingredients like cement, storage for bulk ingredients like aggregate and water, mechanisms for the addition of various additives and amendments, machinery to accurately weigh, move, and mix some or all of those ingredients, and facilities to dispense the mixed concrete, often to a concrete mixer truck.\nModern concrete is usually prepared as a viscous fluid, so that it may be poured into forms, which are containers erected in the field to give the concrete its desired shape. Concrete formwork can be prepared in several ways, such as slip forming and steel plate construction. Alternatively, concrete can be mixed into dryer, non-fluid forms and used in factory settings to manufacture precast concrete products.\nA wide variety of equipment is used for processing concrete, from hand tools to heavy industrial machinery. Whichever equipment builders use, however, the objective is to produce the desired building material; ingredients must be properly mixed, placed, shaped, and retained within time constraints. Any interruption in pouring the concrete can cause the initially placed material to begin to set before the next batch is added on top. This creates a horizontal plane of weakness called a \"cold joint\" between the two batches. Once the mix is where it should be, the curing process must be controlled to ensure that the concrete attains the desired attributes. During concrete preparation, various technical details may affect the quality and nature of the product.\nDesign mix.\n\"Design mix\" ratios are decided by an engineer after analyzing the properties of the specific ingredients being used. Instead of using a 'nominal mix' of 1 part cement, 2 parts sand, and 4 parts aggregate (the second example from above), a civil engineer will custom-design a concrete mix to exactly meet the requirements of the site and conditions, setting material ratios and often designing an admixture package to fine-tune the properties or increase the performance envelope of the mix. Design-mix concrete can have very broad specifications that cannot be met with more basic nominal mixes, but the involvement of the engineer often increases the cost of the concrete mix.\nConcrete Mixes are primarily divided into nominal mix, standard mix and design mix.\nNominal mix ratios are given in volume of formula_1. Nominal mixes are a simple, fast way of getting a basic idea of the properties of the finished concrete without having to perform testing in advance.\nVarious governing bodies (such as British Standards) define nominal mix ratios into a number of grades, usually ranging from lower compressive strength to higher compressive strength. The grades usually indicate the 28-day cube strength.\nMixing.\nThorough mixing is essential to produce uniform, high-quality concrete.\nSeparate paste mixing has shown that the mixing of cement and water into a paste before combining these materials with aggregates can increase the compressive strength of the resulting concrete. The paste is generally mixed in a high-speed, shear-type mixer at a w/c (water to cement ratio) of 0.30 to 0.45 by mass. The cement paste premix may include admixtures such as accelerators or retarders, superplasticizers, pigments, or silica fume. The premixed paste is then blended with aggregates and any remaining batch water and final mixing is completed in conventional concrete mixing equipment.\nSample analysis \u2013 Workability.\nWorkability is the ability of a fresh (plastic) concrete mix to fill the form/mold properly with the desired work (pouring, pumping, spreading, tamping, vibration) and without reducing the concrete's quality. Workability depends on water content, aggregate (shape and size distribution), cementitious content and age (level of hydration) and can be modified by adding chemical admixtures, like superplasticizer. Raising the water content or adding chemical admixtures increases concrete workability. Excessive water leads to increased bleeding or segregation of aggregates (when the cement and aggregates start to separate), with the resulting concrete having reduced quality. Changes in gradation can also affect workability of the concrete, although a wide range of gradation can be used for various applications. An undesirable gradation can mean using a large aggregate that is too large for the size of the formwork, or which has too few smaller aggregate grades to serve to fill the gaps between the larger grades, or using too little or too much sand for the same reason, or using too little water, or too much cement, or even using jagged crushed stone instead of smoother round aggregate such as pebbles. Any combination of these factors and others may result in a mix which is too harsh, i.e., which does not flow or spread out smoothly, is difficult to get into the formwork, and which is difficult to surface finish.\nWorkability can be measured by the concrete slump test, a simple measure of the plasticity of a fresh batch of concrete following the ASTM C 143 or EN 12350-2 test standards. Slump is normally measured by filling an \"Abrams cone\" with a sample from a fresh batch of concrete. The cone is placed with the wide end down onto a level, non-absorptive surface. It is then filled in three layers of equal volume, with each layer being tamped with a steel rod to consolidate the layer. When the cone is carefully lifted off, the enclosed material slumps a certain amount, owing to gravity. A relatively dry sample slumps very little, having a slump value of one or two inches (25 or 50\u00a0mm) out of . A relatively wet concrete sample may slump as much as eight inches. Workability can also be measured by the flow table test.\nSlump can be increased by addition of chemical admixtures such as plasticizer or superplasticizer without changing the water-cement ratio. Some other admixtures, especially air-entraining admixture, can increase the slump of a mix.\nHigh-flow concrete, like self-consolidating concrete, is tested by other flow-measuring methods. One of these methods includes placing the cone on the narrow end and observing how the mix flows through the cone while it is gradually lifted.\nAfter mixing, concrete is a fluid and can be pumped to the location where needed.\nCuring.\nMaintaining optimal conditions for cement hydration.\nConcrete must be kept moist during curing in order to achieve optimal strength and durability. During curing hydration occurs, allowing calcium-silicate hydrate (C-S-H) to form. Over 90% of a mix's final strength is typically reached within four weeks, with the remaining 10% achieved over years or even decades. The conversion of calcium hydroxide in the concrete into calcium carbonate from absorption of CO2 over several decades further strengthens the concrete and makes it more resistant to damage. This carbonation reaction, however, lowers the pH of the cement pore solution and can corrode the reinforcement bars.\nHydration and hardening of concrete during the first three days is critical. Abnormally fast drying and shrinkage due to factors such as evaporation from wind during placement may lead to increased tensile stresses at a time when it has not yet gained sufficient strength, resulting in greater shrinkage cracking. The early strength of the concrete can be increased if it is kept damp during the curing process. Minimizing stress prior to curing minimizes cracking. High-early-strength concrete is designed to hydrate faster, often by increased use of cement that increases shrinkage and cracking. The strength of concrete changes (increases) for up to three years. It depends on cross-section dimension of elements and conditions of structure exploitation. Addition of short-cut polymer fibers can improve (reduce) shrinkage-induced stresses during curing and increase early and ultimate compression strength.\nProperly curing concrete leads to increased strength and lower permeability and avoids cracking where the surface dries out prematurely. Care must also be taken to avoid freezing or overheating due to the exothermic setting of cement. Improper curing can cause scaling, reduced strength, poor abrasion resistance and cracking.\nCuring techniques avoiding water loss by evaporation.\nDuring the curing period, concrete is ideally maintained at controlled temperature and humidity. To ensure full hydration during curing, concrete slabs are often sprayed with \"curing compounds\" that create a water-retaining film over the concrete. Typical films are made of wax or related hydrophobic compounds. After the concrete is sufficiently cured, the film is allowed to abrade from the concrete through normal use.\nTraditional conditions for curing involve spraying or ponding the concrete surface with water. The adjacent picture shows one of many ways to achieve this, ponding\u2014submerging setting concrete in water and wrapping in plastic to prevent dehydration. Additional common curing methods include wet burlap and plastic sheeting covering the fresh concrete.\nFor higher-strength applications, accelerated curing techniques may be applied to the concrete. A common technique involves heating the poured concrete with steam, which serves to both keep it damp and raise the temperature so that the hydration process proceeds more quickly and more thoroughly.\nAlternative types.\nAsphalt.\n\"Asphalt concrete\" (commonly called \"asphalt\", \"blacktop\", or \"pavement\" in North America, and \"tarmac\", \"bitumen macadam\", or \"rolled asphalt\" in the United Kingdom and the Republic of Ireland) is a composite material commonly used to surface roads, parking lots, airports, as well as the core of embankment dams. Asphalt mixtures have been used in pavement construction since the beginning of the twentieth century. It consists of mineral aggregate bound together with asphalt, laid in layers, and compacted. The process was refined and enhanced by Belgian inventor and U.S. immigrant Edward De Smedt.\nThe terms \"asphalt\" (or \"asphaltic\") \"concrete\", \"bituminous asphalt concrete\", and \"bituminous mixture\" are typically used only in engineering and construction documents, which define concrete as any composite material composed of mineral aggregate adhered with a binder. The abbreviation, \"AC\", is sometimes used for \"asphalt concrete\" but can also denote \"asphalt content\" or \"asphalt cement\", referring to the liquid asphalt portion of the composite material.\nGraphene enhanced concrete.\nGraphene enhanced concretes are standard designs of concrete mixes, except that during the cement-mixing or production process, a small amount of chemically engineered graphene (typically &lt; 0.5% by weight) is added. These enhanced graphene concretes are designed around the concrete application.\nMicrobial.\nBacteria such as \"Bacillus pasteurii\", \"Bacillus pseudofirmus\", \"Bacillus cohnii\", \"Sporosarcina pasteuri\", and \"Arthrobacter crystallopoietes\" increase the compression strength of concrete through their biomass. However some forms of bacteria can also be concrete-destroying. Bacillus sp. CT-5. can reduce corrosion of reinforcement in reinforced concrete by up to four times. \"Sporosarcina pasteurii\" reduces water and chloride permeability. \"B. pasteurii\" increases resistance to acid. \"Bacillus pasteurii\" and \"B. sphaericuscan\" induce calcium carbonate precipitation in the surface of cracks, adding compression strength.\nNanoconcrete.\nNanoconcrete (also spelled \"nano concrete\"' or \"nano-concrete\") is a class of materials that contains Portland cement particles that are no greater than 100 \u03bcm and particles of silica no greater than 500 \u03bcm, which fill voids that would otherwise occur in normal concrete, thereby substantially increasing the material's strength. It is widely used in foot and highway bridges where high flexural and compressive strength are indicated.\nPervious.\nPervious concrete is a mix of specially graded coarse aggregate, cement, water, and little-to-no fine aggregates. This concrete is also known as \"no-fines\" or porous concrete. Mixing the ingredients in a carefully controlled process creates a paste that coats and bonds the aggregate particles. The hardened concrete contains interconnected air voids totaling approximately 15 to 25 percent. Water runs through the voids in the pavement to the soil underneath. Air entrainment admixtures are often used in freeze-thaw climates to minimize the possibility of frost damage. Pervious concrete also permits rainwater to filter through roads and parking lots, to recharge aquifers, instead of contributing to runoff and flooding.\nPolymer.\nPolymer concretes are mixtures of aggregate and any of various polymers and may be reinforced. The cement is costlier than lime-based cements, but polymer concretes nevertheless have advantages; they have significant tensile strength even without reinforcement, and they are largely impervious to water. Polymer concretes are frequently used for the repair and construction of other applications, such as drains.\nVolcanic.\nVolcanic concrete substitutes volcanic rock for the limestone that is burned to form clinker. It consumes a similar amount of energy, but does not directly emit carbon as a byproduct. Volcanic rock/ash are used as supplementary cementitious materials in concrete to improve the resistance to sulfate, chloride and alkali silica reaction due to pore refinement. Also, they are generally cost effective in comparison to other aggregates, good for semi and light weight concretes, and good for thermal and acoustic insulation.\nPyroclastic materials, such as pumice, scoria, and ashes are formed from cooling magma during explosive volcanic eruptions. They are used as supplementary cementitious materials (SCM) or as aggregates for cements and concretes. They have been extensively used since ancient times to produce materials for building applications. For example, pumice and other volcanic glasses were added as a natural pozzolanic material for mortars and plasters during the construction of the Villa San Marco in the Roman period (89 BC \u2013 79 AD), which remain one of the best-preserved otium villae of the Bay of Naples in Italy.\nWaste light.\nWaste light is form of polymer modified concrete. The specific polymer admixture allows the replacement of all the traditional aggregates (gravel, sand, stone) by any mixture of solid waste materials in the grain size of 3\u201310\u00a0mm to form a low-compressive-strength (3\u201320\u00a0N/mm2) product for road and building construction. One cubic meter of waste light concrete contains 1.1\u20131.3\u00a0m3 of shredded waste and no other aggregates.\nSulfur concrete.\nSulfur concrete is a special concrete that uses sulfur as a binder and does not require cement or water.\nProperties.\nConcrete has relatively high compressive strength, but much lower tensile strength. Therefore, it is usually reinforced with materials that are strong in tension (often steel). The elasticity of concrete is relatively constant at low stress levels but starts decreasing at higher stress levels as matrix cracking develops. Concrete has a very low coefficient of thermal expansion and shrinks as it matures. All concrete structures crack to some extent, due to shrinkage and tension. Concrete that is subjected to long-duration forces is prone to creep.\nTests can be performed to ensure that the properties of concrete correspond to specifications for the application.\nThe ingredients affect the strengths of the material. Concrete strength values are usually specified as the lower-bound compressive strength of either a cylindrical or cubic specimen as determined by standard test procedures.\nThe strengths of concrete is dictated by its function. Very low-strength\u2014 or less\u2014concrete may be used when the concrete must be lightweight. Lightweight concrete is often achieved by adding air, foams, or lightweight aggregates, with the side effect that the strength is reduced. For most routine uses, concrete is often used. concrete is readily commercially available as a more durable, although more expensive, option. Higher-strength concrete is often used for larger civil projects. Strengths above are often used for specific building elements. For example, the lower floor columns of high-rise concrete buildings may use concrete of or more, to keep the size of the columns small. Bridges may use long beams of high-strength concrete to lower the number of spans required. Occasionally, other structural needs may require high-strength concrete. If a structure must be very rigid, concrete of very high strength may be specified, even much stronger than is required to bear the service loads. Strengths as high as have been used commercially for these reasons.\nEnergy efficiency.\nThe cement produced for making concrete accounts for about 8% of worldwide CO2 emissions per year (compared to, \"e.g.\", global aviation at 1.9%). The two largest sources of CO2 are produced by the cement manufacturing process, arising from (1) the decarbonation reaction of limestone in the cement kiln (T \u2248 950 \u00b0C), and (2) from the combustion of fossil fuel to reach the sintering temperature (T \u2248 1450 \u00b0C) of cement clinker in the kiln. The energy required for extracting, crushing, and mixing the raw materials (construction aggregates used in the concrete production, and also limestone and clay feeding the cement kiln) is lower. Energy requirement for transportation of ready-mix concrete is also lower because it is produced nearby the construction site from local resources, typically manufactured within 100 kilometers of the job site. The overall embodied energy of concrete at roughly 1 to 1.5 megajoules per kilogram is therefore lower than for many structural and construction materials.\nOnce in place, concrete offers a great energy efficiency over the lifetime of a building. Concrete walls leak air far less than those made of wood frames. Air leakage accounts for a large percentage of energy loss from a home. The thermal mass properties of concrete increase the efficiency of both residential and commercial buildings. By storing and releasing the energy needed for heating or cooling, concrete's thermal mass delivers year-round benefits by reducing temperature swings inside and minimizing heating and cooling costs. While insulation reduces energy loss through the building envelope, thermal mass uses walls to store and release energy. Modern concrete wall systems use both external insulation and thermal mass to create an energy-efficient building. Insulating concrete forms (ICFs) are hollow blocks or panels made of either insulating foam or rastra that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\nFire safety.\nConcrete buildings are more resistant to fire than those constructed using steel frames, since concrete has lower heat conductivity than steel and can thus last longer under the same fire conditions. Concrete is sometimes used as a fire protection for steel frames, for the same effect as above. Concrete as a fire shield, for example Fondu fyre, can also be used in extreme environments like a missile launch pad.\nOptions for non-combustible construction include floors, ceilings and roofs made of cast-in-place and hollow-core precast concrete. For walls, concrete masonry technology and Insulating Concrete Forms (ICFs) are additional options. ICFs are hollow blocks or panels made of fireproof insulating foam that are stacked to form the shape of the walls of a building and then filled with reinforced concrete to create the structure.\nConcrete also provides good resistance against externally applied forces such as high winds, hurricanes, and tornadoes owing to its lateral stiffness, which results in minimal horizontal movement. However, this stiffness can work against certain types of concrete structures, particularly where a relatively higher flexing structure is required to resist more extreme forces.\nEarthquake safety.\nAs discussed above, concrete is very strong in compression, but weak in tension. Larger earthquakes can generate very large shear loads on structures. These shear loads subject the structure to both tensile and compressional loads. Concrete structures without reinforcement, like other unreinforced masonry structures, can fail during severe earthquake shaking. Unreinforced masonry structures constitute one of the largest earthquake risks globally. These risks can be reduced through seismic retrofitting of at-risk buildings, (e.g. school buildings in Istanbul, Turkey).\nConstruction with concrete.\nConcrete is one of the most durable building materials. It provides superior fire resistance compared with wooden construction and gains strength over time. Structures made of concrete can have a long service life. Concrete is used more than any other artificial material in the world. As of 2006, about 7.5 billion cubic meters of concrete are made each year, more than one cubic meter for every person on Earth.\nReinforced concrete.\nThe use of reinforcement, in the form of iron was introduced in the 1850s by French industrialist Fran\u00e7ois Coignet, and it was not until the 1880s that German civil engineer G. A. Wayss used steel as reinforcement. Concrete is a relatively brittle material that is strong under compression but less in tension. Plain, unreinforced concrete is unsuitable for many structures as it is relatively poor at withstanding stresses induced by vibrations, wind loading, and so on. Hence, to increase its overall strength, steel rods, wires, mesh or cables can be embedded in concrete before it is set. This reinforcement, often known as rebar, resists tensile forces.\nReinforced concrete (RC) is a versatile composite and one of the most widely used materials in modern construction. It is made up of different constituent materials with very different properties that complement each other. In the case of reinforced concrete, the component materials are almost always concrete and steel. These two materials form a strong bond together and are able to resist a variety of applied forces, effectively acting as a single structural element.\nReinforced concrete can be precast or cast-in-place (in situ) concrete, and is used in a wide range of applications such as; slab, wall, beam, column, foundation, and frame construction. Reinforcement is generally placed in areas of the concrete that are likely to be subject to tension, such as the lower portion of beams. Usually, there is a minimum of 50\u00a0mm cover, both above and below the steel reinforcement, to resist spalling and corrosion which can lead to structural instability. Other types of non-steel reinforcement, such as Fibre-reinforced concretes are used for specialized applications, predominately as a means of controlling cracking.\nPrecast concrete.\nPrecast concrete is concrete which is cast in one place for use elsewhere and is a mobile material. The largest part of precast production is carried out in the works of specialist suppliers, although in some instances, due to economic and geographical factors, scale of product or difficulty of access, the elements are cast on or adjacent to the construction site. Precasting offers considerable advantages because it is carried out in a controlled environment, protected from the elements, but the downside of this is the contribution to greenhouse gas emission from transportation to the construction site.\nAdvantages to be achieved by employing precast concrete:\nMass structures.\nDue to cement's exothermic chemical reaction while setting up, large concrete structures such as dams, navigation locks, large mat foundations, and large breakwaters generate excessive heat during hydration and associated expansion. To mitigate these effects, \"post-cooling\" is commonly applied during construction. An early example at Hoover Dam used a network of pipes between vertical concrete placements to circulate cooling water during the curing process to avoid damaging overheating. Similar systems are still used; depending on volume of the pour, the concrete mix used, and ambient air temperature, the cooling process may last for many months after the concrete is placed. Various methods also are used to pre-cool the concrete mix in mass concrete structures.\nAnother approach to mass concrete structures that minimizes cement's thermal by-product is the use of roller-compacted concrete, which uses a dry mix which has a much lower cooling requirement than conventional wet placement. It is deposited in thick layers as a semi-dry material then roller compacted into a dense, strong mass.\nSurface finishes.\nRaw concrete surfaces tend to be porous and have a relatively uninteresting appearance. Many finishes can be applied to improve the appearance and preserve the surface against staining, water penetration, and freezing.\nExamples of improved appearance include stamped concrete where the wet concrete has a pattern impressed on the surface, to give a paved, cobbled or brick-like effect, and may be accompanied with coloration. Another popular effect for flooring and table tops is polished concrete where the concrete is polished optically flat with diamond abrasives and sealed with polymers or other sealants.\nOther finishes can be achieved with chiseling, or more conventional techniques such as painting or covering it with other materials.\nThe proper treatment of the surface of concrete, and therefore its characteristics, is an important stage in the construction and renovation of architectural structures.\nPrestressed structures.\nPrestressed concrete is a form of reinforced concrete that builds in compressive stresses during construction to oppose tensile stresses experienced in use. This can greatly reduce the weight of beams or slabs, by\nbetter distributing the stresses in the structure to make optimal use of the reinforcement. For example, a horizontal beam tends to sag. Prestressed reinforcement along the bottom of the beam counteracts this.\nIn pre-tensioned concrete, the prestressing is achieved by using steel or polymer tendons or bars that are subjected to a tensile force prior to casting, or for post-tensioned concrete, after casting.\nThere are two different systems being used:\nMore than of highways in the United States are paved with this material. Reinforced concrete, prestressed concrete and precast concrete are the most widely used types of concrete functional extensions in modern days. For more information see Brutalist architecture.\nPlacement.\nOnce mixed, concrete is typically transported to the place where it is intended to become a structural item. Various methods of transportation and placement are used depending on the distances involve, quantity needed, and other details of application. Large amounts are often transported by truck, poured free under gravity or through a tremie, or pumped through a pipe. Smaller amounts may be carried in a skip (a metal container which can be tilted or opened to release the contents, usually transported by crane or hoist), or wheelbarrow, or carried in toggle bags for manual placement underwater.\nCold weather placement.\nExtreme weather conditions (extreme heat or cold; windy conditions, and humidity variations) can significantly alter the quality of concrete. Many precautions are observed in cold weather placement. Low temperatures significantly slow the chemical reactions involved in hydration of cement, thus affecting the strength development. Preventing freezing is the most important precaution, as formation of ice crystals can cause damage to the crystalline structure of the hydrated cement paste. If the surface of the concrete pour is insulated from the outside temperatures, the heat of hydration will prevent freezing.\nThe American Concrete Institute (ACI) definition of cold weather placement, ACI 306, is:\nIn Canada, where temperatures tend to be much lower during the cold season, the following criteria are used by CSA A23.1:\nThe minimum strength before exposing concrete to extreme cold is . CSA A 23.1 specified a compressive strength of 7.0\u00a0MPa to be considered safe for exposure to freezing.\nUnderwater placement.\nConcrete may be placed and cured underwater. Care must be taken in the placement method to prevent washing out the cement. Underwater placement methods include the tremie, pumping, skip placement, manual placement using toggle bags, and bagwork.\n&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;Grouted aggregate is an alternative method of forming a concrete mass underwater, where the forms are filled with coarse aggregate and the voids then completely filled with pumped grout.\nRoads.\nConcrete roads are more fuel efficient to drive on, more reflective and last significantly longer than other paving surfaces, yet have a much smaller market share than other paving solutions. Modern-paving methods and design practices have changed the economics of concrete paving, so that a well-designed and placed concrete pavement will be less expensive on initial costs and significantly less expensive over the life cycle. Another major benefit is that pervious concrete can be used, which eliminates the need to place storm drains near the road, and reducing the need for slightly sloped roadway to help rainwater to run off. No longer requiring discarding rainwater through use of drains also means that less electricity is needed (more pumping is otherwise needed in the water-distribution system), and no rainwater gets polluted as it no longer mixes with polluted water. Rather, it is immediately absorbed by the ground.\nEnvironment, health and safety.\nThe manufacture and use of concrete produce a wide range of environmental, economic and social impacts.\nConcrete, cement and the environment.\nA major component of concrete is cement, a fine, powder, used mainly to bind sand and coarser aggregates together in concrete. Although a variety of cement types exist, the most common is \"Portland cement\", which is produced by mixing clinker with smaller quantities of other additives such as gypsum and ground limestone. The production of clinker, the main constituent of cement, is responsible for the bulk of the sector's greenhouse gas emissions, including both energy intensity and process emissions.\nThe cement industry is one of the three primary producers of carbon dioxide, a major greenhouse gas \u2013 the other two being energy production and transportation industries. On average, every tonne of cement produced releases one tonne of CO2 into the atmosphere. Pioneer cement manufacturers have claimed to reach lower carbon intensities, with 590\u00a0kg of CO2eq per tonne of cement produced. The emissions are due to combustion and calcination processes, which roughly account for 40% and 60% of the greenhouse gases, respectively. Considering that cement is only a fraction of the constituents of concrete, it is estimated that a tonne of concrete is responsible for emitting about 100\u2013200\u00a0kg of CO2. Every year more than 10 billion tonnes of concrete are used worldwide. In the coming years, large quantities of concrete will continue to be used, and the mitigation of CO2 emissions from the sector will be even more critical.\nConcrete is used to create hard surfaces that contribute to surface runoff, which can cause heavy soil erosion, water pollution, and flooding, but conversely can be used to divert, dam, and control flooding. Concrete dust released by building demolition and natural disasters can be a major source of dangerous air pollution. Concrete is a contributor to the urban heat island effect, though less so than asphalt.\nConcrete and climate change mitigation.\nReducing the cement clinker content might have positive effects on the environmental life-cycle assessment of concrete. Some research work on reducing the cement clinker content in concrete has already been carried out. However, there exist different research strategies. Often replacement of some clinker for large amounts of slag or fly ash was investigated based on conventional concrete technology. This could lead to a waste of scarce raw materials such as slag and fly ash. The aim of other research activities is the efficient use of cement and reactive materials like slag and fly ash in concrete based on a modified mix design approach.\nAn environmental investigation found that the embodied carbon of a precast concrete facade can be reduced by 50% when using the presented fiber reinforced high performance concrete in place of typical reinforced concrete cladding.\nStudies have been conducted about commercialization of low-carbon concretes. Life cycle assessment (LCA) of low-carbon concrete was investigated according to the ground granulated blast-furnace slag (GGBS) and fly ash (FA) replacement ratios. Global warming potential (GWP) of GGBS decreased by 1.1\u00a0kg CO2 eq/m3, while FA decreased by 17.3\u00a0kg CO2 eq/m3 when the mineral admixture replacement ratio was increased by 10%. This study also compared the compressive strength properties of binary blended low-carbon concrete according to the replacement ratios, and the applicable range of mixing proportions was derived.\nResearchers at University of Auckland are working on utilizing biochar in concrete applications to reduce carbon emissions during concrete production and to improve strength.\nConcrete and climate change adaptation.\nHigh-performance building materials will be particularly important for enhancing resilience, including for flood defenses and critical-infrastructure protection. Risks to infrastructure and cities posed by extreme weather events are especially serious for those places exposed to flood and hurricane damage, but also where residents need protection from extreme summer temperatures. Traditional concrete can come under strain when exposed to humidity and higher concentrations of atmospheric CO2. While concrete is likely to remain important in applications where the environment is challenging, novel, smarter and more adaptable materials are also needed.\nConcrete \u2013 health and safety.\nGrinding of concrete can produce hazardous dust. Exposure to cement dust can lead to issues such as silicosis, kidney disease, skin irritation and similar effects. The U.S. National Institute for Occupational Safety and Health in the United States recommends attaching local exhaust ventilation shrouds to electric concrete grinders to control the spread of this dust. In addition, the Occupational Safety and Health Administration (OSHA) has placed more stringent regulations on companies whose workers regularly come into contact with silica dust. An updated silica rule, which OSHA put into effect 23 September 2017 for construction companies, restricted the amount of breathable crystalline silica workers could legally come into contact with to 50 micro grams per cubic meter of air per 8-hour workday. That same rule went into effect 23 June 2018 for general industry, hydraulic fracturing and maritime. That the deadline was extended to 23 June 2021 for engineering controls in the hydraulic fracturing industry. Companies which fail to meet the tightened safety regulations can face financial charges and extensive penalties. The presence of some substances in concrete, including useful and unwanted additives, can cause health concerns due to toxicity and radioactivity. Fresh concrete (before curing is complete) is highly alkaline and must be handled with proper protective equipment.\nCircular economy.\nConcrete is an excellent material with which to make long-lasting and energy-efficient buildings. However, even with good design, human needs change and potential waste will be generated.\nEnd-of-life: concrete degradation and waste.\nConcrete can be damaged by many processes, such as the expansion of corrosion products of the steel reinforcement bars, freezing of trapped water, fire or radiant heat, aggregate expansion, sea water effects, bacterial corrosion, leaching, erosion by fast-flowing water, physical damage and chemical damage (from carbonatation, chlorides, sulfates and distillate water). The micro fungi \"Aspergillus alternaria\" and \"Cladosporium\" were able to grow on samples of concrete used as a radioactive waste barrier in the Chernobyl reactor; leaching aluminium, iron, calcium, and silicon.\nConcrete may be considered waste according to the European Commission decision of 2014/955/EU for the List of Waste under the codes: 17 (construction and demolition wastes, including excavated soil from contaminated sites) 01 (concrete, bricks, tiles and ceramics), 01 (concrete), and 17.01.06* (mixtures of, separate fractions of concrete, bricks, tiles and ceramics containing hazardous substances), and 17.01.07 (mixtures of, separate fractions of concrete, bricks, tiles and ceramics other than those mentioned in 17.01.06). It is estimated that in 2018 the European Union generated 371,910 thousand tons of mineral waste from construction and demolition, and close to 4% of this quantity is considered hazardous. Germany, France and the United Kingdom were the top three polluters with 86,412 thousand tons, 68,976 and 68,732 thousand tons of construction waste generation, respectively.\nCurrently, there is not an End-of-Waste criteria for concrete materials in the EU. However, different sectors have been proposing alternatives for concrete waste and re purposing it as a secondary raw material in various applications, including concrete manufacturing itself.\nReuse of concrete.\nReuse of blocks in original form, or by cutting into smaller blocks, has even less environmental impact; however, only a limited market currently exists. Improved building designs that allow for slab reuse and building transformation without demolition could increase this use. Hollow core concrete slabs are easy to dismantle and the span is normally constant, making them good for reuse.\nOther cases of re-use are possible with pre-cast concrete pieces: through selective demolition, such pieces can be disassembled and collected for further use in other building sites. Studies show that back-building and remounting plans for building units (i.e., re-use of pre-fabricated concrete) is an alternative for a kind of construction which protects resources and saves energy. Especially long-living, durable, energy-intensive building materials, such as concrete, can be kept in the life-cycle longer through recycling. Prefabricated constructions are the prerequisites for constructions necessarily capable of being taken apart. In the case of optimal application in the building carcass, savings in costs are estimated in 26%, a lucrative complement to new building methods. However, this depends on several courses to be set. The viability of this alternative has to be studied as the logistics associated with transporting heavy pieces of concrete can impact the operation financially and also increase the carbon footprint of the project. Also, ever changing regulations on new buildings worldwide may require higher quality standards for construction elements and inhibit the use of old elements which may be classified as obsolete.\nRecycling of concrete.\nConcrete recycling is an increasingly common method for disposing of concrete structures. Concrete debris were once routinely shipped to landfills for disposal, but recycling is increasing due to improved environmental awareness, governmental laws and economic benefits.\nContrary to general belief, concrete recovery is achievable \u2013 concrete can be crushed and reused as aggregate in new projects.\nRecycling or recovering concrete reduces natural resource exploitation and associated transportation costs, and reduces waste landfill. However, it has little impact on reducing greenhouse gas emissions as most emissions occur when cement is made, and cement alone cannot be recycled. At present, most recovered concrete is used for road sub-base and civil engineering projects. From a sustainability viewpoint, these relatively low-grade uses currently provide the optimal outcome.\nThe recycling process can be done \"in situ\", with mobile plants, or in specific recycling units. The input material can be returned concrete which is fresh (wet) from ready-mix trucks, production waste at a pre-cast production facility, Waste from construction and demolition. The most significant source is demolition waste, preferably pre-sorted from selective demolition processes.\nBy far the most common method for recycling dry and hardened concrete involves crushing. Mobile sorters and crushers are often installed on construction sites to allow on-site processing. In other situations, specific processing sites are established, which are usually able to produce higher quality aggregate. Screens are used to achieve desired particle size, and remove dirt, foreign particles and fine material from the coarse aggregate.\nChloride and sulfates are undesired contaminants originated from soil and weathering and can provoke corrosion problems on aluminium and steel structures. The final product, Recycled Concrete Aggregate (RCA), presents interesting properties such as: angular shape, rougher surface, lower specific gravity (20%), higher water absorption, and pH greater than 11 \u2013 this elevated pH increases the risk of alkali reactions.\nThe lower density of RCA usually Increases project efficiency and improve job cost \u2013 recycled concrete aggregates yield more volume by weight (up to 15%). The physical properties of coarse aggregates made from crushed demolition concrete make it the preferred material for applications such as road base and sub-base. This is because recycled aggregates often have better compaction properties and require less cement for sub-base uses. Furthermore, it is generally cheaper to obtain than virgin material.\nApplications of recycled concrete aggregate.\nThe main commercial applications of the final recycled concrete aggregate are:\nCradle-to-cradle challenges.\nThe applications developed for RCA so far are not exhaustive, and many more uses are to be developed as regulations, institutions and norms find ways to accommodate construction and demolition waste as secondary raw materials in a safe and economic way. However, considering the purpose of having a circularity of resources in the concrete life cycle, the only application of RCA that could be considered as recycling of concrete is the replacement of natural aggregates on concrete mixes. All the other applications would fall under the category of downcycling. It is estimated that even near complete recovery of concrete from construction and demolition waste will only supply about 20% of total aggregate needs in the developed world.\nThe path towards circularity goes beyond concrete technology itself, depending on multilateral advances in the cement industry, research and development of alternative materials, building design and management, and demolition as well as conscious use of spaces in urban areas to reduce consumption.\nWorld records.\nThe world record for the largest concrete pour in a single project is the Three Gorges Dam in Hubei Province, China by the Three Gorges Corporation. The amount of concrete used in the construction of the dam is estimated at 16 million cubic meters over 17 years. The previous record was 12.3 million cubic meters held by Itaipu hydropower station in Brazil.\nThe world record for concrete pumping was set on 7 August 2009 during the construction of the Parbati Hydroelectric Project, near the village of Suind, Himachal Pradesh, India, when the concrete mix was pumped through a vertical height of .\nThe Polavaram dam works in Andhra Pradesh on 6 January 2019 entered the Guinness World Records by pouring 32,100 cubic metres of concrete in 24 hours. The world record for the largest continuously poured concrete raft was achieved in August 2007 in Abu Dhabi by contracting firm Al Habtoor-CCC Joint Venture and the concrete supplier is Unibeton Ready Mix. The pour (a part of the foundation for the Abu Dhabi's Landmark Tower) was 16,000 cubic meters of concrete poured within a two-day period. The previous record, 13,200 cubic meters poured in 54 hours despite a severe tropical storm requiring the site to be covered with tarpaulins to allow work to continue, was achieved in 1992 by joint Japanese and South Korean consortiums Hazama Corporation and the Samsung C&amp;T Corporation for the construction of the Petronas Towers in Kuala Lumpur, Malaysia.\nThe world record for largest continuously poured concrete floor was completed 8 November 1997, in Louisville, Kentucky by design-build firm EXXCEL Project Management. The monolithic placement consisted of of concrete placed in 30 hours, finished to a flatness tolerance of FF 54.60 and a levelness tolerance of FL 43.83. This surpassed the previous record by 50% in total volume and 7.5% in total area.\nThe record for the largest continuously placed underwater concrete pour was completed 18 October 2010, in New Orleans, Louisiana by contractor C. J. Mahan Construction Company, LLC of Grove City, Ohio. The placement consisted of 10,251 cubic yards of concrete placed in 58.5 hours using two concrete pumps and two dedicated concrete batch plants. Upon curing, this placement allows the cofferdam to be dewatered approximately below sea level to allow the construction of the Inner Harbor Navigation Canal Sill &amp; Monolith Project to be completed in the dry.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5373", "revid": "1159161288", "url": "https://en.wikipedia.org/wiki?curid=5373", "title": "Coitus interruptus", "text": "Withdrawal method of birth-control\nCoitus interruptus, also known as withdrawal, pulling out or the pull-out method, is a method of birth control in which a man, during sexual intercourse, withdraws his penis from a woman's vagina prior to ejaculation and then directs his ejaculate (semen) away from the vagina in an effort to avoid insemination.\nThis method was used by an estimated 38 million couples worldwide in 1991. \"Coitus interruptus\" does not protect against sexually transmitted infections (STIs/STDs).\nHistory.\nPerhaps the oldest description of the use of the withdrawal method to avoid pregnancy is the story of Onan in the Torah and the Bible. This text is believed to have been written down over 2,500 years ago. Societies in the ancient civilizations of Greece and Rome preferred small families and are known to have practiced a variety of birth control methods. There are references that have led historians to believe withdrawal was sometimes used as birth control. However, these societies viewed birth control as a woman's responsibility, and the only well-documented contraception methods were female-controlled devices (both possibly effective, such as pessaries, and ineffective, such as amulets).\nAfter the decline of the Roman Empire in the 5th century AD, contraceptive practices fell out of use in Europe; the use of contraceptive pessaries, for example, is not documented again until the 15th century. If withdrawal was used during the Roman Empire, knowledge of the practice may have been lost during its decline.\nFrom the 18th century until the development of modern methods, withdrawal was one of the most popular methods of birth-control in Europe, North America, and elsewhere.\nEffects.\nLike many methods of birth control, reliable effect is achieved only by correct and consistent use. Observed failure rates of withdrawal vary depending on the population being studied: American studies have found actual failure rates of 15\u201328% per year. One U.S. study, based on self-reported data from the 2006-2010 cycle of the National Survey of Family Growth, found significant differences in failure rate based on parity status. Women with 0 previous births had a 12-month failure rate of only 8.4%, which then increased to 20.4% for those with 1 prior birth and again to 27.7% for those with 2 or more.\nAn analysis of Demographic and Health Surveys in 43 developing countries between 1990 and 2013 found a median 12-month failure rate across subregions of 13.4%, with a range of 7.8-17.1%. Individual countries within the subregions were even more varied. A large scale study of women in England and Scotland during 1968\u20131974 to determine the efficacy of various contraceptive methods found a failure rate of 6.7 per 100 woman-years of use. This was a \u201ctypical use\u201d failure rate, including user failure to use the method correctly. In comparison, the combined oral contraceptive pill has an actual use failure rate of 2\u20138%, while intrauterine devices (IUDs) have an actual use failure rate of 0.1\u20130.8%. Condoms have an actual use failure rate of 10\u201318%. However, some authors suggest that actual effectiveness of withdrawal could be similar to the effectiveness of condoms; this area needs further research. (See Comparison of birth control methods.)\nFor couples that use \"coitus interruptus\" consistently and correctly at every act of intercourse, the failure rate is 4% per year. This rate is derived from an educated guess based on a modest chance of sperm in the pre-ejaculate. In comparison, the pill has a perfect-use failure rate of 0.3%, IUDs a rate of 0.1-0.6%, and internal condoms a rate of 2%.\nIt has been suggested that the pre-ejaculate (\"Cowper's fluid\") emitted by the penis prior to ejaculation may contain spermatozoa (sperm cells), which would compromise the effectiveness of the method. However, several small studies have failed to find any viable sperm in the fluid. While no large conclusive studies have been done, it is believed by some that the cause of method (correct-use) failure is the pre-ejaculate fluid picking up sperm from a previous ejaculation. For this reason, it is recommended that the male partner urinate between ejaculations, to clear the urethra of sperm, and wash any ejaculate from objects that might come near the woman's vulva (e.g. hands and penis).\nHowever, recent research suggests that this might not be accurate. A contrary, yet non-generalizable study that found mixed evidence, including individual cases of a high sperm concentration, was published in March 2011. A noted limitation to these previous studies' findings is that pre-ejaculate samples were analyzed after the critical two-minute point. That is, looking for motile sperm in small amounts of pre-ejaculate via microscope after two minutes \u2013 when the sample has most likely dried \u2013 makes examination and evaluation \"extremely difficult\". Thus, in March 2011 a team of researchers assembled 27 male volunteers and analyzed their pre-ejaculate samples within two minutes after producing them. The researchers found that 11 of the 27 men (41%) produced pre-ejaculatory samples that contained sperm, and 10 of these samples (37%) contained a \"fair amount\" of motile sperm (i.e. as few as 1 million to as many as 35 million).\nThis study therefore recommends, in order to minimize unintended pregnancy and disease transmission, the use of condoms from the first moment of genital contact.\nAs a point of reference, a study showed that, of couples who conceived within a year of trying, only 2.5% included a male partner with a total sperm count (per ejaculate) of 23 million sperm or less.\nHowever, across a wide range of observed values, total sperm count (as with other identified semen and sperm characteristics) has weak power to predict which couples are at risk of pregnancy. Regardless, this study introduced the concept that some men may consistently have sperm in their pre-ejaculate, due to a \"leakage,\" while others may not.\nSimilarly, another robust study performed in 2016 found motile sperm in the pre-ejaculate of 16.7% (7/42) healthy men. What more, this study attempted to exclude contamination of sperm from ejaculate by drying the pre-ejaculate specimens to reveal a fern-like pattern, characteristics of true pre-ejaculate. All pre-ejaculate specimens were examined within an hour of production and then dried; all pre-ejaculate specimens were found to be true pre-ejaculate.\nIt is widely believed that urinating after an ejaculation will flush the urethra of remaining sperm. However, some of the subjects in the March 2011 study who produced sperm in their pre-ejaculate did urinate (sometimes more than once) before producing their sample. Therefore, some males can release the pre-ejaculate fluid containing sperm without a previous ejaculation.\nAdvantages.\nThe advantage of \"coitus interruptus\" is that it can be used by people who have objections to, or do not have access to, other forms of contraception. Some people prefer it so they can avoid possible adverse effects of hormonal contraceptives or so that they can have a full experience and be able to \"feel\" their partner. Other reasons for the popularity of this method are it has no direct monetary cost, requires no artificial devices, has no physical side effects, can be practiced without a prescription or medical consultation, and provides no barriers to stimulation.\nDisadvantages.\nCompared to the other common reversible methods of contraception such as IUDs, hormonal contraceptives, and male condoms, \"coitus interruptus\" is less effective at preventing pregnancy. As a result, it is also less cost-effective than many more effective methods: although the method itself has no direct cost, users have a greater chance of incurring the risks and expenses of either child-birth or abortion. Only models that assume all couples practice perfect use of the method find cost savings associated with the choice of withdrawal as a birth control method.\nThe method is largely ineffective in the prevention of sexually transmitted infections (STIs/STDs), like HIV, since pre-ejaculate may carry viral particles or bacteria which may infect the partner if this fluid comes in contact with mucous membranes. However, a reduction in the volume of bodily fluids exchanged during intercourse may reduce the likelihood of disease transmission compared to using no method due to the smaller number of pathogens present.\nPrevalence.\nBased on data from surveys conducted during the late 1990s, 3% of women of childbearing age worldwide rely on withdrawal as their primary method of contraception. Regional popularity of the method varies widely, from a low of 1% in Africa to 16% in Western Asia.\nIn the United States, according to the National Survey of Family Growth (NSFG) in 2014, 8.1% of reproductive-aged women reported using withdrawal as a primary contraceptive method. This was a significant increase from 2012 when 4.8% of women reported the use of withdrawal as their most effective method. However, when withdrawal is used in addition to or in rotation with another contraceptive method, the percentage of women using withdrawal jumps from 5% for sole use and 11% for any withdrawal use in 2002, and for adolescents from 7.1% of sole withdrawal use to 14.6% of any withdrawal use in 2006\u20132008.\nWhen asked if withdrawal was used at least once in the past month by women, use of withdrawal increased from 13% as sole use to 33% ever use in the past month. These increases are even more pronounced for adolescents 15 to 19 years old and young women 20 to 24 years old Similarly, the NSFG reports that 9.8% of unmarried men who have had sexual intercourse in the last three months in 2002 used withdrawal, which then increased to 14.5% in 2006\u20132010, and then to 18.8% in 2011\u20132015. The use of withdrawal varied by the unmarried man's age and cohabiting status, but not by ethnicity or race. The use of withdrawal decreased significantly with increasing age groups, ranging from 26.2% among men aged 15\u201319 to 12% among men aged 35\u201344. The use of withdrawal was significantly higher for never-married men (23.0%) compared with formerly married (16.3%) and cohabiting (13.0%) men.\nFor 1998, about 18% of married men in Turkey reported using withdrawal as a contraceptive method.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5374", "revid": "34841508", "url": "https://en.wikipedia.org/wiki?curid=5374", "title": "Condom", "text": "Device for birth control and STI prevention\nA condom is a sheath-shaped barrier device used during sexual intercourse to reduce the probability of pregnancy or a sexually transmitted infection (STI). There are both male and female condoms. With proper use\u2014and use at every act of intercourse\u2014women whose partners use male condoms experience a 2% per-year pregnancy rate. With typical use, the rate of pregnancy is 18% per-year. Their use greatly decreases the risk of gonorrhea, chlamydia, trichomoniasis, hepatitis B, and HIV/AIDS. To a lesser extent, they also protect against genital herpes, human papillomavirus (HPV), and syphilis.\nThe male condom is rolled onto an erect penis before intercourse and works by forming a physical barrier which blocks semen from entering the body of a sexual partner. Male condoms are typically made from latex and, less commonly, from polyurethane, polyisoprene, or lamb intestine. Male condoms have the advantages of ease of use, ease of access, and few side effects. Individuals with latex allergy should use condoms made from a material other than latex, such as polyurethane. Female condoms are typically made from polyurethane and may be used multiple times.\nCondoms as a method of preventing STIs have been used since at least 1564. Rubber condoms became available in 1855, followed by latex condoms in the 1920s. It is on the World Health Organization's List of Essential Medicines. As of 2019, globally around 21% of those using birth control use the condom, making it the second-most common method after female sterilization (24%). Rates of condom use are highest in East and Southeast Asia, Europe and North America. About six to nine billion are sold a year.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nMedical uses.\nBirth control.\nThe effectiveness of condoms, as of most forms of contraception, can be assessed two ways. \"Perfect use\" or \"method\" effectiveness rates only include people who use condoms properly and consistently. \"Actual use\", or \"typical use\" effectiveness rates are of all condom users, including those who use condoms incorrectly or do not use condoms at every act of intercourse. Rates are generally presented for the first year of use. Most commonly the Pearl Index is used to calculate effectiveness rates, but some studies use decrement tables.\nThe typical use pregnancy rate among condom users varies depending on the population being studied, ranging from 10 to 18% per year. The perfect use pregnancy rate of condoms is 2% per year. Condoms may be combined with other forms of contraception (such as spermicide) for greater protection.\nSexually transmitted infections.\nCondoms are widely recommended for the prevention of sexually transmitted infections (STIs). They have been shown to be effective in reducing infection rates in both men and women. While not perfect, the condom is effective at reducing the transmission of organisms that cause AIDS, genital herpes, cervical cancer, genital warts, syphilis, chlamydia, gonorrhea, and other diseases. Condoms are often recommended as an adjunct to more effective birth control methods (such as IUD) in situations where STD protection is also desired.\nFor this reason, condoms are frequently used by those in the swinging (sexual practice) community.\nAccording to a 2000 report by the National Institutes of Health (NIH), consistent use of latex condoms reduces the risk of HIV transmission by approximately 85% relative to risk when unprotected, putting the seroconversion rate (infection rate) at 0.9 per 100 person-years with condom, down from 6.7 per 100 person-years. Analysis published in 2007 from the University of Texas Medical Branch and the World Health Organization found similar risk reductions of 80\u201395%.\nThe 2000 NIH review concluded that condom use significantly reduces the risk of gonorrhea for men. A 2006 study reports that proper condom use decreases the risk of transmission of human papillomavirus (HPV) to women by approximately 70%. Another study in the same year found consistent condom use was effective at reducing transmission of herpes simplex virus-2, also known as genital herpes, in both men and women.\nAlthough a condom is effective in limiting exposure, some disease transmission may occur even with a condom. Infectious areas of the genitals, especially when symptoms are present, may not be covered by a condom, and as a result, some diseases like HPV and herpes may be transmitted by direct contact. The primary effectiveness issue with using condoms to prevent STDs, however, is inconsistent use.\nCondoms may also be useful in treating potentially precancerous cervical changes. Exposure to human papillomavirus, even in individuals already infected with the virus, appears to increase the risk of precancerous changes. The use of condoms helps promote regression of these changes. In addition, researchers in the UK suggest that a hormone in semen can aggravate existing cervical cancer, condom use during sex can prevent exposure to the hormone.\nCauses of failure.\nCondoms may slip off the penis after ejaculation, break due to improper application or physical damage (such as tears caused when opening the package), or break or slip due to latex degradation (typically from usage past the expiration date, improper storage, or exposure to oils). The rate of breakage is between 0.4% and 2.3%, while the rate of slippage is between 0.6% and 1.3%. Even if no breakage or slippage is observed, 1\u20133% of women will test positive for semen residue after intercourse with a condom. Failure rates are higher for anal sex, and until 2022, condoms were only approved by the FDA for vaginal sex. The One Male Condom received FDA approval for anal sex on February 23, 2022.\n\"Double bagging\", using two condoms at once, is often believed to cause a higher rate of failure due to the friction of rubber on rubber. This claim is not supported by research. The limited studies that have been done found that the simultaneous use of multiple condoms decreases the risk of condom breakage.\nDifferent modes of condom failure result in different levels of semen exposure. If a failure occurs during application, the damaged condom may be disposed of and a new condom applied before intercourse begins\u00a0\u2013 such failures generally pose no risk to the user. One study found that semen exposure from a broken condom was about half that of unprotected intercourse; semen exposure from a slipped condom was about one-fifth that of unprotected intercourse.\nStandard condoms will fit almost any penis, with varying degrees of comfort or risk of slippage. Many condom manufacturers offer \"snug\" or \"magnum\" sizes. Some manufacturers also offer custom sized-to-fit condoms, with claims that they are more reliable and offer improved sensation/comfort. Some studies have associated larger penises and smaller condoms with increased breakage and decreased slippage rates (and vice versa), but other studies have been inconclusive.\nIt is recommended for condoms manufacturers to avoid very thick or very thin condoms, because they are both considered less effective. Some authors encourage users to choose thinner condoms \"for greater durability, sensation, and comfort\", but others warn that \"the thinner the condom, the smaller the force required to break it\".\nExperienced condom users are significantly less likely to have a condom slip or break compared to first-time users, although users who experience one slippage or breakage are more likely to suffer a second such failure. An article in \"Population Reports\" suggests that education on condom use reduces behaviors that increase the risk of breakage and slippage. A Family Health International publication also offers the view that education can reduce the risk of breakage and slippage, but emphasizes that more research needs to be done to determine all of the causes of breakage and slippage.\nAmong people who intend condoms to be their form of birth control, pregnancy may occur when the user has sex without a condom. The person may have run out of condoms, or be traveling and not have a condom with them, or dislike the feel of condoms and decide to \"take a chance\". This behavior is the primary cause of typical use failure (as opposed to method or perfect use failure).\nAnother possible cause of condom failure is sabotage. One motive is to have a child against a partner's wishes or consent. Some commercial sex workers from Nigeria reported clients sabotaging condoms in retaliation for being coerced into condom use. Using a fine needle to make several pinholes at the tip of the condom is believed to significantly impact on their effectiveness. Cases of such condom sabotage have occurred.\nSide effects.\nThe use of latex condoms by people with an allergy to latex can cause allergic symptoms, such as skin irritation. In people with severe latex allergies, using a latex condom can potentially be life-threatening. Repeated use of latex condoms can also cause the development of a latex allergy in some people. Irritation may also occur due to spermicides that may be present.\nUse.\nMale condoms are usually packaged inside a foil or plastic wrapper, in a rolled-up form, and are designed to be applied to the tip of the penis and then unrolled over the erect penis. It is important that some space be left in the tip of the condom so that semen has a place to collect; otherwise it may be forced out of the base of the device. Most condoms have a teat end for this purpose. After use, it is recommended the condom be wrapped in tissue or tied in a knot, then disposed of in a trash receptacle. Condoms are used to reduce the likelihood of pregnancy during intercourse and to reduce the likelihood of contracting sexually transmitted infections (STIs). Condoms are also used during fellatio to reduce the likelihood of contracting STIs.\nSome couples find that putting on a condom interrupts sex, although others incorporate condom application as part of their foreplay. Some men and women find the physical barrier of a condom dulls sensation. Advantages of dulled sensation can include prolonged erection and delayed ejaculation; disadvantages might include a loss of some sexual excitement. Advocates of condom use also cite their advantages of being inexpensive, easy to use, and having few side effects.\nAdult film industry.\nIn 2012 proponents gathered 372,000 voter signatures through a citizens' initiative in Los Angeles County to put Measure B on the 2012 ballot. As a result, Measure B, a law requiring the use of condoms in the production of pornographic films, was passed. This requirement has received much criticism and is said by some to be counter-productive, merely forcing companies that make pornographic films to relocate to other places without this requirement. Producers claim that condom use depresses sales.\nSex education.\nCondoms are often used in sex education programs, because they have the capability to reduce the chances of pregnancy and the spread of some sexually transmitted diseases when used correctly. A recent American Psychological Association (APA) press release supported the inclusion of information about condoms in sex education, saying \"comprehensive sexuality education programs\u00a0... discuss the appropriate use of condoms\", and \"promote condom use for those who are sexually active.\"\nIn the United States, teaching about condoms in public schools is opposed by some religious organizations. Planned Parenthood, which advocates family planning and sex education, argues that no studies have shown abstinence-only programs to result in delayed intercourse, and cites surveys showing that 76% of American parents want their children to receive comprehensive sexuality education including condom use.\nInfertility treatment.\nCommon procedures in infertility treatment such as semen analysis and intrauterine insemination (IUI) require collection of semen samples. These are most commonly obtained through masturbation, but an alternative to masturbation is use of a special \"collection condom\" to collect semen during sexual intercourse.\nCollection condoms are made from silicone or polyurethane, as latex is somewhat harmful to sperm. Some men prefer collection condoms to masturbation and some religions prohibit masturbation entirely. Also, compared with samples obtained from masturbation, semen samples from collection condoms have higher total sperm counts, sperm motility, and percentage of sperm with normal morphology. For this reason, they are believed to give more accurate results when used for semen analysis, and to improve the chances of pregnancy when used in procedures such as intracervical or intrauterine insemination. Adherents of religions that prohibit contraception, such as Catholicism, may use collection condoms with holes pricked in them.\nFor fertility treatments, a collection condom may be used to collect semen during sexual intercourse where the semen is provided by the woman's partner. Private sperm donors may also use a collection condom to obtain samples through masturbation or by sexual intercourse with a partner and will transfer the ejaculate from the collection condom to a specially designed container. The sperm is transported in such containers, in the case of a donor, to a recipient woman to be used for insemination, and in the case of a woman's partner, to a fertility clinic for processing and use. However, transportation may reduce the fecundity of the sperm. Collection condoms may also be used where semen is produced at a sperm bank or fertility clinic.\n\"Condom therapy\" is sometimes prescribed to infertile couples when the female has high levels of antisperm antibodies. The theory is that preventing exposure to her partner's semen will lower her level of antisperm antibodies, and thus increase her chances of pregnancy when condom therapy is discontinued. However, condom therapy has not been shown to increase subsequent pregnancy rates.\nOther uses.\nCondoms excel as multipurpose containers and barriers because they are waterproof, elastic, durable, and (for military and espionage uses) will not arouse suspicion if found.\nOngoing military utilization began during World War II, and includes covering the muzzles of rifle barrels to prevent fouling, the waterproofing of firing assemblies in underwater demolitions, and storage of corrosive materials and garrotes by paramilitary agencies.\nCondoms have also been used to smuggle alcohol, cocaine, heroin, and other drugs across borders and into prisons by filling the condom with drugs, tying it in a knot and then either swallowing it or inserting it into the rectum. These methods are very dangerous and potentially lethal; if the condom breaks, the drugs inside become absorbed into the bloodstream and can cause an overdose.\nMedically, condoms can be used to cover endovaginal ultrasound probes, or in field chest needle decompressions they can be used to make a one-way valve.\nCondoms have also been used to protect scientific samples from the environment, and to waterproof microphones for underwater recording.\nTypes.\nMost condoms have a reservoir tip or teat end, making it easier to accommodate the man's ejaculate. Condoms come in different sizes and shapes.\nThey also come in a variety of surfaces intended to stimulate the user's partner. Condoms are usually supplied with a lubricant coating to facilitate penetration, while flavored condoms are principally used for oral sex. As mentioned above, most condoms are made of latex, but polyurethane and lambskin condoms also exist.\nFemale condom.\nMale condoms have a tight ring to form a seal around the penis, while female condoms usually have a large stiff ring to prevent them from slipping into the body orifice. The Female Health Company produced a female condom that was initially made of polyurethane, but newer versions are made of nitrile rubber. Medtech Products produces a female condom made of latex.\nMaterials.\nNatural latex.\nLatex has outstanding elastic properties: Its tensile strength exceeds 30\u00a0MPa, and latex condoms may be stretched in excess of 800% before breaking. In 1990 the ISO set standards for condom production (ISO 4074, Natural latex rubber condoms), and the EU followed suit with its CEN standard (Directive 93/42/EEC concerning medical devices). Every latex condom is tested for holes with an electric current. If the condom passes, it is rolled and packaged. In addition, a portion of each batch of condoms is subject to water leak and air burst testing.\nWhile the advantages of latex have made it the most popular condom material, it does have some drawbacks. Latex condoms are damaged when used with oil-based substances as lubricants, such as petroleum jelly, cooking oil, baby oil, mineral oil, skin lotions, suntan lotions, cold creams, butter or margarine. Contact with oil makes latex condoms more likely to break or slip off due to loss of elasticity caused by the oils. Additionally, latex allergy precludes use of latex condoms and is one of the principal reasons for the use of other materials. In May 2009, the U.S. Food and Drug Administration (FDA) granted approval for the production of condoms composed of Vytex, latex that has been treated to remove 90% of the proteins responsible for allergic reactions. An allergen-free condom made of synthetic latex (polyisoprene) is also available.\nSynthetic.\nThe most common non-latex condoms are made from polyurethane. Condoms may also be made from other synthetic materials, such as AT-10 resin, and most polyisoprene.\nPolyurethane condoms tend to be the same width and thickness as latex condoms, with most polyurethane condoms between 0.04\u00a0mm and 0.07\u00a0mm thick.\nPolyurethane can be considered better than latex in several ways: it conducts heat better than latex, is not as sensitive to temperature and ultraviolet light (and so has less rigid storage requirements and a longer shelf life), can be used with oil-based lubricants, is less allergenic than latex, and does not have an odor. Polyurethane condoms have gained FDA approval for sale in the United States as an effective method of contraception and HIV prevention, and under laboratory conditions have been shown to be just as effective as latex for these purposes.\nHowever, polyurethane condoms are less elastic than latex ones, and may be more likely to slip or break than latex, lose their shape or bunch up more than latex, and are more expensive.\nPolyisoprene is a synthetic version of natural rubber latex. While significantly more expensive, it has the advantages of latex (such as being softer and more elastic than polyurethane condoms) without the protein which is responsible for latex allergies. Unlike polyurethane condoms, they cannot be used with an oil-based lubricant.\nLambskin.\nCondoms made from sheep intestines, labeled \"lambskin\", are also available. Although they are generally effective as a contraceptive by blocking sperm, it is presumed that they are less effective than latex in preventing the transmission of sexually transmitted infections because of pores in the material. This is based on the idea that intestines, by their nature, are porous, permeable membranes, and while sperm are too large to pass through the pores, viruses \u2014 such as HIV, herpes, and genital warts \u2014 are small enough to pass. However, there are to date no clinical data confirming or denying this theory.\nAs a result of laboratory data on condom porosity, in 1989, the FDA began requiring lambskin condom manufacturers to indicate that the products were not to be used for the prevention of sexually transmitted infections. This was based on the presumption that lambskin condoms would be less effective than latex in preventing HIV transmission, rather than a conclusion that lambskin condoms lack efficacy in STI prevention altogether. An FDA publication in 1992 states that lambskin condoms \"provide good birth control and a varying degree of protection against some, but not all, sexually transmitted diseases\" and that the labelling requirement was decided upon because the FDA \"cannot expect people to know which STDs they need to be protected against\", and since \"the reality is that you don't know what your partner has, we wanted natural-membrane condoms to have labels that don't allow the user to assume they're effective against the small viral STDs.\"\nSome believe that lambskin condoms provide a more \"natural\" sensation and lack the allergens inherent to latex. Still, because of their lesser protection against infection, other hypoallergenic materials such as polyurethane are recommended for latex-allergic users and partners. Lambskin condoms are also significantly more expensive than different types, and as slaughter by-products, they are also not vegetarian.\nSpermicide.\nSome latex condoms are lubricated at the manufacturer with a small amount of a nonoxynol-9, a spermicidal chemical. According to \"Consumer Reports\", condoms lubricated with spermicide have no additional benefit in preventing pregnancy, have a shorter shelf life, and may cause urinary tract infections in women. In contrast, application of separately packaged spermicide \"is\" believed to increase the contraceptive efficacy of condoms.\nNonoxynol-9 was once believed to offer additional protection against STDs (including HIV) but recent studies have shown that, with frequent use, nonoxynol-9 may increase the risk of HIV transmission. The World Health Organization says that spermicidally lubricated condoms should no longer be promoted. However, it recommends using a nonoxynol-9 lubricated condom over no condom at all. As of 2005[ [update]], nine condom manufacturers have stopped manufacturing condoms with nonoxynol-9 and Planned Parenthood has discontinued the distribution of condoms so lubricated.\nRibbed and studded.\nTextured condoms include studded and ribbed condoms which can provide extra sensations to both partners. The studs or ribs can be located on the inside, outside, or both; alternatively, they are located in specific sections to provide directed stimulation to either the G-spot or frenulum. Many textured condoms which advertise \"mutual pleasure\" also are bulb-shaped at the top, to provide extra stimulation to the penis. Some women experience irritation during vaginal intercourse with studded condoms.\nOther.\nThe anti-rape condom is another variation designed to be worn by women. It is designed to cause pain to the attacker, hopefully allowing the victim a chance to escape.\nA collection condom is used to collect semen for fertility treatments or sperm analysis. These condoms are designed to maximize sperm life.\nSome condom-like devices are intended for entertainment only, such as glow-in-the dark condoms. These novelty condoms may not provide protection against pregnancy and STDs.\nIn February 2022, the U.S. Food and Drug Administration (FDA) approved the first condoms specifically indicated to help reduce transmission of sexually transmitted infections (STIs) during anal intercourse.\nPrevalence.\nThe prevalence of condom use varies greatly between countries. Most surveys of contraceptive use are among married women, or women in informal unions. Japan has the highest rate of condom usage in the world: in that country, condoms account for almost 80% of contraceptive use by married women. On average, in developed countries, condoms are the most popular method of birth control: 28% of married contraceptive users rely on condoms. In the average less-developed country, condoms are less common: only 6\u20138% of married contraceptive users choose condoms.\nHistory.\nBefore the 19th century.\nWhether condoms were used in ancient civilizations is debated by archaeologists and historians. In ancient Egypt, Greece, and Rome, pregnancy prevention was generally seen as a woman's responsibility, and the only well documented contraception methods were female-controlled devices. In Asia before the 15th century, some use of glans condoms (devices covering only the head of the penis) is recorded. Condoms seem to have been used for contraception, and to have been known only by members of the upper classes. In China, glans condoms may have been made of oiled silk paper, or of lamb intestines. In Japan, condoms called \"Kabuto-gata\" (\u7532\u5f62) were made of tortoise shell or animal horn.\nIn 16th-century Italy, anatomist and physician Gabriele Falloppio wrote a treatise on syphilis. The earliest documented strain of syphilis, first appearing in Europe in a 1490s outbreak, caused severe symptoms and often death within a few months of contracting the disease. Falloppio's treatise is the earliest uncontested description of condom use: it describes linen sheaths soaked in a chemical solution and allowed to dry before use. The cloths he described were sized to cover the glans of the penis, and were held on with a ribbon. Falloppio claimed that an experimental trial of the linen sheath demonstrated protection against syphilis.\nAfter this, the use of penis coverings to protect from disease is described in a wide variety of literature throughout Europe. The first indication that these devices were used for birth control, rather than disease prevention, is the 1605 theological publication \"De iustitia et iure\" (On justice and law) by Catholic theologian Leonardus Lessius, who condemned them as immoral. In 1666, the English Birth Rate Commission attributed a recent downward fertility rate to use of \"condons\", the first documented use of that word or any similar spelling. Other early spellings include \"condam\" and \"quondam\", from which the Italian derivation \"guantone\" has been suggested, from \"guanto\", \"a glove\".\nIn addition to linen, condoms during the Renaissance were made out of intestines and bladder. In the late 16th century, Dutch traders introduced condoms made from \"fine leather\" to Japan. Unlike the horn condoms used previously, these leather condoms covered the entire penis.\nCasanova in the 18th century was one of the first reported using \"assurance caps\" to prevent impregnating his mistresses.\nFrom at least the 18th century, condom use was opposed in some legal, religious, and medical circles for essentially the same reasons that are given today: condoms reduce the likelihood of pregnancy, which some thought immoral or undesirable for the nation; they do not provide full protection against sexually transmitted infections, while belief in their protective powers was thought to encourage sexual promiscuity; and, they are not used consistently due to inconvenience, expense, or loss of sensation.\nDespite some opposition, the condom market grew rapidly. In the 18th century, condoms were available in a variety of qualities and sizes, made from either linen treated with chemicals, or \"skin\" (bladder or intestine softened by treatment with sulfur and lye). They were sold at pubs, barbershops, chemist shops, open-air markets, and at the theater throughout Europe and Russia. They later spread to America, although in every place there were generally used only by the middle and upper classes, due to both expense and lack of sex education.\n1800 through 1920s.\nThe early 19th century saw contraceptives promoted to the poorer classes for the first time. Writers on contraception tended to prefer other birth control methods to the condom. By the late 19th century, many feminists expressed distrust of the condom as a contraceptive, as its use was controlled and decided upon by men alone. They advocated instead for methods controlled by women, such as diaphragms and spermicidal douches. Other writers cited both the expense of condoms and their unreliability (they were often riddled with holes and often fell off or tore). Still, they discussed condoms as a good option for some and the only contraceptive that protects from disease.\nMany countries passed laws impeding the manufacture and promotion of contraceptives. In spite of these restrictions, condoms were promoted by traveling lecturers and in newspaper advertisements, using euphemisms in places where such ads were illegal. Instructions on how to make condoms at home were distributed in the United States and Europe. Despite social and legal opposition, at the end of the 19th century the condom was the Western world's most popular birth control method.\nBeginning in the second half of the 19th century, American rates of sexually transmitted diseases skyrocketed. Causes cited by historians include the effects of the American Civil War and the ignorance of prevention methods promoted by the Comstock laws. To fight the growing epidemic, sex education classes were introduced to public schools for the first time, teaching about venereal diseases and how they were transmitted. They generally taught abstinence was the only way to avoid sexually transmitted diseases. Condoms were not promoted for disease prevention because the medical community and moral watchdogs considered STDs to be punishment for sexual misbehavior. The stigma against people with these diseases was so significant that many hospitals refused to treat people with syphilis.\nThe German military was the first to promote condom use among its soldiers in the later 19th century. Early 20th century experiments by the American military concluded that providing condoms to soldiers significantly lowered rates of sexually transmitted diseases. During World War I, the United States and (at the beginning of the war only) Britain were the only countries with soldiers in Europe who did not provide condoms and promote their use.\nIn the decades after World War I, there remained social and legal obstacles to condom use throughout the U.S. and Europe. Founder of psychoanalysis Sigmund Freud opposed all methods of birth control because their failure rates were too high. Freud was especially opposed to the condom because he thought it cut down on sexual pleasure. Some feminists continued to oppose male-controlled contraceptives such as condoms. In 1920 the Church of England's Lambeth Conference condemned all \"unnatural means of conception avoidance\". The Bishop of London, Arthur Winnington-Ingram, complained of the huge number of condoms discarded in alleyways and parks, especially after weekends and holidays.\nHowever, European militaries continued to provide condoms to their members for disease protection, even in countries where they were illegal for the general population. Through the 1920s, catchy names and slick packaging became an increasingly important marketing technique for many consumer items, including condoms and cigarettes. Quality testing became more common, involving filling each condom with air followed by one of several methods intended to detect loss of pressure. Worldwide, condom sales doubled in the 1920s.\nRubber and manufacturing advances.\nIn 1839, Charles Goodyear discovered a way of processing natural rubber, which is too stiff when cold and too soft when warm, in such a way as to make it elastic. This proved to have advantages for the manufacture of condoms; unlike the sheep's gut condoms, they could stretch and did not tear quickly when used. The rubber vulcanization process was patented by Goodyear in 1844. The first rubber condom was produced in 1855. The earliest rubber condoms had a seam and were as thick as a bicycle inner tube. Besides this type, small rubber condoms covering only the glans were often used in England and the United States. There was more risk of losing them and if the rubber ring was too tight, it would constrict the penis. This type of condom was the original \"capote\" (French for condom), perhaps because of its resemblance to a woman's bonnet worn at that time, also called a capote.\nFor many decades, rubber condoms were manufactured by wrapping strips of raw rubber around penis-shaped molds, then dipping the wrapped molds in a chemical solution to cure the rubber. In 1912, Polish-born inventor Julius Fromm developed a new, improved manufacturing technique for condoms: dipping glass molds into a raw rubber solution. Called \"cement dipping\", this method required adding gasoline or benzene to the rubber to make it liquid. \nAround 1920 patent lawyer and vice-president of the United States Rubber Company Ernest Hopkinson invented a new technique of converting latex into rubber without a coagulant (demulsifier), which featured using water as a solvent and warm air to dry the solution, as well as optionally preserving liquid latex with ammonia. Condoms made this way, commonly called \"latex\" ones, required less labor to produce than cement-dipped rubber condoms, which had to be smoothed by rubbing and trimming. The use of water to suspend the rubber instead of gasoline and benzene eliminated the fire hazard previously associated with all condom factories. Latex condoms also performed better for the consumer: they were stronger and thinner than rubber condoms, and had a shelf life of five years (compared to three months for rubber).\nUntil the twenties, all condoms were individually hand-dipped by semi-skilled workers. Throughout the decade of the 1920s, advances in the automation of the condom assembly line were made. The first fully automated line was patented in 1930. Major condom manufacturers bought or leased conveyor systems, and small manufacturers were driven out of business. The skin condom, now significantly more expensive than the latex variety, became restricted to a niche high-end market.\n1930 to present.\nIn 1930 the Anglican Church's sanctioned the use of birth control by married couples. In 1931 the Federal Council of Churches in the U.S. issued a similar statement. The Roman Catholic Church responded by issuing the encyclical \"Casti connubii\" affirming its opposition to all contraceptives, a stance it has never reversed. In the 1930s, legal restrictions on condoms began to be relaxed. But during this period Fascist Italy and Nazi Germany increased restrictions on condoms (limited sales as disease preventatives were still allowed). During the Depression, condom lines by Schmid gained in popularity. Schmid still used the cement-dipping method of manufacture which had two advantages over the latex variety. Firstly, cement-dipped condoms could be safely used with oil-based lubricants. Secondly, while less comfortable, these older-style rubber condoms could be reused and so were more economical, a valued feature in hard times. More attention was brought to quality issues in the 1930s, and the U.S. Food and Drug Administration began to regulate the quality of condoms sold in the United States.\nThroughout World War II, condoms were not only distributed to male U.S. military members, but also heavily promoted with films, posters, and lectures. European and Asian militaries on both sides of the conflict also provided condoms to their troops throughout the war, even Germany which outlawed all civilian use of condoms in 1941. In part because condoms were readily available, soldiers found a number of non-sexual uses for the devices, many of which continue to this day. After the war, condom sales continued to grow. From 1955 to 1965, 42% of Americans of reproductive age relied on condoms for birth control. In Britain from 1950 to 1960, 60% of married couples used condoms. The birth control pill became the world's most popular method of birth control in the years after its 1960 d\u00e9but, but condoms remained a strong second. The U.S. Agency for International Development pushed condom use in developing countries to help solve the \"world population crises\": by 1970 hundreds of millions of condoms were being used each year in India alone.(This number has grown in recent decades: in 2004, the government of India purchased 1.9 billion condoms for distribution at family planning clinics.)\nIn the 1960s and 1970s quality regulations tightened, and more legal barriers to condom use were removed. In Ireland, legal condom sales were allowed for the first time in 1978. Advertising, however was one area that continued to have legal restrictions. In the late 1950s, the American National Association of Broadcasters banned condom advertisements from national television; this policy remained in place until 1979.\nAfter it was discovered in the early 1980s that AIDS can be a sexually transmitted infection, the use of condoms was encouraged to prevent transmission of HIV. Despite opposition by some political, religious, and other figures, national condom promotion campaigns occurred in the U.S. and Europe. These campaigns increased condom use significantly.\nDue to increased demand and greater social acceptance, condoms began to be sold in a wider variety of retail outlets, including in supermarkets and in discount department stores such as Walmart. Condom sales increased every year until 1994, when media attention to the AIDS pandemic began to decline. The phenomenon of decreasing use of condoms as disease preventatives has been called \"prevention fatigue\" or \"condom fatigue\". Observers have cited condom fatigue in both Europe and North America. As one response, manufacturers have changed the tone of their advertisements from scary to humorous.\nNew developments continued to occur in the condom market, with the first polyurethane condom\u2014branded Avanti and produced by the manufacturer of Durex\u2014introduced in the 1990s. Worldwide condom use is expected to continue to grow: one study predicted that developing nations would need 18.6 billion condoms by 2015. As of September\u00a02013[ [update]], condoms are available inside prisons in Canada, most of the European Union, Australia, Brazil, Indonesia, South Africa, and the US states of Vermont (on September 17, 2013, the Californian Senate approved a bill for condom distribution inside the state's prisons, but the bill was not yet law at the time of approval).\nThe global condom market was estimated at US$9.2 billion in 2020.\nEtymology and other terms.\nThe term \"condom\" first appears in the early 18th century: early forms include \"condum\" (1706 and 1717), \"condon\" (1708) and \"cundum\" (1744). The word's etymology is unknown. In popular tradition, the invention and naming of the condom came to be attributed to an associate of England's King Charles II, one \"Dr. Condom\" or \"Earl of Condom\". There is however no evidence of the existence of such a person, and condoms had been used for over one hundred years before King Charles II acceded to the throne in 1660.\nA variety of unproven Latin etymologies have been proposed, including (receptacle), (house), and (scabbard or case). It has also been speculated to be from the Italian word \"guantone\", derived from \"guanto\", meaning glove. William E. Kruck wrote an article in 1981 concluding that, \"As for the word 'condom', I need state only that its origin remains completely unknown, and there ends this search for an etymology.\" Modern dictionaries may also list the etymology as \"unknown\".\nOther terms are also commonly used to describe condoms. In North America condoms are also commonly known as \"prophylactics\", or \"rubbers\". In Britain they may be called \"French letters\" or \"rubber johnnies\". Additionally, condoms may be referred to using the manufacturer's name.\nSociety and culture.\nSome moral and scientific criticism of condoms exists despite the many benefits of condoms agreed on by scientific consensus and sexual health experts.\nCondom usage is typically recommended for new couples who have yet to develop full trust in their partner with regard to STDs. Established couples on the other hand have few concerns about STDs, and can use other methods of birth control such as the pill, which does not act as a barrier to intimate sexual contact. Note that the polar debate with regard to condom usage is attenuated by the target group the argument is directed. Notably the age category and stable partner question are factors, as well as the distinction between heterosexual and homosexuals, who have different kinds of sex and have different risk consequences and factors.\nAmong the prime objections to condom usage is the blocking of erotic sensation, or the intimacy that barrier-free sex provides. As the condom is held tightly to the skin of the penis, it diminishes the delivery of stimulation through rubbing and friction. Condom proponents claim this has the benefit of making sex last longer, by diminishing sensation and delaying male ejaculation. Those who promote condom-free heterosexual sex (slang: \"bareback\") claim that the condom puts a barrier between partners, diminishing what is normally a highly sensual, intimate, and spiritual connection between partners.\nReligious.\nThe United Church of Christ (UCC), a Reformed denomination of the Congregationalist tradition, promotes the distribution of condoms in churches and faith-based educational settings. Michael Shuenemeyer, a UCC minister, has stated that \"The practice of safer sex is a matter of life and death. People of faith make condoms available because we have chosen life so that we and our children may live.\"\nOn the other hand, the Roman Catholic Church opposes all kinds of sexual acts outside of marriage, as well as any sexual act in which the chance of successful conception has been reduced by direct and intentional acts (for example, surgery to prevent conception) or foreign objects (for example, condoms).\nThe use of condoms to prevent STI transmission is not specifically addressed by Catholic doctrine, and is currently a topic of debate among theologians and high-ranking Catholic authorities. A few, such as Belgian Cardinal Godfried Danneels, believe the Catholic Church should actively support condoms used to prevent disease, especially serious diseases such as AIDS. However, the majority view\u2014including all statements from the Vatican\u2014is that condom-promotion programs encourage promiscuity, thereby actually increasing STI transmission. This view was most recently reiterated in 2009 by Pope Benedict XVI.\nThe Roman Catholic Church is the largest organized body of any world religion. The church has hundreds of programs dedicated to fighting the AIDS epidemic in Africa, but its opposition to condom use in these programs has been highly controversial.\nIn a November 2011 interview, Pope Benedict XVI discussed for the first time the use of condoms to prevent STI transmission. He said that the use of a condom can be justified in a few individual cases if the purpose is to reduce the risk of an HIV infection. He gave as an example male prostitutes. There was some confusion at first whether the statement applied only to homosexual prostitutes and thus not to heterosexual intercourse at all. However, Federico Lombardi, spokesman for the Vatican, clarified that it applied to heterosexual and transsexual prostitutes, whether male or female, as well. He did, however, also clarify that the Vatican's principles on sexuality and contraception had not been changed.\nScientific and environmental.\nMore generally, some scientific researchers have expressed objective concern over certain ingredients sometimes added to condoms, notably talc and nitrosamines. Dry dusting powders are applied to latex condoms before packaging to prevent the condom from sticking to itself when rolled up. Previously, talc was used by most manufacturers, but cornstarch is currently the most popular dusting powder. Although rare during normal use, talc is known to be potentially irritant to mucous membranes (such as in the vagina). Cornstarch is generally believed to be safe; however, some researchers have raised concerns over its use as well.\nNitrosamines, which are potentially carcinogenic in humans, are believed to be present in a substance used to improve elasticity in latex condoms. A 2001 review stated that humans regularly receive 1,000 to 10,000 times greater nitrosamine exposure from food and tobacco than from condom use and concluded that the risk of cancer from condom use is very low. However, a 2004 study in Germany detected nitrosamines in 29 out of 32 condom brands tested, and concluded that exposure from condoms might exceed the exposure from food by 1.5- to 3-fold.\nIn addition, the large-scale use of disposable condoms has resulted in concerns over their environmental impact via littering and in landfills, where they can eventually wind up in wildlife environments if not incinerated or otherwise permanently disposed of first. Polyurethane condoms in particular, given they are a form of plastic, are not biodegradable, and latex condoms take a very long time to break down. Experts, such as AVERT, recommend condoms be disposed of in a garbage receptacle, as flushing them down the toilet (which some people do) may cause plumbing blockages and other problems. Furthermore, the plastic and foil wrappers condoms are packaged in are also not biodegradable. However, the benefits condoms offer are widely considered to offset their small landfill mass. Frequent condom or wrapper disposal in public areas such as a parks have been seen as a persistent litter problem.\nWhile biodegradable, latex condoms damage the environment when disposed of improperly. According to the Ocean Conservancy, condoms, along with certain other types of trash, cover the coral reefs and smother sea grass and other bottom dwellers. The United States Environmental Protection Agency also has expressed concerns that many animals might mistake the litter for food.\nCultural barriers to use.\nIn much of the Western world, the introduction of the pill in the 1960s was associated with a decline in condom use. In Japan, oral contraceptives were not approved for use until September 1999, and even then access was more restricted than in other industrialized nations. Perhaps because of this restricted access to hormonal contraception, Japan has the highest rate of condom usage in the world: in 2008, 80% of contraceptive users relied on condoms.\nCultural attitudes toward gender roles, contraception, and sexual activity vary greatly around the world, and range from extremely conservative to extremely liberal. But in places where condoms are misunderstood, mischaracterised, demonised, or looked upon with overall cultural disapproval, the prevalence of condom use is directly affected. In less-developed countries and among less-educated populations, misperceptions about how disease transmission and conception work negatively affect the use of condoms; additionally, in cultures with more traditional gender roles, women may feel uncomfortable demanding that their partners use condoms.\nAs an example, Latino immigrants in the United States often face cultural barriers to condom use. A study on female HIV prevention published in the \"Journal of Sex Health Research\" asserts that Latino women often lack the attitudes needed to negotiate safe sex due to traditional gender-role norms in the Latino community, and may be afraid to bring up the subject of condom use with their partners. Women who participated in the study often reported that because of the general machismo subtly encouraged in Latino culture, their male partners would be angry or possibly violent at the woman's suggestion that they use condoms. A similar phenomenon has been noted in a survey of low-income American black women; the women in this study also reported a fear of violence at the suggestion to their male partners that condoms be used.\nA telephone survey conducted by Rand Corporation and Oregon State University, and published in the \"Journal of Acquired Immune Deficiency Syndromes\" showed that belief in AIDS conspiracy theories among United States black men is linked to rates of condom use. As conspiracy beliefs about AIDS grow in a given sector of these black men, consistent condom use drops in that same sector. Female use of condoms was not similarly affected.\nIn the African continent, condom promotion in some areas has been impeded by anti-condom campaigns by some Muslim and Catholic clerics. Among the Maasai in Tanzania, condom use is hampered by an aversion to \"wasting\" sperm, which is given sociocultural importance beyond reproduction. Sperm is believed to be an \"elixir\" to women and to have beneficial health effects. Maasai women believe that, after conceiving a child, they must have sexual intercourse repeatedly so that the additional sperm aids the child's development. Frequent condom use is also considered by some Maasai to cause impotence. Some women in Africa believe that condoms are \"for prostitutes\" and that respectable women should not use them. A few clerics even promote the lie that condoms are deliberately laced with HIV. In the United States, possession of many condoms has been used by police to accuse women of engaging in prostitution. The Presidential Advisory Council on HIV/AIDS has condemned this practice and there are efforts to end it.\nMiddle-Eastern couples who have not had children, because of the strong desire and social pressure to establish fertility as soon as possible within marriage, rarely use condoms.\nIn 2017, India restricted TV advertisements for condoms to between the hours of 10 pm to 6 am. Family planning advocates were against this, saying it was liable to \"undo decades of progress on sexual and reproductive health\".\nMajor manufacturers.\nOne analyst described the size of the condom market as something that \"boggles the mind\". Numerous small manufacturers, nonprofit groups, and government-run manufacturing plants exist around the world. Within the condom market, there are several major contributors, among them both for-profit businesses and philanthropic organizations. Most large manufacturers have ties to the business that reach back to the end of the 19th century.\nEconomics.\nIn the United States condoms usually cost less than US$1.00.\nResearch.\nA spray-on condom made of latex is intended to be easier to apply and more successful in preventing the transmission of diseases. As of 2009[ [update]], the spray-on condom was not going to market because the drying time could not be reduced below two to three minutes.\nThe Invisible Condom, developed at Universit\u00e9 Laval in Quebec, Canada, is a gel that hardens upon increased temperature after insertion into the vagina or rectum. In the lab, it has been shown to effectively block HIV and herpes simplex virus. The barrier breaks down and liquefies after several hours. As of 2005[ [update]], the invisible condom is in the clinical trial phase, and has not yet been approved for use.\nAlso developed in 2005 is a condom treated with an erectogenic compound. The drug-treated condom is intended to help the wearer maintain his erection, which should also help reduce slippage. If approved, the condom would be marketed under the Durex brand. As of 2007[ [update]], it was still in clinical trials. In 2009, Ansell Healthcare, the makers of Lifestyle condoms, introduced the X2 condom lubricated with \"Excite Gel\" which contains the amino acid L-arginine and is intended to improve the strength of the erectile response.\nIn March 2013, philanthropist Bill Gates offered US$100,000 grants through his foundation for a condom design that \"significantly preserves or enhances pleasure\" to encourage more males to adopt the use of condoms for safer sex. The grant information stated: \"The primary drawback from the male perspective is that condoms decrease pleasure as compared to no condom, creating a trade-off that many men find unacceptable, particularly given that the decisions about use must be made just prior to intercourse. Is it possible to develop a product without this stigma, or better, one that is felt to enhance pleasure?\" In November of the same year, 11 research teams were selected to receive the grant money.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5375", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=5375", "title": "Country code", "text": "Identification code for countries\nA country code is a short alphanumeric identification code for countries and dependent areas. Its primary use is in data processing and communications. Several identification systems have been developed.\nThe term \"country code\" frequently refers to ISO 3166-1 alpha-2, as well as the telephone country code, which is embodied in the E.164 recommendation by the International Telecommunication Union (ITU).\nISO 3166-1.\nThe standard ISO 3166-1 defines short identification codes for most countries and dependent areas:\nThe two-letter codes are used as the basis for other codes and applications, for example,\nOther applications are defined in ISO 3166-1 alpha-2.\nITU country codes.\nIn telecommunication, a \"country code\", or \"international subscriber dialing\" (ISD) code, is a telephone number prefix used in international direct dialing (IDD) and for destination routing of telephone calls to a country other than the caller's. A country or region with an autonomous telephone administration must apply for membership in the International Telecommunication Union (ITU) to participate in the international public switched telephone network (PSTN). County codes are defined by the ITU-T section of the ITU in standards E.123 and E.164.\nCountry codes constitute the international telephone numbering plan, and are dialed only when calling a telephone number in another country. They are dialed before the national telephone number. International calls require at least one additional prefix to be dialing before the country code, to connect the call to international circuits, the international call prefix. When printing telephone numbers this is indicated by a plus-sign (\"+\") in front of a complete international telephone number, per recommendation E164 by the ITU.\nOther country codes.\nThe developers of ISO 3166 intended that in time it would replace other coding systems.\nOther codings.\nCountry identities may be encoded in the following coding systems:\nLists of country codes by country.\n -\n -\n -\n -\n -\n -\n -\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5376", "revid": "1750837", "url": "https://en.wikipedia.org/wiki?curid=5376", "title": "Cladistics", "text": "Method of biological systematics in evolutionary biology\nCladistics (; from grc \" \"\" (kl\u00e1dos)\"\u00a0'branch') is an approach to biological classification in which organisms are categorized in groups (\"clades\") based on hypotheses of most recent common ancestry. The evidence for hypothesized relationships is typically shared derived characteristics (synapomorphies) that are not present in more distant groups and ancestors. However, from an empirical perspective, common ancestors are inferences based on a cladistic hypothesis of relationships of taxa whose character states can be observed. Theoretically, a last common ancestor and all its descendants constitute a (minimal) clade. Importantly, all descendants stay in their overarching ancestral clade. For example, if the terms \"worms\" or \"fishes\" were used within a \"strict\" cladistic framework, these terms would include humans. Many of these terms are normally used paraphyletically, outside of cladistics, e.g. as a 'grade', which are fruitless to precisely delineate, especially when including extinct species. Radiation results in the generation of new subclades by bifurcation, but in practice sexual hybridization may blur very closely related groupings.\nAs a hypothesis, a clade can only be rejected if some groupings were explicitly excluded. It may then be found that the excluded group did actually descend from the last common ancestor of the group, and thus emerged within the group. (\"Evolved from\" is misleading, because in cladistics all descendants stay in the ancestral group). Upon finding that the group is paraphyletic this way, either such excluded groups should be granted to the clade, or the group should be abolished.\nBranches down to the divergence to the next significant (e.g. extant) sister are considered stem-groupings of the clade, but in principle each level stands on its own, to be assigned a unique name. For a fully bifurcated tree, adding a group to a tree also adds an additional (named) clade, and a new level on that branch. Specifically, also extinct groups are always put on a side-branch, not distinguishing whether an actual ancestor of other groupings was found.\nThe techniques and nomenclature of cladistics have been applied to disciplines other than biology. (See phylogenetic nomenclature.)\nCladistics findings are posing a difficulty for taxonomy, where the rank and (genus-)naming of established groupings may turn out to be inconsistent.\nCladistics is now the most commonly used method to classify organisms.\nHistory.\nThe original methods used in cladistic analysis and the school of taxonomy derived from the work of the German entomologist Willi Hennig, who referred to it as phylogenetic systematics (also the title of his 1966 book); the terms \"cladistics\" and \"clade\" were popularized by other researchers. Cladistics in the original sense refers to a particular set of methods used in phylogenetic analysis, although it is now sometimes used to refer to the whole field.\nWhat is now called the cladistic method appeared as early as 1901 with a work by Peter Chalmers Mitchell for birds and subsequently by Robert John Tillyard (for insects) in 1921, and W. Zimmermann (for plants) in 1943.\nThe term \"clade\" was introduced in 1958 by Julian Huxley after having been coined by Lucien Cu\u00e9not in 1940, \"cladogenesis\" in 1958, \"cladistic\" by Arthur Cain and Harrison in 1960, \"cladist\" (for an adherent of Hennig's school) by Ernst Mayr in 1965, and \"cladistics\" in 1966. Hennig referred to his own approach as \"phylogenetic systematics\". From the time of his original formulation until the end of the 1970s, cladistics competed as an analytical and philosophical approach to systematics with phenetics and so-called evolutionary taxonomy. Phenetics was championed at this time by the numerical taxonomists Peter Sneath and Robert Sokal, and evolutionary taxonomy by Ernst Mayr.\nOriginally conceived, if only in essence, by Willi Hennig in a book published in 1950, cladistics did not flourish until its translation into English in 1966 (Lewin 1997). Today, cladistics is the most popular method for inferring phylogenetic trees from morphological data.\nIn the 1990s, the development of effective polymerase chain reaction techniques allowed the application of cladistic methods to biochemical and molecular genetic traits of organisms, vastly expanding the amount of data available for phylogenetics. At the same time, cladistics rapidly became popular in evolutionary biology, because computers made it possible to process large quantities of data about organisms and their characteristics.\nMethodology.\nThe cladistic method interprets each shared character state transformation as a potential piece of evidence for grouping. Synapomorphies (shared, derived character states) are viewed as evidence of grouping, while symplesiomorphies (shared ancestral character states) are not. The outcome of a cladistic analysis is a cladogram \u2013 a tree-shaped diagram (dendrogram) that is interpreted to represent the best hypothesis of phylogenetic relationships. Although traditionally such cladograms were generated largely on the basis of morphological characters and originally calculated by hand, genetic sequencing data and computational phylogenetics are now commonly used in phylogenetic analyses, and the parsimony criterion has been abandoned by many phylogeneticists in favor of more \"sophisticated\" but less parsimonious evolutionary models of character state transformation. Cladists contend that these models are unjustified because there is no evidence that they recover more \"true\" or \"correct\" results from actual empirical data sets \nEvery cladogram is based on a particular dataset analyzed with a particular method. Datasets are tables consisting of molecular, morphological, ethological and/or other characters and a list of operational taxonomic units (OTUs), which may be genes, individuals, populations, species, or larger taxa that are presumed to be monophyletic and therefore to form, all together, one large clade; phylogenetic analysis infers the branching pattern within that clade. Different datasets and different methods, not to mention violations of the mentioned assumptions, often result in different cladograms. Only scientific investigation can show which is more likely to be correct.\nUntil recently, for example, cladograms like the following have generally been accepted as accurate representations of the ancestral relations among turtles, lizards, crocodilians, and birds:\nIf this phylogenetic hypothesis is correct, then the last common ancestor of turtles and birds, at the branch near the \u25bc lived earlier than the last common ancestor of lizards and birds, near the \u2666. Most molecular evidence, however, produces cladograms more like this:\nIf this is accurate, then the last common ancestor of turtles and birds lived later than the last common ancestor of lizards and birds. Since the cladograms show two mutually exclusive hypotheses to describe the evolutionary history, at most one of them is correct.\nThe cladogram to the right represents the current universally accepted hypothesis that all primates, including strepsirrhines like the lemurs and lorises, had a common ancestor all of whose descendants are or were primates, and so form a clade; the name Primates is therefore recognized for this clade. Within the primates, all anthropoids (monkeys, apes, and humans) are hypothesized to have had a common ancestor all of whose descendants are or were anthropoids, so they form the clade called Anthropoidea. The \"prosimians\", on the other hand, form a paraphyletic taxon. The name Prosimii is not used in phylogenetic nomenclature, which names only clades; the \"prosimians\" are instead divided between the clades Strepsirhini and Haplorhini, where the latter contains Tarsiiformes and Anthropoidea.\nLemurs and tarsiers may have looked closely related to humans, in the sense of being close on the evolutionary tree to humans. However, from the perspective of a tarsier, humans and lemurs would have looked close, in the exact same sense. Cladistics forces a neutral perspective, treating all branches (extant or extinct) in the same manner. It also forces one to try to make statements, and honestly take into account findings, about the exact historic relationships between the groups.\nTerminology for character states.\nThe following terms, coined by Hennig, are used to identify shared or distinct character states among groups:\nThe terms plesiomorphy and apomorphy are relative; their application depends on the position of a group within a tree. For example, when trying to decide whether the tetrapods form a clade, an important question is whether having four limbs is a synapomorphy of the earliest taxa to be included within Tetrapoda: did all the earliest members of the Tetrapoda inherit four limbs from a common ancestor, whereas all other vertebrates did not, or at least not homologously? By contrast, for a group within the tetrapods, such as birds, having four limbs is a plesiomorphy. Using these two terms allows a greater precision in the discussion of homology, in particular allowing clear expression of the hierarchical relationships among different homologous features.\nIt can be difficult to decide whether a character state is in fact the same and thus can be classified as a synapomorphy, which may identify a monophyletic group, or whether it only appears to be the same and is thus a homoplasy, which cannot identify such a group. There is a danger of circular reasoning: assumptions about the shape of a phylogenetic tree are used to justify decisions about character states, which are then used as evidence for the shape of the tree. Phylogenetics uses various forms of parsimony to decide such questions; the conclusions reached often depend on the dataset and the methods. Such is the nature of empirical science, and for this reason, most cladists refer to their cladograms as hypotheses of relationship. Cladograms that are supported by a large number and variety of different kinds of characters are viewed as more robust than those based on more limited evidence.\nTerminology for taxa.\nMono-, para- and polyphyletic taxa can be understood based on the shape of the tree (as done above), as well as based on their character states. These are compared in the table below.\nCriticism.\nCladistics, either generally or in specific applications, has been criticized from its beginnings. Decisions as to whether particular character states are homologous, a precondition of their being synapomorphies, have been challenged as involving circular reasoning and subjective judgements. Of course, the potential unreliability of evidence is a problem for any systematic method, or for that matter, for any empirical scientific endeavor at all.\nTransformed cladistics arose in the late 1970s in an attempt to resolve some of these problems by removing a priori assumptions about phylogeny from cladistic analysis, but it has remained unpopular.\nIssues.\nAncestors.\nThe cladistic method does not identify fossil species as actual ancestors of a clade. Instead, fossil taxa are identified as belonging to separate extinct branches. While a fossil species could be the actual ancestor of a clade, there is no way to know that. Therefore, a more conservative hypothesis is that the fossil taxon is related to other fossil and extant taxa, as implied by the pattern of shared apomorphic features.\nExtinction status.\nAn otherwise extinct group with any extant descendants, is not considered (literally) extinct, and for instance does not have a date of extinction.\nHybridization, interbreeding.\nAnything having to do with biology and sex is complicated and messy, and cladistics is no exception. Many species reproduce sexually, and are capable of interbreeding for millions of years. Worse, during such a period, many branches may have radiated, and it may take hundreds of millions of years for them to have whittled down to just two. Only then one can theoretically assign proper last common ancestors of groupings which do not inadvertently include earlier branches. The process of true cladistic bifurcation can thus take a much more extended time than one is usually aware of. In practice, for recent radiations, cladistically guided findings only give a coarse impression of the complexity. A more detailed account will give details about fractions of introgressions between groupings, and even geographic variations thereof. This has been used as an argument for the use of paraphyletic groupings, but typically other reasons are quoted.\nHorizontal gene transfer.\nHorizontal gene transfer is the mobility of genetic info between different organisms that can have immediate or delayed effects for the reciprocal host. There are several processes in nature which can cause horizontal gene transfer. This does typically not directly interfere with ancestry of the organism, but can complicate the determination of that ancestry. On another level, one can map the horizontal gene transfer processes, by determining the phylogeny of the individual genes using cladistics.\nNaming stability.\nIf there is unclarity in mutual relationships, there are a lot of possible trees. Assigning names to each possible clade may not be prudent. Furthermore, established names are discarded in cladistics, or alternatively carry connotations which may no longer hold, such as when additional groups are found to have emerged in them. Naming changes are the direct result of changes in the recognition of mutual relationships, which often is still in flux, especially for extinct species. Hanging on to older naming and/or connotations is counter-productive, as they typically do not reflect actual mutual relationships precisely at all. E.g. Archaea, Asgard archaea, protists, slime molds, worms, invertebrata, fishes, reptilia, monkeys, \"Ardipithecus\", \"Australopithecus\", \"Homo erectus\" all contain \"Homo sapiens\" cladistically, in their \"sensu lato\" meaning. For originally extinct stem groups, \"sensu lato\" generally means generously keeping previously included groups, which then may come to include even living species. A pruned \"sensu stricto\" meaning is often adopted instead, but the group would need to be restricted to a single branche on the stem. Other branches then get their own name and level. This is commensurate to the fact that more senior stem branches are in fact closer related to the resulting group than the more basal stem branches; that those stem branches only may have lived for a short time does not affect that assessment in cladistics.\nIn disciplines other than biology.\nThe comparisons used to acquire data on which cladograms can be based are not limited to the field of biology. Any group of individuals or classes that are hypothesized to have a common ancestor, and to which a set of common characteristics may or may not apply, can be compared pairwise. Cladograms can be used to depict the hypothetical descent relationships within groups of items in many different academic realms. The only requirement is that the items have characteristics that can be identified and measured.\nAnthropology and archaeology: Cladistic methods have been used to reconstruct the development of cultures or artifacts using groups of cultural traits or artifact features.\nComparative mythology and folktale use cladistic methods to reconstruct the protoversion of many myths. Mythological phylogenies constructed with mythemes clearly support low horizontal transmissions (borrowings), historical (sometimes Palaeolithic) diffusions and punctuated evolution. They also are a powerful way to test hypotheses about cross-cultural relationships among folktales.\nLiterature: Cladistic methods have been used in the classification of the surviving manuscripts of the \"Canterbury Tales\", and the manuscripts of the Sanskrit \"Charaka Samhita\".\nHistorical linguistics: Cladistic methods have been used to reconstruct the phylogeny of languages using linguistic features. This is similar to the traditional comparative method of historical linguistics, but is more explicit in its use of parsimony and allows much faster analysis of large datasets (computational phylogenetics).\nTextual criticism or stemmatics: Cladistic methods have been used to reconstruct the phylogeny of manuscripts of the same work (and reconstruct the lost original) using distinctive copying errors as apomorphies. This differs from traditional historical-comparative linguistics in enabling the editor to evaluate and place in genetic relationship large groups of manuscripts with large numbers of variants that would be impossible to handle manually. It also enables parsimony analysis of contaminated traditions of transmission that would be impossible to evaluate manually in a reasonable period of time.\nAstrophysics infers the history of relationships between galaxies to create branching diagram hypotheses of galaxy diversification.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5377", "revid": "774897", "url": "https://en.wikipedia.org/wiki?curid=5377", "title": "Calendar", "text": "System for organizing the days of year\nA calendar is a system of organizing days. This is done by giving names to periods of time, typically days, weeks, months and years. A date is the designation of a single and specific day within such a system. A calendar is also a physical record (often paper) of such a system. A calendar can also mean a list of planned events, such as a court calendar or a partly or fully chronological list of documents, such as a calendar of wills.\nPeriods in a calendar (such as years and months) are usually, though not necessarily, synchronized with the cycle of the sun or the moon. The most common type of pre-modern calendar was the lunisolar calendar, a lunar calendar that occasionally adds one intercalary month to remain synchronized with the solar year over the long term.\nEtymology.\nThe term \"calendar\" is taken from , the term for the first day of the month in the Roman calendar, related to the verb 'to call out', referring to the \"calling\" of the new moon when it was first seen. Latin meant 'account book, register' (as accounts were settled and debts were collected on the calends of each month). The Latin term was adopted in Old French as and from there in Middle English as by the 13th\u00a0century (the spelling \"calendar\" is early modern).\nHistory.\nThe course of the sun and the moon are the most salient regularly recurring natural events useful for timekeeping, and in pre-modern societies around the world lunation and the year were most commonly used as time units. Nevertheless, the Roman calendar contained remnants of a very ancient pre-Etruscan 10-month solar year. \nThe first recorded physical calendars, dependent on the development of writing in the Ancient Near East, are the Bronze Age Egyptian and Sumerian calendars.\nDuring the Vedic period India developed a sophisticated timekeeping methodology and calendars for Vedic rituals. According to Yukio Ohashi, the Vedanga calendar in ancient India was based on astronomical studies during the Vedic Period and was not derived from other cultures.\nA large number of calendar systems in the Ancient Near East were based on the Babylonian calendar dating from the Iron Age, among them the calendar system of the Persian Empire, which in turn gave rise to the Zoroastrian calendar and the Hebrew calendar. \nA great number of Hellenic calendars were developed in Classical Greece, and during the Hellenistic period they gave rise to the ancient Roman calendar and to various Hindu calendars. \nCalendars in antiquity were lunisolar, depending on the introduction of intercalary months to align the solar and the lunar years. This was mostly based on observation, but there may have been early attempts to model the pattern of intercalation algorithmically, as evidenced in the fragmentary 2nd-century Coligny calendar.\nThe Roman calendar was reformed by Julius Caesar in 46 BC. His \"Julian\" calendar was no longer dependent on the observation of the new moon, but followed an algorithm of introducing a leap day every four years. This created a dissociation of the calendar month from lunation.\nThe Islamic calendar is based on the prohibition of intercalation (\"nasi\"') by Muhammad, in Islamic tradition dated to a sermon given on 9 Dhu al-Hijjah AH 10 (Julian date: 6 March 632). This resulted in an observation-based lunar calendar that shifts relative to the seasons of the solar year.\nModern reforms.\nThe first calendar reform of the early modern era resulted in the Gregorian calendar, introduced in 1582 and based on the observation of a long-term shift between the Julian calendar and the solar year.\nThere have been several modern proposals for reform of the modern calendar, such as the World Calendar, the International Fixed Calendar, the Holocene calendar, and the Hanke-Henry Permanent Calendar. Such ideas are mooted from time to time, but have failed to gain traction because of the loss of continuity and the massive upheaval that implementing them would involve, as well as their effect on cycles of religious activity.\nSystems.\nA full calendar system has a different calendar date for every day. Thus the week cycle is by itself not a full calendar system; neither is a system to name the days within a year without a system for identifying the years.\nThe simplest calendar system just counts time periods from a reference date. This applies for the Julian day or Unix Time. Virtually the only possible variation is using a different reference date, in particular, one less distant in the past to make the numbers smaller. Computations in these systems are just a matter of addition and subtraction.\nOther calendars have one (or multiple) larger units of time.\nCalendars that contain one level of cycles:\nCalendars with two levels of cycles:\nCycles can be synchronized with periodic phenomena:\nVery commonly a calendar includes more than one type of cycle or has both cyclic and non-cyclic elements.\nMost calendars incorporate more complex cycles. For example, the vast majority of them track years, months, weeks and days. The seven-day week is practically universal, though its use varies. It has run uninterrupted for millennia.\nSolar.\nSolar calendars assign a \"date\" to each solar day. A day may consist of the period between sunrise and sunset, with a following period of night, or it may be a period between successive events such as two sunsets. The length of the interval between two such successive events may be allowed to vary slightly during the year, or it may be averaged into a mean solar day. Other types of calendar may also use a solar day.\nLunar.\nNot all calendars use the solar year as a unit. A lunar calendar is one in which days are numbered within each lunar phase cycle. Because the length of the lunar month is not an even fraction of the length of the tropical year, a purely lunar calendar quickly drifts against the seasons, which do not vary much near the equator. It does, however, stay constant with respect to other phenomena, notably tides. An example is the Islamic calendar.\nAlexander Marshack, in a controversial reading, believed that marks on a bone baton (c.\u200925,000 BC) represented a lunar calendar. Other marked bones may also represent lunar calendars. Similarly, Michael Rappenglueck believes that marks on a 15,000-year-old cave painting represent a lunar calendar.\nLunisolar.\nA lunisolar calendar is a lunar calendar that compensates by adding an extra month as needed to realign the months with the seasons. Prominent examples of lunisolar calendar are Hindu calendar and Buddhist calendar that are popular in South Asia and Southeast Asia. Another example is the Hebrew calendar, which uses a 19-year cycle.\nSubdivisions.\nNearly all calendar systems group consecutive days into \"months\" and also into \"years\". In a \"solar calendar\" a \"year\" approximates Earth's tropical year (that is, the time it takes for a complete cycle of seasons), traditionally used to facilitate the planning of agricultural activities. In a \"lunar calendar\", the \"month\" approximates the cycle of the moon phase. Consecutive days may be grouped into other periods such as the week.\nBecause the number of days in the \"tropical year\" is not a whole number, a solar calendar must have a different number of days in different years. This may be handled, for example, by adding an extra day in leap years. The same applies to months in a lunar calendar and also the number of months in a year in a lunisolar calendar. This is generally known as intercalation. Even if a calendar is solar, but not lunar, the year cannot be divided entirely into months that never vary in length.\nCultures may define other units of time, such as the week, for the purpose of scheduling regular activities that do not easily coincide with months or years. Many cultures use different baselines for their calendars' starting years. Historically, several countries have based their calendars on regnal years, a calendar based on the reign of their current sovereign. For example, the year 2006 in Japan is year 18 Heisei, with Heisei being the era name of Emperor Akihito.\nOther types.\nArithmetical and astronomical.\nAn \"astronomical calendar\" is based on ongoing observation; examples are the religious Islamic calendar and the old religious Jewish calendar in the time of the Second Temple. Such a calendar is also referred to as an \"observation-based\" calendar. The advantage of such a calendar is that it is perfectly and perpetually accurate. The disadvantage is that working out when a particular date would occur is difficult.\nAn \"arithmetic calendar\" is one that is based on a strict set of rules; an example is the current Jewish calendar. Such a calendar is also referred to as a \"rule-based\" calendar. The advantage of such a calendar is the ease of calculating when a particular date occurs. The disadvantage is imperfect accuracy. Furthermore, even if the calendar is very accurate, its accuracy diminishes slowly over time, owing to changes in Earth's rotation. This limits the lifetime of an accurate arithmetic calendar to a few thousand years. After then, the rules would need to be modified from observations made since the invention of the calendar.\nComplete and incomplete.\nCalendars may be either complete or incomplete. Complete calendars provide a way of naming each consecutive day, while incomplete calendars do not. The early Roman calendar, which had no way of designating the days of the winter months other than to lump them together as \"winter\", is an example of an incomplete calendar, while the Gregorian calendar is an example of a complete calendar.\nUsage.\nThe primary practical use of a calendar is to identify days: to be informed about or to agree on a future event and to record an event that has happened. Days may be significant for agricultural, civil, religious, or social reasons. For example, a calendar provides a way to determine when to start planting or harvesting, which days are religious or civil holidays, which days mark the beginning and end of business accounting periods, and which days have legal significance, such as the day taxes are due or a contract expires. Also, a calendar may, by identifying a day, provide other useful information about the day such as its season.\nCalendars are also used to help people manage their personal schedules, time, and activities, particularly when individuals have numerous work, school, and family commitments. People frequently use multiple systems and may keep both a business and family calendar to help prevent them from overcommitting their time.\nCalendars are also used as part of a complete timekeeping system: date and time of day together specify a moment in time. In the modern world, timekeepers can show time, date, and weekday. Some may also show the lunar phase.\nGregorian.\nThe Gregorian calendar is the \"de facto\" international standard and is used almost everywhere in the world for civil purposes. The widely used solar aspect is a cycle of leap days in a 400-year cycle designed to keep the duration of the year aligned with the solar year. There is a lunar aspect which approximates the position of the moon during the year, and is used in the calculation of the date of Easter. Each Gregorian year has either 365 or 366 days (the leap day being inserted as 29 February), amounting to an average Gregorian year of 365.2425 days (compared to a solar year of 365.2422 days).\nThe calendar was introduced in 1582 as a refinement to the Julian calendar, which had been in use throughout the European Middle Ages, amounting to a 0.002% correction in the length of the year. During the Early Modern period, its adoption was mostly limited to Roman Catholic nations, but by the 19th century it had become widely adopted for the sake of convenience in international trade. The last European country to adopt it was Greece, in 1923.\nThe calendar epoch used by the Gregorian calendar is inherited from the medieval convention established by Dionysius Exiguus and associated with the Julian calendar. The year number is variously given as AD (for \"Anno Domini\") or CE (for \"Common Era\" or \"Christian Era\").\nReligious.\nThe most important use of pre-modern calendars is keeping track of the liturgical year and the observation of religious feast days.\nWhile the Gregorian calendar is itself historically motivated to the calculation of the Easter date, it is now in worldwide secular use as the \"de facto\" standard. Alongside the use of the Gregorian calendar for secular matters, there remain several calendars in use for religious purposes.\nWestern Christian liturgical calendars are based on the cycle of the Roman Rite of the Catholic Church and generally include the liturgical seasons of Advent, Christmas, Ordinary Time (Time after Epiphany), Lent, Easter, and Ordinary Time (Time after Pentecost). Some Christian calendars do not include Ordinary Time and every day falls into a denominated season.\nEastern Christians, including the Orthodox Church, use the Julian calendar.\nThe Islamic calendar or Hijri calendar is a lunar calendar consisting of 12 lunar months in a year of 354 or 355 days. It is used to date events in most of the Muslim countries (concurrently with the Gregorian calendar) and used by Muslims everywhere to determine the proper day on which to celebrate Islamic holy days and festivals. Its epoch is the Hijra (corresponding to AD 622)\nWith an annual drift of 11 or 12 days, the seasonal relation is repeated approximately every 33 Islamic years.\nVarious Hindu calendars remain in use in the Indian subcontinent, including the Nepali calendars, Bengali calendar, Malayalam calendar, Tamil calendar, Vikrama Samvat used in Northern India, and Shalivahana calendar in the Deccan states.\nThe Buddhist calendar and the traditional lunisolar calendars of Cambodia, Laos, Myanmar, Sri Lanka and Thailand are also based on an older version of the Hindu calendar.\nMost of the Hindu calendars are inherited from a system first enunciated in Vedanga Jyotisha of Lagadha, standardized in the \"S\u016brya Siddh\u0101nta\" and subsequently reformed by astronomers such as \u0100ryabha\u1e6da (AD 499), Var\u0101hamihira (6th century) and Bh\u0101skara II (12th century).\nThe Hebrew calendar is used by Jews worldwide for religious and cultural affairs, also influences civil matters in Israel (such as national holidays) and can be used business dealings (such as for the dating of cheques).\nFollowers of the Bah\u00e1\u02bc\u00ed Faith use the Bah\u00e1\u02bc\u00ed calendar. The Bah\u00e1\u02bc\u00ed Calendar, also known as the Badi Calendar was first established by the Bab in the Kitab-i-Asma. The Bah\u00e1\u02bc\u00ed Calendar is also purely a solar calendar and comprises 19 months each having nineteen days.\nNational.\nThe Chinese, Hebrew, Hindu, and Julian calendars are widely used for religious and social purposes.\nThe Iranian (Persian) calendar is used in Iran and some parts of Afghanistan. The Assyrian calendar is in use by the members of the Assyrian community in the Middle East (mainly Iraq, Syria, Turkey, and Iran) and the diaspora. The first year of the calendar is exactly 4750 years prior to the start of the Gregorian calendar. The Ethiopian calendar or Ethiopic calendar is the principal calendar used in Ethiopia and Eritrea, with the Oromo calendar also in use in some areas. In neighboring Somalia, the Somali calendar co-exists alongside the Gregorian and Islamic calendars. In Thailand, where the Thai solar calendar is used, the months and days have adopted the western standard, although the years are still based on the traditional Buddhist calendar.\nFiscal.\nA fiscal calendar generally means the accounting year of a government or a business. It is used for budgeting, keeping accounts, and taxation. It is a set of 12 months that may start at any date in a year. The US government's fiscal year starts on 1 October and ends on 30 September. The government of India's fiscal year starts on 1 April and ends on 31 March. Small traditional businesses in India start the fiscal year on Diwali festival and end the day before the next year's Diwali festival.\nIn accounting (and particularly accounting software), a fiscal calendar (such as a 4/4/5 calendar) fixes each month at a specific number of weeks to facilitate comparisons from month to month and year to year. January always has exactly 4 weeks (Sunday through Saturday), February has 4 weeks, March has 5 weeks, etc. Note that this calendar will normally need to add a 53rd week to every 5th or 6th year, which might be added to December or might not be, depending on how the organization uses those dates. There exists an international standard way to do this (the ISO week). The ISO week starts on a Monday and ends on a Sunday. Week 1 is always the week that contains 4 January in the Gregorian calendar.\nFormats.\nThe term \"calendar\" applies not only to a given scheme of timekeeping but also to a specific record or device displaying such a scheme, for example, an appointment book in the form of a pocket calendar (or personal organizer), desktop calendar, a wall calendar, etc.\nIn a paper calendar, one or two sheets can show a single day, a week, a month, or a year. If a sheet is for a single day, it easily shows the date and the weekday. If a sheet is for multiple days it shows a conversion table to convert from weekday to date and back. With a special pointing device, or by crossing out past days, it may indicate the current date and weekday. This is the most common usage of the word.\nIn the US Sunday is considered the first day of the week and so appears on the far left and Saturday the last day of the week appearing on the far right. In Britain, the weekend may appear at the end of the week so the first day is Monday and the last day is Sunday. The US calendar display is also used in Britain.\nIt is common to display the Gregorian calendar in separate monthly grids of seven columns (from Monday to Sunday, or Sunday to Saturday depending on which day is considered to start the week\u00a0\u2013 this varies according to country) and five to six rows (or rarely, four rows when the month of February contains 28 days in common years beginning on the first day of the week), with the day of the month numbered in each cell, beginning with 1. The sixth row is sometimes eliminated by marking 23/30 and 24/31 together as necessary.\nWhen working with weeks rather than months, a continuous format is sometimes more convenient, where no blank cells are inserted to ensure that the first day of a new month begins on a fresh row.\nSoftware.\nCalendaring software provides users with an electronic version of a calendar, and may additionally provide an appointment book, address book, or contact list.\nCalendaring is a standard feature of many PDAs, EDAs, and smartphones. The software may be a local package designed for individual use (e.g., Lightning extension for Mozilla Thunderbird, Microsoft Outlook without Exchange Server, or Windows Calendar) or maybe a networked package that allows for the sharing of information between users (e.g., Mozilla Sunbird, Windows Live Calendar, Google Calendar, or Microsoft Outlook with Exchange Server).\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5378", "revid": "8372814", "url": "https://en.wikipedia.org/wiki?curid=5378", "title": "Physical cosmology", "text": "Branch of cosmology which studies mathematical models of the universe\nPhysical cosmology is a branch of cosmology concerned with the study of cosmological models. A cosmological model, or simply cosmology, provides a description of the largest-scale structures and dynamics of the universe and allows study of fundamental questions about its origin, structure, evolution, and ultimate fate. Cosmology as a science originated with the Copernican principle, which implies that celestial bodies obey identical physical laws to those on Earth, and Newtonian mechanics, which first allowed those physical laws to be understood.\nPhysical cosmology, as it is now understood, began with the development in 1915 of Albert Einstein's general theory of relativity, followed by major observational discoveries in the 1920s: first, Edwin Hubble discovered that the universe contains a huge number of external galaxies beyond the Milky Way; then, work by Vesto Slipher and others showed that the universe is expanding. These advances made it possible to speculate about the origin of the universe, and allowed the establishment of the Big Bang theory, by Georges Lema\u00eetre, as the leading cosmological model. A few researchers still advocate a handful of alternative cosmologies; however, most cosmologists agree that the Big Bang theory best explains the observations.\nDramatic advances in observational cosmology since the 1990s, including the cosmic microwave background, distant supernovae and galaxy redshift surveys, have led to the development of a standard model of cosmology. This model requires the universe to contain large amounts of dark matter and dark energy whose nature is currently not well understood, but the model gives detailed predictions that are in excellent agreement with many diverse observations.\nCosmology draws heavily on the work of many disparate areas of research in theoretical and applied physics. Areas relevant to cosmology include particle physics experiments and theory, theoretical and observational astrophysics, general relativity, quantum mechanics, and plasma physics.\nSubject history.\nModern cosmology developed along tandem tracks of theory and observation. In 1916, Albert Einstein published his theory of general relativity, which provided a unified description of gravity as a geometric property of space and time. At the time, Einstein believed in a static universe, but found that his original formulation of the theory did not permit it. This is because masses distributed throughout the universe gravitationally attract, and move toward each other over time. However, he realized that his equations permitted the introduction of a constant term which could counteract the attractive force of gravity on the cosmic scale. Einstein published his first paper on relativistic cosmology in 1917, in which he added this \"cosmological constant\" to his field equations in order to force them to model a static universe. The Einstein model describes a static universe; space is finite and unbounded (analogous to the surface of a sphere, which has a finite area but no edges). However, this so-called Einstein model is unstable to small perturbations\u2014it will eventually start to expand or contract. It was later realized that Einstein's model was just one of a larger set of possibilities, all of which were consistent with general relativity and the cosmological principle. The cosmological solutions of general relativity were found by Alexander Friedmann in the early 1920s. His equations describe the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker universe, which may expand or contract, and whose geometry may be open, flat, or closed.\nIn the 1910s, Vesto Slipher (and later Carl Wilhelm Wirtz) interpreted the red shift of spiral nebulae as a Doppler shift that indicated they were receding from Earth. However, it is difficult to determine the distance to astronomical objects. One way is to compare the physical size of an object to its angular size, but a physical size must be assumed to do this. Another method is to measure the brightness of an object and assume an intrinsic luminosity, from which the distance may be determined using the inverse-square law. Due to the difficulty of using these methods, they did not realize that the nebulae were actually galaxies outside our own Milky Way, nor did they speculate about the cosmological implications. In 1927, the Belgian Roman Catholic priest Georges Lema\u00eetre independently derived the Friedmann\u2013Lema\u00eetre\u2013Robertson\u2013Walker equations and proposed, on the basis of the recession of spiral nebulae, that the universe began with the \"explosion\" of a \"primeval atom\"\u2014which was later called the Big Bang. In 1929, Edwin Hubble provided an observational basis for Lema\u00eetre's theory. Hubble showed that the spiral nebulae were galaxies by determining their distances using measurements of the brightness of Cepheid variable stars. He discovered a relationship between the redshift of a galaxy and its distance. He interpreted this as evidence that the galaxies are receding from Earth in every direction at speeds proportional to their distance. This fact is now known as Hubble's law, though the numerical factor Hubble found relating recessional velocity and distance was off by a factor of ten, due to not knowing about the types of Cepheid variables.\nGiven the cosmological principle, Hubble's law suggested that the universe was expanding. Two primary explanations were proposed for the expansion. One was Lema\u00eetre's Big Bang theory, advocated and developed by George Gamow. The other explanation was Fred Hoyle's steady state model in which new matter is created as the galaxies move away from each other. In this model, the universe is roughly the same at any point in time.\nFor a number of years, support for these theories was evenly divided. However, the observational evidence began to support the idea that the universe evolved from a hot dense state. The discovery of the cosmic microwave background in 1965 lent strong support to the Big Bang model, and since the precise measurements of the cosmic microwave background by the Cosmic Background Explorer in the early 1990s, few cosmologists have seriously proposed other theories of the origin and evolution of the cosmos. One consequence of this is that in standard general relativity, the universe began with a singularity, as demonstrated by Roger Penrose and Stephen Hawking in the 1960s.\nAn alternative view to extend the Big Bang model, suggesting the universe had no beginning or singularity and the age of the universe is infinite, has been presented.\nEnergy of the cosmos.\nThe lightest chemical elements, primarily hydrogen and helium, were created during the Big Bang through the process of nucleosynthesis. In a sequence of stellar nucleosynthesis reactions, smaller atomic nuclei are then combined into larger atomic nuclei, ultimately forming stable iron group elements such as iron and nickel, which have the highest nuclear binding energies. The net process results in a \"later energy release\", meaning subsequent to the Big Bang. Such reactions of nuclear particles can lead to \"sudden energy releases\" from cataclysmic variable stars such as novae. Gravitational collapse of matter into black holes also powers the most energetic processes, generally seen in the nuclear regions of galaxies, forming \"quasars\" and \"active galaxies\".\nCosmologists cannot explain all cosmic phenomena exactly, such as those related to the accelerating expansion of the universe, using conventional forms of energy. Instead, cosmologists propose a new form of energy called dark energy that permeates all space. One hypothesis is that dark energy is just the vacuum energy, a component of empty space that is associated with the virtual particles that exist due to the uncertainty principle.\nThere is no clear way to define the total energy in the universe using the most widely accepted theory of gravity, general relativity. Therefore, it remains controversial whether the total energy is conserved in an expanding universe. For instance, each photon that travels through intergalactic space loses energy due to the redshift effect. This energy is not transferred to any other system, so seems to be permanently lost. On the other hand, some cosmologists insist that energy is conserved in some sense; this follows the law of conservation of energy.\nDifferent forms of energy may dominate the cosmos\u2014relativistic particles which are referred to as radiation, or non-relativistic particles referred to as matter. Relativistic particles are particles whose rest mass is zero or negligible compared to their kinetic energy, and so move at the speed of light or very close to it; non-relativistic particles have much higher rest mass than their energy and so move much slower than the speed of light.\nAs the universe expands, both matter and radiation become diluted. However, the energy densities of radiation and matter dilute at different rates. As a particular volume expands, mass-energy density is changed only by the increase in volume, but the energy density of radiation is changed both by the increase in volume and by the increase in the wavelength of the photons that make it up. Thus the energy of radiation becomes a smaller part of the universe's total energy than that of matter as it expands. The very early universe is said to have been 'radiation dominated' and radiation controlled the deceleration of expansion. Later, as the average energy per photon becomes roughly 10 eV and lower, matter dictates the rate of deceleration and the universe is said to be 'matter dominated'. The intermediate case is not treated well analytically. As the expansion of the universe continues, matter dilutes even further and the cosmological constant becomes dominant, leading to an acceleration in the universe's expansion.\nHistory of the universe.\nThe history of the universe is a central issue in cosmology. The history of the universe is divided into different periods called epochs, according to the dominant forces and processes in each period. The standard cosmological model is known as the Lambda-CDM model.\nEquations of motion.\nWithin the standard cosmological model, the equations of motion governing the universe as a whole are derived from general relativity with a small, positive cosmological constant. The solution is an expanding universe; due to this expansion, the radiation and matter in the universe cool down and become diluted. At first, the expansion is slowed down by gravitation attracting the radiation and matter in the universe. However, as these become diluted, the cosmological constant becomes more dominant and the expansion of the universe starts to accelerate rather than decelerate. In our universe this happened billions of years ago.\nParticle physics in cosmology.\nDuring the earliest moments of the universe, the average energy density was very high, making knowledge of particle physics critical to understanding this environment. Hence, scattering processes and decay of unstable elementary particles are important for cosmological models of this period.\nAs a rule of thumb, a scattering or a decay process is cosmologically important in a certain epoch if the time scale describing that process is smaller than, or comparable to, the time scale of the expansion of the universe. The time scale that describes the expansion of the universe is formula_1 with formula_2 being the Hubble parameter, which varies with time. The expansion timescale formula_1 is roughly equal to the age of the universe at each point in time.\nTimeline of the Big Bang.\nObservations suggest that the universe began around 13.8 billion years ago. Since then, the evolution of the universe has passed through three phases. The very early universe, which is still poorly understood, was the split second in which the universe was so hot that particles had energies higher than those currently accessible in particle accelerators on Earth. Therefore, while the basic features of this epoch have been worked out in the Big Bang theory, the details are largely based on educated guesses.\nFollowing this, in the early universe, the evolution of the universe proceeded according to known high energy physics. This is when the first protons, electrons and neutrons formed, then nuclei and finally atoms. With the formation of neutral hydrogen, the cosmic microwave background was emitted. Finally, the epoch of structure formation began, when matter started to aggregate into the first stars and quasars, and ultimately galaxies, clusters of galaxies and superclusters formed. The future of the universe is not yet firmly known, but according to the \u039bCDM model it will continue expanding forever.\nAreas of study.\nBelow, some of the most active areas of inquiry in cosmology are described, in roughly chronological order. This does not include all of the Big Bang cosmology, which is presented in \"Timeline of the Big Bang.\"\nVery early universe.\nThe early, hot universe appears to be well explained by the Big Bang from roughly 10\u221233 seconds onwards, but there are several problems. One is that there is no compelling reason, using current particle physics, for the universe to be flat, homogeneous, and isotropic \"(see the cosmological principle)\". Moreover, grand unified theories of particle physics suggest that there should be magnetic monopoles in the universe, which have not been found. These problems are resolved by a brief period of cosmic inflation, which drives the universe to flatness, smooths out anisotropies and inhomogeneities to the observed level, and exponentially dilutes the monopoles. The physical model behind cosmic inflation is extremely simple, but it has not yet been confirmed by particle physics, and there are difficult problems reconciling inflation and quantum field theory. Some cosmologists think that string theory and brane cosmology will provide an alternative to inflation.\nAnother major problem in cosmology is what caused the universe to contain far more matter than antimatter. Cosmologists can observationally deduce that the universe is not split into regions of matter and antimatter. If it were, there would be X-rays and gamma rays produced as a result of annihilation, but this is not observed. Therefore, some process in the early universe must have created a small excess of matter over antimatter, and this (currently not understood) process is called \"baryogenesis\". Three required conditions for baryogenesis were derived by Andrei Sakharov in 1967, and requires a violation of the particle physics symmetry, called CP-symmetry, between matter and antimatter. However, particle accelerators measure too small a violation of CP-symmetry to account for the baryon asymmetry. Cosmologists and particle physicists look for additional violations of the CP-symmetry in the early universe that might account for the baryon asymmetry.\nBoth the problems of baryogenesis and cosmic inflation are very closely related to particle physics, and their resolution might come from high energy theory and experiment, rather than through observations of the universe.\nBig Bang Theory.\nBig Bang nucleosynthesis is the theory of the formation of the elements in the early universe. It finished when the universe was about three minutes old and its temperature dropped below that at which nuclear fusion could occur. Big Bang nucleosynthesis had a brief period during which it could operate, so only the very lightest elements were produced. Starting from hydrogen ions (protons), it principally produced deuterium, helium-4, and lithium. Other elements were produced in only trace abundances. The basic theory of nucleosynthesis was developed in 1948 by George Gamow, Ralph Asher Alpher, and Robert Herman. It was used for many years as a probe of physics at the time of the Big Bang, as the theory of Big Bang nucleosynthesis connects the abundances of primordial light elements with the features of the early universe. Specifically, it can be used to test the equivalence principle, to probe dark matter, and test neutrino physics. Some cosmologists have proposed that Big Bang nucleosynthesis suggests there is a fourth \"sterile\" species of neutrino.\nStandard model of Big Bang cosmology.\nThe \u039bCDM (Lambda cold dark matter) or Lambda-CDM model is a parametrization of the Big Bang cosmological model in which the universe contains a cosmological constant, denoted by Lambda (Greek \u039b), associated with dark energy, and cold dark matter (abbreviated CDM). It is frequently referred to as the standard model of Big Bang cosmology.\nCosmic microwave background.\nThe cosmic microwave background is radiation left over from decoupling after the epoch of recombination when neutral atoms first formed. At this point, radiation produced in the Big Bang stopped Thomson scattering from charged ions. The radiation, first observed in 1965 by Arno Penzias and Robert Woodrow Wilson, has a perfect thermal black-body spectrum. It has a temperature of 2.7 kelvins today and is isotropic to one part in 105. Cosmological perturbation theory, which describes the evolution of slight inhomogeneities in the early universe, has allowed cosmologists to precisely calculate the angular power spectrum of the radiation, and it has been measured by the recent satellite experiments (COBE and WMAP) and many ground and balloon-based experiments (such as Degree Angular Scale Interferometer, Cosmic Background Imager, and Boomerang). One of the goals of these efforts is to measure the basic parameters of the Lambda-CDM model with increasing accuracy, as well as to test the predictions of the Big Bang model and look for new physics. The results of measurements made by WMAP, for example, have placed limits on the neutrino masses.\nNewer experiments, such as QUIET and the Atacama Cosmology Telescope, are trying to measure the polarization of the cosmic microwave background. These measurements are expected to provide further confirmation of the theory as well as information about cosmic inflation, and the so-called secondary anisotropies, such as the Sunyaev-Zel'dovich effect and Sachs-Wolfe effect, which are caused by interaction between galaxies and clusters with the cosmic microwave background.\nOn 17 March 2014, astronomers of the BICEP2 Collaboration announced the apparent detection of \"B\"-mode polarization of the CMB, considered to be evidence of primordial gravitational waves that are predicted by the theory of inflation to occur during the earliest phase of the Big Bang. However, later that year the Planck collaboration provided a more accurate measurement of cosmic dust, concluding that the B-mode signal from dust is the same strength as that reported from BICEP2. On 30 January 2015, a joint analysis of BICEP2 and Planck data was published and the European Space Agency announced that the signal can be entirely attributed to interstellar dust in the Milky Way.\nFormation and evolution of large-scale structure.\nUnderstanding the formation and evolution of the largest and earliest structures (i.e., quasars, galaxies, clusters and superclusters) is one of the largest efforts in cosmology. Cosmologists study a model of hierarchical structure formation in which structures form from the bottom up, with smaller objects forming first, while the largest objects, such as superclusters, are still assembling. One way to study structure in the universe is to survey the visible galaxies, in order to construct a three-dimensional picture of the galaxies in the universe and measure the matter power spectrum. This is the approach of the \"Sloan Digital Sky Survey\" and the 2dF Galaxy Redshift Survey.\nAnother tool for understanding structure formation is simulations, which cosmologists use to study the gravitational aggregation of matter in the universe, as it clusters into filaments, superclusters and voids. Most simulations contain only non-baryonic cold dark matter, which should suffice to understand the universe on the largest scales, as there is much more dark matter in the universe than visible, baryonic matter. More advanced simulations are starting to include baryons and study the formation of individual galaxies. Cosmologists study these simulations to see if they agree with the galaxy surveys, and to understand any discrepancy.\nOther, complementary observations to measure the distribution of matter in the distant universe and to probe reionization include:\nThese will help cosmologists settle the question of when and how structure formed in the universe.\nDark matter.\nEvidence from Big Bang nucleosynthesis, the cosmic microwave background, structure formation, and galaxy rotation curves suggests that about 23% of the mass of the universe consists of non-baryonic dark matter, whereas only 4% consists of visible, baryonic matter. The gravitational effects of dark matter are well understood, as it behaves like a cold, non-radiative fluid that forms haloes around galaxies. Dark matter has never been detected in the laboratory, and the particle physics nature of dark matter remains completely unknown. Without observational constraints, there are a number of candidates, such as a stable supersymmetric particle, a weakly interacting massive particle, a gravitationally-interacting massive particle, an axion, and a massive compact halo object. Alternatives to the dark matter hypothesis include a modification of gravity at small accelerations (MOND) or an effect from brane cosmology. TeVeS is a version of MOND that can explain gravitational lensing.\nDark energy.\nIf the universe is flat, there must be an additional component making up 73% (in addition to the 23% dark matter and 4% baryons) of the energy density of the universe. This is called dark energy. In order not to interfere with Big Bang nucleosynthesis and the cosmic microwave background, it must not cluster in haloes like baryons and dark matter. There is strong observational evidence for dark energy, as the total energy density of the universe is known through constraints on the flatness of the universe, but the amount of clustering matter is tightly measured, and is much less than this. The case for dark energy was strengthened in 1999, when measurements demonstrated that the expansion of the universe has begun to gradually accelerate.\nApart from its density and its clustering properties, nothing is known about dark energy. \"Quantum field theory\" predicts a cosmological constant (CC) much like dark energy, but 120 orders of magnitude larger than that observed. Steven Weinberg and a number of string theorists \"(see string landscape)\" have invoked the 'weak anthropic principle': i.e. the reason that physicists observe a universe with such a small cosmological constant is that no physicists (or any life) could exist in a universe with a larger cosmological constant. Many cosmologists find this an unsatisfying explanation: perhaps because while the weak anthropic principle is self-evident (given that living observers exist, there must be at least one universe with a cosmological constant which allows for life to exist) it does not attempt to explain the context of that universe. For example, the weak anthropic principle alone does not distinguish between:\nOther possible explanations for dark energy include quintessence or a modification of gravity on the largest scales. The effect on cosmology of the dark energy that these models describe is given by the dark energy's equation of state, which varies depending upon the theory. The nature of dark energy is one of the most challenging problems in cosmology.\nA better understanding of dark energy is likely to solve the problem of the ultimate fate of the universe. In the current cosmological epoch, the accelerated expansion due to dark energy is preventing structures larger than superclusters from forming. It is not known whether the acceleration will continue indefinitely, perhaps even increasing until a big rip, or whether it will eventually reverse, lead to a Big Freeze, or follow some other scenario.\nGravitational waves.\nGravitational waves are ripples in the curvature of spacetime that propagate as waves at the speed of light, generated in certain gravitational interactions that propagate outward from their source. Gravitational-wave astronomy is an emerging branch of observational astronomy which aims to use gravitational waves to collect observational data about sources of detectable gravitational waves such as binary star systems composed of white dwarfs, neutron stars, and black holes; and events such as supernovae, and the formation of the early universe shortly after the Big Bang.\nIn 2016, the LIGO Scientific Collaboration and Virgo Collaboration teams announced that they had made the first observation of gravitational waves, originating from a pair of merging black holes using the Advanced LIGO detectors. On 15 June 2016, a second detection of gravitational waves from coalescing black holes was announced. Besides LIGO, many other gravitational-wave observatories (detectors) are under construction.\nOther areas of inquiry.\nCosmologists also study:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5382", "revid": "46090146", "url": "https://en.wikipedia.org/wiki?curid=5382", "title": "Inflation (cosmology)", "text": "Theory of rapid universe expansion\nIn physical cosmology, cosmic inflation, cosmological inflation, or just inflation, is a theory of exponential expansion of space in the early universe. The inflationary epoch is believed to have lasted from 10\u221236\u00a0seconds to between 10\u221233 and 10\u221232\u00a0seconds after the Big Bang. Following the inflationary period, the universe continued to expand, but at a slower rate. The acceleration of this expansion due to dark energy began after the universe was already over 7.7\u00a0billion years old (5.4\u00a0billion years ago).\nInflation theory was developed in the late 1970s and early 80s, with notable contributions by several theoretical physicists, including Alexei Starobinsky at Landau Institute for Theoretical Physics, Alan Guth at Cornell University, and Andrei Linde at Lebedev Physical Institute. Alexei Starobinsky, Alan Guth, and Andrei Linde won the 2014 Kavli Prize \"for pioneering the theory of cosmic inflation\". It was developed further in the early 1980s. It explains the origin of the large-scale structure of the cosmos. Quantum fluctuations in the microscopic inflationary region, magnified to cosmic size, become the seeds for the growth of structure in the Universe (see galaxy formation and evolution and structure formation). Many physicists also believe that inflation explains why the universe appears to be the same in all directions (isotropic), why the cosmic microwave background radiation is distributed evenly, why the universe is flat, and why no magnetic monopoles have been observed.\nThe detailed particle physics mechanism responsible for inflation is unknown. The basic inflationary paradigm is accepted by most physicists, as a number of inflation model predictions have been confirmed by observation; however, a substantial minority of scientists dissent from this position. The hypothetical field thought to be responsible for inflation is called the inflaton.\nIn 2002 three of the original architects of the theory were recognized for their major contributions; physicists Alan Guth of M.I.T., Andrei Linde of Stanford, and Paul Steinhardt of Princeton shared the prestigious Dirac Prize \"for development of the concept of inflation in cosmology\". In 2012 Guth and Linde were awarded the Breakthrough Prize in Fundamental Physics for their invention and development of inflationary cosmology.\nOverview.\nAround 1930, Edwin Hubble discovered that light from remote galaxies was redshifted; the more remote, the more shifted. This was quickly interpreted as meaning galaxies were receding from Earth. If Earth is not in some special, privileged, central position in the universe, then it would mean all galaxies are moving apart, and the further away, the faster they are moving away. It is now understood that the universe is expanding, carrying the galaxies with it, and causing this observation. Many other observations agree, and also lead to the same conclusion. However, for many years it was not clear why or how the universe might be expanding, or what it might signify.\nBased on a huge amount of experimental observation and theoretical work, it is now believed that the reason for the observation is that \"space itself is expanding\", and that it expanded very rapidly within the first fraction of a second after the Big Bang. This kind of expansion is known as a \"metric\" expansion. In the terminology of mathematics and physics, a \"metric\" is a measure of distance that satisfies a specific list of properties, and the term implies that \"the sense of distance within the universe is itself changing\". Today, metric variation is far too small an effect to see on less than an intergalactic scale.\nThe modern explanation for the metric expansion of space was proposed by physicist Alan Guth in 1979, while investigating the problem of why no magnetic monopoles are seen today. He found that if the universe contained a field in a positive-energy false vacuum state, then according to general relativity it would generate an exponential expansion of space. It was very quickly realized that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does \"today\", the Universe would have to have started from very finely tuned, or \"special\" initial conditions at the Big Bang. Inflation theory largely resolves these problems as well, thus making a universe like ours much more likely in the context of Big Bang theory.\nNo physical field has yet been discovered that is responsible for this inflation. However such a field would be scalar and the first relativistic scalar field proven to exist, the Higgs field, was only discovered in 2012\u20132013 and is still being researched. So it is not seen as problematic that a field responsible for cosmic inflation and the metric expansion of space has not yet been discovered. The proposed field and its quanta (the subatomic particles related to it) have been named the inflaton. If this field did not exist, scientists would have to propose a different explanation for all the observations that strongly suggest a metric expansion of space has occurred, and is still occurring (much more slowly) today.\nTheory.\nAn expanding universe generally has a cosmological horizon, which, by analogy with the more familiar horizon caused by the curvature of Earth's surface, marks the boundary of the part of the Universe that an observer can see. Light (or other radiation) emitted by objects beyond the cosmological horizon in an accelerating universe never reaches the observer, because the space in between the observer and the object is expanding too rapidly.\nThe observable universe is one \"causal patch\" of a much larger unobservable universe; other parts of the Universe cannot communicate with Earth yet. These parts of the Universe are outside our current cosmological horizon. In the standard hot big bang model, without inflation, the cosmological horizon moves out, bringing new regions into view. Yet as a local observer sees such a region for the first time, it looks no different from any other region of space the local observer has already seen: its background radiation is at nearly the same temperature as the background radiation of other regions, and its space-time curvature is evolving lock-step with the others. This presents a mystery: how did these new regions know what temperature and curvature they were supposed to have? They couldn't have learned it by getting signals, because they were not previously in communication with our past light cone.\nInflation answers this question by postulating that all the regions come from an earlier era with a big vacuum energy, or cosmological constant. A space with a cosmological constant is qualitatively different: instead of moving outward, the cosmological horizon stays put. For any one observer, the distance to the cosmological horizon is constant. With exponentially expanding space, two nearby observers are separated very quickly; so much so, that the distance between them quickly exceeds the limits of communications. The spatial slices are expanding very fast to cover huge volumes. Things are constantly moving beyond the cosmological horizon, which is a fixed distance away, and everything becomes homogeneous.\nAs the inflationary field slowly relaxes to the vacuum, the cosmological constant goes to zero and space begins to expand normally. The new regions that come into view during the normal expansion phase are exactly the same regions that were pushed out of the horizon during inflation, and so they are at nearly the same temperature and curvature, because they come from the same originally small patch of space.\nThe theory of inflation thus explains why the temperatures and curvatures of different regions are so nearly equal. It also predicts that the total curvature of a space-slice at constant global time is zero. This prediction implies that the total ordinary matter, dark matter and residual vacuum energy in the Universe have to add up to the critical density, and the evidence supports this. More strikingly, inflation allows physicists to calculate the minute differences in temperature of different regions from quantum fluctuations during the inflationary era, and many of these quantitative predictions have been confirmed.\nSpace expands.\nIn a space that expands exponentially (or nearly exponentially) with time, any pair of free-floating objects that are initially at rest will move apart from each other at an accelerating rate, at least as long as they are not bound together by any force. From the point of view of one such object, the spacetime is something like an inside-out Schwarzschild black hole\u2014each object is surrounded by a spherical event horizon. Once the other object has fallen through this horizon it can never return, and even light signals it sends will never reach the first object (at least so long as the space continues to expand exponentially).\nIn the approximation that the expansion is exactly exponential, the horizon is static and remains a fixed physical distance away. This patch of an inflating universe can be described by the following metric:\nformula_1\nThis exponentially expanding spacetime is called a de Sitter space, and to sustain it there must be a cosmological constant, a vacuum energy density that is constant in space and time and proportional to \u039b in the above metric. For the case of exactly exponential expansion, the vacuum energy has a negative pressure \"p\" equal in magnitude to its energy density \"\u03c1\"; the equation of state is \"p=\u2212\u03c1\".\nInflation is typically not an exactly exponential expansion, but rather quasi- or near-exponential. In such a universe the horizon will slowly grow with time as the vacuum energy density gradually decreases.\nFew inhomogeneities remain.\nBecause the accelerating expansion of space stretches out any initial variations in density or temperature to very large length scales, an essential feature of inflation is that it smooths out inhomogeneities and anisotropies, and reduces the curvature of space. This pushes the Universe into a very simple state in which it is completely dominated by the inflaton field and the only significant inhomogeneities are tiny quantum fluctuations. Inflation also dilutes exotic heavy particles, such as the magnetic monopoles predicted by many extensions to the Standard Model of particle physics. If the Universe was only hot enough to form such particles \"before\" a period of inflation, they would not be observed in nature, as they would be so rare that it is quite likely that there are none in the observable universe. Together, these effects are called the inflationary \"no-hair theorem\" by analogy with the no hair theorem for black holes.\nThe \"no-hair\" theorem works essentially because the cosmological horizon is no different from a black-hole horizon, except for philosophical disagreements about what is on the other side. The interpretation of the no-hair theorem is that the Universe (observable and unobservable) expands by an enormous factor during inflation. In an expanding universe, energy densities generally fall, or get diluted, as the volume of the Universe increases. For example, the density of ordinary \"cold\" matter (dust) goes down as the inverse of the volume: when linear dimensions double, the energy density goes down by a factor of eight; the radiation energy density goes down even more rapidly as the Universe expands since the wavelength of each photon is stretched (redshifted), in addition to the photons being dispersed by the expansion. When linear dimensions are doubled, the energy density in radiation falls by a factor of sixteen (see the solution of the energy density continuity equation for an ultra-relativistic fluid). During inflation, the energy density in the inflaton field is roughly constant. However, the energy density in everything else, including inhomogeneities, curvature, anisotropies, exotic particles, and standard-model particles is falling, and through sufficient inflation these all become negligible. This leaves the Universe flat and symmetric, and (apart from the homogeneous inflaton field) mostly empty, at the moment inflation ends and reheating begins.\nDuration.\nA key requirement is that inflation must continue long enough to produce the present observable universe from a single, small inflationary Hubble volume. This is necessary to ensure that the Universe appears flat, homogeneous and isotropic at the largest observable scales. This requirement is generally thought to be satisfied if the Universe expanded by a factor of at least 1026 during inflation.\nReheating.\nInflation is a period of supercooled expansion, when the temperature drops by a factor of 100,000 or so. (The exact drop is model-dependent, but in the first models it was typically from 1027\u00a0K down to 1022\u00a0K.) This relatively low temperature is maintained during the inflationary phase. When inflation ends the temperature returns to the pre-inflationary temperature; this is called \"reheating\" or thermalization because the large potential energy of the inflaton field decays into particles and fills the Universe with Standard Model particles, including electromagnetic radiation, starting the radiation dominated phase of the Universe. Because the nature of the inflation is not known, this process is still poorly understood, although it is believed to take place through a parametric resonance.\nMotivations.\nInflation resolves several problems in Big Bang cosmology that were discovered in the 1970s. Inflation was first proposed by Alan Guth in 1979 while investigating the problem of why no magnetic monopoles are seen today; he found that a positive-energy false vacuum would, according to general relativity, generate an exponential expansion of space. It was very quickly realised that such an expansion would resolve many other long-standing problems. These problems arise from the observation that to look like it does \"today\", the Universe would have to have started from very finely tuned, or \"special\" initial conditions at the Big Bang. Inflation attempts to resolve these problems by providing a dynamical mechanism that drives the Universe to this special state, thus making a universe like ours much more likely in the context of the Big Bang theory.\nHorizon problem.\nThe horizon problem is the problem of determining why the Universe appears statistically homogeneous and isotropic in accordance with the cosmological principle. For example, molecules in a canister of gas are distributed homogeneously and isotropically because they are in thermal equilibrium: gas throughout the canister has had enough time to interact to dissipate inhomogeneities and anisotropies. The situation is quite different in the big bang model without inflation, because gravitational expansion does not give the early universe enough time to equilibrate. In a big bang with only the matter and radiation known in the Standard Model, two widely separated regions of the observable universe cannot have equilibrated because they move apart from each other faster than the speed of light and thus have never come into causal contact. In the early Universe, it was not possible to send a light signal between the two regions. Because they have had no interaction, it is difficult to explain why they have the same temperature (are thermally equilibrated). Historically, proposed solutions included the \"Phoenix universe\" of Georges Lema\u00eetre, the related oscillatory universe of Richard Chase Tolman, and the Mixmaster universe of Charles Misner. Lema\u00eetre and Tolman proposed that a universe undergoing a number of cycles of contraction and expansion could come into thermal equilibrium. Their models failed, however, because of the buildup of entropy over several cycles. Misner made the (ultimately incorrect) conjecture that the Mixmaster mechanism, which made the Universe \"more\" chaotic, could lead to statistical homogeneity and isotropy.\nFlatness problem.\nThe flatness problem is sometimes called one of the Dicke coincidences (along with the cosmological constant problem).\nIt became known in the 1960s that the density of matter in the Universe was comparable to the critical density necessary for a flat universe (that is, a universe whose large scale geometry is the usual Euclidean geometry, rather than a non-Euclidean hyperbolic or spherical geometry).(p61)\nTherefore, regardless of the shape of the universe the contribution of spatial curvature to the expansion of the Universe could not be much greater than the contribution of matter. But as the Universe expands, the curvature redshifts away more slowly than matter and radiation. Extrapolated into the past, this presents a fine-tuning problem because the contribution of curvature to the Universe must be exponentially small (sixteen orders of magnitude less than the density of radiation at Big Bang nucleosynthesis, for example). This problem is exacerbated by recent observations of the cosmic microwave background that have demonstrated that the Universe is flat to within a few percent.\nMagnetic-monopole problem.\nThe magnetic monopole problem, sometimes called \"the exotic-relics problem\", says that if the early universe were very hot, a large number of very heavy, stable magnetic monopoles would have been produced.\nStable magnetic monopoles are a problem for Grand Unified Theories, which propose that at high temperatures (such as in the early universe) the electromagnetic force, strong, and weak nuclear forces are not actually fundamental forces but arise due to spontaneous symmetry breaking from a single gauge theory.\nThese theories predict a number of heavy, stable particles that have not been observed in nature. The most notorious is the magnetic monopole, a kind of stable, heavy \"charge\" of magnetic field.\nMonopoles are predicted to be copiously produced following Grand Unified Theories at high temperature,\nand they should have persisted to the present day, to such an extent that they would become the primary constituent of the Universe.\nNot only is that not the case, but all searches for them have failed, placing stringent limits on the density of relic magnetic monopoles in the Universe.\nA period of inflation that occurs below the temperature where magnetic monopoles can be produced would offer a possible resolution of this problem: Monopoles would be separated from each other as the Universe around them expands, potentially lowering their observed density by many orders of magnitude. Though, as cosmologist Martin Rees has written,\n \"Skeptics about exotic physics might not be hugely impressed by a theoretical argument to explain the absence of particles that are themselves only hypothetical. Preventive medicine can readily seem 100\u00a0percent effective against a disease that doesn't exist!\"\nHistory.\nPrecursors.\nIn the early days of General Relativity, Albert Einstein introduced the cosmological constant to allow a static solution, which was a three-dimensional sphere with a uniform density of matter. Later, Willem de Sitter found a highly symmetric inflating universe, which described a universe with a cosmological constant that is otherwise empty.\nIt was discovered that Einstein's universe is unstable, and that small fluctuations cause it to collapse or turn into a de Sitter universe.\nIn the early 1970s Zeldovich noticed the flatness and horizon problems of Big Bang cosmology; before his work, cosmology was presumed to be symmetrical on purely philosophical grounds. In the Soviet Union, this and other considerations led Belinski and Khalatnikov to analyze the chaotic BKL singularity in General Relativity. Misner's Mixmaster universe attempted to use this chaotic behavior to solve the cosmological problems, with limited success.\nFalse vacuum.\nIn the late 1970s, Sidney Coleman applied the instanton techniques developed by Alexander Polyakov and collaborators to study the fate of the false vacuum in quantum field theory. Like a metastable phase in statistical mechanics\u2014water below the freezing temperature or above the boiling point\u2014a quantum field would need to nucleate a large enough bubble of the new vacuum, the new phase, in order to make a transition. Coleman found the most likely decay pathway for vacuum decay and calculated the inverse lifetime per unit volume. He eventually noted that gravitational effects would be significant, but he did not calculate these effects and did not apply the results to cosmology.\nThe universe could have been spontaneously created from nothing (no space, time, nor matter) by quantum fluctuations of metastable false vacuum causing an expanding bubble of true vacuum.\nStarobinsky inflation.\nIn the Soviet Union, Alexei Starobinsky noted that quantum corrections to general relativity should be important for the early universe. These generically lead to curvature-squared corrections to the Einstein\u2013Hilbert action and a form of \"f\"(\"R\") modified gravity. The solution to Einstein's equations in the presence of curvature squared terms, when the curvatures are large, leads to an effective cosmological constant. Therefore, he proposed that the early universe went through an inflationary de Sitter era.\nThis resolved the cosmology problems and led to specific predictions for the corrections to the microwave background radiation, corrections that were then calculated in detail. Starobinsky used the action \nformula_2\nwhich corresponds to the potential\nformula_3\nin the Einstein frame. This results in the observables:\nformula_4\nMonopole problem.\nIn 1978, Zeldovich noted the monopole problem, which was an unambiguous quantitative version of the horizon problem, this time in a subfield of particle physics, which led to several speculative attempts to resolve it. In 1980 Alan Guth realized that false vacuum decay in the early universe would solve the problem, leading him to propose a scalar-driven inflation. Starobinsky's and Guth's scenarios both predicted an initial de Sitter phase, differing only in mechanistic details.\nEarly inflationary models.\nGuth proposed inflation in January 1981 to explain the nonexistence of magnetic monopoles;\nit was Guth who coined the term \"inflation\". At the same time, Starobinsky argued that quantum corrections to gravity would replace the supposed initial singularity of the Universe with an exponentially expanding de Sitter phase.\nIn October 1980, Demosthenes Kazanas suggested that exponential expansion could eliminate the particle horizon and perhaps solve the horizon problem,\nwhile Sato suggested that an exponential expansion could eliminate domain walls (another kind of exotic relic). In 1981 Einhorn and Sato published a model similar to Guth's and showed that it would resolve the puzzle of the magnetic monopole abundance in Grand Unified Theories. Like Guth, they concluded that such a model not only required fine tuning of the cosmological constant, but also would likely lead to a much too granular universe, i.e., to large density variations resulting from bubble wall collisions.\nGuth proposed that as the early universe cooled, it was trapped in a false vacuum with a high energy density, which is much like a cosmological constant. As the very early universe cooled it was trapped in a metastable state (it was supercooled), which it could only decay out of through the process of bubble nucleation via quantum tunneling. Bubbles of true vacuum spontaneously form in the sea of false vacuum and rapidly begin expanding at the speed of light. Guth recognized that this model was problematic because the model did not reheat properly: when the bubbles nucleated, they did not generate any radiation. Radiation could only be generated in collisions between bubble walls. But if inflation lasted long enough to solve the initial conditions problems, collisions between bubbles became exceedingly rare. In any one causal patch it is likely that only one bubble would nucleate.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;... Kazanas (1980) called this phase of the early Universe \"de Sitter's phase.\" The name \"inflation\" was given by Guth (1981). ... Guth himself did not refer to work of Kazanas until he published a book on the subject under the title \"The Inflationary Universe: The quest for a new theory of cosmic origin\" (1997), where he apologizes for not having referenced the work of Kazanas and of others, related to inflation.\nSlow-roll inflation.\nThe bubble collision problem was solved by Linde and independently by Andreas Albrecht and Paul Steinhardt in a model named \"new inflation\" or \"slow-roll inflation\" (Guth's model then became known as \"old inflation\"). In this model, instead of tunneling out of a false vacuum state, inflation occurred by a scalar field rolling down a potential energy hill. When the field rolls very slowly compared to the expansion of the Universe, inflation occurs. However, when the hill becomes steeper, inflation ends and reheating can occur.\nEffects of asymmetries.\nEventually, it was shown that new inflation does not produce a perfectly symmetric universe, but that quantum fluctuations in the inflaton are created. These fluctuations form the primordial seeds for all structure created in the later universe. These fluctuations were first calculated by Viatcheslav Mukhanov and G. V. Chibisov in analyzing Starobinsky's similar model. In the context of inflation, they were worked out independently of the work of Mukhanov and Chibisov at the three-week 1982 Nuffield Workshop on the Very Early Universe at Cambridge University. The fluctuations were calculated by four groups working separately over the course of the workshop: Stephen Hawking; Starobinsky; Guth and So-Young Pi; and Bardeen, Steinhardt and Turner.\nObservational status.\nInflation is a mechanism for realizing the cosmological principle, which is the basis of the standard model of physical cosmology: it accounts for the homogeneity and isotropy of the observable universe. In addition, it accounts for the observed flatness and absence of magnetic monopoles. Since Guth's early work, each of these observations has received further confirmation, most impressively by the detailed observations of the cosmic microwave background made by the Planck spacecraft. This analysis shows that the Universe is flat to within percent, and that it is homogeneous and isotropic to one part in 100,000.\nInflation predicts that the structures visible in the Universe today formed through the gravitational collapse of perturbations that were formed as quantum mechanical fluctuations in the inflationary epoch. The detailed form of the spectrum of perturbations, called a nearly-scale-invariant Gaussian random field is very specific and has only two free parameters. One is the amplitude of the spectrum and the \"spectral index\", which measures the slight deviation from scale invariance predicted by inflation (perfect scale invariance corresponds to the idealized de Sitter universe).\nThe other free parameter is the tensor to scalar ratio. The simplest inflation models, those without fine-tuning, predict a tensor to scalar ratio near 0.1\u00a0.\nInflation predicts that the observed perturbations should be in thermal equilibrium with each other (these are called \"adiabatic\" or \"isentropic\" perturbations). This structure for the perturbations has been confirmed by the Planck spacecraft, WMAP spacecraft and other cosmic microwave background (CMB) experiments, and galaxy surveys, especially the ongoing Sloan Digital Sky Survey. These experiments have shown that the one part in 100,000 inhomogeneities observed have exactly the form predicted by theory. There is evidence for a slight deviation from scale invariance. The \"spectral index\", ns is one for a scale-invariant Harrison\u2013Zel'dovich spectrum. The simplest inflation models predict that ns is between 0.92 and 0.98\u00a0. This is the range that is possible without fine-tuning of the parameters related to energy. From Planck data it can be inferred that ns=0.968 \u00b1 0.006, and a tensor to scalar ratio that is less than 0.11\u00a0. These are considered an important confirmation of the theory of inflation.\nVarious inflation theories have been proposed that make radically different predictions, but they generally have much more fine-tuning than should be necessary. As a physical model, however, inflation is most valuable in that it robustly predicts the initial conditions of the Universe based on only two adjustable parameters: the spectral index (that can only change in a small range) and the amplitude of the perturbations. Except in contrived models, this is true regardless of how inflation is realized in particle physics.\nOccasionally, effects are observed that appear to contradict the simplest models of inflation. The first-year WMAP data suggested that the spectrum might not be nearly scale-invariant, but might instead have a slight curvature. However, the third-year data revealed that the effect was a statistical anomaly. Another effect remarked upon since the first cosmic microwave background satellite, the Cosmic Background Explorer is that the amplitude of the quadrupole moment of the CMB is unexpectedly low and the other low multipoles appear to be preferentially aligned with the ecliptic plane. Some have claimed that this is a signature of non-Gaussianity and thus contradicts the simplest models of inflation. Others have suggested that the effect may be due to other new physics, foreground contamination, or even publication bias.\nAn experimental program is underway to further test inflation with more precise CMB measurements. In particular, high precision measurements of the so-called \"B-modes\" of the polarization of the background radiation could provide evidence of the gravitational radiation produced by inflation, and could also show whether the energy scale of inflation predicted by the simplest models (1015~1016 GeV) is correct. In March 2014, the BICEP2 team announced B-mode CMB polarization confirming inflation had been demonstrated. The team announced the tensor-to-scalar power ratio r was between 0.15 and 0.27 (rejecting the null hypothesis; r is expected to be 0 in the absence of inflation). However, on 19 June 2014, lowered confidence in confirming the findings was reported; on 19 September 2014, a further reduction in confidence was reported and, on 30 January 2015, even less confidence yet was reported. By 2018, additional data suggested, with 95% confidence, that formula_5 is 0.06 or lower: consistent with the null hypothesis, but still also consistent with many remaining models of inflation.\nOther potentially corroborating measurements are expected from the Planck spacecraft, although it is unclear if the signal will be visible, or if contamination from foreground sources will interfere. Other forthcoming measurements, such as those of 21 centimeter radiation (radiation emitted and absorbed from neutral hydrogen before the first stars formed), may measure the power spectrum with even greater resolution than the CMB and galaxy surveys, although it is not known if these measurements will be possible or if interference with radio sources on Earth and in the galaxy will be too great.\nTheoretical status.\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in physics:\nIs the theory of cosmological inflation correct, and if so, what are the details of this epoch? What is the hypothetical inflaton field giving rise to inflation?\nIn Guth's early proposal, it was thought that the inflaton was the Higgs field, the field that explains the mass of the elementary particles. It is now believed by some that the inflaton cannot be the Higgs field\nalthough the recent discovery of the Higgs boson has increased the number of works considering the Higgs field as inflaton.\nOne problem of this identification is the current tension with experimental data at the electroweak scale, which is currently under study at the Large Hadron Collider (LHC). Other models of inflation relied on the properties of Grand Unified Theories. Since the simplest models of grand unification have failed, it is now thought by many physicists that inflation will be included in a supersymmetric theory such as string theory or a supersymmetric grand unified theory. At present, while inflation is understood principally by its detailed predictions of the initial conditions for the hot early universe, the particle physics is largely \"ad hoc\" modelling. As such, although predictions of inflation have been consistent with the results of observational tests, many open questions remain.\nFine-tuning problem.\nOne of the most severe challenges for inflation arises from the need for fine tuning. In new inflation, the \"slow-roll conditions\" must be satisfied for inflation to occur. The slow-roll conditions say that the inflaton potential must be flat (compared to the large vacuum energy) and that the inflaton particles must have a small mass. New inflation requires the Universe to have a scalar field with an especially flat potential and special initial conditions. However, explanations for these fine-tunings have been proposed. For example, classically scale invariant field theories, where scale invariance is broken by quantum effects, provide an explanation of the flatness of inflationary potentials, as long as the theory can be studied through perturbation theory.\nLinde proposed a theory known as \"chaotic inflation\" in which he suggested that the conditions for inflation were actually satisfied quite generically. Inflation will occur in virtually any universe that begins in a chaotic, high energy state that has a scalar field with unbounded potential energy. However, in his model the inflaton field necessarily takes values larger than one Planck unit: for this reason, these are often called \"large field\" models and the competing new inflation models are called \"small field\" models. In this situation, the predictions of effective field theory are thought to be invalid, as renormalization should cause large corrections that could prevent inflation.\nThis problem has not yet been resolved and some cosmologists argue that the small field models, in which inflation can occur at a much lower energy scale, are better models. While inflation depends on quantum field theory (and the semiclassical approximation to quantum gravity) in an important way, it has not been completely reconciled with these theories.\nBrandenberger commented on fine-tuning in another situation. The amplitude of the primordial inhomogeneities produced in inflation is directly tied to the energy scale of inflation. This scale is suggested to be around 1016 GeV or 10\u22123 times the Planck energy. The natural scale is na\u00efvely the Planck scale so this small value could be seen as another form of fine-tuning (called a hierarchy problem): the energy density given by the scalar potential is down by 10\u221212 compared to the Planck density. This is not usually considered to be a critical problem, however, because the scale of inflation corresponds naturally to the scale of gauge unification.\nEternal inflation.\nIn many models, the inflationary phase of the Universe's expansion lasts forever in at least some regions of the Universe. This occurs because inflating regions expand very rapidly, reproducing themselves. Unless the rate of decay to the non-inflating phase is sufficiently fast, new inflating regions are produced more rapidly than non-inflating regions. In such models, most of the volume of the Universe is continuously inflating at any given time.\nAll models of eternal inflation produce an infinite, hypothetical multiverse, typically a fractal. The multiverse theory has created significant dissension in the scientific community about the viability of the inflationary model.\nPaul Steinhardt, one of the original architects of the inflationary model, introduced the first example of eternal inflation in 1983. He showed that the inflation could proceed forever by producing bubbles of non-inflating space filled with hot matter and radiation surrounded by empty space that continues to inflate. The bubbles could not grow fast enough to keep up with the inflation. Later that same year, Alexander Vilenkin showed that eternal inflation is generic.\nAlthough new inflation is classically rolling down the potential, quantum fluctuations can sometimes lift it to previous levels. These regions in which the inflaton fluctuates upwards expand much faster than regions in which the inflaton has a lower potential energy, and tend to dominate in terms of physical volume. It has been shown that any inflationary theory with an unbounded potential is eternal. There are well-known theorems that this steady state cannot continue forever into the past. Inflationary spacetime, which is similar to de Sitter space, is incomplete without a contracting region. However, unlike de Sitter space, fluctuations in a contracting inflationary space collapse to form a gravitational singularity, a point where densities become infinite. Therefore, it is necessary to have a theory for the Universe's initial conditions.\nIn eternal inflation, regions with inflation have an exponentially growing volume, while regions that are not inflating don't. This suggests that the volume of the inflating part of the Universe in the global picture is always unimaginably larger than the part that has stopped inflating, even though inflation eventually ends as seen by any single pre-inflationary observer. Scientists disagree about how to assign a probability distribution to this hypothetical anthropic landscape. If the probability of different regions is counted by volume, one should expect that inflation will never end or applying boundary conditions that a local observer exists to observe it, that inflation will end as late as possible.\nSome physicists believe this paradox can be resolved by weighting observers by their pre-inflationary volume. Others believe that there is no resolution to the paradox and that the multiverse is a critical flaw in the inflationary paradigm. Paul Steinhardt, who first introduced the eternal inflationary model, later became one of its most vocal critics for this reason.\nInitial conditions.\nSome physicists have tried to avoid the initial conditions problem by proposing models for an eternally inflating universe with no origin. These models propose that while the Universe, on the largest scales, expands exponentially it was, is and always will be, spatially infinite and has existed, and will exist, forever.\nOther proposals attempt to describe the ex nihilo creation of the Universe based on quantum cosmology and the following inflation. Vilenkin put forth one such scenario. Hartle and Hawking offered the no-boundary proposal for the initial creation of the Universe in which inflation comes about naturally.\nGuth described the inflationary universe as the \"ultimate free lunch\": new universes, similar to our own, are continually produced in a vast inflating background. Gravitational interactions, in this case, circumvent (but do not violate) the first law of thermodynamics (energy conservation) and the second law of thermodynamics (entropy and the arrow of time problem). However, while there is consensus that this solves the initial conditions problem, some have disputed this, as it is much more likely that the Universe came about by a quantum fluctuation. Don Page was an outspoken critic of inflation because of this anomaly. He stressed that the thermodynamic arrow of time necessitates low entropy initial conditions, which would be highly unlikely. According to them, rather than solving this problem, the inflation theory aggravates it \u2013 the reheating at the end of the inflation era increases entropy, making it necessary for the initial state of the Universe to be even more orderly than in other Big Bang theories with no inflation phase.\nHawking and Page later found ambiguous results when they attempted to compute the probability of inflation in the Hartle-Hawking initial state. Other authors have argued that, since inflation is eternal, the probability doesn't matter as long as it is not precisely zero: once it starts, inflation perpetuates itself and quickly dominates the Universe. However, Albrecht and Lorenzo Sorbo argued that the probability of an inflationary cosmos, consistent with today's observations, emerging by a random fluctuation from some pre-existent state is much higher than that of a non-inflationary cosmos. This is because the \"seed\" amount of non-gravitational energy required for the inflationary cosmos is so much less than that for a non-inflationary alternative, which outweighs any entropic considerations.\nAnother problem that has occasionally been mentioned is the trans-Planckian problem or trans-Planckian effects. Since the energy scale of inflation and the Planck scale are relatively close, some of the quantum fluctuations that have made up the structure in our universe were smaller than the Planck length before inflation. Therefore, there ought to be corrections from Planck-scale physics, in particular the unknown quantum theory of gravity. Some disagreement remains about the magnitude of this effect: about whether it is just on the threshold of detectability or completely undetectable.\nHybrid inflation.\nAnother kind of inflation, called \"hybrid inflation\", is an extension of new inflation. It introduces additional scalar fields, so that while one of the scalar fields is responsible for normal slow roll inflation, another triggers the end of inflation: when inflation has continued for sufficiently long, it becomes favorable to the second field to decay into a much lower energy state.\nIn hybrid inflation, one scalar field is responsible for most of the energy density (thus determining the rate of expansion), while another is responsible for the slow roll (thus determining the period of inflation and its termination). Thus fluctuations in the former inflaton would not affect inflation termination, while fluctuations in the latter would not affect the rate of expansion. Therefore, hybrid inflation is not eternal. When the second (slow-rolling) inflaton reaches the bottom of its potential, it changes the location of the minimum of the first inflaton's potential, which leads to a fast roll of the inflaton down its potential, leading to termination of inflation.\nRelation to dark energy.\nDark energy is broadly similar to inflation and is thought to be causing the expansion of the present-day universe to accelerate. However, the energy scale of dark energy is much lower, 10\u221212\u00a0GeV, roughly 27 orders of magnitude less than the scale of inflation.\nInflation and string cosmology.\nThe discovery of flux compactifications opened the way for reconciling inflation and string theory. \"Brane inflation\" suggests that inflation arises from the motion of D-branes in the compactified geometry, usually towards a stack of anti-D-branes. This theory, governed by the \"Dirac-Born-Infeld action\", is different from ordinary inflation. The dynamics are not completely understood. It appears that special conditions are necessary since inflation occurs in tunneling between two vacua in the string landscape. The process of tunneling between two vacua is a form of old inflation, but new inflation must then occur by some other mechanism.\nInflation and loop quantum gravity.\nWhen investigating the effects the theory of loop quantum gravity would have on cosmology, a loop quantum cosmology model has evolved that provides a possible mechanism for cosmological inflation. Loop quantum gravity assumes a quantized spacetime. If the energy density is larger than can be held by the quantized spacetime, it is thought to bounce back.\nAlternatives and adjuncts.\nOther models have been advanced that are claimed to explain some or all of the observations addressed by inflation.\nBig bounce.\nThe big bounce hypothesis attempts to replace the cosmic singularity with a cosmic contraction and bounce, thereby explaining the initial conditions that led to the big bang.\nThe flatness and horizon problems are naturally solved in the Einstein-Cartan-Sciama-Kibble theory of gravity, without needing an exotic form of matter or free parameters.\nThis theory extends general relativity by removing a constraint of the symmetry of the affine connection and regarding its antisymmetric part, the torsion tensor, as a dynamical variable. The minimal coupling between torsion and Dirac spinors generates a spin-spin interaction that is significant in fermionic matter at extremely high densities. Such an interaction averts the unphysical Big Bang singularity, replacing it with a cusp-like bounce at a finite minimum scale factor, before which the Universe was contracting. The rapid expansion immediately after the Big Bounce explains why the present Universe at largest scales appears spatially flat, homogeneous and isotropic. As the density of the Universe decreases, the effects of torsion weaken and the Universe smoothly enters the radiation-dominated era.\nEkpyrotic and cyclic models.\nThe ekpyrotic and cyclic models are also considered adjuncts to inflation. These models solve the horizon problem through an expanding epoch well \"before\" the Big Bang, and then generate the required spectrum of primordial density perturbations during a contracting phase leading to a Big Crunch. The Universe passes through the Big Crunch and emerges in a hot Big Bang phase. In this sense they are reminiscent of Richard Chace Tolman's oscillatory universe; in Tolman's model, however, the total age of the Universe is necessarily finite, while in these models this is not necessarily so. Whether the correct spectrum of density fluctuations can be produced, and whether the Universe can successfully navigate the Big Bang/Big Crunch transition, remains a topic of controversy and current research. Ekpyrotic models avoid the magnetic monopole problem as long as the temperature at the Big Crunch/Big Bang transition remains below the Grand Unified Scale, as this is the temperature required to produce magnetic monopoles in the first place. As things stand, there is no evidence of any 'slowing down' of the expansion, but this is not surprising as each cycle is expected to last on the order of a trillion years.\nString gas cosmology.\nString theory requires that, in addition to the three observable spatial dimensions, additional dimensions exist that are curled up or compactified (see also Kaluza\u2013Klein theory). Extra dimensions appear as a frequent component of supergravity models and other approaches to quantum gravity. This raised the contingent question of why four space-time dimensions became large and the rest became unobservably small. An attempt to address this question, called \"string gas cosmology\", was proposed by Robert Brandenberger and Cumrun Vafa. This model focuses on the dynamics of the early universe considered as a hot gas of strings. Brandenberger and Vafa show that a dimension of spacetime can only expand if the strings that wind around it can efficiently annihilate each other. Each string is a one-dimensional object, and the largest number of dimensions in which two strings will generically intersect (and, presumably, annihilate) is three. Therefore, the most likely number of non-compact (large) spatial dimensions is three. Current work on this model centers on whether it can succeed in stabilizing the size of the compactified dimensions and produce the correct spectrum of primordial density perturbations. The original model did not \"solve the entropy and flatness problems of standard cosmology\", although Brandenburger and coauthors later argued that these problems can be eliminated by implementing string gas cosmology in the context of a bouncing-universe scenario.\nVarying \"c\".\nCosmological models employing a variable speed of light have been proposed to resolve the horizon problem of and provide an alternative to cosmic inflation. In the VSL models, the fundamental constant \"c\", denoting the speed of light in vacuum, is greater in the early universe than its present value, effectively increasing the particle horizon at the time of decoupling sufficiently to account for the observed isotropy of the CMB.\nCriticisms.\nSince its introduction by Alan Guth in 1980, the inflationary paradigm has become widely accepted. Nevertheless, many physicists, mathematicians, and philosophers of science have voiced criticisms, claiming untestable predictions and a lack of serious empirical support. In 1999, John Earman and Jes\u00fas Moster\u00edn published a thorough critical review of inflationary cosmology, concluding,\n \"we do not think that there are, as yet, good grounds for admitting any of the models of inflation into the standard core of cosmology.\"\nAs pointed out by Roger Penrose from 1986 on, in order to work, inflation requires extremely specific initial conditions of its own, so that the problem (or pseudo-problem) of initial conditions is not solved:\n \"There is something fundamentally misconceived about trying to explain the uniformity of the early universe as resulting from a thermalization process. ... For, if the thermalization is actually doing anything ... then it represents a definite increasing of the entropy. Thus, the universe would have been even more special before the thermalization than after.\"\nThe problem of specific or \"fine-tuned\" initial conditions would not have been solved; it would have gotten worse. At a conference in 2015, Penrose said that\n \"inflation isn't falsifiable, it's falsified. ... BICEP did a wonderful service by bringing all the Inflation-ists out of their shell, and giving them a black eye.\"\nA recurrent criticism of inflation is that the invoked inflaton field does not correspond to any known physical field, and that its potential energy curve seems to be an ad hoc contrivance to accommodate almost any data obtainable. Paul Steinhardt, one of the founding fathers of inflationary cosmology, has recently become one of its sharpest critics. He calls 'bad inflation' a period of accelerated expansion whose outcome conflicts with observations, and 'good inflation' one compatible with them:\n \"Not only is bad inflation more likely than good inflation, but no inflation is more likely than either ... Roger Penrose considered all the possible configurations of the inflaton and gravitational fields. Some of these configurations lead to inflation ... Other configurations lead to a uniform, flat universe directly \u2013 without inflation. Obtaining a flat universe is unlikely overall. Penrose's shocking conclusion, though, was that obtaining a flat universe without inflation is much more likely than with inflation \u2013 by a factor of 10 to the googol power!\"\nTogether with Anna Ijjas and Abraham Loeb, he wrote articles claiming that the inflationary paradigm is in trouble in view of the data from the Planck satellite.\nCounter-arguments were presented by Alan Guth, David Kaiser, and Yasunori Nomura\nand by Andrei Linde,\nsaying that\n \"cosmic inflation is on a stronger footing than ever before\".\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;* Brane cosmology\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;* "}
{"id": "5385", "revid": "1160845878", "url": "https://en.wikipedia.org/wiki?curid=5385", "title": "Candela", "text": "SI unit of luminous intensity\nThe candela ( or ; symbol: cd) is the unit of luminous intensity in the International System of Units (SI). It measures luminous power per unit solid angle emitted by a light source in a particular direction. Luminous intensity is analogous to radiant intensity, but instead of simply adding up the contributions of every wavelength of light in the source's spectrum, the contribution of each wavelength is weighted by the luminosity function, the model of the sensitivity of the human eye to different wavelengths, standardized by the CIE and ISO. A common wax candle emits light with a luminous intensity of roughly one candela. If emission in some directions is blocked by an opaque barrier, the emission would still be approximately one candela in the directions that are not obscured.\nThe word \"candela\" is Latin for \"candle\". The old name \"candle\" is still sometimes used, as in \"foot-candle\" and the modern definition of \"candlepower\".\nDefinition.\nThe 26th General Conference on Weights and Measures (CGPM) redefined the candela in 2018. The new definition, which took effect on 20 May 2019, is:\nThe candela [...] is defined by taking the fixed numerical value of the luminous efficacy of monochromatic radiation of frequency 540\u00a0\u00d7\u00a01012\u00a0Hz, \"K\"cd, to be 683 when expressed in the unit lm\u00a0W\u22121, which is equal to cd sr W\u22121, or cd sr kg\u22121 m\u22122 s3, where the kilogram, metre and second are defined in terms of \"h\", \"c\" and \u0394\"\u03bd\"Cs.\nExplanation.\nThe frequency chosen is in the visible spectrum near green, corresponding to a wavelength of about 555\u00a0nanometres. The human eye, when adapted for bright conditions, is most sensitive near this frequency. Under these conditions, photopic vision dominates the visual perception of our eyes over the scotopic vision. At other frequencies, more radiant intensity is required to achieve the same luminous intensity, according to the frequency response of the human eye. The luminous intensity for light of a particular wavelength \"\u03bb\" is given by\nformula_1\nwhere \"I\"v(\"\u03bb\") is the luminous intensity, \"I\"e(\"\u03bb\") is the radiant intensity and formula_2 is the photopic luminosity function. If more than one wavelength is present (as is usually the case), one must integrate over the spectrum of wavelengths to get the total luminous intensity.\nHistory.\nPrior to 1948, various standards for luminous intensity were in use in a number of countries. These were typically based on the brightness of the flame from a \"standard candle\" of defined composition, or the brightness of an incandescent filament of specific design. One of the best-known of these was the English standard of candlepower. One candlepower was the light produced by a pure spermaceti candle weighing one sixth of a pound and burning at a rate of 120\u00a0grains per hour. Germany, Austria and Scandinavia used the Hefnerkerze, a unit based on the output of a Hefner lamp.\nIt became clear that a better-defined unit was needed. Jules Violle had proposed a standard based on the light emitted by 1\u00a0cm2 of platinum at its melting point (or freezing point), calling this the Violle. The light intensity was due to the Planck radiator (a black body) effect, and was thus independent of the construction of the device. \nThe \"Commission Internationale de l'\u00c9clairage\" (International Commission on Illumination) and the CIPM proposed a \"new candle\" based on this basic concept. However, the value of the new unit was chosen to make it similar to the earlier unit candlepower by dividing the Violle by 60. The decision was promulgated by the CIPM in 1946:\nThe value of the new candle is such that the brightness of the full radiator at the temperature of solidification of platinum is 60\u00a0new candles per square centimetre.\nIt was then ratified in 1948 by the 9th\u00a0CGPM which adopted a new name for this unit, the \"candela\". In 1967 the 13th CGPM removed the term \"new candle\" and gave an amended version of the candela definition, specifying the atmospheric pressure applied to the freezing platinum:\nThe candela is the luminous intensity, in the perpendicular direction, of a surface of 1\u2009/\u2009600\u2009000 square\u00a0metre of a black body at the temperature of freezing platinum under a pressure of 101\u2009325\u00a0newtons per square\u00a0metre.\nIn 1979, because of the difficulties in realizing a Planck radiator at high temperatures and the new possibilities offered by radiometry, the 16th\u00a0CGPM adopted a new definition of the candela:\nThe candela is the luminous intensity, in a given direction, of a source that emits monochromatic radiation of frequency and that has a radiant intensity in that direction of \u00a0watt per steradian.\nThe definition describes how to produce a light source that (by definition) emits one candela, but does not specify the luminosity function for weighting radiation at other frequencies. Such a source could then be used to calibrate instruments designed to measure luminous intensity with reference to a specified luminosity function. An appendix to the SI Brochure makes it clear that the luminosity function is not uniquely specified, but must be selected to fully define the candela.\nThe arbitrary (1/683) term was chosen so that the new definition would precisely match the old definition. Although the candela is now defined in terms of the second (an SI base unit) and the watt (a derived SI unit), the candela remains a base unit of the SI system, by definition.\nThe 26th CGPM approved the modern definition of the candela in 2018 as part of the 2019 redefinition of SI base units, which redefined the SI base units in terms of fundamental physical constants.\nSI photometric light units.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nRelationships between luminous intensity, luminous flux, and illuminance.\nIf a source emits a known luminous intensity \"I\"v (in candelas) in a well-defined cone, the total luminous flux \"\u03a6\"v in lumens is given by\n\"\u03a6\"\"v\"\u00a0= \"I\"\"v\" 2\u03c0 [1\u00a0\u2212 cos(\"A\"/2)],\nwhere \"A\" is the \"radiation angle\" of the lamp\u2014the full vertex angle of the emission cone. For example, a lamp that emits 590\u00a0cd with a radiation angle of 40\u00b0 emits about 224\u00a0lumens. See MR16 for emission angles of some common lamps.\nIf the source emits light uniformly in all directions, the flux can be found by multiplying the intensity by 4\u03c0: a uniform 1\u00a0candela source emits 12.6\u00a0lumens.\nFor the purpose of measuring illumination, the candela is not a practical unit, as it only applies to idealized point light sources, each approximated by a source small compared to the distance from which its luminous radiation is measured, also assuming that it is done so in the absence of other light sources. What gets directly measured by a light meter is incident light on a sensor of finite area, i.e. illuminance in lm/m2 (lux). However, if designing illumination from many point light sources, like light bulbs, of known approximate omnidirectionally uniform intensities, the contributions to illuminance from incoherent light being additive, it is mathematically estimated as follows. If r\"i\" is the position of the \"i\"th source of uniform intensity \"Ii\", and \u00e2 is the unit vector normal to the illuminated elemental opaque area \"dA\" being measured, and provided that all light sources lie in the same half-space divided by the plane of this area,\nformula_4\nIn the case of a single point light source of intensity \"Iv\", at a distance \"r\" and normally incident, this reduces to\nformula_5\nSI multiples.\nLike other SI units, the candela can also be modified by adding a metric prefix that multiplies it by a power of 10, for example millicandela (mcd) for 10\u22123 candela.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5387", "revid": "31026899", "url": "https://en.wikipedia.org/wiki?curid=5387", "title": "Condensed matter physics", "text": "Branch of physics dealing with a property of matter\nCondensed matter physics is the field of physics that deals with the macroscopic and microscopic physical properties of matter, especially the solid and liquid phases which arise from electromagnetic forces between atoms. More generally, the subject deals with condensed phases of matter: systems of many constituents with strong interactions among them. More exotic condensed phases include the superconducting phase exhibited by certain materials at extremely low cryogenic temperature, the ferromagnetic and antiferromagnetic phases of spins on crystal lattices of atoms, and the Bose\u2013Einstein condensate found in ultracold atomic systems. Condensed matter physicists seek to understand the behavior of these phases by experiments to measure various material properties, and by applying the physical laws of quantum mechanics, electromagnetism, statistical mechanics, and other physics theories to develop mathematical models.\nThe diversity of systems and phenomena available for study makes condensed matter physics the most active field of contemporary physics: one third of all American physicists self-identify as condensed matter physicists, and the Division of Condensed Matter Physics is the largest division at the American Physical Society. The field overlaps with chemistry, materials science, engineering and nanotechnology, and relates closely to atomic physics and biophysics. The theoretical physics of condensed matter shares important concepts and methods with that of particle physics and nuclear physics.\nA variety of topics in physics such as crystallography, metallurgy, elasticity, magnetism, etc., were treated as distinct areas until the 1940s, when they were grouped together as \"solid-state physics\". Around the 1960s, the study of physical properties of liquids was added to this list, forming the basis for the more comprehensive specialty of condensed matter physics. The Bell Telephone Laboratories was one of the first institutes to conduct a research program in condensed matter physics. According to founding director of the Max Planck Institute for Solid State Research, physics professor Manuel Cardona, it was Albert Einstein who created the modern field of condensed matter physics starting with his seminal 1905 article on the photoelectric effect and photoluminescence which opened the fields of photoelectron spectroscopy and photoluminescence spectroscopy, and later his 1907 article on the specific heat of solids which introduced, for the first time, the effect of lattice vibrations on the thermodynamic properties of crystals, in particular the specific heat. Deputy Director of the Yale Quantum Institute A. Douglas Stone makes a similar priority case for Einstein in his work on the synthetic history of quantum mechanics.\nEtymology.\nAccording to physicist Philip Warren Anderson, the use of the term \"condensed matter\" to designate a field of study was coined by him and Volker Heine, when they changed the name of their group at the Cavendish Laboratories, Cambridge from \"Solid state theory\" to \"Theory of Condensed Matter\" in 1967, as they felt it better included their interest in liquids, nuclear matter, and so on. Although Anderson and Heine helped popularize the name \"condensed matter\", it had been used in Europe for some years, most prominently in the Springer-Verlag journal \"Physics of Condensed Matter\", launched in 1963. The name \"condensed matter physics\" emphasized the commonality of scientific problems encountered by physicists working on solids, liquids, plasmas, and other complex matter, whereas \"solid state physics\" was often associated with restricted industrial applications of metals and semiconductors. In the 1960s and 70s, some physicists felt the more comprehensive name better fit the funding environment and Cold War politics of the time.\nReferences to \"condensed\" states can be traced to earlier sources. For example, in the introduction to his 1947 book \"Kinetic Theory of Liquids\", Yakov Frenkel proposed that \"The kinetic theory of liquids must accordingly be developed as a generalization and extension of the kinetic theory of solid bodies. As a matter of fact, it would be more correct to unify them under the title of 'condensed bodies'\".\nHistory of condensed matter physics.\nClassical physics.\nOne of the first studies of condensed states of matter was by English chemist Humphry Davy, in the first decades of the nineteenth century. Davy observed that of the forty chemical elements known at the time, twenty-six had metallic properties such as lustre, ductility and high electrical and thermal conductivity. This indicated that the atoms in John Dalton's atomic theory were not indivisible as Dalton claimed, but had inner structure. Davy further claimed that elements that were then believed to be gases, such as nitrogen and hydrogen could be liquefied under the right conditions and would then behave as metals.\nIn 1823, Michael Faraday, then an assistant in Davy's lab, successfully liquefied chlorine and went on to liquefy all known gaseous elements, except for nitrogen, hydrogen, and oxygen. Shortly after, in 1869, Irish chemist Thomas Andrews studied the phase transition from a liquid to a gas and coined the term critical point to describe the condition where a gas and a liquid were indistinguishable as phases, and Dutch physicist Johannes van der Waals supplied the theoretical framework which allowed the prediction of critical behavior based on measurements at much higher temperatures. By 1908, James Dewar and Heike Kamerlingh Onnes were successfully able to liquefy hydrogen and then newly discovered helium, respectively.\nPaul Drude in 1900 proposed the first theoretical model for a classical electron moving through a metallic solid. Drude's model described properties of metals in terms of a gas of free electrons, and was the first microscopic model to explain empirical observations such as the Wiedemann\u2013Franz law. However, despite the success of Drude's free electron model, it had one notable problem: it was unable to correctly explain the electronic contribution to the specific heat and magnetic properties of metals, and the temperature dependence of resistivity at low temperatures.\nIn 1911, three years after helium was first liquefied, Onnes working at University of Leiden discovered superconductivity in mercury, when he observed the electrical resistivity of mercury to vanish at temperatures below a certain value. The phenomenon completely surprised the best theoretical physicists of the time, and it remained unexplained for several decades. Albert Einstein, in 1922, said regarding contemporary theories of superconductivity that \"with our far-reaching ignorance of the quantum mechanics of composite systems we are very far from being able to compose a theory out of these vague ideas.\"\nAdvent of quantum mechanics.\nDrude's classical model was augmented by Wolfgang Pauli, Arnold Sommerfeld, Felix Bloch and other physicists. Pauli realized that the free electrons in metal must obey the Fermi\u2013Dirac statistics. Using this idea, he developed the theory of paramagnetism in 1926. Shortly after, Sommerfeld incorporated the Fermi\u2013Dirac statistics into the free electron model and made it better to explain the heat capacity. Two years later, Bloch used quantum mechanics to describe the motion of an electron in a periodic lattice. The mathematics of crystal structures developed by Auguste Bravais, Yevgraf Fyodorov and others was used to classify crystals by their symmetry group, and tables of crystal structures were the basis for the series \"International Tables of Crystallography\", first published in 1935. Band structure calculations was first used in 1930 to predict the properties of new materials, and in 1947 John Bardeen, Walter Brattain and William Shockley developed the first semiconductor-based transistor, heralding a revolution in electronics.\nIn 1879, Edwin Herbert Hall working at the Johns Hopkins University discovered a voltage developed across conductors transverse to an electric current in the conductor and magnetic field perpendicular to the current. This phenomenon arising due to the nature of charge carriers in the conductor came to be termed the Hall effect, but it was not properly explained at the time, since the electron was not experimentally discovered until 18 years later. After the advent of quantum mechanics, Lev Landau in 1930 developed the theory of Landau quantization and laid the foundation for the theoretical explanation for the quantum Hall effect discovered half a century later.\nMagnetism as a property of matter has been known in China since 4000 BC. However, the first modern studies of magnetism only started with the development of electrodynamics by Faraday, Maxwell and others in the nineteenth century, which included classifying materials as ferromagnetic, paramagnetic and diamagnetic based on their response to magnetization. Pierre Curie studied the dependence of magnetization on temperature and discovered the Curie point phase transition in ferromagnetic materials. In 1906, Pierre Weiss introduced the concept of magnetic domains to explain the main properties of ferromagnets. The first attempt at a microscopic description of magnetism was by Wilhelm Lenz and Ernst Ising through the Ising model that described magnetic materials as consisting of a periodic lattice of spins that collectively acquired magnetization. The Ising model was solved exactly to show that spontaneous magnetization cannot occur in one dimension but is possible in higher-dimensional lattices. Further research such as by Bloch on spin waves and N\u00e9el on antiferromagnetism led to developing new magnetic materials with applications to magnetic storage devices.\nModern many-body physics.\nThe Sommerfeld model and spin models for ferromagnetism illustrated the successful application of quantum mechanics to condensed matter problems in the 1930s. However, there still were several unsolved problems, most notably the description of superconductivity and the Kondo effect. After World War II, several ideas from quantum field theory were applied to condensed matter problems. These included recognition of collective excitation modes of solids and the important notion of a quasiparticle. Russian physicist Lev Landau used the idea for the Fermi liquid theory wherein low energy properties of interacting fermion systems were given in terms of what are now termed Landau-quasiparticles. Landau also developed a mean-field theory for continuous phase transitions, which described ordered phases as spontaneous breakdown of symmetry. The theory also introduced the notion of an order parameter to distinguish between ordered phases. Eventually in 1956, John Bardeen, Leon Cooper and John Schrieffer developed the so-called BCS theory of superconductivity, based on the discovery that arbitrarily small attraction between two electrons of opposite spin mediated by phonons in the lattice can give rise to a bound state called a Cooper pair.\nThe study of phase transitions and the critical behavior of observables, termed critical phenomena, was a major field of interest in the 1960s. Leo Kadanoff, Benjamin Widom and Michael Fisher developed the ideas of critical exponents and widom scaling. These ideas were unified by Kenneth G. Wilson in 1972, under the formalism of the renormalization group in the context of quantum field theory.\nThe quantum Hall effect was discovered by Klaus von Klitzing, Dorda and Pepper in 1980 when they observed the Hall conductance to be integer multiples of a fundamental constant formula_1.(see figure) The effect was observed to be independent of parameters such as system size and impurities. In 1981, theorist Robert Laughlin proposed a theory explaining the unanticipated precision of the integral plateau. It also implied that the Hall conductance is proportional to a topological invariant, called Chern number, whose relevance for the band structure of solids was formulated by David J. Thouless and collaborators. Shortly after, in 1982, Horst St\u00f6rmer and Daniel Tsui observed the fractional quantum Hall effect where the conductance was now a rational multiple of the constant formula_1. Laughlin, in 1983, realized that this was a consequence of quasiparticle interaction in the Hall states and formulated a variational method solution, named the Laughlin wavefunction. The study of topological properties of the fractional Hall effect remains an active field of research. Decades later, the aforementioned topological band theory advanced by David J. Thouless and collaborators was further expanded leading to the discovery of topological insulators. \nIn 1986, Karl M\u00fcller and Johannes Bednorz discovered the first high temperature superconductor, a material which was superconducting at temperatures as high as 50 kelvins. It was realized that the high temperature superconductors are examples of strongly correlated materials where the electron\u2013electron interactions play an important role. A satisfactory theoretical description of high-temperature superconductors is still not known and the field of strongly correlated materials continues to be an active research topic.\nIn 2009, David Field and researchers at Aarhus University discovered spontaneous electric fields when creating prosaic films of various gases. This has more recently expanded to form the research area of spontelectrics.\nIn 2012, several groups released preprints which suggest that samarium hexaboride has the properties of a topological insulator in accord with the earlier theoretical predictions. Since samarium hexaboride is an established Kondo insulator, i.e. a strongly correlated electron material, it is expected that the existence of a topological Dirac surface state in this material would lead to a topological insulator with strong electronic correlations.\nTheoretical.\nTheoretical condensed matter physics involves the use of theoretical models to understand properties of states of matter. These include models to study the electronic properties of solids, such as the Drude model, the band structure and the density functional theory. Theoretical models have also been developed to study the physics of phase transitions, such as the Ginzburg\u2013Landau theory, critical exponents and the use of mathematical methods of quantum field theory and the renormalization group. Modern theoretical studies involve the use of numerical computation of electronic structure and mathematical tools to understand phenomena such as high-temperature superconductivity, topological phases, and gauge symmetries.\nEmergence.\nTheoretical understanding of condensed matter physics is closely related to the notion of emergence, wherein complex assemblies of particles behave in ways dramatically different from their individual constituents. For example, a range of phenomena related to high temperature superconductivity are understood poorly, although the microscopic physics of individual electrons and lattices is well known. Similarly, models of condensed matter systems have been studied where collective excitations behave like photons and electrons, thereby describing electromagnetism as an emergent phenomenon. Emergent properties can also occur at the interface between materials: one example is the lanthanum aluminate-strontium titanate interface, where two band-insulators are joined to create conductivity and superconductivity.\nElectronic theory of solids.\nThe metallic state has historically been an important building block for studying properties of solids. The first theoretical description of metals was given by Paul Drude in 1900 with the Drude model, which explained electrical and thermal properties by describing a metal as an ideal gas of then-newly discovered electrons. He was able to derive the empirical Wiedemann-Franz law and get results in close agreement with the experiments. This classical model was then improved by Arnold Sommerfeld who incorporated the Fermi\u2013Dirac statistics of electrons and was able to explain the anomalous behavior of the specific heat of metals in the Wiedemann\u2013Franz law. In 1912, The structure of crystalline solids was studied by Max von Laue and Paul Knipping, when they observed the X-ray diffraction pattern of crystals, and concluded that crystals get their structure from periodic lattices of atoms. In 1928, Swiss physicist Felix Bloch provided a wave function solution to the Schr\u00f6dinger equation with a periodic potential, known as Bloch's theorem.\nCalculating electronic properties of metals by solving the many-body wavefunction is often computationally hard, and hence, approximation methods are needed to obtain meaningful predictions. The Thomas\u2013Fermi theory, developed in the 1920s, was used to estimate system energy and electronic density by treating the local electron density as a variational parameter. Later in the 1930s, Douglas Hartree, Vladimir Fock and John Slater developed the so-called Hartree\u2013Fock wavefunction as an improvement over the Thomas\u2013Fermi model. The Hartree\u2013Fock method accounted for exchange statistics of single particle electron wavefunctions. In general, it is very difficult to solve the Hartree\u2013Fock equation. Only the free electron gas case can be solved exactly. Finally in 1964\u201365, Walter Kohn, Pierre Hohenberg and Lu Jeu Sham proposed the density functional theory (DFT) which gave realistic descriptions for bulk and surface properties of metals. The density functional theory has been widely used since the 1970s for band structure calculations of variety of solids.\nSymmetry breaking.\nSome states of matter exhibit \"symmetry breaking\", where the relevant laws of physics possess some form of symmetry that is broken. A common example is crystalline solids, which break continuous translational symmetry. Other examples include magnetized ferromagnets, which break rotational symmetry, and more exotic states such as the ground state of a BCS superconductor, that breaks U(1) phase rotational symmetry.\nGoldstone's theorem in quantum field theory states that in a system with broken continuous symmetry, there may exist excitations with arbitrarily low energy, called the Goldstone bosons. For example, in crystalline solids, these correspond to phonons, which are quantized versions of lattice vibrations.\nPhase transition.\nPhase transition refers to the change of phase of a system, which is brought about by change in an external parameter such as temperature. Classical phase transition occurs at finite temperature when the order of the system was destroyed. For example, when ice melts and becomes water, the ordered crystal structure is destroyed.\nIn quantum phase transitions, the temperature is set to absolute zero, and the non-thermal control parameter, such as pressure or magnetic field, causes the phase transitions when order is destroyed by quantum fluctuations originating from the Heisenberg uncertainty principle. Here, the different quantum phases of the system refer to distinct ground states of the Hamiltonian matrix. Understanding the behavior of quantum phase transition is important in the difficult tasks of explaining the properties of rare-earth magnetic insulators, high-temperature superconductors, and other substances.\nTwo classes of phase transitions occur: \"first-order transitions\" and \"second-order\" or \"continuous transitions\". For the latter, the two phases involved do not co-exist at the transition temperature, also called the critical point. Near the critical point, systems undergo critical behavior, wherein several of their properties such as correlation length, specific heat, and magnetic susceptibility diverge exponentially. These critical phenomena present serious challenges to physicists because normal macroscopic laws are no longer valid in the region, and novel ideas and methods must be invented to find the new laws that can describe the system.\nThe simplest theory that can describe continuous phase transitions is the Ginzburg\u2013Landau theory, which works in the so-called mean-field approximation. However, it can only roughly explain continuous phase transition for ferroelectrics and type I superconductors which involves long range microscopic interactions. For other types of systems that involves short range interactions near the critical point, a better theory is needed.\nNear the critical point, the fluctuations happen over broad range of size scales while the feature of the whole system is scale invariant. Renormalization group methods successively average out the shortest wavelength fluctuations in stages while retaining their effects into the next stage. Thus, the changes of a physical system as viewed at different size scales can be investigated systematically. The methods, together with powerful computer simulation, contribute greatly to the explanation of the critical phenomena associated with continuous phase transition.\nExperimental.\nExperimental condensed matter physics involves the use of experimental probes to try to discover new properties of materials. Such probes include effects of electric and magnetic fields, measuring response functions, transport properties and thermometry. Commonly used experimental methods include spectroscopy, with probes such as X-rays, infrared light and inelastic neutron scattering; study of thermal response, such as specific heat and measuring transport via thermal and heat conduction.\nScattering.\nSeveral condensed matter experiments involve scattering of an experimental probe, such as X-ray, optical photons, neutrons, etc., on constituents of a material. The choice of scattering probe depends on the observation energy scale of interest. Visible light has energy on the scale of 1 electron volt (eV) and is used as a scattering probe to measure variations in material properties such as dielectric constant and refractive index. X-rays have energies of the order of 10 keV and hence are able to probe atomic length scales, and are used to measure variations in electron charge density.\nNeutrons can also probe atomic length scales and are used to study scattering off nuclei and electron spins and magnetization (as neutrons have spin but no charge). Coulomb and Mott scattering measurements can be made by using electron beams as scattering probes. Similarly, positron annihilation can be used as an indirect measurement of local electron density. Laser spectroscopy is an excellent tool for studying the microscopic properties of a medium, for example, to study forbidden transitions in media with nonlinear optical spectroscopy. \nExternal magnetic fields.\nIn experimental condensed matter physics, external magnetic fields act as thermodynamic variables that control the state, phase transitions and properties of material systems. Nuclear magnetic resonance (NMR) is a method by which external magnetic fields are used to find resonance modes of individual electrons, thus giving information about the atomic, molecular, and bond structure of their neighborhood. NMR experiments can be made in magnetic fields with strengths up to 60 tesla. Higher magnetic fields can improve the quality of NMR measurement data. Quantum oscillations is another experimental method where high magnetic fields are used to study material properties such as the geometry of the Fermi surface. High magnetic fields will be useful in experimentally testing of the various theoretical predictions such as the quantized magnetoelectric effect, image magnetic monopole, and the half-integer quantum Hall effect.\nNuclear spectroscopy.\nThe local structure, the structure of the nearest neighbour atoms, of condensed matter can be investigated with methods of nuclear spectroscopy, which are very sensitive to small changes. Using specific and radioactive nuclei, the nucleus becomes the probe that interacts with its surrounding electric and magnetic fields (hyperfine interactions). The methods are suitable to study defects, diffusion, phase change, magnetism. Common methods are e.g. NMR, M\u00f6ssbauer spectroscopy, or perturbed angular correlation (PAC). Especially PAC is ideal for the study of phase changes at extreme temperature above 2000\u00a0\u00b0C due to no temperature dependence of the method.\nCold atomic gases.\nUltracold atom trapping in optical lattices is an experimental tool commonly used in condensed matter physics, and in atomic, molecular, and optical physics. The method involves using optical lasers to form an interference pattern, which acts as a \"lattice\", in which ions or atoms can be placed at very low temperatures. Cold atoms in optical lattices are used as \"quantum simulators\", that is, they act as controllable systems that can model behavior of more complicated systems, such as frustrated magnets. In particular, they are used to engineer one-, two- and three-dimensional lattices for a Hubbard model with pre-specified parameters, and to study phase transitions for antiferromagnetic and spin liquid ordering.\nIn 1995, a gas of rubidium atoms cooled down to a temperature of 170 nK was used to experimentally realize the Bose\u2013Einstein condensate, a novel state of matter originally predicted by S. N. Bose and Albert Einstein, wherein a large number of atoms occupy one quantum state.\nApplications.\nResearch in condensed matter physics has given rise to several device applications, such as the development of the semiconductor transistor, laser technology, and several phenomena studied in the context of nanotechnology. Methods such as scanning-tunneling microscopy can be used to control processes at the nanometer scale, and have given rise to the study of nanofabrication. Such molecular machines were developed for example by Nobel laurate in chemistry Ben Feringa. He and his team developed multiple molecular machines such as molecular car, molecular windmill and many more.\nIn quantum computation, information is represented by quantum bits, or qubits. The qubits may decohere quickly before useful computation is completed. This serious problem must be solved before quantum computing may be realized. To solve this problem, several promising approaches are proposed in condensed matter physics, including Josephson junction qubits, spintronic qubits using the spin orientation of magnetic materials, or the topological non-Abelian anyons from fractional quantum Hall effect states.\nCondensed matter physics also has important uses for biophysics, for example, the experimental method of magnetic resonance imaging, which is widely used in medical diagnosis.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5388", "revid": "1153571569", "url": "https://en.wikipedia.org/wiki?curid=5388", "title": "Cultural anthropology", "text": "Branch of anthropology focused on the study of cultural variation among humans\nCultural anthropology is a branch of anthropology focused on the study of cultural variation among humans. It is in contrast to social anthropology, which perceives cultural variation as a subset of a posited anthropological constant. The portmanteau term sociocultural anthropology includes both cultural and social anthropology traditions.\nAnthropologists have pointed out that through culture, people can adapt to their environment in non-genetic ways, so people living in different environments will often have different cultures. Much of anthropological theory has originated in an appreciation of and interest in the tension between the local (particular cultures) and the global (a universal human nature, or the web of connections between people in distinct places/circumstances).\nCultural anthropology has a rich methodology, including participant observation (often called fieldwork because it requires the anthropologist spending an extended period of time at the research location), interviews, and surveys.\nHistory.\nThe rise of cultural anthropology took place within the context of the late 19th century, when questions regarding which cultures were \"primitive\" and which were \"civilized\" occupied the mind of not only Freud, but many others. Colonialism and its processes increasingly brought European thinkers into direct or indirect contact with \"primitive others\". The relative status of various humans, some of whom had modern advanced technologies that included engines and telegraphs, while others lacked anything but face-to-face communication techniques and still lived a Paleolithic lifestyle, was of interest to the first generation of cultural anthropologists.\nTheoretical foundations.\nThe concept of culture.\nOne of the earliest articulations of the anthropological meaning of the term \"culture\" came from Sir Edward Tylor who writes on the first page of his 1871 book: \"Culture, or civilization, taken in its broad, ethnographic sense, is that complex whole which includes knowledge, belief, art, morals, law, custom, and any other capabilities and habits acquired by man as a member of society.\" The term \"civilization\" later gave way to definitions given by V. Gordon Childe, with culture forming an umbrella term and civilization becoming a particular kind of culture.\nAccording to Kay Milton, former director of anthropology research at Queens University Belfast, culture can be general or specific. This means culture can be something applied to all human beings or it can be specific to a certain group of people such as African American culture or Irish American culture. Specific cultures are structured systems which means they are organized very specifically and adding or taking away any element from that system may disrupt it.\nThe critique of evolutionism.\nAnthropology is concerned with the lives of people in different parts of the world, particularly in relation to the discourse of beliefs and practices. In addressing this question, ethnologists in the 19th century divided into two schools of thought. Some, like Grafton Elliot Smith, argued that different groups must have learned from one another somehow, however indirectly; in other words, they argued that cultural traits spread from one place to another, or \"diffused\".\nOther ethnologists argued that different groups had the capability of creating similar beliefs and practices independently. Some of those who advocated \"independent invention\", like Lewis Henry Morgan, additionally supposed that similarities meant that different groups had passed through the same stages of cultural evolution (See also classical social evolutionism). Morgan, in particular, acknowledged that certain forms of society and culture could not possibly have arisen before others. For example, industrial farming could not have been invented before simple farming, and metallurgy could not have developed without previous non-smelting processes involving metals (such as simple ground collection or mining). Morgan, like other 19th century social evolutionists, believed there was a more or less orderly progression from the primitive to the civilized.\n20th-century anthropologists largely reject the notion that all human societies must pass through the same stages in the same order, on the grounds that such a notion does not fit the empirical facts. Some 20th-century ethnologists, like Julian Steward, have instead argued that such similarities reflected similar adaptations to similar environments. Although 19th-century ethnologists saw \"diffusion\" and \"independent invention\" as mutually exclusive and competing theories, most ethnographers quickly reached a consensus that both processes occur, and that both can plausibly account for cross-cultural similarities. But these ethnographers also pointed out the superficiality of many such similarities. They noted that even traits that spread through diffusion often were given different meanings and function from one society to another. Analyses of large human concentrations in big cities, in multidisciplinary studies by Ronald Daus, show how new methods may be applied to the understanding of man living in a global world and how it was caused by the action of extra-European nations, so highlighting the role of Ethics in modern anthropology.\nAccordingly, most of these anthropologists showed less interest in comparing cultures, generalizing about human nature, or discovering universal laws of cultural development, than in understanding particular cultures in those cultures' own terms. Such ethnographers and their students promoted the idea of \"cultural relativism\", the view that one can only understand another person's beliefs and behaviors in the context of the culture in which they live or lived.\nOthers, such as Claude L\u00e9vi-Strauss (who was influenced both by American cultural anthropology and by French Durkheimian sociology), have argued that apparently similar patterns of development reflect fundamental similarities in the structure of human thought (see structuralism). By the mid-20th century, the number of examples of people skipping stages, such as going from hunter-gatherers to post-industrial service occupations in one generation, were so numerous that 19th-century evolutionism was effectively disproved.\nCultural relativism.\nCultural relativism is a principle that was established as axiomatic in anthropological research by Franz Boas and later popularized by his students. Boas first articulated the idea in 1887: \"...civilization is not something absolute, but ... is relative, and ... our ideas and conceptions are true only so far as our civilization goes.\" Although Boas did not coin the term, it became common among anthropologists after Boas' death in 1942, to express their synthesis of a number of ideas Boas had developed. Boas believed that the sweep of cultures, to be found in connection with any sub-species, is so vast and pervasive that there cannot be a relationship between culture and race. Cultural relativism involves specific epistemological and methodological claims. Whether or not these claims require a specific ethical stance is a matter of debate. This principle should not be confused with moral relativism.\nCultural relativism was in part a response to Western ethnocentrism. Ethnocentrism may take obvious forms, in which one consciously believes that one's people's arts are the most beautiful, values the most virtuous, and beliefs the most truthful. Boas, originally trained in physics and geography, and heavily influenced by the thought of Kant, Herder, and von Humboldt, argued that one's culture may mediate and thus limit one's perceptions in less obvious ways. This understanding of culture confronts anthropologists with two problems: first, how to escape the unconscious bonds of one's own culture, which inevitably bias our perceptions of and reactions to the world, and second, how to make sense of an unfamiliar culture. The principle of cultural relativism thus forced anthropologists to develop innovative methods and heuristic strategies.\nBoas and his students realized that if they were to conduct scientific research in other cultures, they would need to employ methods that would help them escape the limits of their own ethnocentrism. One such method is that of ethnography: basically, they advocated living with people of another culture for an extended period of time, so that they could learn the local language and be enculturated, at least partially, into that culture. In this context, cultural relativism is of fundamental methodological importance, because it calls attention to the importance of the local context in understanding the meaning of particular human beliefs and activities. Thus, in 1948 Virginia Heyer wrote, \"Cultural relativity, to phrase it in starkest abstraction, states the relativity of the part to the whole. The part gains its cultural significance by its place in the whole, and cannot retain its integrity in a different situation.\"\nTheoretical approaches.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nComparison with social anthropology.\nThe rubric \"cultural\" anthropology is generally applied to ethnographic works that are holistic in approach, are oriented to the ways in which culture affects individual experience, or aim to provide a rounded view of the knowledge, customs, and institutions of a people. \"Social\" anthropology is a term applied to ethnographic works that attempt to isolate a particular system of social relations such as those that comprise domestic life, economy, law, politics, or religion, give analytical priority to the organizational bases of social life, and attend to cultural phenomena as somewhat secondary to the main issues of social scientific inquiry.\nParallel with the rise of cultural anthropology in the United States, social anthropology developed as an academic discipline in Britain and in France.\nFoundational thinkers.\nLewis Henry Morgan.\nLewis Henry Morgan (1818\u20131881), a lawyer from Rochester, New York, became an advocate for and ethnological scholar of the Iroquois. His comparative analyses of religion, government, material culture, and especially kinship patterns proved to be influential contributions to the field of anthropology. Like other scholars of his day (such as Edward Tylor), Morgan argued that human societies could be classified into categories of cultural evolution on a scale of progression that ranged from \"savagery\", to \"barbarism\", to \"civilization\". Generally, Morgan used technology (such as bowmaking or pottery) as an indicator of position on this scale.\nFranz Boas, founder of the modern discipline.\nFranz Boas (1858\u20131942) established academic anthropology in the United States in opposition to Morgan's evolutionary perspective. His approach was empirical, skeptical of overgeneralizations, and eschewed attempts to establish universal laws. For example, Boas studied immigrant children to demonstrate that biological race was not immutable, and that human conduct and behavior resulted from nurture, rather than nature.\nInfluenced by the German tradition, Boas argued that the world was full of distinct \"cultures,\" rather than societies whose evolution could be measured by how much or how little \"civilization\" they had. He believed that each culture has to be studied in its particularity, and argued that cross-cultural generalizations, like those made in the natural sciences, were not possible.\nIn doing so, he fought discrimination against immigrants, blacks, and indigenous peoples of the Americas. Many American anthropologists adopted his agenda for social reform, and theories of race continue to be popular subjects for anthropologists today. The so-called \"Four Field Approach\" has its origins in Boasian Anthropology, dividing the discipline in the four crucial and interrelated fields of sociocultural, biological, linguistic, and archaic anthropology (e.g. archaeology). Anthropology in the United States continues to be deeply influenced by the Boasian tradition, especially its emphasis on culture.\nKroeber, Mead, and Benedict.\nBoas used his positions at Columbia University and the American Museum of Natural History (AMNH) to train and develop multiple generations of students. His first generation of students included Alfred Kroeber, Robert Lowie, Edward Sapir, and Ruth Benedict, who each produced richly detailed studies of indigenous North American cultures. They provided a wealth of details used to attack the theory of a single evolutionary process. Kroeber and Sapir's focus on Native American languages helped establish linguistics as a truly general science and free it from its historical focus on Indo-European languages.\nThe publication of Alfred Kroeber's textbook \"Anthropology\" (1923) marked a turning point in American anthropology. After three decades of amassing material, Boasians felt a growing urge to generalize. This was most obvious in the 'Culture and Personality' studies carried out by younger Boasians such as Margaret Mead and Ruth Benedict. Influenced by psychoanalytic psychologists including Sigmund Freud and Carl Jung, these authors sought to understand the way that individual personalities were shaped by the wider cultural and social forces in which they grew up.\nThough such works as Mead's \"Coming of Age in Samoa\" (1928) and Benedict's \"The Chrysanthemum and the Sword\" (1946) remain popular with the American public, Mead and Benedict never had the impact on the discipline of anthropology that some expected. Boas had planned for Ruth Benedict to succeed him as chair of Columbia's anthropology department, but she was sidelined in favor of Ralph Linton, and Mead was limited to her offices at the AMNH.\nWolf, Sahlins, Mintz, and political economy.\nIn the 1950s and mid-1960s anthropology tended increasingly to model itself after the natural sciences. Some anthropologists, such as Lloyd Fallers and Clifford Geertz, focused on processes of modernization by which newly independent states could develop. Others, such as Julian Steward and Leslie White, focused on how societies evolve and fit their ecological niche\u2014an approach popularized by Marvin Harris.\nEconomic anthropology as influenced by Karl Polanyi and practiced by Marshall Sahlins and George Dalton challenged standard neoclassical economics to take account of cultural and social factors, and employed Marxian analysis into anthropological study. In England, British Social Anthropology's paradigm began to fragment as Max Gluckman and Peter Worsley experimented with Marxism and authors such as Rodney Needham and Edmund Leach incorporated L\u00e9vi-Strauss's structuralism into their work. Structuralism also influenced a number of developments in the 1960s and 1970s, including cognitive anthropology and componential analysis.\nIn keeping with the times, much of anthropology became politicized through the Algerian War of Independence and opposition to the Vietnam War; Marxism became an increasingly popular theoretical approach in the discipline. By the 1970s the authors of volumes such as \"Reinventing Anthropology\" worried about anthropology's relevance.\nSince the 1980s issues of power, such as those examined in Eric Wolf's \"Europe and the People Without History\", have been central to the discipline. In the 1980s books like \"Anthropology and the Colonial Encounter\" pondered anthropology's ties to colonial inequality, while the immense popularity of theorists such as Antonio Gramsci and Michel Foucault moved issues of power and hegemony into the spotlight. Gender and sexuality became popular topics, as did the relationship between history and anthropology, influenced by Marshall Sahlins, who drew on L\u00e9vi-Strauss and Fernand Braudel to examine the relationship between symbolic meaning, sociocultural structure, and individual agency in the processes of historical transformation. Jean and John Comaroff produced a whole generation of anthropologists at the University of Chicago that focused on these themes. Also influential in these issues were Nietzsche, Heidegger, the critical theory of the Frankfurt School, Derrida and Lacan.\nGeertz, Schneider, and interpretive anthropology.\nMany anthropologists reacted against the renewed emphasis on materialism and scientific modelling derived from Marx by emphasizing the importance of the concept of culture. Authors such as David Schneider, Clifford Geertz, and Marshall Sahlins developed a more fleshed-out concept of culture as a web of meaning or signification, which proved very popular within and beyond the discipline. Geertz was to state:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Believing, with Max Weber, that man is an animal suspended in webs of significance he himself has spun, I take culture to be those webs, and the analysis of it to be therefore not an experimental science in search of law but an interpretive one in search of meaning.\"\nGeertz's interpretive method involved what he called \"thick description\". The cultural symbols of rituals, political and economic action, and of kinship, are \"read\" by the anthropologist as if they are a document in a foreign language. The interpretation of those symbols must be re-framed for their anthropological audience, i.e. transformed from the \"experience-near\" but foreign concepts of the other culture, into the \"experience-distant\" theoretical concepts of the anthropologist. These interpretations must then be reflected back to its originators, and its adequacy as a translation fine-tuned in a repeated way, a process called the hermeneutic circle. Geertz applied his method in a number of areas, creating programs of study that were very productive. His analysis of \"religion as a cultural system\" was particularly influential outside of anthropology. David Schnieder's cultural analysis of American kinship has proven equally influential. Schneider demonstrated that the American folk-cultural emphasis on \"blood connections\" had an undue influence on anthropological kinship theories, and that kinship is not a biological characteristic but a cultural relationship established on very different terms in different societies.\nProminent British symbolic anthropologists include Victor Turner and Mary Douglas.\nThe post-modern turn.\nIn the late 1980s and 1990s authors such as James Clifford pondered ethnographic authority, in particular how and why anthropological knowledge was possible and authoritative. They were reflecting trends in research and discourse initiated by feminists in the academy, although they excused themselves from commenting specifically on those pioneering critics. Nevertheless, key aspects of feminist theory and methods became \"de rigueur\" as part of the 'post-modern moment' in anthropology: Ethnographies became more interpretative and reflexive, explicitly addressing the author's methodology; cultural, gendered, and racial positioning; and their influence on the ethnographic analysis. This was part of a more general trend of postmodernism that was popular contemporaneously. Currently anthropologists pay attention to a wide variety of issues pertaining to the contemporary world, including globalization, medicine and biotechnology, indigenous rights, virtual communities, and the anthropology of industrialized societies.\nSocio-cultural anthropology subfields.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nMethods.\nModern cultural anthropology has its origins in, and developed in reaction to, 19th century ethnology, which involves the organized comparison of human societies. Scholars like E.B. Tylor and J.G. Frazer in England worked mostly with materials collected by others\u2014usually missionaries, traders, explorers, or colonial officials\u2014earning them the moniker of \"arm-chair anthropologists\".\nParticipant observation.\nParticipant observation is one of the principal research methods of cultural anthropology. It relies on the assumption that the best way to understand a group of people is to interact with them closely over a long period of time. The method originated in the field research of social anthropologists, especially Bronislaw Malinowski in Britain, the students of Franz Boas in the United States, and in the later urban research of the Chicago School of Sociology. Historically, the group of people being studied was a small, non-Western society. However, today it may be a specific corporation, a church group, a sports team, or a small town. There are no restrictions as to what the subject of participant observation can be, as long as the group of people is studied intimately by the observing anthropologist over a long period of time. This allows the anthropologist to develop trusting relationships with the subjects of study and receive an inside perspective on the culture, which helps him or her to give a richer description when writing about the culture later. Observable details (like daily time allotment) and more hidden details (like taboo behavior) are more easily observed and interpreted over a longer period of time, and researchers can discover discrepancies between what participants say\u2014and often believe\u2014should happen (the formal system) and what actually does happen, or between different aspects of the formal system; in contrast, a one-time survey of people's answers to a set of questions might be quite consistent, but is less likely to show conflicts between different aspects of the social system or between conscious representations and behavior.\nInteractions between an ethnographer and a cultural informant must go both ways. Just as an ethnographer may be naive or curious about a culture, the members of that culture may be curious about the ethnographer. To establish connections that will eventually lead to a better understanding of the cultural context of a situation, an anthropologist must be open to becoming part of the group, and willing to develop meaningful relationships with its members. One way to do this is to find a small area of common experience between an anthropologist and their subjects, and then to expand from this common ground into the larger area of difference. Once a single connection has been established, it becomes easier to integrate into the community, and more likely that accurate and complete information is being shared with the anthropologist.\nBefore participant observation can begin, an anthropologist must choose both a location and a focus of study. This focus may change once the anthropologist is actively observing the chosen group of people, but having an idea of what one wants to study before beginning fieldwork allows an anthropologist to spend time researching background information on their topic. It can also be helpful to know what previous research has been conducted in one's chosen location or on similar topics, and if the participant observation takes place in a location where the spoken language is not one the anthropologist is familiar with, they will usually also learn that language. This allows the anthropologist to become better established in the community. The lack of need for a translator makes communication more direct, and allows the anthropologist to give a richer, more contextualized representation of what they witness. In addition, participant observation often requires permits from governments and research institutions in the area of study, and always needs some form of funding.\nThe majority of participant observation is based on conversation. This can take the form of casual, friendly dialogue, or can also be a series of more structured interviews. A combination of the two is often used, sometimes along with photography, mapping, artifact collection, and various other methods. In some cases, ethnographers also turn to structured observation, in which an anthropologist's observations are directed by a specific set of questions they are trying to answer. In the case of structured observation, an observer might be required to record the order of a series of events, or describe a certain part of the surrounding environment. While the anthropologist still makes an effort to become integrated into the group they are studying, and still participates in the events as they observe, structured observation is more directed and specific than participant observation in general. This helps to standardize the method of study when ethnographic data is being compared across several groups or is needed to fulfill a specific purpose, such as research for a governmental policy decision.\nOne common criticism of participant observation is its lack of objectivity. Because each anthropologist has their own background and set of experiences, each individual is likely to interpret the same culture in a different way. Who the ethnographer is has a lot to do with what they will eventually write about a culture, because each researcher is influenced by their own perspective. This is considered a problem especially when anthropologists write in the ethnographic present, a present tense which makes a culture seem stuck in time, and ignores the fact that it may have interacted with other cultures or gradually evolved since the anthropologist made observations. To avoid this, past ethnographers have advocated for strict training, or for anthropologists working in teams. However, these approaches have not generally been successful, and modern ethnographers often choose to include their personal experiences and possible biases in their writing instead.\nParticipant observation has also raised ethical questions, since an anthropologist is in control of what they report about a culture. In terms of representation, an anthropologist has greater power than their subjects of study, and this has drawn criticism of participant observation in general. Additionally, anthropologists have struggled with the effect their presence has on a culture. Simply by being present, a researcher causes changes in a culture, and anthropologists continue to question whether or not it is appropriate to influence the cultures they study, or possible to avoid having influence.\nEthnography.\nIn the 20th century, most cultural and social anthropologists turned to the crafting of ethnographies. An ethnography is a piece of writing about a people, at a particular place and time. Typically, the anthropologist lives among people in another society for a period of time, simultaneously participating in and observing the social and cultural life of the group.\nNumerous other ethnographic techniques have resulted in ethnographic writing or details being preserved, as cultural anthropologists also curate materials, spend long hours in libraries, churches and schools poring over records, investigate graveyards, and decipher ancient scripts. A typical ethnography will also include information about physical geography, climate and habitat. It is meant to be a holistic piece of writing about the people in question, and today often includes the longest possible timeline of past events that the ethnographer can obtain through primary and secondary research.\nBronis\u0142aw Malinowski developed the ethnographic method, and Franz Boas taught it in the United States. Boas' students such as Alfred L. Kroeber, Ruth Benedict and Margaret Mead drew on his conception of culture and cultural relativism to develop cultural anthropology in the United States. Simultaneously, Malinowski and A.R. Radcliffe Brown's students were developing social anthropology in the United Kingdom. Whereas cultural anthropology focused on symbols and values, social anthropology focused on social groups and institutions. Today socio-cultural anthropologists attend to all these elements.\nIn the early 20th century, socio-cultural anthropology developed in different forms in Europe and in the United States. European \"social anthropologists\" focused on observed social behaviors and on \"social structure\", that is, on relationships among social roles (for example, husband and wife, or parent and child) and social institutions (for example, religion, economy, and politics).\nAmerican \"cultural anthropologists\" focused on the ways people expressed their view of themselves and their world, especially in symbolic forms, such as art and myths. These two approaches frequently converged and generally complemented one another. For example, kinship and leadership function both as symbolic systems and as social institutions. Today almost all socio-cultural anthropologists refer to the work of both sets of predecessors, and have an equal interest in what people do and in what people say.\nCross-cultural comparison.\nOne means by which anthropologists combat ethnocentrism is to engage in the process of cross-cultural comparison. It is important to test so-called \"human universals\" against the ethnographic record. Monogamy, for example, is frequently touted as a universal human trait, yet comparative study shows that it is not.\nThe Human Relations Area Files, Inc. (HRAF) is a research agency based at Yale University. Since 1949, its mission has been to encourage and facilitate worldwide comparative studies of human culture, society, and behavior in the past and present. The name came from the Institute of Human Relations, an interdisciplinary program/building at Yale at the time. The Institute of Human Relations had sponsored HRAF's precursor, the \"Cross-Cultural Survey\" (see George Peter Murdock), as part of an effort to develop an integrated science of human behavior and culture. The two eHRAF databases on the Web are expanded and updated annually. \"eHRAF World Cultures\" includes materials on cultures, past and present, and covers nearly 400 cultures. The second database, \"eHRAF Archaeology\", covers major archaeological traditions and many more sub-traditions and sites around the world.\nComparison across cultures includies the industrialized (or de-industrialized) West. Cultures in the more traditional standard cross-cultural sample of small scale societies are:\nMulti-sited ethnography.\nEthnography dominates socio-cultural anthropology. Nevertheless, many contemporary socio-cultural anthropologists have rejected earlier models of ethnography as treating local cultures as bounded and isolated. These anthropologists continue to concern themselves with the distinct ways people in different locales experience and understand their lives, but they often argue that one cannot understand these particular ways of life solely from a local perspective; they instead combine a focus on the local with an effort to grasp larger political, economic, and cultural frameworks that impact local lived realities. Notable proponents of this approach include Arjun Appadurai, James Clifford, George Marcus, Sidney Mintz, Michael Taussig, Eric Wolf and Ronald Daus.\nA growing trend in anthropological research and analysis is the use of multi-sited ethnography, discussed in George Marcus' article, \"Ethnography In/Of the World System: the Emergence of Multi-Sited Ethnography\". Looking at culture as embedded in macro-constructions of a global social order, multi-sited ethnography uses traditional methodology in various locations both spatially and temporally. Through this methodology, greater insight can be gained when examining the impact of world-systems on local and global communities.\nAlso emerging in multi-sited ethnography are greater interdisciplinary approaches to fieldwork, bringing in methods from cultural studies, media studies, science and technology studies, and others. In multi-sited ethnography, research tracks a subject across spatial and temporal boundaries. For example, a multi-sited ethnography may follow a \"thing\", such as a particular commodity, as it is transported through the networks of global capitalism.\nMulti-sited ethnography may also follow ethnic groups in diaspora, stories or rumours that appear in multiple locations and in multiple time periods, metaphors that appear in multiple ethnographic locations, or the biographies of individual people or groups as they move through space and time. It may also follow conflicts that transcend boundaries. An example of multi-sited ethnography is Nancy Scheper-Hughes' work on the international black market for the trade of human organs. In this research, she follows organs as they are transferred through various legal and illegal networks of capitalism, as well as the rumours and urban legends that circulate in impoverished communities about child kidnapping and organ theft.\nSociocultural anthropologists have increasingly turned their investigative eye on to \"Western\" culture. For example, Philippe Bourgois won the Margaret Mead Award in 1997 for \"In Search of Respect\", a study of the entrepreneurs in a Harlem crack-den. Also growing more popular are ethnographies of professional communities, such as laboratory researchers, Wall Street investors, law firms, or information technology (IT) computer employees.\nTopics in cultural anthropology.\nKinship and family.\nKinship refers to the anthropological study of the ways in which humans form and maintain relationships with one another, and further, how those relationships operate within and define social organization.\nResearch in kinship studies often crosses over into different anthropological subfields including medical, feminist, and public anthropology. This is likely due to its fundamental concepts, as articulated by linguistic anthropologist Patrick McConvell: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Kinship is the bedrock of all human societies that we know. All humans recognize fathers and mothers, sons and daughters, brothers and sisters, uncles and aunts, husbands and wives, grandparents, cousins, and often many more complex types of relationships in the terminologies that they use. That is the matrix into which human children are born in the great majority of cases, and their first words are often kinship terms.Throughout history, kinship studies have primarily focused on the topics of marriage, descent, and procreation. Anthropologists have written extensively on the variations within marriage across cultures and its legitimacy as a human institution. There are stark differences between communities in terms of marital practice and value, leaving much room for anthropological fieldwork. For instance, the Nuer of Sudan and the Brahmans of Nepal practice polygyny, where one man has several marriages to two or more women. The Nyar of India and Nyimba of Tibet and Nepal practice polyandry, where one woman is often married to two or more men. The marital practice found in most cultures, however, is monogamy, where one woman is married to one man. Anthropologists also study different marital taboos across cultures, most commonly the incest taboo of marriage within sibling and parent-child relationships. It has been found that all cultures have an incest taboo to some degree, but the taboo shifts between cultures when the marriage extends beyond the nuclear family unit.\nThere are similar foundational differences where the act of procreation is concerned. Although anthropologists have found that biology is acknowledged in every cultural relationship to procreation, there are differences in the ways in which cultures assess the constructs of parenthood. For example, in the Nuyoo municipality of Oaxaca, Mexico, it is believed that a child can have partible maternity and partible paternity. In this case, a child would have multiple biological mothers in the case that it is born of one woman and then breastfed by another. A child would have multiple biological fathers in the case that the mother had sex with multiple men, following the commonplace belief in Nuyoo culture that pregnancy must be preceded by sex with multiple men in order have the necessary accumulation of semen.\nLate twentieth-century shifts in interest.\nIn the twenty-first century, Western ideas of kinship have evolved beyond the traditional assumptions of the nuclear family, raising anthropological questions of consanguinity, lineage, and normative marital expectation. The shift can be traced back to the 1960s, with the reassessment of kinship's basic principles offered by Edmund Leach, Rodney Neeham, David Schneider, and others. Instead of relying on narrow ideas of Western normalcy, kinship studies increasingly catered to \"more ethnographic voices, human agency, intersecting power structures, and historical contex\". The study of kinship evolved to accommodate for the fact that it cannot be separated from its institutional roots and must pay respect to the society in which it lives, including that society's contradictions, hierarchies, and individual experiences of those within it. This shift was progressed further by the emergence of second-wave feminism in the early 1970s, which introduced ideas of marital oppression, sexual autonomy, and domestic subordination. Other themes that emerged during this time included the frequent comparisons between Eastern and Western kinship systems and the increasing amount of attention paid to anthropologists' own societies, a swift turn from the focus that had traditionally been paid to largely \"foreign\", non-Western communities.\nKinship studies began to gain mainstream recognition in the late 1990s with the surging popularity of feminist anthropology, particularly with its work related to biological anthropology and the intersectional critique of gender relations. At this time, there was the arrival of \"Third World feminism\", a movement that argued kinship studies could not examine the gender relations of developing countries in isolation, and must pay respect to racial and economic nuance as well. This critique became relevant, for instance, in the anthropological study of Jamaica: race and class were seen as the primary obstacles to Jamaican liberation from economic imperialism, and gender as an identity was largely ignored. Third World feminism aimed to combat this in the early twenty-first century by promoting these categories as coexisting factors. In Jamaica, marriage as an institution is often substituted for a series of partners, as poor women cannot rely on regular financial contributions in a climate of economic instability. In addition, there is a common practice of Jamaican women artificially lightening their skin tones in order to secure economic survival. These anthropological findings, according to Third World feminism, cannot see gender, racial, or class differences as separate entities, and instead must acknowledge that they interact together to produce unique individual experiences.\nRise of reproductive anthropology.\nKinship studies have also experienced a rise in the interest of reproductive anthropology with the advancement of assisted reproductive technologies (ARTs), including in vitro fertilization (IVF). These advancements have led to new dimensions of anthropological research, as they challenge the Western standard of biogenetically based kinship, relatedness, and parenthood. According to anthropologists Maria C. Inhorn and Daphna Birenbaum-Carmeli, \"ARTs have pluralized notions of relatedness and led to a more dynamic notion of \"kinning\" namely, kinship as a process, as something under construction, rather than a natural given\". With this technology, questions of kinship have emerged over the difference between biological and genetic relatedness, as gestational surrogates can provide a biological environment for the embryo while the genetic ties remain with a third party. If genetic, surrogate, and adoptive maternities are involved, anthropologists have acknowledged that there can be the possibility for three \"biological\" mothers to a single child. With ARTs, there are also anthropological questions concerning the intersections between wealth and fertility: ARTs are generally only available to those in the highest income bracket, meaning the infertile poor are inherently devalued in the system. There have also been issues of reproductive tourism and bodily commodification, as individuals seek economic security through hormonal stimulation and egg harvesting, which are potentially harmful procedures. With IVF, specifically, there have been many questions of embryotic value and the status of life, particularly as it relates to the manufacturing of stem cells, testing, and research.\nCurrent issues in kinship studies, such as adoption, have revealed and challenged the Western cultural disposition towards the genetic, \"blood\" tie. Western biases against single parent homes have also been explored through similar anthropological research, uncovering that a household with a single parent experiences \"greater levels of scrutiny and [is] routinely seen as the 'other' of the nuclear, patriarchal family\". The power dynamics in reproduction, when explored through a comparative analysis of \"conventional\" and \"unconventional\" families, have been used to dissect the Western assumptions of child bearing and child rearing in contemporary kinship studies.\nCritiques of kinship studies.\nKinship, as an anthropological field of inquiry, has been heavily criticized across the discipline. One critique is that, as its inception, the framework of kinship studies was far too structured and formulaic, relying on dense language and stringent rules. Another critique, explored at length by American anthropologist David Schneider, argues that kinship has been limited by its inherent Western ethnocentrism. Schneider proposes that kinship is not a field that can be applied cross-culturally, as the theory itself relies on European assumptions of normalcy. He states in the widely circulated 1984 book \"A critique of the study of kinship\" that \"[K]inship has been defined by European social scientists, and European social scientists use their own folk culture as the source of many, if not all of their ways of formulating and understanding the world about them\". However, this critique has been challenged by the argument that it is linguistics, not cultural divergence, that has allowed for a European bias, and that the bias can be lifted by centering the methodology on fundamental human concepts. Polish anthropologist Anna Wierzbicka argues that \"mother\" and \"father\" are examples of such fundamental human concepts, and can only be Westernized when conflated with English concepts such as \"parent\" and \"sibling\".\nA more recent critique of kinship studies is its solipsistic focus on privileged, Western human relations and its promotion of normative ideals of human exceptionalism. In \"Critical Kinship Studies\", social psychologists Elizabeth Peel and Damien Riggs argue for a move beyond this human-centered framework, opting instead to explore kinship through a \"posthumanist\" vantage point where anthropologists focus on the intersecting relationships of human animals, non-human animals, technologies and practices.\nInstitutional anthropology.\nThe role of anthropology in institutions has expanded significantly since the end of the 20th century. Much of this development can be attributed to the rise in anthropologists working outside of academia and the increasing importance of globalization in both institutions and the field of anthropology. Anthropologists can be employed by institutions such as for-profit business, nonprofit organizations, and governments. For instance, cultural anthropologists are commonly employed by the United States federal government.\nThe two types of institutions defined in the field of anthropology are total institutions and social institutions. Total institutions are places that comprehensively coordinate the actions of people within them, and examples of total institutions include prisons, convents, and hospitals. Social institutions, on the other hand, are constructs that regulate individuals' day-to-day lives, such as kinship, religion, and economics. Anthropology of institutions may analyze labor unions, businesses ranging from small enterprises to corporations, government, medical organizations, education, prisons, and financial institutions. Nongovernmental organizations have garnered particular interest in the field of institutional anthropology because they are capable of fulfilling roles previously ignored by governments, or previously realized by families or local groups, in an attempt to mitigate social problems.\nThe types and methods of scholarship performed in the anthropology of institutions can take a number of forms. Institutional anthropologists may study the relationship between organizations or between an organization and other parts of society. Institutional anthropology may also focus on the inner workings of an institution, such as the relationships, hierarchies and cultures formed, and the ways that these elements are transmitted and maintained, transformed, or abandoned over time. Additionally, some anthropology of institutions examines the specific design of institutions and their corresponding strength. More specifically, anthropologists may analyze specific events within an institution, perform semiotic investigations, or analyze the mechanisms by which knowledge and culture are organized and dispersed.\nIn all manifestations of institutional anthropology, participant observation is critical to understanding the intricacies of the way an institution works and the consequences of actions taken by individuals within it. Simultaneously, anthropology of institutions extends beyond examination of the commonplace involvement of individuals in institutions to discover how and why the organizational principles evolved in the manner that they did.\nCommon considerations taken by anthropologists in studying institutions include the physical location at which a researcher places themselves, as important interactions often take place in private, and the fact that the members of an institution are often being examined in their workplace and may not have much idle time to discuss the details of their everyday endeavors. The ability of individuals to present the workings of an institution in a particular light or frame must additionally be taken into account when using interviews and document analysis to understand an institution, as the involvement of an anthropologist may be met with distrust when information being released to the public is not directly controlled by the institution and could potentially be damaging.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5390", "revid": "28598214", "url": "https://en.wikipedia.org/wiki?curid=5390", "title": "Conversion of units", "text": "Comparison of various scales\nConversion of units is the conversion between different units of measurement for the same quantity, typically through multiplicative conversion factors which change the measured quantity value without changing its effects. Unit conversion is often easier within the metric or the SI than in others, due to the regular 10-base in all units and the prefixes that increase or decrease by 3 powers of 10 at a time. \nOverview.\nThe process of conversion depends on the specific situation and the intended purpose. This may be governed by regulation, contract, technical specifications or other published standards. Engineering judgment may include such factors as:\nSome conversions from one system of units to another need to be exact, without increasing or decreasing the precision of the first measurement. This is sometimes called \"soft conversion\". It does not involve changing the physical configuration of the item being measured.\nBy contrast, a \"hard conversion\" or an \"adaptive conversion\" may not be exactly equivalent. It changes the measurement to convenient and workable numbers and units in the new system. It sometimes involves a slightly different configuration, or size substitution, of the item. Nominal values are sometimes allowed and used.\nFactor-label method.\nThe factor-label method, also known as the unit-factor method or the unity bracket method, is a widely used technique for unit conversions using the rules of algebra.\nThe factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to metres per second by using a sequence of conversion factors as shown below:\nformula_1\nEach conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as \"mile\" is the numerator in the original fraction and formula_2, \"mile\" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields formula_3, which when simplified results in the dimensionless formula_4. Because of the identity property of multiplication, multiplying any quantity (physical or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units \"mile\" and \"hour\", 10 miles per hour converts to 4.4704 metres per second.\nAs a more complex example, the concentration of nitrogen oxides (NO\"x\") in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (g/h) of NO\"x\" by using the following information as shown below:\n The flue gas exits the furnace at 0\u00a0\u00b0C temperature and 101.325\u00a0kPa absolute pressure.\n The molar volume of a gas at 0\u00a0\u00b0C temperature and 101.325\u00a0kPa is 22.414\u00a0m3/kmol.\nformula_5\nAfter canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NO\"x\" concentration of 10\u00a0ppmv converts to mass flow rate of 24.63\u00a0grams per hour.\nChecking equations that involve dimensions.\nThe factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.\nFor example, check the universal gas law equation of \"PV\" = \"nRT\", when:\nformula_6\nAs can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions \u2013 dimensional adjusters \u2013 that can then be assigned physical significance. It is important to point out that such 'mathematical manipulation' is neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck constant, a fundamental physical constant, was 'discovered' as a purely mathematical abstraction or representation that built on the Rayleigh\u2013Jeans law for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment \u2013 not earlier.\nLimitations.\nThe factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (formula_7, rather than a linear transform formula_8) between them.\nFor example, the freezing point of water is 0\u00a0\u00b0C and 32\u00a0\u00b0F, and a 5\u00a0\u00b0C change is the same as a 9\u00a0\u00b0F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32\u00a0\u00b0F (the offset from the point of reference), divides by 9\u00a0\u00b0F and multiplies by 5\u00a0\u00b0C (scales by the ratio of units), and adds 0\u00a0\u00b0C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100\u00a0\u00b0C and 212\u00a0\u00b0F, though this would yield the same formula at the end.\nHence, to convert the numerical quantity value of a temperature \"T\"[F] in degrees Fahrenheit to a numerical quantity value \"T\"[C] in degrees Celsius, this formula may be used:\n\"T\"[C] = (\"T\"[F] \u2212 32) \u00d7 5/9.\nTo convert \"T\"[C] in degrees Celsius to \"T\"[F] in degrees Fahrenheit, this formula may be used:\n\"T\"[F] = (\"T\"[C] \u00d7 9/5) + 32.\nExample.\nStarting with:\nformula_9\nreplace the original unit formula_10 with its meaning in terms of the desired unit formula_11, e.g. if formula_12, then:\nformula_13\nNow formula_14 and formula_15 are both numerical values, so just calculate their product.\nOr, which is just mathematically the same thing, multiply \"Z\" by unity, the product is still \"Z\":\nformula_16\nFor example, you have an expression for a physical value \"Z\" involving the unit \"feet per second\" (formula_10) and you want it in terms of the unit \"miles per hour\" (formula_11):\nformula_19\nOr as an example using the metric system, you have a value of fuel economy in the unit \"litres per 100 kilometres\" and you want it in terms of the unit \"microlitres per metre\":\nformula_20\nCalculation involving non-SI Units.\nIn the cases where non-SI units are used, the numerical calculation of a formula can be done by first working out the pre-factor, and then plug in the numerical values of the given/known quantities.\nFor example, in the study of Bose\u2013Einstein condensate, atomic mass \"m\" is usually given in daltons, instead of kilograms, and chemical potential \"\u03bc\" is often given in the Boltzmann constant times nanokelvin. The condensate's healing length is given by:\nformula_21\nFor a 23Na condensate with chemical potential of (the Boltzmann constant times) 128\u00a0nK, the calculation of healing length (in micrometres) can be done in two steps:\nCalculate the pre-factor.\nAssume that formula_22 this gives\nformula_23\nwhich is our pre-factor.\nCalculate the numbers.\nNow, make use of the fact that formula_24. With formula_25, formula_26.\nThis method is especially useful for programming and/or making a worksheet, where input quantities are taking multiple different values; For example, with the pre-factor calculated above, it is very easy to see that the healing length of 174Yb with chemical potential 20.3\u00a0nK is formula_27.\nSoftware tools.\nThere are many conversion tools. They are found in the function libraries of applications such as spreadsheets databases, in calculators, and in macro packages and plugins for many other applications such as the mathematical, scientific and technical applications.\nThere are many standalone applications that offer the thousands of the various units with conversions. For example, the free software movement offers a command line utility GNU units for Linux and Windows.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5391", "revid": "1161009229", "url": "https://en.wikipedia.org/wiki?curid=5391", "title": "City", "text": "Large permanent human settlement\nA city is a human settlement of a notable size. It can be defined as a permanent and densely settled place with administratively defined boundaries whose members work primarily on non-agricultural tasks. Cities generally have extensive systems for housing, transportation, sanitation, utilities, land use, production of goods, and communication. Their density facilitates interaction between people, government organizations, and businesses, sometimes benefiting different parties in the process, such as improving the efficiency of goods and service distribution.\nHistorically, city dwellers have been a small proportion of humanity overall, but following two centuries of unprecedented and rapid urbanization, more than half of the world population now lives in cities, which has had profound consequences for global sustainability. Present-day cities usually form the core of larger metropolitan areas and urban areas\u2014creating numerous commuters traveling toward city centres for employment, entertainment, and education. However, in a world of intensifying globalization, all cities are to varying degrees also connected globally beyond these regions. This increased influence means that cities also have significant influences on global issues, such as sustainable development, climate change, and global health. Because of these major influences on global issues, the international community has prioritized investment in sustainable cities through Sustainable Development Goal 11. Due to the efficiency of transportation and the smaller land consumption, dense cities hold the potential to have a smaller ecological footprint per inhabitant than more sparsely populated areas. Therefore, compact cities are often referred to as a crucial element in fighting climate change. However, this concentration can also have significant negative consequences, such as forming urban heat islands, concentrating pollution, and stressing water supplies and other resources.\nOther important traits of cities besides population include the capital status and relative continued occupation of the city. For example, country capitals such as Beijing, Jakarta, Kuala Lumpur, London, Manila, Mexico City, Moscow, Nairobi, New Delhi, Paris, Rome, Athens, Seoul, Singapore, Tokyo, and Washington, D.C. reflect the identity and apex of their respective nations. Some historic capitals, such as Kyoto, Yogyakarta, and Xi'an, maintain their reflection of cultural identity even without modern capital status. Religious holy sites offer another example of capital status within a religion, Jerusalem, Mecca, Varanasi, Ayodhya, Haridwar, and Prayagraj each holds significance.\nMeaning.\nA city can be distinguished from other human settlements by its relatively great size, but also by its functions and its special symbolic status, which may be conferred by a central authority. The term can also refer either to the physical streets and buildings of the city or to the collection of people who dwell there and can be used in a general sense to mean urban rather than rural territory.\nNational censuses use a variety of definitions \u2013 invoking factors such as population, population density, number of dwellings, economic function, and infrastructure \u2013 to classify populations as urban. Typical working definitions for small-city populations start at around 100,000 people. Common population definitions for an urban area (city or town) range between 1,500 and 50,000 people, with most U.S. states using a minimum between 1,500 and 5,000 inhabitants. Some jurisdictions set no such minima. In the United Kingdom, city status is awarded by the Crown and then remains permanent. (Historically, the qualifying factor was the presence of a cathedral, resulting in some very small cities such as Wells, with a population of 12,000 as of 2018[ [update]], and St Davids, with a population of 1,841 as of 2011[ [update]].) According to the \"functional definition\", a city is not distinguished by size alone, but also by the role it plays within a larger political context. Cities serve as administrative, commercial, religious, and cultural hubs for their larger surrounding areas.\nThe presence of a\u00a0literate elite\u00a0is often associated with cities because of the cultural diversities present in a city. A typical city has professional administrators, regulations, and some form of taxation (food and other necessities or means to trade for them) to support the government workers. (This arrangement contrasts with the more typically horizontal relationships in a tribe or village accomplishing common goals through informal agreements between neighbors, or the leadership of a chief.) The governments may be based on heredity, religion, military power, work systems such as canal-building, food distribution, land-ownership, agriculture, commerce, manufacturing, finance, or a combination of these. Societies that live in cities are often called civilizations.\nThe \"degree of urbanization\" is a modern metric to help define what comprises a city: \"a population of at least 50,000 inhabitants in contiguous dense grid cells (&gt;1,500 inhabitants per square kilometer)\". This metric was \"devised over years by the European Commission, OECD, World Bank and others, and endorsed in March [2021] by the United Nations ... largely for the purpose of international statistical comparison\".\nEtymology.\nThe word \"city\" and the related \"civilization\" come from the Latin root \"civitas\", originally meaning 'citizenship' or 'community member' and eventually coming to correspond with \"urbs\", meaning 'city' in a more physical sense. The Roman \"civitas\" was closely linked with the Greek \"polis\"\u2014another common root appearing in English words such as \"metropolis\".\nIn toponymic terminology, names of individual cities and towns are called \"astionyms\" (from Ancient Greek \u1f04\u03c3\u03c4\u03c5 'city or town' and \u1f44\u03bd\u03bf\u03bc\u03b1 'name').\nGeography.\nUrban geography deals both with cities in their larger context and with their internal structure. Cities are estimated to cover about 3% of the land surface of the Earth.\nSite.\nTown siting has varied through history according to natural, technological, economic, and military contexts. Access to water has long been a major factor in city placement and growth, and despite exceptions enabled by the advent of rail transport in the nineteenth century, through the present most of the world's urban population lives near the coast or on a river.\nUrban areas as a rule cannot produce their own food and therefore must develop some relationship with a hinterland that sustains them. Only in special cases such as mining towns which play a vital role in long-distance trade, are cities disconnected from the countryside which feeds them. Thus, centrality within a productive region influences siting, as economic forces would, in theory, favor the creation of marketplaces in optimal mutually reachable locations.\nCenter.\nThe vast majority of cities have a central area containing buildings with special economic, political, and religious significance. Archaeologists refer to this area by the Greek term temenos or if fortified as a citadel. These spaces historically reflect and amplify the city's centrality and importance to its wider sphere of influence. Today cities have a city center or downtown, sometimes coincident with a central business district.\nPublic space.\nCities typically have public spaces where anyone can go. These include privately owned spaces open to the public as well as forms of public land such as public domain and the commons. Western philosophy since the time of the Greek agora has considered physical public space as the substrate of the symbolic public sphere. Public art adorns (or disfigures) public spaces. Parks and other natural sites within cities provide residents with relief from the hardness and regularity of typical built environments. Urban green spaces are another component of public space that provides the benefit of mitigating the urban heat island effect, especially in cities that are in warmer climates. These spaces prevent carbon imbalances, extreme habitat losses, electricity and water consumption, and human health risks.\nInternal structure.\nThe urban structure generally follows one or more basic patterns: geomorphic, radial, concentric, rectilinear, and curvilinear. The physical environment generally constrains the form in which a city is built. If located on a mountainside, urban structures may rely on terraces and winding roads. It may be adapted to its means of subsistence (e.g. agriculture or fishing). And it may be set up for optimal defense given the surrounding landscape. Beyond these \"geomorphic\" features, cities can develop internal patterns, due to natural growth or to city planning.\nIn a radial structure, main roads converge on a central point. This form could evolve from successive growth over a long time, with concentric traces of town walls and citadels marking older city boundaries. In more recent history, such forms were supplemented by ring roads moving traffic around the outskirts of a town. Dutch cities such as Amsterdam and Haarlem are structured as a central square surrounded by concentric canals marking every expansion. In cities such as Moscow, this pattern is still clearly visible.\nA system of rectilinear city streets and land plots, known as the grid plan, has been used for millennia in Asia, Europe, and the Americas. The Indus Valley civilization built Mohenjo-Daro, Harappa, and other cities on a grid pattern, using ancient principles described by Kautilya, and aligned with the compass points. The ancient Greek city of Priene exemplifies a grid plan with specialized districts used across the Hellenistic Mediterranean.\nUrban areas.\nThe urban-type settlement extends far beyond the traditional boundaries of the city proper in a form of development sometimes described critically as urban sprawl. Decentralization and dispersal of city functions (commercial, industrial, residential, cultural, political) has transformed the very meaning of the term and has challenged geographers seeking to classify territories according to an urban-rural binary.\nMetropolitan areas include suburbs and exurbs organized around the needs of commuters, and sometimes edge cities characterized by a degree of economic and political independence. (In the US these are grouped into metropolitan statistical areas for purposes of demography and marketing.) Some cities are now part of a continuous urban landscape called urban agglomeration, conurbation, or megalopolis (exemplified by the BosWash corridor of the Northeastern United States.)\nHistory.\nThe cities of Jericho, Aleppo, Faiyum, Yerevan, Athens, Matera, Damascus, and Argos are among those laying claim to the longest continual inhabitation.\nCities, characterized by population density, symbolic function, and urban planning, have existed for thousands of years. In the conventional view, civilization and the city were both followed by the development of agriculture, which enabled the production of surplus food and thus a social division of labor (with concomitant social stratification) and trade. Early cities often featured granaries, sometimes within a temple. A minority viewpoint considers that cities may have arisen without agriculture, due to alternative means of subsistence (fishing), to use as communal seasonal shelters, to their value as bases for defensive and offensive military organization, or to their inherent economic function. Cities played a crucial role in the establishment of political power over an area, and ancient leaders such as Alexander the Great founded and created them with zeal.\nAncient times.\nJericho and \u00c7atalh\u00f6y\u00fck, dated to the eighth millennium BC, are among the earliest proto-cities known to archaeologists. However, the Mesopotamian city of Uruk from the mid-fourth millennium BC (ancient Iraq) is considered by most archaeologists to be the first true city, innovating many characteristics for cities to follow, with its name attributed to the Uruk period.\nIn the fourth and third millennium BC, complex civilizations flourished in the river valleys of Mesopotamia, India, China, and Egypt. Excavations in these areas have found the ruins of cities geared variously towards trade, politics, or religion. Some had large, dense populations, but others carried out urban activities in the realms of politics or religion without having large associated populations.\nAmong the early Old World cities, Mohenjo-Daro of the Indus Valley civilization in present-day Pakistan, existing from about 2600 BC, was one of the largest, with a population of 50,000 or more and a sophisticated sanitation system. China's planned cities were constructed according to sacred principles to act as celestial microcosms.\nThe Ancient Egyptian cities known physically by archaeologists are not extensive. They include (known by their Arab names) El Lahun, a workers' town associated with the pyramid of Senusret II, and the religious city Amarna built by Akhenaten and abandoned. These sites appear planned in a highly regimented and stratified fashion, with a minimalistic grid of rooms for the workers and increasingly more elaborate housing available for higher classes.\nIn Mesopotamia, the civilization of Sumer, followed by Assyria and Babylon, gave rise to numerous cities, governed by kings and fostered multiple languages written in cuneiform. The Phoenician trading empire, flourishing around the turn of the first millennium BC, encompassed numerous cities extending from Tyre, Cydon, and Byblos to Carthage and C\u00e1diz.\nIn the following centuries, independent city-states of Greece, especially Athens, developed the \"polis\", an association of male landowning citizens who collectively constituted the city. The agora, meaning \"gathering place\" or \"assembly\", was the center of the athletic, artistic, spiritual, and political life of the polis. Rome was the first city that surpassed one million inhabitants. Under the authority of its empire, Rome transformed and founded many cities (\"Colonia\"), and with them brought its principles of urban architecture, design, and society.\nIn the ancient Americas, early urban traditions developed in the Andes and Mesoamerica. In the Andes, the first urban centers developed in the Norte Chico civilization, Chavin and Moche cultures, followed by major cities in the Huari, Chimu, and Inca cultures. The Norte Chico civilization included as many as 30 major population centers in what is now the Norte Chico region of north-central coastal Peru. It is the oldest known civilization in the Americas, flourishing between the 30th and 18th centuries BC. Mesoamerica saw the rise of early urbanism in several cultural regions, beginning with the Olmec and spreading to the Preclassic Maya, the Zapotec of Oaxaca, and Teotihuacan in central Mexico. Later cultures such as the Aztec, Andean civilizations, Mayan, Mississippians, and Pueblo peoples drew on these earlier urban traditions. Many of their ancient cities continue to be inhabited, including major metropolitan cities such as Mexico City, in the same location as Tenochtitlan; while ancient continuously inhabited Pueblos are near modern urban areas in New Mexico, such as Acoma Pueblo near the Albuquerque metropolitan area and Taos Pueblo near Taos; while others like Lima are located nearby ancient Peruvian sites such as Pachacamac.\nJenn\u00e9-Jeno, located in present-day Mali and dating to the third century BC, lacked monumental architecture and a distinctive elite social class\u2014but nevertheless had specialized production and relations with a hinterland. Pre-Arabic trade contacts probably existed between Jenn\u00e9-Jeno and North Africa. Other early urban centers in sub-Saharan Africa, dated to around 500 AD, include Awdaghust, Kumbi-Saleh the ancient capital of Ghana, and Maranda a center located on a trade route between Egypt and Gao.\nMiddle Ages.\nIn the remnants of the Roman Empire, cities of late antiquity gained independence but soon lost population and importance. The locus of power in the West shifted to Constantinople and to the ascendant Islamic civilization with its major cities Baghdad, Cairo, and C\u00f3rdoba. From the 9th through the end of the 12th century, Constantinople, the capital of the Eastern Roman Empire, was the largest and wealthiest city in Europe, with a population approaching 1 million. The Ottoman Empire gradually gained control over many cities in the Mediterranean area, including Constantinople in 1453.\nIn the Holy Roman Empire, beginning in the 12th century, free imperial cities such as Nuremberg, Strasbourg, Frankfurt, Basel, Zurich, and Nijmegen became a privileged elite among towns having won self-governance from their local lord or having been granted self-governance by the emperor and being placed under his immediate protection. By 1480, these cities, as far as still part of the empire, became part of the Imperial Estates governing the empire with the emperor through the Imperial Diet.\nBy the 13th and 14th centuries, some cities become powerful states, taking surrounding areas under their control or establishing extensive maritime empires. In Italy, medieval communes developed into city-states including the Republic of Venice and the Republic of Genoa. In Northern Europe, cities including L\u00fcbeck and Bruges formed the Hanseatic League for collective defense and commerce. Their power was later challenged and eclipsed by the Dutch commercial cities of Ghent, Ypres, and Amsterdam. Similar phenomena existed elsewhere, as in the case of Sakai, which enjoyed considerable autonomy in late medieval Japan.\nIn the first millennium AD, the Khmer capital of Angkor in Cambodia grew into the most extensive preindustrial settlement in the world by area, covering over 1,000\u00a0km2 and possibly supporting up to one million people.\nEarly modern.\nIn the West, nation-states became the dominant unit of political organization following the Peace of Westphalia in the seventeenth century. Western Europe's larger capitals (London and Paris) benefited from the growth of commerce following the emergence of an Atlantic trade. However, most towns remained small.\nDuring the Spanish colonization of the Americas, the old Roman city concept was extensively used. Cities were founded in the middle of the newly conquered territories and were bound to several laws regarding administration, finances, and urbanism.\nIndustrial age.\nThe growth of the modern industry from the late 18th century onward led to massive urbanization and the rise of new great cities, first in Europe and then in other regions, as new opportunities brought huge numbers of migrants from rural communities into urban areas. England led the way as London became the capital of a world empire and cities across the country grew in locations strategic for manufacturing. In the United States from 1860 to 1910, the introduction of railroads reduced transportation costs, and large manufacturing centers began to emerge, fueling migration from rural to city areas.\nSome industrialized cities were confronted with health challenges associated with overcrowding, occupational hazards of industry, contaminated water and air, poor sanitation, and communicable diseases such as typhoid and cholera. Factories and slums emerged as regular features of the urban landscape.\nPost-industrial age.\nIn the second half of the 20th century, deindustrialization (or \"economic restructuring\") in the West led to poverty, homelessness, and urban decay in formerly prosperous cities. America's \"Steel Belt\" became a \"Rust Belt\" and cities such as Detroit, Michigan, and Gary, Indiana began to shrink, contrary to the global trend of massive urban expansion. Such cities have shifted with varying success into the service economy and public-private partnerships, with concomitant gentrification, uneven revitalization efforts, and selective cultural development. Under the Great Leap Forward and subsequent five-year plans continuing today, China has undergone concomitant urbanization and industrialization and become the world's leading manufacturer.\nAmidst these economic changes, high technology and instantaneous telecommunication enable select cities to become centers of the knowledge economy. A new smart city paradigm, supported by institutions such as the RAND Corporation and IBM, is bringing computerized surveillance, data analysis, and governance to bear on cities and city dwellers. Some companies are building brand-new master-planned cities from scratch on greenfield sites.\nUrbanization.\nUrbanization is the process of migration from rural to urban areas, driven by various political, economic, and cultural factors. Until the 18th century, an equilibrium existed between the rural agricultural population and towns featuring markets and small-scale manufacturing. With the agricultural and industrial revolutions urban population began its unprecedented growth, both through migration and demographic expansion. In England, the proportion of the population living in cities jumped from 17% in 1801 to 72% in 1891. In 1900, 15% of the world's population lived in cities. The cultural appeal of cities also plays a role in attracting residents.\nUrbanization rapidly spread across Europe and the Americas and since the 1950s has taken hold in Asia and Africa as well. The Population Division of the United Nations Department of Economic and Social Affairs reported in 2014 that for the first time, more than half of the world population lives in cities.\nLatin America is the most urban continent, with four-fifths of its population living in cities, including one-fifth of the population said to live in shantytowns (favelas, poblaciones callampas, etc.). Batam, Indonesia, Mogadishu, Somalia, Xiamen, China, and Niamey, Niger, are considered among the world's fastest-growing cities, with annual growth rates of 5\u20138%. In general, the more developed countries of the \"Global North\" remain more urbanized than the less developed countries of the \"Global South\"\u2014but the difference continues to shrink because urbanization is happening faster in the latter group. Asia is home to by far the greatest absolute number of city-dwellers: over two billion and counting. The UN predicts an additional 2.5 billion city dwellers (and 300 million fewer country dwellers) worldwide by 2050, with 90% of urban population expansion occurring in Asia and Africa.\nMegacities, cities with populations in the multi-millions, have proliferated into the dozens, arising especially in Asia, Africa, and Latin America. Economic globalization fuels the growth of these cities, as new torrents of foreign capital arrange for rapid industrialization, as well as the relocation of major businesses from Europe and North America, attracting immigrants from near and far. A deep gulf divides the rich and poor in these cities, with usually contain a super-wealthy elite living in gated communities and large masses of people living in substandard housing with inadequate infrastructure and otherwise poor conditions.\nCities around the world have expanded physically as they grow in population, with increases in their surface extent, with the creation of high-rise buildings for residential and commercial use, and with development underground.\nUrbanization can create rapid demand for water resources management, as formerly good sources of freshwater become overused and polluted, and the volume of sewage begins to exceed manageable levels.\nGovernment.\nThe local government of cities takes different forms including prominently the municipality (especially in England, in the United States, India, and other British colonies; legally, the municipal corporation; \"municipio\" in Spain and Portugal, and, along with \"municipalidad\", in most former parts of the Spanish and Portuguese empires) and the \"commune\" (in France and Chile; or \"comune\" in Italy).\nThe chief official of the city has the title of mayor. Whatever their true degree of political authority, the mayor typically acts as the figurehead or personification of their city.\nLegal conflicts and issues arise more frequently in cities than elsewhere due to the bare fact of their greater density. Modern city governments thoroughly regulate everyday life in many dimensions, including public and personal health, transport, burial, resource use and extraction, recreation, and the nature and use of buildings. Technologies, techniques, and laws governing these areas\u2014developed in cities\u2014have become ubiquitous in many areas.\nMunicipal officials may be appointed from a higher level of government or elected locally.\nIn the USA cities and towns are utilized as an organizational form to interact with the state and national government for the building and maintenance of infrastructure and utilities.\nlaws are only made be the international, national, and state governments. \nMunicipal services.\nCities typically provide municipal services such as education, through school systems; policing, through police departments; and firefighting, through fire departments; as well as the city's basic infrastructure. These are provided more or less routinely, in a more or less equal fashion. Responsibility for administration usually falls on the city government, but some services may be operated by a higher level of government, while others may be privately run. Armies may assume responsibility for policing cities in states of domestic turmoil such as America's King assassination riots of 1968.\nFinance.\nThe traditional basis for municipal finance is local property tax levied on real estate within the city. Local government can also collect revenue for services, or by leasing land that it owns. However, financing municipal services, as well as urban renewal and other development projects, is a perennial problem, which cities address through appeals to higher governments, arrangements with the private sector, and techniques such as privatization (selling services into the private sector), corporatization (formation of quasi-private municipally-owned corporations), and financialization (packaging city assets into tradeable financial public contracts and other related rights). This situation has become acute in deindustrialized cities and in cases where businesses and wealthier citizens have moved outside of city limits and therefore beyond the reach of taxation. Cities in search of ready cash increasingly resort to the municipal bond, essentially a loan with interest and a repayment date. City governments have also begun to use tax increment financing, in which a development project is financed by loans based on future tax revenues which it is expected to yield. Under these circumstances, creditors and consequently city governments place a high importance on city credit ratings.\nGovernance.\nGovernance includes government but refers to a wider domain of social control functions implemented by many actors including non-governmental organizations. The impact of globalization and the role of multinational corporations in local governments worldwide, has led to a shift in perspective on urban governance, away from the \"urban regime theory\" in which a coalition of local interests functionally govern, toward a theory of outside economic control, widely associated in academics with the philosophy of neoliberalism. In the neoliberal model of governance, public utilities are privatized, the industry is deregulated, and corporations gain the status of governing actors\u2014as indicated by the power they wield in public-private partnerships and over business improvement districts, and in the expectation of self-regulation through corporate social responsibility. The biggest investors and real estate developers act as the city's de facto urban planners.\nThe related concept of good governance places more emphasis on the state, with the purpose of assessing urban governments for their suitability for development assistance. The concepts of governance and good governance are especially invoked in emergent megacities, where international organizations consider existing governments inadequate for their large populations.\nUrban planning.\nUrban planning, the application of forethought to city design, involves optimizing land use, transportation, utilities, and other basic systems, in order to achieve certain objectives. Urban planners and scholars have proposed overlapping theories as ideals for how plans should be formed. Planning tools, beyond the original design of the city itself, include public capital investment in infrastructure and land-use controls such as zoning. The continuous process of comprehensive planning involves identifying general objectives as well as collecting data to evaluate progress and inform future decisions.\nGovernment is legally the final authority on planning but in practice, the process involves both public and private elements. The legal principle of eminent domain is used by the government to divest citizens of their property in cases where its use is required for a project. Planning often involves tradeoffs\u2014decisions in which some stand to gain and some to lose\u2014and thus is closely connected to the prevailing political situation.\nThe history of urban planning dates to some of the earliest known cities, especially in the Indus Valley and Mesoamerican civilizations, which built their cities on grids and apparently zoned different areas for different purposes. The effects of planning, ubiquitous in today's world, can be seen most clearly in the layout of planned communities, fully designed prior to construction, often with consideration for interlocking physical, economic, and cultural systems.\nSociety.\nSocial structure.\nUrban society is typically stratified. Spatially, cities are formally or informally segregated along ethnic, economic, and racial lines. People living relatively close together may live, work, and play in separate areas, and associate with different people, forming ethnic or lifestyle enclaves or, in areas of concentrated poverty, ghettoes. While in the US and elsewhere poverty became associated with the inner city, in France it has become associated with the \"banlieues\", areas of urban development that surround the city proper. Meanwhile, across Europe and North America, the racially white majority is empirically the most segregated group. Suburbs in the West, and, increasingly, gated communities and other forms of \"privatopia\" around the world, allow local elites to self-segregate into secure and exclusive neighborhoods.\nLandless urban workers, contrasted with peasants and known as the proletariat, form a growing stratum of society in the age of urbanization. In Marxist doctrine, the proletariat will inevitably revolt against the bourgeoisie as their ranks swell with disenfranchised and disaffected people lacking all stake in the status quo. The global urban proletariat of today, however, generally lacks the status of factory workers which in the nineteenth century provided access to the means of production.\nEconomics.\nHistorically, cities rely on rural areas for intensive farming to yield surplus crops, in exchange for which they provide money, political administration, manufactured goods, and culture. Urban economics tends to analyze larger agglomerations, stretching beyond city limits, in order to reach a more complete understanding of the local labor market.\nAs hubs of trade cities have long been home to retail commerce and consumption through the interface of shopping. In the 20th century, department stores using new techniques of advertising, public relations, decoration, and design, transformed urban shopping areas into fantasy worlds encouraging self-expression and escape through consumerism.\nIn general, the density of cities expedites commerce and facilitates knowledge spillovers, helping people and firms exchange information and generate new ideas. A thicker labor market allows for better skill matching between firms and individuals. Population density enables also sharing of common infrastructure and production facilities; however, in very dense cities, increased crowding and waiting times may lead to some negative effects.\nAlthough manufacturing fueled the growth of cities, many now rely on a tertiary or service economy. The services in question range from tourism, hospitality, entertainment, housekeeping, and prostitution to grey-collar work in law, finance, and administration.\nAccording to a scientific model of cities by Professor Geoffrey West, with the doubling of a city's size, salaries per capita will generally increase by 15%.\nCulture and communications.\nCities are typically hubs for education and the arts, supporting universities, museums, temples, and other cultural institutions. They feature impressive displays of architecture ranging from small to enormous and ornate to brutal; skyscrapers, providing thousands of offices or homes within a small footprint, and visible from miles away, have become iconic urban features. Cultural elites tend to live in cities, bound together by shared cultural capital, and themselves play some role in governance. By virtue of their status as centers of culture and literacy, cities can be described as the locus of civilization, human history, and social change.\nDensity makes for effective mass communication and transmission of news, through heralds, printed proclamations, newspapers, and digital media. These communication networks, though still using cities as hubs, penetrate extensively into all populated areas. In the age of rapid communication and transportation, commentators have described urban culture as nearly ubiquitous or as no longer meaningful.\nToday, a city's promotion of its cultural activities dovetails with place branding and city marketing, public diplomacy techniques used to inform development strategy; attract businesses, investors, residents, and tourists; and to create shared identity and sense of place within the metropolitan area. Physical inscriptions, plaques, and monuments on display physically transmit a historical context for urban places. Some cities, such as Jerusalem, Mecca, and Rome have indelible religious status and for hundreds of years have attracted pilgrims. Patriotic tourists visit Agra to see the Taj Mahal, or New York City to visit the World Trade Center. Elvis lovers visit Memphis to pay their respects at Graceland. Place brands (which include place satisfaction and place loyalty) have great economic value (comparable to the value of commodity brands) because of their influence on the decision-making process of people thinking about doing business in\u2014\"purchasing\" (the brand of)\u2014a city.\nBread and circuses among other forms of cultural appeal, attract and entertain the masses. Sports also play a major role in city branding and local identity formation. Cities go to considerable lengths in competing to host the Olympic Games, which bring global attention and tourism. Paris, a city known for its cultural history, is the site of the next Olympics in the summer of 2024.\nWarfare.\nCities play a crucial strategic role in warfare due to their economic, demographic, symbolic, and political centrality. For the same reasons, they are targets in asymmetric warfare. Many cities throughout history were founded under military auspices, a great many have incorporated fortifications, and military principles continue to influence urban design. Indeed, war may have served as the social rationale and economic basis for the very earliest cities.\nPowers engaged in geopolitical conflict have established fortified settlements as part of military strategies, as in the case of garrison towns, America's Strategic Hamlet Program during the Vietnam War, and Israeli settlements in Palestine. While occupying the Philippines, the US Army ordered local people to concentrate in cities and towns, in order to isolate committed insurgents and battle freely against them in the countryside.\nDuring World War II, national governments on occasion declared certain cities open, effectively surrendering them to an advancing enemy in order to avoid damage and bloodshed. Urban warfare proved decisive, however, in the Battle of Stalingrad, where Soviet forces repulsed German occupiers, with extreme casualties and destruction. In an era of low-intensity conflict and rapid urbanization, cities have become sites of long-term conflict waged both by foreign occupiers and by local governments against insurgency. Such warfare, known as counterinsurgency, involves techniques of surveillance and psychological warfare as well as close combat, and functionally extends modern urban crime prevention, which already uses concepts such as defensible space.\nAlthough capture is the more common objective, warfare has in some cases spelled complete destruction for a city. Mesopotamian tablets and ruins attest to such destruction, as does the Latin motto \"Carthago delenda est\". Since the atomic bombings of Hiroshima and Nagasaki and throughout the Cold War, nuclear strategists continued to contemplate the use of \"counter-value\" targeting: crippling an enemy by annihilating its valuable cities, rather than aiming primarily at its military forces.\nInfrastructure.\nUrban infrastructure involves various physical networks and spaces necessary for transportation, water use, energy, recreation, and public functions. Infrastructure carries a high initial cost in fixed capital but lower marginal costs and thus positive economies of scale. Because of the higher barriers to entry, these networks have been classified as natural monopolies, meaning that economic logic favors control of each network by a single organization, public or private.\nInfrastructure in general plays a vital role in a city's capacity for economic activity and expansion, underpinning the very survival of the city's inhabitants, as well as technological, commercial, industrial, and social activities. Structurally, many infrastructure systems take the form of networks with redundant links and multiple pathways, so that the system as a whole continue to operate even if parts of it fail. The particulars of a city's infrastructure systems have historical path dependence because new development must build from what exists already.\nMegaprojects such as the construction of airports, power plants, and railways require large upfront investments and thus tend to require funding from the national government or the private sector. Privatization may also extend to all levels of infrastructure construction and maintenance.\nUrban infrastructure ideally serves all residents equally but in practice may prove uneven\u2014with, in some cities, clear first-class and second-class alternatives.\nUtilities.\nPublic utilities (literally, useful things with general availability) include basic and essential infrastructure networks, chiefly concerned with the supply of water, electricity, and telecommunications capability to the populace.\nSanitation, necessary for good health in crowded conditions, requires water supply and waste management as well as individual hygiene. Urban water systems include principally a water supply network and a network (sewerage system) for sewage and stormwater. Historically, either local governments or private companies have administered urban water supply, with a tendency toward government water supply in the 20th century and a tendency toward private operation at the turn of the twenty-first. The market for private water services is dominated by two French companies, Veolia Water (formerly Vivendi) and Engie (formerly Suez), said to hold 70% of all water contracts worldwide.\nModern urban life relies heavily on the energy transmitted through electricity for the operation of electric machines (from household appliances to industrial machines to now-ubiquitous electronic systems used in communications, business, and government) and for traffic lights, street lights, and indoor lighting. Cities rely to a lesser extent on hydrocarbon fuels such as gasoline and natural gas for transportation, heating, and cooking. Telecommunications infrastructure such as telephone lines and coaxial cables also traverse cities, forming dense networks for mass and point-to-point communications.\nTransportation.\nBecause cities rely on specialization and an economic system based on wage labor, their inhabitants must have the ability to regularly travel between home, work, commerce, and entertainment. City dwellers travel by foot or by wheel on roads and walkways, or use special rapid transit systems based on underground, overground, and elevated rail. Cities also rely on long-distance transportation (truck, rail, and airplane) for economic connections with other cities and rural areas.\nCity streets historically were the domain of horses and their riders and pedestrians, who only sometimes had sidewalks and special walking areas reserved for them. In the West, bicycles or (velocipedes), efficient human-powered machines for short- and medium-distance travel, enjoyed a period of popularity at the beginning of the twentieth century before the rise of automobiles. Soon after, they gained a more lasting foothold in Asian and African cities under European influence. In Western cities, industrializing, expanding, and electrifying public transit systems, and especially streetcars enabled urban expansion as new residential neighborhoods sprung up along transit lines and workers rode to and from work downtown.\nSince the mid-20th century, cities have relied heavily on motor vehicle transportation, with major implications for their layout, environment, and aesthetics. (This transformation occurred most dramatically in the US\u2014where corporate and governmental policies favored automobile transport systems\u2014and to a lesser extent in Europe.) The rise of personal cars accompanied the expansion of urban economic areas into much larger metropolises, subsequently creating ubiquitous traffic issues with the accompanying construction of new highways, wider streets, and alternative walkways for pedestrians. However, severe traffic jams still occur regularly in cities around the world, as private car ownership and urbanization continue to increase, overwhelming existing urban street networks.\nThe urban bus system, the world's most common form of public transport, uses a network of scheduled routes to move people through the city, alongside cars, on the roads. The economic function itself also became more decentralized as concentration became impractical and employers relocated to more car-friendly locations (including edge cities). Some cities have introduced bus rapid transit systems which include exclusive bus lanes and other methods for prioritizing bus traffic over private cars. Many big American cities still operate conventional public transit by rail, as exemplified by the ever-popular New York City Subway system. Rapid transit is widely used in Europe and has increased in Latin America and Asia.\nWalking and cycling (\"non-motorized transport\") enjoy increasing favor (more pedestrian zones and bike lanes) in American and Asian urban transportation planning, under the influence of such trends as the Healthy Cities movement, the drive for sustainable development, and the idea of a carfree city. Techniques such as road space rationing and road use charges have been introduced to limit urban car traffic.\nHousing.\nThe housing of residents presents one of the major challenges every city must face. Adequate housing entails not only physical shelters but also the physical systems necessary to sustain life and economic activity.\nHomeownership represents status and a modicum of economic security, compared to renting which may consume much of the income of low-wage urban workers. Homelessness, or lack of housing, is a challenge currently faced by millions of people in countries rich and poor. Because cities generally have higher population densities than rural areas, city dwellers are more likely to reside in apartments and less likely to live in a single-family home.\nEcology.\nUrban ecosystems, influenced as they are by the density of human buildings and activities, differ considerably from those of their rural surroundings. Anthropogenic buildings and waste, as well as cultivation in gardens, create physical and chemical environments which have no equivalents in the wilderness, in some cases enabling exceptional biodiversity. They provide homes not only for immigrant humans but also for immigrant plants, bringing about interactions between species that never previously encountered each other. They introduce frequent disturbances (construction, walking) to plant and animal habitats, creating opportunities for recolonization and thus favoring young ecosystems with r-selected species dominant. On the whole, urban ecosystems are less complex and productive than others, due to the diminished absolute amount of biological interactions.\nTypical urban fauna includes insects (especially ants), rodents (mice, rats), and birds, as well as cats and dogs (domesticated and feral). Large predators are scarce. However, in North America, large predators such as coyotes and white-tailed deer roam in urban wildlife \nCities generate considerable ecological footprints, locally and at longer distances, due to concentrated populations and technological activities. From one perspective, cities are not ecologically sustainable due to their resource needs. From another, proper management may be able to ameliorate a city's ill effects. Air pollution arises from various forms of combustion, including fireplaces, wood or coal-burning stoves, other heating systems, and internal combustion engines. Industrialized cities, and today third-world megacities, are notorious for veils of smog (industrial haze) that envelop them, posing a chronic threat to the health of their millions of inhabitants. Urban soil contains higher concentrations of heavy metals (especially lead, copper, and nickel) and has lower pH than soil in the comparable wilderness.\nModern cities are known for creating their own microclimates, due to concrete, asphalt, and other artificial surfaces, which heat up in sunlight and channel rainwater into underground ducts. The temperature in New York City exceeds nearby rural temperatures by an average of 2\u20133\u00a0\u00b0C and at times 5\u201310\u00a0\u00b0C differences have been recorded. This effect varies nonlinearly with population changes (independently of the city's physical size). Aerial particulates increase rainfall by 5\u201310%. Thus, urban areas experience unique climates, with earlier flowering and later leaf dropping than in nearby countries.\nPoor and working-class people face disproportionate exposure to environmental risks (known as environmental racism when intersecting also with racial segregation). For example, within the urban microclimate, less-vegetated poor neighborhoods bear more of the heat (but have fewer means of coping with it).\nOne of the main methods of improving the urban ecology is including in the cities more urban green spaces: parks, gardens, lawns, and trees. These areas improve the health and well-being of the human, animal, and plant populations of the cities. Well-maintained urban trees can provide many social, ecological, and physical benefits to the residents of the city.\nA study published in \"Nature's Scientific Reports\" journal in 2019 found that people who spent at least two hours per week in nature were 23 percent more likely to be satisfied with their life and were 59 percent more likely to be in good health than those who had zero exposure. The study used data from almost 20,000 people in the UK. Benefits increased for up to 300 minutes of exposure. The benefits are applied to men and women of all ages, as well as across different ethnicities, socioeconomic statuses, and even those with long-term illnesses and disabilities. People who did not get at least two hours \u2013 even if they surpassed an hour per week \u2013 did not get the benefits. The study is the latest addition to a compelling body of evidence for the health benefits of nature. Many doctors already give nature prescriptions to their patients. The study didn't count time spent in a person's own yard or garden as time in nature, but the majority of nature visits in the study took place within two miles of home. \"Even visiting local urban green spaces seems to be a good thing,\" Dr. White said in a press release. \"Two hours a week is hopefully a realistic target for many people, especially given that it can be spread over an entire week to get the benefit.\"\nWorld city system.\nAs the world becomes more closely linked through economics, politics, technology, and culture (a process called globalization), cities have come to play a leading role in transnational affairs, exceeding the limitations of international relations conducted by national governments. This phenomenon, resurgent today, can be traced back to the Silk Road, Phoenicia, and the Greek city-states, through the Hanseatic League and other alliances of cities. Today the information economy based on high-speed internet infrastructure enables instantaneous telecommunication around the world, effectively eliminating the distance between cities for the purposes of the international markets and other high-level elements of the world economy, as well as personal communications and mass media.\nGlobal city.\nA global city, also known as a world city, is a prominent centre of trade, banking, finance, innovation, and markets. Saskia Sassen used the term \"global city\" in her 1991 work, \"The Global City: New York, London, Tokyo\" to refer to a city's power, status, and cosmopolitanism, rather than to its size. Following this view of cities, it is possible to rank the world's cities hierarchically. Global cities form the capstone of the global hierarchy, exerting command and control through their economic and political influence. Global cities may have reached their status due to early transition to post-industrialism or through inertia which has enabled them to maintain their dominance from the industrial era. This type of ranking exemplifies an emerging discourse in which cities, considered variations on the same ideal type, \"must\" compete with each other globally to achieve prosperity.\nCritics of the notion point to the different realms of power and interchange. The term \"global city\" is heavily influenced by economic factors and, thus, may not account for places that are otherwise significant. Paul James, for example argues that the term is \"reductive and skewed\" in its focus on financial systems.\nMultinational corporations and banks make their headquarters in global cities and conduct much of their business within this context. American firms dominate the international markets for law and engineering and maintain branches in the biggest foreign global cities.\nLarge cities have a great divide between populations of both ends of the financial spectrum. Regulations on immigration promote the exploitation of low- and high-skilled immigrant workers from poor areas. During employment, migrant workers may be subject to unfair working conditions, including working overtime, low wages, and lack of safety in workplaces.\nTransnational activity.\nCities increasingly participate in world political activities independently of their enclosing nation-states. Early examples of this phenomenon are the sister city relationship and the promotion of multi-level governance within the European Union as a technique for European integration. Cities including Hamburg, Prague, Amsterdam, The Hague, and City of London maintain their own embassies to the European Union at Brussels.\nNew urban dwellers are increasingly transmigrants, keeping one foot each (through telecommunications if not travel) in their old and their new homes.\nGlobal governance.\nCities participate in global governance by various means including membership in global networks which transmit norms and regulations. At the general, global level, United Cities and Local Governments (UCLG) is a significant umbrella organization for cities; regionally and nationally, Eurocities, Asian Network of Major Cities 21, the Federation of Canadian Municipalities the National League of Cities, and the United States Conference of Mayors play similar roles. UCLG took responsibility for creating Agenda 21 for culture, a program for cultural policies promoting sustainable development, and has organized various conferences and reports for its furtherance.\nNetworks have become especially prevalent in the arena of environmentalism and specifically climate change following the adoption of Agenda 21. Environmental city networks include the C40 Cities Climate Leadership Group, the United Nations Global Compact Cities Programme, the Carbon Neutral Cities Alliance (CNCA), the Covenant of Mayors and the Compact of Mayors, ICLEI \u2013 Local Governments for Sustainability, and the Transition Towns network.\nCities with world political status as meeting places for advocacy groups, non-governmental organizations, lobbyists, educational institutions, intelligence agencies, military contractors, information technology firms, and other groups with a stake in world policymaking. They are consequently also sites for symbolic protest. South Africa has one of the highest rate of protests in the world. Pretoria, a city in South Africa had a rally where 5 thousand people took part in order to advocate for increasing wages to afford living costs.\nUnited Nations System.\nThe United Nations System has been involved in a series of events and declarations dealing with the development of cities during this period of rapid urbanization.\nUN-Habitat coordinates the U.N. urban agenda, working with the UN Environmental Programme, the UN Development Programme, the Office of the High Commissioner for Human Rights, the World Health Organization, and the World Bank.\nThe World Bank, a U.N. specialized agency, has been a primary force in promoting the Habitat conferences, and since the first Habitat conference has used their declarations as a framework for issuing loans for urban infrastructure. The bank's structural adjustment programs contributed to urbanization in the Third World by creating incentives to move to cities. The World Bank and UN-Habitat in 1999 jointly established the Cities Alliance (based at the World Bank headquarters in Washington, D.C.) to guide policymaking, knowledge sharing, and grant distribution around the issue of urban poverty. (UN-Habitat plays an advisory role in evaluating the quality of a locality's governance.) The Bank's policies have tended to focus on bolstering real estate markets through credit and technical assistance.\nThe United Nations Educational, Scientific and Cultural Organization, UNESCO has increasingly focused on cities as key sites for influencing cultural governance. It has developed various city networks including the International Coalition of Cities against Racism and the Creative Cities Network. UNESCO's capacity to select World Heritage Sites gives the organization significant influence over cultural capital, tourism, and historic preservation funding.\nRepresentation in culture.\nCities figure prominently in traditional Western culture, appearing in the Bible in both evil and holy forms, symbolized by Babylon and Jerusalem. Cain and Nimrod are the first city builders in the Book of Genesis. In Sumerian mythology Gilgamesh built the walls of Uruk.\nCities can be perceived in terms of extremes or opposites: at once liberating and oppressive, wealthy and poor, organized and chaotic. The name anti-urbanism refers to various types of ideological opposition to cities, whether because of their culture or their political relationship with the country. Such opposition may result from identification of cities with oppression and the ruling elite. This and other political ideologies strongly influence narratives and themes in discourse about cities. In turn, cities symbolize their home societies.\nWriters, painters, and filmmakers have produced innumerable works of art concerning the urban experience. Classical and medieval literature includes a genre of \"descriptiones\" which treat of city features and history. Modern authors such as Charles Dickens and James Joyce are famous for evocative descriptions of their home cities. Fritz Lang conceived the idea for his influential 1927 film \"Metropolis\" while visiting Times Square and marveling at the nighttime neon lighting. Other early cinematic representations of cities in the twentieth century generally depicted them as technologically efficient spaces with smoothly functioning systems of automobile transport. By the 1960s, however, traffic congestion began to appear in such films as \"The Fast Lady\" (1962) and \"Playtime\" (1967).\nLiterature, film, and other forms of popular culture have supplied visions of future cities both utopian and dystopian. The prospect of expanding, communicating, and increasingly interdependent world cities has given rise to images such as Nylonkong (New York, London, Hong Kong) and visions of a single world-encompassing ecumenopolis.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5392", "revid": "9092818", "url": "https://en.wikipedia.org/wiki?curid=5392", "title": "Cilantro", "text": ""}
{"id": "5393", "revid": "1135346512", "url": "https://en.wikipedia.org/wiki?curid=5393", "title": "Coriander (plant)", "text": ""}
{"id": "5394", "revid": "31873261", "url": "https://en.wikipedia.org/wiki?curid=5394", "title": "Chervil", "text": "Species of plants\nChervil (; Anthriscus cerefolium), sometimes called French parsley or garden chervil (to distinguish it from similar plants also called chervil), is a delicate annual herb related to parsley. It was formerly called myrhis due to its volatile oil with an aroma similar to the resinous substance myrrh. It is commonly used to season mild-flavoured dishes and is a constituent of the French herb mixture .\nName.\nThe name \"chervil\" is from Anglo-Norman, from Latin or , meaning \"leaves of joy\"; the Latin is formed, as from an Ancient Greek word ().\nBiology.\nA member of the Apiaceae, chervil is native to the Caucasus but was spread by the Romans through most of Europe, where it is now naturalised. It is also grown frequently in the United States, where it sometimes escapes cultivation. Such escape can be recognized, however, as garden chervil is distinguished from all other Anthriscus species growing in North America (i.e., A. caucalis and A. sylvestris) by its having lanceolate-linear bracteoles and a fruit with a relatively long beak.\nThe plants grow to , with tripinnate leaves that may be curly. The small white flowers form small umbels, across. The fruit is about 1\u00a0cm long, oblong-ovoid with a slender, ridged beak.\nUses and impact.\nCulinary arts.\nChervil is used, particularly in France, to season poultry, seafood, young spring vegetables (such as carrots), soups, and sauces. More delicate than parsley, it has a faint taste of liquorice or aniseed.\nChervil is one of the four traditional French , along with tarragon, chives, and parsley, which are essential to French cooking. Unlike the more pungent, robust herbs such as thyme and rosemary, which can take prolonged cooking, the are added at the last minute, to salads, omelettes, and soups.\nChemistry.\nEssential oil obtained via water distillation of wild Turkish Anthriscus cerefolium was analyzed by gas chromatography - mass spectrometry identifying 4 compounds: methyl chavicol (83.10%), 1-allyl-2,4-dimethoxybenzene (15.15%), undecane (1.75%) and \u03b2-pinene (&lt;0.01%).\nHorticulture.\nAccording to some, slugs are attracted to chervil and the plant is sometimes used to bait them.\nHealth.\nChervil has had various uses in folk medicine. It was claimed to be useful as a digestive aid, for lowering high blood pressure, and, infused with vinegar, for curing hiccups. Besides its digestive properties, it is used as a mild stimulant.\nChervil has also been implicated in \"strimmer dermatitis\", another name for phytophotodermatitis, due to spray from weed trimmers and similar forms of contact. Other plants in the family Apiaceae can have similar effects.\nCultivation.\nTransplanting chervil can be difficult, due to the long taproot. It prefers a cool and moist location; otherwise, it rapidly goes to seed (also known as bolting). It is usually grown as a cool-season crop, like lettuce, and should be planted in early spring and late fall or in a winter greenhouse. Regular harvesting of leaves also helps to prevent bolting. If plants bolt despite precautions, the plant can be periodically re-sown throughout the growing season, thus producing fresh plants as older plants bolt and go out of production.\nChervil grows to a height of , and a width of .\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5395", "revid": "35936988", "url": "https://en.wikipedia.org/wiki?curid=5395", "title": "Chives", "text": "Edible species of flowering plant in the family Amaryllidaceae\nChives, scientific name Allium schoenoprasum, is a species of flowering plant in the family Amaryllidaceae that produces edible leaves and flowers. Their close relatives include the common onions, garlic, shallot, leek, scallion, and Chinese onion.\nA perennial plant, it is widespread in nature across much of Europe, Asia, and North America.\n\"A. schoenoprasum\" is the only species of \"Allium\" native to both the New and the Old Worlds.\nChives are a commonly used herb and can be found in grocery stores or grown in home gardens. In culinary use, the green stalks (scapes) and the unopened, immature flower buds are diced and used as an ingredient for omelettes, fish, potatoes, soups, and many other dishes. The edible flowers can be used in salads. Chives have insect-repelling properties that can be used in gardens to control pests.\nThe plant provides a great deal of nectar for pollinators. It was rated in the top 10 for most nectar production (nectar per unit cover per year) in a UK plants survey conducted by the AgriLand project which is supported by the UK Insect Pollinators Initiative.\nDescription.\nChives are a bulb-forming herbaceous perennial plant, growing to tall. The bulbs are slender, conical, long and broad, and grow in dense clusters from the roots. The scapes (or stems) are hollow and tubular, up to long and across, with a soft texture, although, prior to the emergence of a flower, they may appear stiffer than usual. The grass-like leaves, which are shorter than the scapes, are also hollow and tubular, or terete, (round in cross-section) which distinguishes it at a glance from garlic chives (\"Allium tuberosum\"). \nThe flowers are pale purple, and star-shaped with six petals, wide, and produced in a dense inflorescence of 10-30 together; before opening, the inflorescence is surrounded by a papery bract. The seeds are produced in a small, three-valved capsule, maturing in summer. The herb flowers from April to May in the southern parts of its habitat zones and in June in the northern parts.\nChives are the only species of \"Allium\" native to both the New and the Old Worlds. Sometimes, the plants found in North America are classified as \"A. schoenoprasum\" var. \"sibiricum\", although this is disputed. Differences between specimens are significant. One example was found in northern Maine growing solitary, instead of in clumps, also exhibiting dingy grey flowers.\nAlthough chives are repulsive to insects in general, due to their sulfur compounds, their flowers attract bees, and they are at times kept to increase desired insect life.\nTaxonomy.\nIt was formally described by the Swedish botanist Carl Linnaeus in his seminal publication \"Species Plantarum\" in 1753.\nThe name of the species derives from the Greek \u03c3\u03c7\u03bf\u03af\u03bd\u03bf\u03c2, \"skho\u00ednos\" (sedge or rush) and \u03c0\u03c1\u03ac\u03c3\u03bf\u03bd, \"pr\u00e1son\" (leek). Its English name, chives, derives from the French word \"cive\", from \"cepa\", the Latin word for onion. In the Middle Ages, it was known as 'rush leek'.\nSome subspecies have been proposed, but are not accepted by Plants of the World Online, as of July\u00a02021[ [update]], which sinks them into the species:\nVarieties have also been proposed, including \"A.\u00a0schoenoprasum\" var. \"sibiricum\". The Flora of North America notes that the species is very variable, and considers recognition of varieties as \"unsound\".\nDistribution and habitat.\nChives are native to temperate areas of Europe, Asia and North America.\nRange.\nIt is found in Asia within the Caucasus (in Armenia, Azerbaijan and Georgia), also in China, Iran, Iraq, Japan (within the islands of Hokkaido and Honshu), Kazakhstan, Kyrgyzstan, Mongolia, Pakistan, Russian Federation (within the krais of Kamchatka, Khabarovsk, and Primorye) Siberia and Turkey.\nIn middle Europe, it is found within Austria, the Czech Republic, Germany, the Netherlands, Poland and Switzerland. In northern Europe, in Denmark, Finland, Norway, Sweden and the United Kingdom. In southeastern Europe, within Bulgaria, Greece, Italy and Romania. It is also found in southwestern Europe, in France, Portugal and Spain.\nIn North America, it is found in Canada (within the provinces and territories of Alberta, British Columbia, Manitoba, Northwest Territories, Nova Scotia, New Brunswick, Newfoundland, Nunavut, Ontario, Prince Edward Island, Quebec, Saskatchewan and Yukon), and the United States (within the states of Alaska, Colorado, Connecticut, Idaho, Maine, Maryland, Massachusetts, Michigan, Minnesota, Montana, New Hampshire, New Jersey, New York, Ohio, Oregon, Pennsylvania, Rhode Island, Vermont, Washington, West Virginia, Wisconsin and Wyoming).\nUses.\nCulinary arts.\nChives are grown for their scapes and leaves, which are used for culinary purposes as a flavoring herb, and provide a somewhat milder onion-like flavor than those of other \"Allium\" species.\nChives have a wide variety of culinary uses, such as in traditional dishes in France, Sweden, and elsewhere. In his 1806 book \"Attempt at a Flora\" (\"F\u00f6rs\u00f6k til en flora\"), Anders Jahan Retzius describes how chives are used with pancakes, soups, fish, and sandwiches. They are also an ingredient of the \"gr\u00e4ddfil\" sauce with the traditional herring dish served at Swedish midsummer celebrations. The flowers may also be used to garnish dishes. \nIn Poland and Germany, chives are served with quark. Chives are one of the \"fines herbes\" of French cuisine, the others being tarragon, chervil and parsley. Chives can be found fresh at most markets year-round, making them readily available; they can also be dry-frozen without much impairment to the taste, giving home growers the opportunity to store large quantities harvested from their own gardens.\nUses in plant cultivation.\nRetzius also describes how farmers would plant chives between the rocks making up the borders of their flowerbeds, to keep the plants free from pests (such as Japanese beetles). The growing plant repels unwanted insect life, and the juice of the leaves can be used for the same purpose, as well as fighting fungal infections, mildew, and scab.\nCultivation.\nChives are cultivated both for their culinary uses and for their ornamental value; the violet flowers are often used in ornamental dry bouquets. The flowers are also edible and are used in salads, or used to make blossom vinegars.\nChives thrive in well-drained soil, rich in organic matter, with a pH of 6-7 and full sun. They can be grown from seed and mature in summer, or early the following spring. Typically, chives need to be germinated at a temperature of 15 to 20\u00a0\u00b0C (60-70\u00a0\u00b0F) and kept moist. They can also be planted under a cloche or germinated indoors in cooler climates, then planted out later. After at least four weeks, the young shoots should be ready to be planted out. They are also easily propagated by division.\nIn cold regions, chives die back to the underground bulbs in winter, with the new leaves appearing in early spring. Chives starting to look old can be cut back to about 2\u20135\u00a0cm. When harvesting, the needed number of stalks should be cut to the base. During the growing season, the plant continually regrows leaves, allowing for a continuous harvest.\nChives are susceptible to damage by leek moth larvae, which bore into the leaves or bulbs of the plant.\nHistory and cultural importance.\nChives have been cultivated in Europe since the Middle Ages (from the fifth until the 15th centuries), although their usage dates back 5,000 years. They were sometimes referred to as \"rush leeks\".\nIt was mentioned in 80 A.D. by Marcus Valerius Martialis in his \"Epigrams\".\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe Romans believed chives could relieve the pain from sunburn or a sore throat. They believed eating chives could increase blood pressure and act as a diuretic.\nRomani have used chives in fortune telling. Bunches of dried chives hung around a house were believed to ward off disease and evil.\nIn the 19th century, Dutch farmers fed cattle on the herb to give a different taste to their milk.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5396", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=5396", "title": "ChrisMorris", "text": ""}
{"id": "5397", "revid": "39616039", "url": "https://en.wikipedia.org/wiki?curid=5397", "title": "Chris Morris (satirist)", "text": "English comedian\nChristopher J Morris (born 15 June 1962) is an English comedian, radio presenter, actor, and filmmaker. Known for his deadpan, dark humour, surrealism, and controversial subject matter, he has been praised by the British Film Institute for his \"uncompromising, moralistic drive\".\nIn the early 1990s, Morris teamed up with his radio producer Armando Iannucci to create \"On the Hour\", a satire of news programmes. This was expanded into a television spin off, \"The Day Today\", which launched the career of comedian Steve Coogan and has since been hailed as one of the most important satirical shows of the 1990s. Morris further developed the satirical news format with \"Brass Eye\", which lampooned celebrities whilst focusing on themes such as crime and drugs. For many, the apotheosis of Morris' career was a \"Brass Eye\" special, which dealt with the moral panic surrounding paedophilia. It quickly became one of the most complained-about programmes in British television history, leading the \"Daily Mail\" to describe him as \"the most loathed man on TV\".\nMeanwhile, Morris' postmodern sketch comedy and ambient music radio show \"Blue Jam\", which had seen controversy similar to \"Brass Eye\", helped him to gain a cult following. \"Blue Jam\" was adapted into the TV series \"Jam\", which some hailed as \"the most radical and original television programme broadcast in years\", and he went on to win the BAFTA Award for Best Short Film after expanding a \"Blue Jam\" sketch into \"My Wrongs 8245\u20138249 &amp; 117\", which starred Paddy Considine. This was followed by \"Nathan Barley\", a sitcom written in collaboration with a then little-known Charlie Brooker that satirised hipsters, which had low ratings but found success upon its DVD release. Morris followed this by joining the cast of the sitcom \"The IT Crowd\", his first project in which he did not have writing or producing input.\nIn 2010, Morris directed his first feature-length film, \"Four Lions\", which satirised Islamic terrorism through a group of inept British Muslims. Reception of the film was largely positive, earning Morris his second BAFTA Film Award, this time for Outstanding Debut. Since 2012, he has directed four episodes of Iannucci's political comedy \"Veep\" and appeared onscreen in \"The Double\" and \"Stewart Lee's Comedy Vehicle\". His second feature-length film, \"The Day Shall Come\", was released in 2019.\nEarly life.\nChristopher J Morris was born on 15 June 1962 in Colchester, Essex, the son of Rosemary Parrington and Paul Michael Morris. His father was a GP. Morris has a large red birthmark almost completely covering the left side of his face and neck, which he disguises with makeup when acting. He grew up in a Victorian farmhouse in the village of Buckden, Cambridgeshire, which he described as \"very dull\". He has two younger brothers, including theatre director Tom Morris. From an early age, he was a prankster and had a passion for radio. From the age of 10, he was educated at the independent Jesuit boarding school Stonyhurst College in Stonyhurst, Lancashire. He went to study zoology at the University of Bristol, where he gained a .\nCareer.\nRadio.\nOn graduating, Morris pursued a career as a musician in various bands, for which he played the bass guitar. He then went to work for Radio West, a local radio station in Bristol. He then took up a news traineeship with BBC Radio Cambridgeshire, where he took advantage of access to editing and recording equipment to create elaborate spoofs and parodies. He also spent time in early 1987 hosting a 2\u20134pm afternoon show and finally ended up presenting Saturday morning show \"I.T.\"\nIn July 1987, he moved on to BBC Radio Bristol to present his own show \"No Known Cure\", broadcast on Saturday and Sunday mornings. The show was surreal and satirical, with odd interviews conducted with unsuspecting members of the public. He was fired from Bristol in 1990 after \"talking over the news bulletins and making silly noises\". In 1988 he also joined, from its launch, Greater London Radio (GLR). He presented \"The Chris Morris Show\" on GLR until 1993, when one show got suspended after a sketch was broadcast involving a child \"outing\" celebrities.\nIn 1991, Morris joined Armando Iannucci's spoof news project \"On the Hour\". Broadcast on BBC Radio 4, it saw him work alongside Iannucci, Steve Coogan, Stewart Lee, Richard Herring and Rebecca Front. In 1992, Morris hosted Danny Baker's Radio 5 Morning Edition show for a week whilst Baker was on holiday. In 1994, Morris began a weekly evening show, the \"Chris Morris Music Show\", on BBC Radio 1 alongside Peter Baynham and 'man with a mobile phone' Paul Garner. In the shows, Morris perfected the spoof interview style that would become a central component of his \"Brass Eye\" programme. In the same year, Morris teamed up with Peter Cook (as Sir Arthur Streeb-Greebling), in a series of improvised conversations for BBC Radio 3 entitled \"Why Bother?\".\nMove into television and film.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"If you make a joke in an area which is for some reason, normally random, out of bounds, then you might find something out, you might put your finger on something.\"\nChris Morris\nIn 1994, a BBC Two television series based on \"On the Hour\" was broadcast under the name \"The Day Today\". \"The Day Today\" made a star of Morris, and marked the television debut of Steve Coogan's Alan Partridge character. The programme ended on a high after just one series, with Morris winning the 1994 British Comedy Award for Best Newcomer for his lead role as the Paxmanesque news anchor.\nIn 1996, Morris appeared on the daytime programme \"The Time, The Place\", posing as an academic, Thurston Lowe, in a discussion entitled \"Are British Men Lousy Lovers?\", but was found out when a producer alerted the show's host, John Stapleton.\nIn 1997, the black humour which had featured in \"On the Hour\" and \"The Day Today\" became more prominent in \"Brass Eye\", another spoof of current affairs television documentary, shown on Channel 4. All three series satirised and exaggerated issues expected of news shows. The second episode of \"Brass Eye,\" for example, satirised drugs and the political rhetoric surrounding them. To help convey the satire, Morris invented a fictional drug by the name of \"cake\". In the episode, British celebrities and politicians describe the supposed symptoms in detail; David Amess mentioned the fictional drug at Parliament. In 2001, Morris' satirized the moral panic regarding pedophilia in the most controversial episode of \"Brass Eye\", \"Paedogeddon\". Channel 4 apologised for the episode after receiving criticism from tabloids and around 3,000 complaints from viewers, which, at the time, was the most for an episode of British television. \nFrom 1997 to 1999, Morris created \"Blue Jam\" for BBC Radio 1, a surreal taboo-breaking radio show set to an ambient soundtrack. In 2000, this was followed by \"Jam\", a television reworking. Morris released a 'remix' version of this, entitled \"Jaaaaam\".\nIn 2002, Morris ventured into film, directing the short \"My Wrongs#8245\u20138249 &amp; 117\", adapted from a \"Blue Jam\" monologue about a man led astray by a sinister talking dog. It was the first film project of Warp Films, a branch of Warp Records. In 2002 it won the BAFTA for best short film. In 2005 Morris worked on a sitcom entitled \"Nathan Barley\", based on the character created by Charlie Brooker for his website TVGoHome (Morris had contributed to TVGoHome on occasion, under the pseudonym 'Sid Peach'). Co-written by Brooker and Morris, the series was broadcast on Channel 4 in early 2005.\n\"The IT Crowd\" and \"Comedy Vehicle\".\nMorris was a cast member in \"The IT Crowd\", a Channel 4 sitcom which focused on the information technology department of the fictional company Reynholm Industries. The series was written and directed by Graham Linehan (with whom Morris collaborated on \"The Day Today\", \"Brass Eye\" and \"Jam\") and produced by Ash Atalla. Morris played Denholm Reynholm, the eccentric managing director of the company. This marked the first time Morris has acted in a substantial role in a project which he has not developed himself. Morris' character appeared to leave the series during episode two of the second series. His character made a brief return in the first episode of the third series.\nIn November 2007, Morris wrote an article for \"The Observer\" in response to Ronan Bennett's article published six days earlier in \"The Guardian\". Bennett's article, \"Shame on us\", accused the novelist Martin Amis of racism. Morris' response, \"The absurd world of Martin Amis\", was also highly critical of Amis; although he did not accede to Bennett's accusation of racism, Morris likened Amis to the Muslim cleric Abu Hamza (who was jailed for inciting racial hatred in 2006), suggesting that both men employ \"mock erudition, vitriol and decontextualised quotes from the Qu'ran\" to incite hatred.\nMorris served as script editor for the 2009 series \"Stewart Lee's Comedy Vehicle\", working with former colleagues Stewart Lee, Kevin Eldon and Armando Iannucci. He maintained this role for the second (2011) and third series (2014), also appearing as a mock interviewer dubbed the \"hostile interrogator\" in the third and fourth series.\n\"Four Lions\", \"Veep\", and other appearances.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"I don't really see the point of comedy unless there's something underpinning it. I mean, what are you doing? Are you doing some kind of exotic display for the court, to be patted on the head by the court, or are you trying to change something?\"\n\u2014 Morris discussing the motives behind his comedy\nMorris completed his debut feature film \"Four Lions\" in late 2009, a satire based on a group of Islamist terrorists in Sheffield.\nIt premiered at the Sundance Film Festival in January 2010 and was short-listed for the festival's World Cinema Narrative prize. The film (working title \"Boilerhouse\") was picked up by Film Four. Morris told \"The Sunday Times\" that the film sought to do for Islamic terrorism what \"Dad's Army\", the classic BBC comedy, did for the Nazis by showing them as \"scary but also ridiculous\".\nIn 2012, Morris directed the seventh and penultimate episode of the first season of \"Veep\", an Armando Iannucci-devised American version of \"The Thick of It\". In 2013, he returned to direct two episodes for the second season of \"Veep\", and a further episode for season three in 2014.\nIn 2013, Morris appeared briefly in Richard Ayoade's \"The Double\", a black comedy film based on the Fyodor Dostoyevsky novella of the same name. Morris had previously worked with Ayoade on \"Nathan Barley\" and \"The IT Crowd\".\nIn February 2014, Morris made a surprise appearance at the beginning of a Stewart Lee live show, introducing the comedian with fictional anecdotes about their work together. The following month, Morris appeared in the third series of \"Stewart Lee's Comedy Vehicle\" as a \"hostile interrogator\", a role previously occupied by Armando Iannucci.\nIn December 2014, it was announced that a short radio collaboration with Noel Fielding and Richard Ayoade would be broadcast on BBC Radio 6. According to Fielding, the work had been in progress since around 2006. However, in January 2015 it was decided, 'in consultation with [Morris]', that the project was not yet complete, and so the intended broadcast did not go ahead.\n\"The Day Shall Come\".\nA statement released by Film4 in February 2016 made reference to funding what would be Morris' second feature film. In November 2017 it was reported that Morris had shot the movie, starring Anna Kendrick, in the Dominican Republic but the title was not made public. It was later reported in January 2018 that Jim Gaffigan and Rupert Friend had joined the cast of the still-untitled film, and that the plot would revolve around an FBI hostage situation gone wrong. The completed film, titled \"The Day Shall Come\", had its world premiere at South by Southwest on 11 March 2019.\nMusic.\nMorris often co-writes and performs incidental music for his television shows, notably with \"Jam\" and the 'extended remix' version, \"Jaaaaam\". In the early 1990s Morris contributed a Pixies parody track entitled \"Motherbanger\" to a flexi-disc given away with an edition of Select music magazine. Morris supplied sketches for British band Saint Etienne's 1993 single \"You're in a Bad Way\" (the sketch 'Spongbake' appears at the end of the 4th track on the CD single).\nIn 2000, he collaborated by mail with Amon Tobin to create the track \"Bad Sex\", which was released as a B-side on the Tobin single \"Slowly\".\nBritish band Stereolab's song \"Nothing to Do with Me\" from their 2001 album \"Sound-Dust\" featured various lines from Chris Morris sketches as lyrics.\nStyle.\nRamsey Ess of \"Vulture\" described Morris' comedy style as \"crass\" and \"shocking\", but noted an \"underlying morality\" and integrity, as well as the humor being Morris' priority.\nRecognition.\nIn 2003, Morris was listed in \"The Observer\" as one of the 50 funniest acts in British comedy. In 2005, Channel 4 aired a show called \"The Comedian's Comedian\" in which foremost writers and performers of comedy ranked their 50 favourite acts. Morris was at number eleven. Morris won the BAFTA for outstanding debut with his film \"Four Lions\". Adeel Akhtar and Nigel Lindsay collected the award in his absence. Lindsay stated that Morris had sent him a text message before they collected the award reading, 'Doused in petrol, Zippo at the ready'. In June 2012 Morris was placed at number 16 in the Top 100 People in UK Comedy.\nIn 2010, a biography, \"Disgusting Bliss: The Brass Eye of Chris Morris\", was published. Written by Lucian Randall, the book depicted Morris as \"brilliant but uncompromising\", and a \"frantic-minded perfectionist\".\nIn November 2014, a three-hour retrospective of Morris' radio career was broadcast on BBC Radio 4 Extra under the title 'Raw Meat Radio', presented by Mary Anne Hobbs and featuring interviews with Armando Iannucci, Peter Baynham, Paul Garner, and others.\nAwards.\nMorris won the Best TV Comedy Newcomer award from the British Comedy Awards in 1994 for his performance in \"The Day Today\". He has won two BAFTA awards: the BAFTA Award for Best Short Film in 2002 for \"My Wrongs#8245\u20138249 &amp; 117\", and the BAFTA Award for Outstanding Debut by a British director, writer or producer in 2011 for \"Four Lions\".\nPersonal life.\nMorris and his wife, actress-turned-literary agent Jo Unwin, live in the Brixton district of London. The pair met in 1984 at the Edinburgh Festival, when he was playing bass guitar for the Cambridge Footlights Revue and she was in a comedy troupe called the Millies. They have two sons, Charles and Frederick, both of whom were born in Lambeth in south London. \nGiving very few interviews and avoiding all social media, Morris has been described as a recluse. He can be heard in a 2008 podcast for CERN, being taken on a tour of the facility by the physicist Brian Cox. In 2010, he made numerous media appearances to promote and support \"Four Lions\" in both the UK and US, on 8 November 2010,&lt;ref name=\"gettyimages/138236390\"&gt;&lt;/ref&gt; appearing as a guest on \"Late Night with Jimmy Fallon\". In 2019, two lengthy interviews conducted with Morris on \"The Adam Buxton Podcast\" were released in the run-up to the release of Morris's film \"The Day Shall Come\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
