{"id": "7293", "revid": "5662528", "url": "https://en.wikipedia.org/wiki?curid=7293", "title": "Commodore 64", "text": "8-bit home computer introduced in 1982\nThe Commodore 64, also known as the C64, is an 8-bit home computer introduced in January 1982 by Commodore International (first shown at the Consumer Electronics Show, January 7\u201310, 1982, in Las Vegas). It has been listed in the Guinness World Records as the highest-selling single computer model of all time, with independent estimates placing the number sold between 12.5 and 17 million units. Volume production started in early 1982, marketing in August for US$. Preceded by the VIC-20 and Commodore PET, the C64 took its name from its 64 kilobytes (65,536 bytes) of RAM. With support for multicolor sprites and a custom chip for waveform generation, the C64 could create superior visuals and audio compared to systems without such custom hardware.\nThe C64 dominated the low-end computer market (except in the UK and Japan, lasting only about six months in Japan) for most of the later years of the 1980s. For a substantial period (1983\u20131986), the C64 had between 30% and 40% share of the US market and two million units sold per year, outselling IBM PC compatibles, Apple computers, and the Atari 8-bit family of computers. Sam Tramiel, a later Atari president and the son of Commodore's founder, said in a 1989 interview, \"When I was at Commodore we were building 400,000 C64s a month for a couple of years.\" In the UK market, the C64 faced competition from the BBC Micro, the ZX Spectrum, and later the Amstrad CPC 464. but the C64 was still the second-most-popular computer in the UK after the ZX Spectrum. The Commodore 64 failed to make any impact in Japan, as their market was dominated by Japanese computers, such as the NEC PC-8801, Sharp X1, Fujitsu FM-7, and MSX.\nPart of the Commodore 64's success was its sale in regular retail stores instead of only electronics or computer hobbyist specialty stores. Commodore produced many of its parts in-house to control costs, including custom integrated circuit chips from MOS Technology. In the United States, it has been compared to the Ford Model T automobile for its role in bringing a new technology to middle-class households via creative and affordable mass-production. Approximately 10,000 commercial software titles have been made for the Commodore\u00a064, including development tools, office productivity applications, and video games. C64 emulators allow anyone with a modern computer, or a compatible video game console, to run these programs today. The C64 is also credited with popularizing the computer demoscene and is still used today by some computer hobbyists. In 2011, 17 years after it was taken off the market, research showed that brand recognition for the model was still at 87%.\nHistory.\nIn January 1981, MOS Technology, Inc., Commodore's integrated circuit design subsidiary, initiated a project to design the graphic and audio chips for a next-generation video game console. Design work for the chips, named MOS Technology VIC-II (Video Integrated Circuit for graphics) and MOS Technology SID (Sound Interface Device for audio), was completed in November 1981. Commodore then began a game console project that would use the new chips\u2014called the \"Ultimax\" or the \"MAX Machine\", engineered by Yash Terakura from Commodore Japan. This project was eventually cancelled after just a few machines were manufactured for the Japanese market. At the same time, Robert \"Bob\" Russell (system programmer and architect on the VIC-20) and Robert \"Bob\" Yannes (engineer of the SID) were critical of the current product line-up at Commodore, which was a continuation of the Commodore PET line aimed at business users. With the support of Al Charpentier (engineer of the VIC-II) and Charles Winterble (manager of MOS Technology), they proposed to Commodore CEO Jack Tramiel a low-cost sequel to the VIC-20. Tramiel dictated that the machine should have 64 KB of random-access memory (RAM). Although 64-Kbit dynamic random-access memory (DRAM) chips cost over US$ at the time, he knew that 64K DRAM prices were falling and would drop to an acceptable level before full production was reached. The team was able to quickly design the computer because, unlike most other home-computer companies, Commodore had its own semiconductor fab to produce test chips; because the fab was not running at full capacity, development costs were part of existing corporate overhead. The chips were complete by November, by which time Charpentier, Winterble, and Tramiel had decided to proceed with the new computer; the latter set a final deadline for the first weekend of January, to coincide with the 1982 Consumer Electronics Show (CES).\nThe product was code named the VIC-40 as the successor to the popular VIC-20. The team that constructed it consisted of Yash Terakura, Shiraz Shivji, Bob Russell, Bob Yannes, and David A. Ziembicki. The design, prototypes, and some sample software were finished in time for the show, after the team had worked tirelessly over both Thanksgiving and Christmas weekends. The machine used the same case, same-sized motherboard, and same Commodore BASIC 2.0 in ROM as the VIC-20. BASIC also served as the user interface shell and was available immediately on startup at the codice_1 prompt. When the product was to be presented, the VIC-40 product was renamed C64. The C64 made an impressive debut at the January 1982 Consumer Electronics Show, as recalled by Production Engineer David A. Ziembicki: \"All we saw at our booth were Atari people with their mouths dropping open, saying, 'How can you do that for $595?'\" The answer was vertical integration; due to Commodore's ownership of MOS Technology's semiconductor fabrication facilities, each C64 had an estimated production cost of US$.\nReception.\nIn July 1983, \"BYTE\" magazine stated that \"the 64 retails for $. At that price it promises to be one of the hottest contenders in the under-$ personal computer market.\" It described the SID as \"a true music synthesizer\u00a0... the quality of the sound has to be heard to be believed\", while criticizing the use of Commodore BASIC\u00a02.0, the floppy disk performance which is \"even slower than the Atari 810 drive\", and Commodore's quality control. \"BYTE\" gave more details, saying the C64 had \"inadequate Commodore BASIC 2.0. An 8K-byte interpreted BASIC\" which they assumed was because \"Obviously, Commodore feels that most home users will be running prepackaged software - there is no provision for using graphics (or sound as mentioned above) from within a BASIC program except by means of POKE commands.\" This was one of very few warnings about C64 BASIC published in any computer magazines. \n \"Creative Computing\" said in December 1984 that the C64 was \"the overwhelming winner\" in the category of home computers under $. Despite criticizing its \"slow disk drive, only two cursor directional keys, zero manufacturer support, non-standard interfaces, etc.\", the magazine said that at the C64's price of less than $ \"you can't get another system with the same features: 64K, color, sprite graphics, and barrels of available software\". The Tandy Color Computer was the runner up. The Apple II was the winner in the category of home computer over $, which was the category the Commodore 64 was in when it was first released at the price of $.\nMarket war: 1982\u20131983.\nCommodore had a reputation for announcing products that never appeared, so sought to quickly ship the C64. Production began in the spring of 1982, and volume shipments began in August. The C64 faced a wide range of competing home computers, but with a lower price and more flexible hardware, it quickly outsold many of its competitors.\nIn the United States, the greatest competitors were the Atari 8-bit 400, the Atari 800, and the Apple II. The Atari 400 and 800 had been designed to accommodate previously stringent FCC emissions requirements and so were expensive to manufacture. Though similar in specifications, the C64 and Apple II represented differing design philosophies; as an open architecture system, upgrade capability for the Apple II was granted by internal expansion slots, whereas the C64's comparatively closed architecture had only a single external ROM cartridge port for bus expansion. However, the Apple II used its expansion slots for interfacing with common peripherals like disk drives, printers, and modems; the C64 had a variety of ports integrated into its motherboard, which were used for these purposes, usually leaving the cartridge port free. Commodore's was not a completely closed system, however, the company had published detailed specifications for most of their models since the Commodore PET and VIC-20 days, and the C64 was no exception. C64 sales were nonetheless relatively slow due to a lack of software, reliability issues with early production models, particularly high failure rates of the PLA chip, which used a new production process, and a shortage of 1541 disk drives, which also suffered rather severe reliability issues. During 1983, however, a trickle of software turned into a flood and sales began rapidly climbing, especially with price cuts from $ to just $ (equivalent to $ to $ in 2022).\nCommodore sold the C64 not only through its network of authorized dealers but also through department stores, discount stores, toy stores and college bookstores. The C64 had a built-in RF modulator and thus could be plugged into any television set. This allowed it (like its predecessor, the VIC-20) to compete directly against video game consoles such as the Atari 2600. Like the Apple IIe, the C64 could also output a composite video signal, avoiding the RF modulator altogether. This allowed the C64 to be plugged into a specialized monitor for a sharper picture. Unlike the IIe, the C64's NTSC output capability also included separate luminance/chroma signal output equivalent to (and electrically compatible with) S-Video, for connection to the Commodore 1702 monitor, providing even better video quality than a composite signal.\nAggressive pricing of the C64 is considered to have been a major catalyst in the video game crash of 1983. In January 1983, Commodore offered a $100 rebate in the United States on the purchase of a C64 to anyone that traded in another video game console or computer. To take advantage of this rebate, some mail-order dealers and retailers offered a Timex Sinclair 1000 (TS1000) for as little as $ with the purchase of a C64. This deal meant that the consumer could send the TS1000 to Commodore, collect the rebate, and pocket the difference; Timex Corporation departed the computer market within a year. Commodore's tactics soon led to a price war with the major home computer manufacturers. The success of the VIC-20 and C64 contributed significantly to the exit from the field of Texas Instruments and other smaller competitors.\nThe price war with Texas Instruments was seen as a personal battle for Commodore president Jack Tramiel. Commodore dropped the C64's list price by $ within two months of its release. In June 1983 the company lowered the price to $300, and some stores sold the computer for $. At one point, the company was selling as many C64s as all computers sold by the rest of the industry combined. Meanwhile, TI lost money by selling the TI-99/4A for $. TI's subsequent demise in the home computer industry in October 1983 was seen as revenge for TI's tactics in the electronic calculator market in the mid-1970s, when Commodore was almost bankrupted by TI.\nAll four machines had similar memory configurations which were standard in 1982\u201383: for the Apple II+ (upgraded within months of C64's release to with the Apple IIe) and for the Atari 800. At upwards of $, the Apple\u00a0II was about twice as expensive, while the Atari 800 cost $899. One key to the C64's success was Commodore's aggressive marketing tactics, and they were quick to exploit the relative price/performance divisions between its competitors with a series of television commercials after the C64's launch in late 1982. The company also published detailed documentation to help developers, while Atari initially kept technical information secret.\nAlthough many early C64 games were inferior Atari 8-bit ports, by late 1983, the growing installed base caused developers to create new software with better graphics and sound. It was the only non-discontinued, widely available home computer by then, with more than 500,000 sold during the Christmas season; because of production problems in Atari's supply chain, by the start of 1984 \"the Commodore 64 largely has [the low-end] market to itself right now\", \"The Washington Post\" reported.\n1984\u20131987.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Some of the graphics modes on the 64 are really strange, and they have no analogs to the Atari or Apple, like the ability to change color of the character basis across the screen. That gave us a lot of color capability that had not been exploited.\nWith sales booming and the early reliability issues with the hardware addressed, software for the C64 began to grow in size and ambition during 1984. This growth shifted to the primary focus of most US game developers. The two holdouts were Sierra, who largely skipped over the C64 in favor of Apple and PC-compatible machines, and Broderbund, who were heavily invested in educational software and developed primarily around the Apple II. In the North American market, the disk format had become nearly universal while cassette and cartridge-based software all but disappeared. So most US-developed games by this point grew large enough to require multi-loading.\nAt a mid-1984 conference of game developers and experts at Origins Game Fair, Dan Bunten, Sid Meier, and a representative of Avalon Hill said that they were developing games for the C64 first as the most promising market. By 1985, games were an estimated 60 to 70% of Commodore 64 software. \"Computer Gaming World\" stated in January 1985 that companies such as Epyx that survived the video game crash did so because they \"jumped on the Commodore bandwagon early\". Over 35% of SSI's 1986 sales were for the C64, ten points higher than for the Apple\u00a0II. The C64 was even more important for other companies, which often found that more than half the sales for a title ported to six platforms came from the C64 version. That year, \"Computer Gaming World\" published a survey of ten game publishers that found that they planned to release forty-three Commodore 64 games that year, compared to nineteen for Atari and forty-eight for Apple\u00a0II, and Alan Miller stated that Accolade developed first for the C64 because \"it will sell the most on that system\".\nIn Europe, the primary competitors to the C64 were British-built computers: the Sinclair ZX Spectrum, the BBC Micro, and the Amstrad CPC 464. In the UK, the 48K Spectrum had not only been released a few months ahead of the C64's early 1983 debut, but it was also selling for \u00a3175, less than half the C64's \u00a3399 price. The Spectrum quickly became the market leader and Commodore had an uphill struggle against it in the marketplace. The C64 did however go on to rival the Spectrum in popularity in the latter half of the 1980s. Adjusted to the population size, the popularity of Commodore\u00a064 was the highest in Finland at roughly 3 units per 100 inhabitants, where it was subsequently marketed as \"the Computer of the Republic\".\nRumors spread in late 1983 that Commodore would discontinue the C64. By early 1985 the C64's price was $; with an estimated production cost of , its profitability was still within the industry-standard markup of two to three times. Commodore sold about one million C64s in 1985 and a total of 3.5 million by mid-1986. Although the company reportedly attempted to discontinue the C64 more than once in favor of more expensive computers such as the Commodore 128, demand remained strong. In 1986, Commodore introduced the 64C, a redesigned 64, which \"Compute!\" saw as evidence that\u2014contrary to C64 owners' fears that the company would abandon them in favor of the Amiga and 128\u2014\"the 64 refuses to die\". Its introduction also meant that Commodore raised the price of the C64 for the first time, which the magazine cited as the end of the home-computer price war. Software sales also remained strong; MicroProse, for example, in 1987 cited the Commodore and IBM PC markets as its top priorities.\n1988\u20131994.\nBy 1988, PC compatibles were the largest and fastest-growing home and entertainment software markets, displacing former leader Commodore. Commodore 64 software sales were almost unchanged in the third quarter of 1988 year over year while the overall market grew 42%, but the company was still selling 1 to 1.5 million units worldwide each year of what \"Computer Chronicles\" that year called \"the Model T of personal computers\". Epyx CEO David Shannon Morse cautioned that \"there are no new 64 buyers, or very few. It's a consistent group that's not growing... it's going to shrink as part of our business.\" One computer gaming executive stated that the Nintendo Entertainment System's enormous popularity\u00a0\u2013 seven million sold in 1988, almost as many as the number of C64s sold in its first five years\u00a0\u2013 had stopped the C64's growth. Trip Hawkins reinforced that sentiment, stating that Nintendo was \"the last hurrah of the 8-bit world\".\nSSI exited the Commodore 64 market in 1991, after most competitors. \"Ultima VI\", released in 1991, was the last major C64 game release from a North American developer, and \"The Simpsons\", published by Ultra Games, was the last arcade conversion. The latter was a somewhat uncommon example of a US-developed arcade port as after the early years of the C64, most arcade conversions were produced by UK developers and converted to NTSC and disk format for the US market, American developers instead focusing on more computer-centered game genres such as RPGs and simulations. In the European market, disk software was rarer and cassettes were the most common distribution method; this led to a higher prevalence of arcade titles and smaller, lower-budget games that could fit entirely in the computer's memory without requiring multiloads. European programmers also tended to exploit advanced features of the C64's hardware more than their US counterparts.\nIn the United States, demand for 8-bit computers all but ceased as the 1990s began and PC compatibles completely dominated the computer market. However, the C64 continued to be popular in the UK and other European countries. The machine's eventual demise was not due to lack of demand or the cost of the C64 itself (still profitable at a retail price point between \u00a344 and \u00a350), but rather because of the cost of producing the disk drive. In March 1994, at CeBIT in Hanover, Germany, Commodore announced that the C64 would be finally discontinued in 1995, noting that the Commodore 1541 cost more than the C64 itself.\nHowever, only one month later in April 1994, the company filed for bankruptcy. When Commodore went bankrupt, all production on their inventory, including the C64, was discontinued, thus ending the C64's 11 and a half year production. Claims of sales of 17, 22 and 30 million of C64 units sold worldwide have been made. Company sales records, however, indicate that the total number was about 12.5 million. Based on that figure, the Commodore 64 was still the third most popular computing platform into the 21st century until 2017 when the Raspberry Pi family replaced it. While 360,000 C64s were sold in 1982, about 1.3 million were sold in 1983, followed by a large spike in 1984 when 2.6 million were sold. After that, sales held steady at between 1.3 and 1.6 million a year for the remainder of the decade and then dropped off after 1989. North American sales peaked between 1983 and 1985 and gradually tapered off afterward, while European sales remained quite strong into the early 1990s.\nC64 family.\nCommodore MAX.\nIn 1982, Commodore released the MAX Machine in Japan. It was called the Ultimax in the United States and VC-10 in Germany. The MAX was intended to be a game console with limited computing capability and was based on a cut-down version of the hardware family later used in the C64. The MAX was discontinued months after its introduction because of poor sales in Japan.\nCommodore Educator 64.\n1983 saw Commodore attempt to compete with the Apple\u00a0II's hold on the US education market with the Educator 64, essentially a C64 and \"greenscale\" monochrome monitor in a PET case. Schools preferred the all-in-one metal construction of the PET over the standard C64's separate components, which could be easily damaged, vandalized, or stolen. Schools did not prefer the Educator 64 to the wide range of software and hardware options the Apple\u00a0IIe was able to offer, and it was produced in limited quantities.\nSX-64.\nAlso in 1983, Commodore released the SX-64, a portable version of the C64. The SX-64 has the distinction of being the first commercial \"full-color\" portable computer. While earlier computers using this form factor only incorporate monochrome (\"green screen\") displays, the base SX-64 unit features a color cathode-ray tube (CRT) and one integrated 1541 floppy disk drive. Even though Commodore claimed in advertisements that it would have dual 1541 drives, when the SX-64 was released there was only one and the other became a floppy disk storage slot. Also, unlike most other C64s, the SX-64 does not have a datasette connector so an external cassette was not an option.\nCommodore 128.\nTwo designers at Commodore, Fred Bowen and Bil Herd, were determined to rectify the problems of the Plus/4. They intended that the eventual successors to the C64\u2014the Commodore 128 and 128D computers (1985)\u2014were to build upon the C64, avoiding the Plus/4's flaws. The successors had many improvements such as a BASIC with graphics and sound commands (like almost all home computers not made by Commodore), 80-column display ability, and full CP/M compatibility. The decision to make the Commodore 128 plug compatible with the C64 was made quietly by Bowen and Herd, software and hardware designers respectively, without the knowledge or approval by the management in the post Jack Tramiel era. The designers were careful not to reveal their decision until the project was too far along to be challenged or changed and still make the impending Consumer Electronics Show (CES) in Las Vegas. Upon learning that the C128 was designed to be compatible with the C64, Commodore's marketing department independently announced that the C128 would be 100% compatible with the C64, thereby raising the bar for C64 support. In a case of malicious compliance, the 128 design was altered to include a separate \"64 mode\" using a complete C64 environment to try to ensure total compatibility.\nCommodore 64C.\nThe C64's designers intended the computer to have a new, wedge-shaped case within a year of release, but the change did not occur. In 1986, Commodore released the 64C computer, which is functionally identical to the original. The exterior design was remodeled in the sleeker style of the Commodore 128. The 64C uses new versions of the SID, VIC-II, and I/O chips being deployed. Models with the C64E board had the graphic symbols printed on the top of the keys, instead of the normal location on the front. The sound chip (SID) was changed to use the MOS 8580 chip, with the core voltage reduced from 12V to 9V. The most significant changes include different behavior in the filters and in the volume control, which result in some music/sound effects sounding differently than intended, and in digitally-sampled audio being almost inaudible, respectively (though both of these can mostly be corrected-for in software). The 64\u00a0KB RAM memory went from eight chips to two chips. BASIC and the KERNAL went from two separate chips into one 16\u00a0KB ROM chip. The PLA chip and some TTL chips were integrated into a DIL 64-pin chip. The \"252535-01\" PLA integrated the color RAM as well into the same chip. The smaller physical space made it impossible to put in some internal expansions like a floppy-speeder. In the United States, the 64C was often bundled with the third-party GEOS graphical user interface (GUI)-based operating system, as well as the software needed to access Quantum Link. The 1541 drive received a matching face-lift, resulting in the 1541C. Later, a smaller, sleeker 1541-II model was introduced, along with the 800 KB 3.5-inch microfloppy 1581.\nCommodore 64 Games System.\nIn 1990, the C64 was repackaged in the form of a game console, called the C64 Games System (C64GS), with most external connectivity removed. A simple modification to the 64C's motherboard was made to allow cartridges to be inserted from above. A modified ROM replaced the BASIC interpreter with a boot screen to inform the user to insert a cartridge. Designed to compete with the Nintendo Entertainment System and Sega's Master System, it suffered from very low sales compared to its rivals. It was another commercial failure for Commodore, and it was never released outside Europe. The Commodore game system lacked a keyboard, so any software that required a keyboard could not be used.\nCommodore 65.\nIn 1990, an advanced successor to the C64, the Commodore 65 (also known as the \"C64DX\"), was prototyped, but the project was canceled by Commodore's chairman Irving Gould in 1991. The C65's specifications were impressive for an 8-bit computer, bringing specs comparable to the 16-bit Apple\u00a0IIGS. For example, it could display 256 colors on the screen, while OCS based Amigas could only display 64 in HalfBrite mode (32 colors and half-bright transformations). Although no specific reason was given for the C65's cancellation, it would have competed in the marketplace with Commodore's lower-end Amigas and the Commodore CDTV.\nSoftware.\nIn 1982, the C64's graphics and sound capabilities were rivaled only by the Atari 8-bit family and appeared exceptional when compared with the widely publicized Atari VCS and Apple\u00a0II. The C64 is often credited with starting the computer subculture known as the demoscene (see Commodore 64 demos). It is still being actively used in the demoscene, especially for music (its SID sound chip even being used in special sound cards for PCs, and the Elektron SidStation synthesizer). Even though other computers quickly caught up with it, the C64 remained a strong competitor to the later video game consoles Nintendo Entertainment System (NES) and Master System, thanks in part to its by-then established software base, especially outside North America, where it comprehensively outsold the NES.\nBecause of lower incomes and the domination of the ZX Spectrum in the UK, almost all British C64 software used cassette tapes. Few cassette C64 programs were released in the US after 1983 and, in North America, the diskette was the principal method of software distribution. The cartridge slot on the C64 was also mainly a feature used in the computer's first two years on the US market and became rapidly obsolete once the price and reliability of 1541 drives improved. A handful of PAL region games used bank switched cartridges to get around the 16\u00a0KB memory limit.\nBASIC.\nAs is common for home computers of the early 1980s, the C64 comes with a BASIC interpreter, in ROM. KERNAL, I/O, and tape/disk drive operations are accessed via custom BASIC language commands. The disk drive has its own interfacing microprocessor and ROM (firmware) I/O routines, much like the earlier CBM/PET systems and the Atari 400 and Atari 800. This means that no memory space is dedicated to running a disk operating system, as was the case with earlier systems such as the Apple\u00a0II and TRS-80.\nCommodore BASIC 2.0 is used instead of the more advanced BASIC\u00a04.0 from the PET series, since C64 users were not expected to need the disk-oriented enhancements of BASIC\u00a04.0. The company did not expect many to buy a disk drive, and using BASIC\u00a02.0 simplified VIC-20 owners' transition to the 64. \"The choice of BASIC\u00a02.0 instead of 4.0 was made with some soul-searching, not just at random. The typical user of a C64 is not expected to need the direct disk commands as much as other extensions, and the amount of memory to be committed to BASIC were to be limited. We chose to leave expansion space for color and sound extensions instead of the disk features. As a result, you will have to handle the disk in the more cumbersome manner of the 'old days'.\"\nThe version of Microsoft BASIC is not very comprehensive and does not include specific commands for sound or graphics manipulation, instead requiring users to use the \"PEEK and POKE\" commands to access the graphics and sound chip registers directly. To provide extended commands, including graphics and sound, Commodore produced two different cartridge-based extensions to BASIC\u00a02.0: Simons' BASIC and Super Expander 64. Other languages available for the C64 include Pascal, C, Logo, Forth, and FORTRAN. Compilers for BASIC\u00a02.0 such as Petspeed\u00a02 (from Commodore), Blitz (from Jason Ranheim), and Turbo Lightning (from Ocean Software) were produced. Most commercial C64 software was written in assembly language, either cross-developed on a larger computer, or directly on the C64 using a machine code monitor or an assembler. This maximized speed and minimized memory use. Some games, particularly adventures, used high-level scripting languages and sometimes mixed BASIC and machine language.\nAlternative operating systems.\nMany third-party operating systems have been developed for the C64. As well as the original GEOS, two third-party GEOS-compatible systems have been written: Wheels and GEOS megapatch. Both of these require hardware upgrades to the original C64. Several other operating systems are or have been available, including WiNGS OS, the Unix-like LUnix, operated from a command-line, and the embedded systems OS Contiki, with full GUI. Other less well-known OSes include ACE, Asterix, DOS/65, and GeckOS. A version of CP/M was released, but this requires the addition of an external Z80 processor to the expansion bus. Furthermore, the Z80 processor is underclocked to be compatible with the C64's memory bus, so performance is poor compared to other CP/M implementations. C64 CP/M and C128 CP/M both suffer a lack of software; although most commercial CP/M software can run on these systems, software media is incompatible between platforms. The low usage of CP/M on Commodores means that software houses saw no need to invest in mastering versions for the Commodore disk format. The C64 CP/M cartridge is also not compatible with anything except the early 326298 motherboards.\nNetworking software.\nDuring the 1980s, the Commodore 64 was used to run bulletin board systems using software packages such as Punter BBS, Bizarre 64, Blue Board, C-Net, Color 64, CMBBS, C-Base, DMBBS, Image BBS, EBBS, and The Deadlock Deluxe BBS Construction Kit, often with sysop-made modifications. These boards sometimes were used to distribute cracked software. As late as December 2013, there were 25 such Bulletin Board Systems in operation, reachable via the Telnet protocol. There were major commercial online services, such as Compunet (UK), CompuServe (US\u00a0\u2013 later bought by America Online), The Source (US), and Minitel (France) among many others. These services usually required custom software which was often bundled with a modem and included free online time as they were billed by the minute. Quantum Link (or Q-Link) was a US and Canadian online service for Commodore 64 and 128 personal computers that operated from November 5, 1985, to November 1, 1994. It was operated by Quantum Computer Services of Vienna, Virginia, which in October 1991 changed its name to America Online and continued to operate its AOL service for the IBM PC compatible and Apple Macintosh. Q-Link was a modified version of the PlayNET system, which Control Video Corporation (CVC, later renamed Quantum Computer Services) licensed.\nOnline gaming.\nThe first graphical character-based interactive environment is \"Club Caribe\". First released as \"Habitat\" in 1988, \"Club Caribe\" was introduced by LucasArts for Q-Link customers on their Commodore 64 computers. Users could interact with one another, chat and exchange items. Although the game's open world was very basic, its use of online avatars and the combination of chat and graphics was revolutionary. Online graphics in the late 1980s were severely restricted by the need to support modem data transfer rates as low as 300 bits per second. Habitat's graphics were stored locally on floppy disk, eliminating the need for network transfer.\nHardware.\nCPU and memory.\nThe C64 uses an 8-bit MOS Technology 6510 microprocessor. It is almost identical to the 6502 but with three-state buses, a different pinout, slightly different clock signals and other minor changes for this specific application. It also has six I/O lines on otherwise unused legs on the 40-pin IC package. These are used for two purposes in the C64: to bank-switch the machine's read-only memory (ROM) in and out of the processor's address space, and to operate the datasette tape recorder. The C64 has of 8-bit-wide dynamic RAM, of 4-bit-wide static color RAM for text mode, and are available to built-in Commodore BASIC 2.0 on startup. There is of ROM, made up of the BASIC interpreter, the KERNAL, and the character ROM. As the processor could only address at a time, the ROM was mapped into memory, and only of RAM (plus in between the ROMs) were available at startup. Most \"breadbin\" Commodore 64s used 4164 DRAM, with eight chips to total up 64K of system RAM. Later models, featuring Assy 250466 and Assy 250469 motherboards, used 41464 DRAM (64K\u00d74) chips which stored per chip, so only two were required. Since 4164 DRAMs are 64K\u00d71, eight chips are needed to make an entire byte, and the computer will not function without all of them present. Thus, the first chip contains Bit 0 for the entire memory space, the second chip contains Bit 1, and so forth. This also makes detecting faulty RAM easy, as a bad chip will display random characters on the screen and the character displayed can be used to determine the faulty RAM.\nThe C64 performs a RAM test on power up and if a RAM error is detected, the amount of free BASIC memory will be lower than the normal 38911 figure. If the faulty chip is in lower memory, then an codice_2 error is displayed rather than the usual BASIC startup banner. The color RAM at $D800 uses a separate 2114 SRAM chip and is gated directly to the VIC-II.\nThe C64 uses a somewhat complicated memory banking scheme; the normal power-on default is to have the BASIC ROM mapped in at $A000-$BFFF and the screen editor/KERNAL\u00a0ROM at $E000\u2013$FFFF. RAM underneath the system ROMs can be written to, but not read back without swapping out the ROMs. Memory location $01 contains a register with control bits for enabling/disabling the system ROMs as well as the I/O area at $D000. If the KERNAL\u00a0ROM is swapped out, BASIC will be removed at the same time,264 and it is not possible to have BASIC active without the KERNAL (as BASIC often calls KERNAL routines and part of the ROM code for BASIC is in fact located in the KERNAL ROM).\nThe character ROM is normally not visible to the CPU. It has two mirrors at $1000 and $9000, but only the VIC-II can see them; the CPU will see RAM in those locations. The character ROM may be mapped into $D000\u2013$DFFF where it is then visible to the CPU. Since doing so necessitates swapping out the I/O registers, interrupts must be disabled first. Graphics memory and data cannot be placed at $1000 or $9000 as the VIC-II will see the character ROM there instead.\nBy removing I/O from the memory map, $D000\u2013$DFFF becomes free RAM. The color RAM at $D800 is swapped out along with the I/O registers and this area can be used for static graphics data such as character sets since the VIC-II cannot see the I/O registers (or color RAM via the CPU mapping). If all ROMs and the I/O area are swapped out, the entire 64k RAM space is available aside for locations $0/$1.\n$C000\u2013$CFFF is free RAM and not used by BASIC or KERNAL routines; because of this, it is an ideal location to store short machine language programs that can be accessed from BASIC. The cassette buffer at $0334\u2013$03FF can also be used to store short machine language routines provided that a Datasette is not used, which will overwrite the buffer.\nC64 cartridges map into assigned ranges in the CPU's address space and the most common cartridge auto starting requires the presence of a special string at $8000 which contains \"CBM80\" followed by the address where program execution begins. A few early C64 cartridges released in 1982 use Ultimax mode (or MAX mode), a leftover feature of the failed MAX Machine. These cartridges map into $F000 and displace the KERNAL\u00a0ROM. If Ultimax mode is used, the programmer will have to provide code for handling system interrupts. The cartridge port has 16 address lines, which grants access to the entire address space of the computer if needed. Disk and tape software normally load at the start of BASIC memory ($0801) and use a small BASIC stub (e.g., codice_3) to jump to the start of the program. Although no Commodore 8-bit machine except the C128 can automatically boot from a floppy disk, some software intentionally overwrites certain BASIC vectors in the process of loading so that execution begins automatically rather than requiring the user to type RUN at the BASIC prompt following loading.\nAround 300 cartridges were released for the C64, mostly in the machine's first &lt;templatestyles src=\"Fraction/styles.css\" /&gt;2+1\u20442 years on the market, after which most software outgrew the cartridge limit. In the final years of the C64, larger software companies such as Ocean Software began releasing games on bank-switched cartridges to overcome this cartridge limit.\nCommodore did not include a reset button on any of their computers until the CBM-II line, but there were third-party cartridges with a reset button on them. It is possible to trigger a soft reset by jumping to the CPU reset routine at $FCE2 (64738). A few programs use this as an \"exit\" feature, although it does not clear memory.\nThe KERNAL\u00a0ROM went through three separate revisions, mostly designed to fix bugs. The initial version is only found on 326298 motherboards, used in the first production models, and cannot detect whether an NTSC or PAL VIC-II is present. The second revision is found on all C64s made from late 1982 through 1985. The third and last KERNAL\u00a0ROM revision was introduced on the 250466 motherboard (late breadbin models with 41464 RAM) and is found in all C64Cs. The 6510 CPU is clocked at (NTSC) and (PAL), lower than some competing systems (for example, the Atari 800 is clocked at ). A small performance boost can be gained by disabling the VIC-II's video output via a register write. This feature is often used by tape and disk fastloaders as well as the KERNAL cassette routine to keep a standard CPU cycle timing not modified by the VIC-II's sharing of the bus.\nThe Restore key is gated directly to the CPU's NMI line and will generate an NMI if pressed. The KERNAL handler for the NMI checks if Run/Stop is also pressed; if not, it ignores the NMI and simply exits back out. Run/Stop-Restore normally functions as a soft reset in BASIC that restores all I/O registers to their power on default state, but does not clear memory or reset pointers, so any BASIC programs in memory will be left untouched. Machine language software usually disables Run/Stop-Restore by remapping the NMI vector to a dummy RTI instruction. The NMI can be used for an extra interrupt thread by programs as well, but runs the risk of a system lockup or undesirable side effects if the Restore key is accidentally pressed, as this will trigger an inadvertent activation of the NMI thread.\nJoysticks, mice, and paddles.\nThe C64 retained the DE-9 joystick Atari joystick port from the VIC-20 and added another; any Atari-specification game controller can be used on a C64. The joysticks are read from the registers at $DC00 and $DC01, and most software is designed to use a joystick in port 2 for control rather than port 1, as the upper bits of $DC00 are used by the keyboard and an I/O conflict can result. Although it is possible to use Sega game pads on a C64, it is not recommended as the slightly different signal generated by them can damage the CIA chip. The SID chip's register $D419 is used to control paddles and is an analog input. Atari paddles are electrically compatible with the C64, but have different resistance values than Commodore's paddles, which means most software will not work properly with them. However, only a handful of games, mostly ones released early in the computer's life cycle, can use paddles. In 1986, Commodore released two mice for the C64 and C128, the 1350 and 1351. The 1350 is a digital device, read from the joystick registers (and can be used with any program supporting joystick input); while the 1351 is a true, analog potentiometer based, mouse, read with the SID's analog-to-digital converter.\nGraphics.\nThe graphics chip, VIC-II, features 16 colors, eight hardware sprites per scanline (enabling up to 112 sprites per PAL screen), scrolling capabilities, and two bitmap graphics modes.\nText modes.\nThe standard text mode features 40 columns, like most Commodore PET models; the built-in character encoding is not standard ASCII but PETSCII, an extended form of ASCII-1963. The KERNAL\u00a0ROM sets the VIC-II to a dark blue background on power up with a light blue text and border. Unlike the PET and VIC-20, the C64 uses \"fat\" double-width text as some early VIC-IIs had poor video quality that resulted in a fuzzy picture. Most screenshots show borders around the screen, which is a feature of the VIC-II chip. By utilizing interrupts to reset various hardware registers on precise timings it was possible to place graphics within the borders and thus use the full screen.\nThe C64 has a resolution of 320\u00d7200 pixels, consisting of a 40\u00d725 grid of 8\u00d78 character blocks. The C64 has 255 predefined character blocks, called PETSCII. The character set can be copied into RAM and altered by a programmer.\nThere are two colour modes, high resolution, with two colours available per character block (one foreground and one background) and multicolour with four colours per character block (three foreground and one background). In multicolour mode, attributes are shared between pixel pairs, so the effective visible resolution is 160\u00d7200 pixels. This is necessary since only 16\u00a0KB of memory is available for the VIC-II video processor.\nAs the C64 has a bitmapped screen, it is possible to draw each pixel individually. This is, however, \"very\" slow. Most programmers used techniques developed for earlier non-bitmapped systems, like the Commodore PET and TRS-80. A programmer redraws the character set and the video processor fills the screen block by block from the top left corner to the bottom right corner.\nTwo different types of animation are used: character block animation and hardware sprites.\nCharacter block animation.\nThe user draws a series of characters of a person walking, say, two in the middle of the block, and another two walking in and out of the block. Then the user sequences them so the character walks into the block and out again. Drawing a series of these and the user gets a person walking across the screen. By timing the redraw to occur when the television screen blanks out to restart drawing the screen there will be no flicker. For this to happen, the user programs the VIC-II that it generates a raster interrupt when the video flyback occurs. This is the technique used in the classic \"Space Invaders\" arcade game.\nHorizontal and vertical pixelwise scrolling of up to one character block is supported by two hardware scroll registers. Depending on timing, hardware scrolling affects the entire screen or just selected lines of character blocks. On a non-emulated C64, scrolling is glasslike and blur-free.\nHardware sprites.\nA sprite is a movable character which moves over an area of the screen, draws over the background and then redraws it after it moves. Note this is very different from character block animation, where the user is just flipping character blocks. On the C64, the VIC-II video processor handles most of the legwork in sprite emulation; the programmer simply defines the sprite and where they want it to go.\nThe C64 has two types of sprites, respecting their colour mode limitations. Hi-res sprites have one colour (one background and one foreground) and multicolour sprites three (one background and three foreground). Colour modes can be split or windowed on a single screen. Sprites can be doubled in size vertically and horizontally up to four times their size, but the pixel attributes are the same \u2013 the pixels become \"fatter\". There are 8 sprites in total and all 8 can be shown in each horizontal line concurrently. Sprites can move with glassy smoothness in front of and behind screen characters and other sprites.\nThe hardware sprites of a C64 can be displayed on either a bitmapped (high resolution) screen or, alternatively, on a text mode screen in conjunction with fast and smooth character block animation. In contrast, software emulated sprites found on systems without support for hardware sprites such as the Apple II and ZX Spectrum required a bitmapped screen. \nSprite-sprite and sprite-background collisions are detected in hardware and the VIC-II can be programmed to trigger an interrupt accordingly.\nSound.\nThe SID chip has three channels, each with its own ADSR envelope generator and filter capabilities. Ring modulation makes use of channel no. 3, to work with the other two channels. Bob Yannes developed the SID chip and later co-founded synthesizer company Ensoniq. Yannes criticized other contemporary computer sound chips as \"primitive, obviously\u00a0... designed by people who knew nothing about music\". Often the game music has become a hit of its own among C64 users. Well-known composers and programmers of game music on the C64 are Rob Hubbard, Jeroen Tel, Tim Follin, David Whittaker, Chris H\u00fclsbeck, Ben Daglish, Martin Galway, Kjell Nordb\u00f8 and David Dunn among many others. Due to the chip's three channels, chords are often played as arpeggios, coining the C64's characteristic lively sound. It was also possible to continuously update the master volume with sampled data to enable the playback of 4-bit digitized audio. As of 2008, it became possible to play four channel 8-bit audio samples, 2 SID channels and still use filtering.\nThere are two versions of the SID chip: the 6581 and the 8580. The MOS Technology 6581 was used in the original (\"breadbin\") C64s, the early versions of the 64C, and the Commodore 128. The 6581 was replaced with the MOS Technology 8580 in 1987. While the 6581 sound quality is a little crisper and many Commodore 64 fans say they prefer its sound, it lacks some versatility available in the 8580 \u2013 for example, the 8580 can mix all available waveforms on each channel, whereas the 6581 can only mix waveforms in a channel in a much more limited fashion. The main difference between the 6581 and the 8580 is the supply voltage. The 6581 uses a 12 volt supply\u2014the 8580, a 9 volt supply. A modification can be made to use the 6581 in a newer 64C board (which uses the 9 volt chip). The SID chip's distinctive sound has allowed it to retain a following long after its host computer was discontinued. A number of audio enthusiasts and companies have designed SID-based products as add-ons for the C64, x86 PCs, and standalone or Musical Instrument Digital Interface (MIDI) music devices such as the Elektron SidStation. These devices use chips taken from excess stock, or removed from used computers. In 2007, Timbaland's extensive use of the SidStation led to the plagiarism controversy for \"Block Party\" and \"Do It\" (written for Nelly Furtado).\nIn 1986, the Sound Expander was released for the Commodore 64. It was a sound module that contained a Yamaha YM3526 sound chip capable of FM synthesis. It was primarily intended for professional music production.\nHardware revisions.\nCommodore made many changes to the C64's hardware during its lifetime, sometimes causing compatibility issues. The computer's rapid development, and Commodore and Tramiel's focus on cost cutting instead of product testing, resulted in several defects that caused developers like Epyx to complain and required many revisions to fix; Charpentier said that \"not coming a little close to quality\" was one of the company's mistakes.\nCost reduction was the reason for most of the revisions. Reducing manufacturing costs was vitally important to Commodore's survival during the price war and leaner years of the 16-bit era. The C64's original (NMOS based) motherboard went through two major redesigns and numerous sub-revisions, exchanging positions of the VIC-II, SID and PLA chips. Initially, a large portion of the cost was eliminated by reducing the number of discrete components, such as diodes and resistors, which enabled the use of a smaller printed circuit board. There were 16 total C64 motherboard revisions, aimed at simplifying and reducing manufacturing costs. Some board revisions were exclusive to PAL regions. All C64 motherboards were manufactured in Hong Kong.\nIC locations changed frequently on each motherboard revision, as did the presence or lack thereof of the metal RF shield around the VIC-II. PAL boards often had aluminized cardboard instead of a metal shield. The SID and VIC-II are socketed on all boards; however, the other ICs may be either socketed or soldered. The first production C64s, made in 1982 to early 1983, are known as \"silver label\" models due to the case sporting a silver-colored \"Commodore\" logo. The power LED had a separate silver badge around it reading \"64\". These machines also have only a 5-pin video cable and cannot output S-video. In late 1982, Commodore introduced the familiar \"rainbow badge\" case, but many machines produced into early 1983 also used silver label cases until the existing stock of them was used up. In the spring of 1983, the original 326298 board was replaced by the 250407 motherboard which sported an 8-pin video connector and added S-video support for the first time. This case design was used until the C64C appeared in 1986. All ICs switched to using plastic shells while the silver label C64s had some ceramic ICs, notably the VIC-II. The case is made from ABS plastic which may become brown with time. This can be reversed by using a process known as \"retrobright\".\nICs.\nThe VIC-II was manufactured with 5 micrometer NMOS technology and was clocked at either 17.73447 MHz (PAL) or 14.31818 MHz (NTSC). Internally, the clock was divided down to generate the dot clock (about 8\u00a0MHz) and the two-phase system clocks (about 1\u00a0MHz; the exact pixel and system clock speeds are slightly different between NTSC and PAL machines). At such high clock rates, the chip generated a lot of heat, forcing MOS Technology to use a ceramic dual in-line package called a \"CERDIP\". The ceramic package was more expensive, but it dissipated heat more effectively than plastic.\nAfter a redesign in 1983, the VIC-II was encased in a plastic dual in-line package, which reduced costs substantially, but it did not totally eliminate the heat problem. Without a ceramic package, the VIC-II required the use of a heat sink. To avoid extra cost, the metal RF shielding doubled as the heat sink for the VIC, although not all units shipped with this type of shielding. Most C64s in Europe shipped with a cardboard RF shield, coated with a layer of metal foil. The effectiveness of the cardboard was highly questionable and, worse still, it acted as an insulator, blocking airflow which trapped heat generated by the SID, VIC, and PLA chips. The SID was originally manufactured using NMOS at 7 micrometers and in some areas 6 micrometers. The prototype SID and some very early production models featured a ceramic dual in-line package, but unlike the VIC-II, these are extremely rare as the SID was encased in plastic when production started in early 1982.\nMotherboard.\nIn 1986, Commodore released the last revision to the classic C64 motherboard. It was otherwise identical to the 1984 design, except for the two 64 kilobit \u00d7 4 bit DRAM chips that replaced the original eight 64 kilobit \u00d7 1\u00a0bit ICs. After the release of the Commodore 64C, MOS Technology began to reconfigure the original C64's chipset to use HMOS production technology. The main benefit of using HMOS was that it required less voltage to drive the IC, which consequently generates less heat. This enhanced the overall reliability of the SID and VIC-II. The new chipset was renumbered to 85xx to reflect the change to HMOS.\nIn 1987, Commodore released a 64C variant with a highly redesigned motherboard commonly known as a \"short board\". The new board used the new HMOS chipset, featuring a new 64-pin PLA chip. The new \"SuperPLA\", as it was dubbed, integrated many discrete components and transistor\u2013transistor logic (TTL) chips. In the last revision of the 64C motherboard, the 2114 4-bit-wide color RAM was integrated into the SuperPLA.\nPower supply.\nThe C64 used an external power supply, a conventional transformer with multiple tappings (as opposed to switch mode, the type now used on PC power supplies). It was encased in an epoxy resin gel, which discouraged tampering but tended to increase the heat level during use. The design saved space within the computer's case and allowed international versions to be more easily manufactured. The 1541-II and 1581 disk drives, along with various third-party clones, also come with their own external power supply \"bricks\", as did most peripherals leading to a \"spaghetti\" of cables and the use of numerous double adapters by users.\nCommodore power supplies often failed sooner than expected. The computer reportedly had a 30% return rate in late 1983, compared to the 5\u20137% the industry considered acceptable. \"Creative Computing\" reported four working computers out of seven C64s. Malfunctioning power bricks were particularly notorious for damaging the RAM chips. Due to their higher density and single supply (+5V), they had less tolerance for an overvoltage condition. The usually failing voltage regulator could be replaced by piggy-backing a new regulator onto the board and fitting a heat sink on top.\nThe original PSU included on early 1982\u201383 machines had a 5-pin connector that could accidentally be plugged into the video output of the computer. To prevent the user from making this damaging mistake, Commodore changed the plug design on 250407 motherboards to a 3-pin connector in 1984. Commodore later changed the design yet again, omitting the resin gel in order to reduce costs. The follow-on model, the Commodore 128, used a larger, improved power supply that included a fuse. The power supply that came with the Commodore REU was similar to that of the Commodore 128's unit, providing an upgrade for customers who purchased that accessory.\nSpecifications.\nInternal hardware.\nCreative Micro Designs also produced a 2\u00a0MB REU for the C64 and C128, called the 1750\u00a0XL. The technology actually supported up to 16\u00a0MB, but 2\u00a0MB was the biggest one officially made. Expansions of up to 16\u00a0MB were also possible via the CMD SuperCPU.\nInput/output (I/O) ports and power supply.\nThe 9 volt AC is used to supply power via a charge pump to the SID sound generator chip, provide 6.8V via a rectifier to the cassette motor, a \"0\" pulse for every positive half wave to the time-of-day (TOD) input on the CIA chips, and 9 volts AC directly to the user-port. Thus, as a minimum, a 12 V square wave is required. But a 9 V sine wave is preferred.\nMemory map.\nNote that even if an I/O chip like the VIC-II only uses 64 positions in the memory address space, it will occupy 1,024 addresses because some address bits are left undecoded.\nManufacturing cost.\nVertical integration was the key to keeping Commodore 64 production costs low. At the introduction in 1982, the production cost was US$135 and the retail price US$595. In 1985, the retail price went down to US$149 (US$ today) and the production costs were believed to be somewhere between US$35\u201350 ( Commodore would not confirm this cost figure. Dougherty of the Berkeley Softworks estimated the costs of the Commodore 64 parts based on his experience at Mattel and Imagic.\nTo lower costs, TTL chips were replaced with less expensive custom chips and ways to increase the yields on the sound and graphics chips were found. The video chip 6567 had the ceramic package replaced with plastic but heat dissipation demanded a redesign of the chip and the development of a plastic package that can dissipate heat as well as ceramic.\nClones.\nClones are computers that imitate C64 functions. In the middle of 2004, after an absence from the marketplace of more than 10 years, PC manufacturer Tulip Computers BV (owners of the Commodore brand since 1997) announced the C64 Direct-to-TV (C64DTV), a joystick-based TV game based on the C64 with 30 video games built into ROM. Designed by Jeri Ellsworth, a self-taught computer designer who had earlier designed the modern C-One C64 implementation, the C64DTV was similar in concept to other mini-consoles based on the Atari 2600 and Intellivision, which had gained modest success earlier in the decade. The product was advertised on QVC in the United States for the 2004 holiday season. By modifying the circuit board, it is possible to attach C1541 floppy disk drives, a second joystick, and PS/2 keyboards to these units, which gives the DTV devices nearly all the capabilities of a full Commodore 64. The DTV hardware is also used in the mini-console \"Hummer\", sold at RadioShack in mid-2005.\nIn 2015, a Commodore 64 compatible motherboard was produced by Individual Computers. Dubbed the \"C64 Reloaded\", it is a modern redesign of the Commodore 64 motherboard revision 250466 with a few new features. The motherboard itself is designed to be placed in an empty C64 or C64C case already owned by the user. Produced in limited quantities, models of this Commodore 64 \"clone\" sport either machined or ZIF sockets in which the custom C64 chips would be placed. The board also contains jumpers to accept different revisions of the VIC-II and SID chips, as well as the ability to jumper between the analogue video system modes PAL and NTSC. The motherboard contains several innovations, including selection via the RESTORE key of multiple KERNAL and character ROMs, built-in reset toggle on the power switch, and an S-video socket to replace the original TV modulator. The motherboard is powered by a DC-to-DC converter that uses a single power input of 12 V DC from a mains adapter to power the unit rather than the original and failure-prone Commodore 64 power supply brick.\nNewer compatible hardware.\nAs of 2008, C64 enthusiasts still develop new hardware, including Ethernet cards, specially adapted hard disks and flash card interfaces (sd2iec). In 2022 a product called A-SID was introduced that turns the C-64 into a WAH effect.\nBrand reuse.\nIn 1998, the C64 brand was reused for the \"Web.it Internet Computer\", a low-powered Internet-oriented all-in-one x86 PC running MS-DOS and Windows 3.1. It uses a AMD \u00c9lan SC400 SoC with 16 MB of RAM, a 3.5\" floppy disk drive 56k-modem and PCMCIA. Despite its \"Commodore 64\" nameplate, the \"C64 Web.it\" is not directly compatible with the original (except via included emulation software), nor does it share its appearance. PC clones branded as C64x sold by Commodore USA, LLC, a company licensing the Commodore trademark, began shipping in June 2011. The C64x has a case resembling the original C64 computer, but \u2013 as with the \"Web.it\" \u2013 it is based on a x86 architecture and is not compatible with the Commodore 64 on either hardware or software levels.\nVirtual Console.\nSeveral Commodore 64 games were released on the Nintendo Wii's Virtual Console service in Europe and North America only. The games were unlisted from the service as of August 2013 for unknown reasons.\nTHEC64 and THEC64 Mini.\nTHEC64 Mini is an unofficial Linux-based console that emulates the Commodore 64, released in 2018 by UK-based Retro Games. The console takes the form of a decorative half-scale Commodore 64 with two USB and one HDMI port, plus a mini USB connection to power the system. The console's decorative keyboard is non-functional \u2013 the system is controlled via the included THEC64 joystick or a separate USB keyboard. It is possible to load new software ROMs into the console, which uses emulator x64 (as part of VICE) to run software, and has a built-in graphical operating system.\nThe full-size THEC64 was released in 2019 in Europe and Australia, and was scheduled for release in November 2020 in the North American market. The console and built-in keyboard are built to scale with the original Commodore 64, including a functional keyboard. Enhancements include VIC-20 emulation, four USB ports, and an upgraded joystick.\nNeither product features any of Commodore's trademarks \u2013 the Commodore key on the original keyboard is replaced with a THEC64 key, and Retro Games can call neither product a \"C64\" \u2013 although the system ROMs are licensed from Cloanto Corporation. The consoles can be switched between \"carousel mode\" for accessing the built-in game library, and \"classic mode\" in which the machine operates similarly to a traditional Commodore 64. USB storage can be used to hold disk, cartridge and tape images for use with the machine.\nEmulators.\nCommodore 64 emulators include the open source VICE, Hoxs64, and CCS64. An iPhone app was also released with a compilation of C64 ports.\nFootnotes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7294", "revid": "13286072", "url": "https://en.wikipedia.org/wiki?curid=7294", "title": "Cartography", "text": "Study and practice of making maps\nCartography (; from , \"papyrus, sheet of paper, map\"; and , \"write\") is the study and practice of making and using maps. Combining science, aesthetics and technique, cartography builds on the premise that reality (or an imagined reality) can be modeled in ways that communicate spatial information effectively.\nThe fundamental objectives of traditional cartography are to:\nModern cartography constitutes many theoretical and practical foundations of geographic information systems (GIS) and geographic information science (GISc).\nHistory.\nAncient times.\nWhat is the earliest known map is a matter of some debate, both because the term \"map\" is not well-defined and because some artifacts that might be maps might actually be something else. A wall painting that might depict the ancient Anatolian city of \u00c7atalh\u00f6y\u00fck (previously known as Catal Huyuk or \u00c7atal H\u00fcy\u00fck) has been dated to the late 7th millennium BCE. Among the prehistoric alpine rock carvings of Mount Bego (France) and Valcamonica (Italy), dated to the 4th millennium BCE, geometric patterns consisting of dotted rectangles and lines are widely interpreted in archaeological literature as depicting cultivated plots. Other known maps of the ancient world include the Minoan \"House of the Admiral\" wall painting from c.\u20091600 BCE, showing a seaside community in an oblique perspective, and an engraved map of the holy Babylonian city of Nippur, from the Kassite period (14th\u00a0\u2013 12th centuries BCE). The oldest surviving world maps are from 9th century BCE Babylonia. One shows Babylon on the Euphrates, surrounded by Assyria, Urartu and several cities, all, in turn, surrounded by a \"bitter river\" (Oceanus). Another depicts Babylon as being north of the center of the world.\nThe ancient Greeks and Romans created maps from the time of Anaximander in the 6th century BCE. In the 2nd century CE, Ptolemy wrote his treatise on cartography, Geographia. This contained Ptolemy's world map \u2013 the world then known to Western society \"(Ecumene)\". As early as the 8th century, Arab scholars were translating the works of the Greek geographers into Arabic.\nIn ancient China, geographical literature dates to the 5th century BCE. The oldest extant Chinese maps come from the State of Qin, dated back to the 4th century BCE, during the Warring States period. In the book of the \"Xin Yi Xiang Fa Yao\", published in 1092 by the Chinese scientist Su Song, a star map on the equidistant cylindrical projection. Although this method of charting seems to have existed in China even before this publication and scientist, the greatest significance of the star maps by Su Song is that they represent the oldest existent star maps in printed form.\nEarly forms of cartography of India included depictions of the pole star and surrounding constellations. These charts may have been used for navigation.\nMiddle Ages and Renaissance.\n (\"maps of the world\") are the medieval European maps of the world. About 1,100 of these are known to have survived: of these, some 900 are found illustrating manuscripts and the remainder exist as stand-alone documents.\nThe Arab geographer Muhammad al-Idrisi produced his medieval atlas \"Tabula Rogeriana (Book of Roger)\" in 1154. By combining the knowledge of Africa, the Indian Ocean, Europe, and the Far East (which he learned through contemporary accounts from Arab merchants and explorers) with the information he inherited from the classical geographers, he was able to write detailed descriptions of a multitude of countries. Along with the substantial text he had written, he created a world map influenced mostly by the Ptolemaic conception of the world, but with significant influence from multiple Arab geographers. It remained the most accurate world map for the next three centuries. The map was divided into seven climatic zones, with detailed descriptions of each zone. As part of this work, a smaller, circular map was made depicting the south on top and Arabia in the center. Al-Idrisi also made an estimate of the circumference of the world, accurate to within 10%.\nIn the Age of Exploration, from the 15th century to the 17th century, European cartographers both copied earlier maps (some of which had been passed down for centuries) and drew their own, based on explorers' observations and new surveying techniques. The invention of the magnetic compass, telescope and sextant enabled increasing accuracy. In 1492, Martin Behaim, a German cartographer, made the oldest extant globe of the Earth.\nIn 1507, Martin Waldseem\u00fcller produced a globular world map and a large 12-panel world wall map (\"Universalis Cosmographia\") bearing the first use of the name \"America\". Portuguese cartographer Diego Ribero was the author of the first known planisphere with a graduated Equator (1527). Italian cartographer Battista Agnese produced at least 71 manuscript atlases of sea charts. Johannes Werner refined and promoted the Werner projection. This was an equal-area, heart-shaped world map projection (generally called a cordiform projection) which was used in the 16th and 17th centuries. Over time, other iterations of this map type arose; most notable are the sinusoidal projection and the Bonne projection. The Werner projection places its standard parallel at the North Pole; a sinusoidal projection places its standard parallel at the equator; and the Bonne projection is intermediate between the two.\nIn 1569, mapmaker Gerardus Mercator first published a map based on his Mercator projection, which uses equally-spaced parallel vertical lines of longitude and parallel latitude lines spaced farther apart as they get farther away from the equator. By this construction, courses of constant bearing are conveniently represented as straight lines for navigation. The same property limits its value as a general-purpose world map because regions are shown as increasingly larger than they actually are the further from the equator they are. Mercator is also credited as the first to use the word \"atlas\" to describe a collection of maps. In the later years of his life, Mercator resolved to create his Atlas, a book filled with many maps of different regions of the world, as well as a chronological history of the world from the Earth's creation by God until 1568. He was unable to complete it to his satisfaction before he died. Still, some additions were made to the Atlas after his death, and new editions were published after his death.\nIn the Renaissance, maps were used to impress viewers and establish the owner's reputation as sophisticated, educated, and worldly. Because of this, towards the end of the Renaissance, maps were displayed with equal importance of painting, sculptures, and other pieces of art. In the sixteenth century, maps were becoming increasingly available to consumers through the introduction of printmaking, with about 10% of Venetian homes having some sort of map by the late 1500s.\nThere were three main functions of maps in the Renaissance:\nIn medieval times, written directions of how to get somewhere were more common than the use of maps. With the Renaissance, cartography began to be seen as a metaphor for power. Political leaders could lay claim on territories through the use of maps, and this was greatly aided by the religious and colonial expansion of Europe. The most commonly mapped places during the Renaissance were the Holy Land and other religious places.\nIn the late 1400s to the late 1500s, Rome, Florence, and Venice dominated map-making and trade. It started in Florence in the mid- to late 1400s. Map trade quickly shifted to Rome and Venice but then was overtaken by atlas makers in the late 16th century. Map publishing in Venice was completed with humanities and book publishing in mind, rather than just informational use.\nPrinting technology.\nThere were two main printmaking technologies in the Renaissance: woodcut and copper-plate intaglio, referring to the medium used to transfer the image onto paper.\nIn woodcut, the map image is created as a relief chiseled from medium-grain hardwood. The areas intended to be printed are inked and pressed against the sheet. Being raised from the rest of the block, the map lines cause indentations in the paper that can often be felt on the back of the map. There are advantages to using relief to make maps. For one, a printmaker doesn't need a press because the maps could be developed as rubbings. Woodblock is durable enough to be used many times before defects appear. Existing printing presses can be used to create the prints rather than having to create a new one. On the other hand, it is hard to achieve fine detail with the relief technique. Inconsistencies in linework are more apparent in woodcut than in intaglio. To improve quality in the late fifteenth century, a style of relief craftsmanship developed using fine chisels to carve the wood, rather than the more commonly used knife.\nIn intaglio, lines are engraved into workable metals, typically copper but sometimes brass. The engraver spreads a thin sheet of wax over the metal plate and uses ink to draw the details. Then, the engraver traces the lines with a stylus to etch them into the plate beneath. The engraver can also use styli to prick holes along the drawn lines, trace along them with colored chalk, and then engrave the map.\u00a0Lines going in the same direction are carved at the same time, and then the plate is turned to carve lines going in a different direction. To print from the finished plate, ink is spread over the metal surface and scraped off such that it remains only in the etched channels. Then the plate is pressed forcibly against the paper so that the ink in the channels is transferred to the paper. The pressing is so forceful that it leaves a \"plate mark\" around the border of the map at the edge of the plate, within which the paper is depressed compared to the margins. Copper and other metals were expensive at the time, so the plate was often reused for new maps or melted down for other purposes.\nWhether woodcut or intaglio, the printed map is hung out to dry. Once dry, it is usually placed in another press to flatten the paper. Any type of paper that was available at the time could be used to print the map on, but thicker paper was more durable.\nBoth relief and intaglio were used about equally by the end of the fifteenth century.\nLettering.\nLettering in mapmaking is important for denoting information. Fine lettering is difficult in woodcut, where it often turned out square and blocky, contrary to the stylized, rounded writing style popular in Italy at the time. To improve quality, mapmakers developed fine chisels to carve the relief. Intaglio lettering did not suffer the troubles of a coarse medium and so was able to express the looping cursive that came to be known as cancellaresca. There were custom-made reverse punches that were also used in metal engraving alongside freehand lettering.\nColor.\nThe first use of color in map-making cannot be narrowed down to one reason. There are arguments that color started as a way to indicate information on the map, with aesthetics coming second. There are also arguments that color was first used on maps for aesthetics but then evolved into conveying information. Either way, many maps of the Renaissance left the publisher without being colored, a practice that continued all the way into the 1800s. However, most publishers accepted orders from their patrons to have their maps or atlases colored if they wished. Because all coloring was done by hand, the patron could request simple, cheap color, or more expensive, elaborate color, even going so far as silver or gold gilding. The simplest coloring was merely outlines, such as of borders and along rivers. Wash color meant painting regions with inks or watercolors. Limning meant adding silver and gold leaf to the map to illuminate lettering, heraldic arms, or other decorative elements.\nEarly-Modern Period.\nThe Early Modern Period saw the convergence of cartographical techniques across Eurasia and the exchange of mercantile mapping techniques via the Indian Ocean.\nIn the early seventeenth century, the Selden map was created by a Chinese cartographer. Historians have put its date of creation around 1620, but there is debate in this regard. This map's significance draws from historical misconceptions of East Asian cartography, the main one being that East Asians didn't do cartography until Europeans arrived. The map's depiction of trading routes, a compass rose, and scale bar points to the culmination of many map-making techniques incorporated into Chinese mercantile cartography.\nIn 1689, representatives of the Russian tsar and Qing Dynasty met near the border town of Nerchinsk, which was near the disputed border of the two powers, in eastern Siberia. The two parties, with the Qing negotiation party bringing Jesuits as intermediaries, managed to work a treaty which placed the Amur River as the border between the Eurasian powers, and opened up trading relations between the two. This treaty's significance draws from the interaction between the two sides, and the intermediaries who were drawn from a wide variety of nationalities.\nThe Enlightenment.\nMaps of the Enlightenment period practically universally used copper plate intaglio, having abandoned the fragile, coarse woodcut technology. Use of map projections evolved, with the double hemisphere being very common and Mercator's prestigious navigational projection gradually making more appearances.\nDue to the paucity of information and the immense difficulty of surveying during the period, mapmakers frequently plagiarized material without giving credit to the original cartographer. For example, a famous map of North America known as the \"Beaver Map\" was published in 1715 by Herman Moll. This map is a close reproduction of a 1698 work by Nicolas de Fer. De Fer, in turn, had copied images that were first printed in books by Louis Hennepin, published in 1697, and Fran\u00e7ois Du Creux, in 1664. By the late 18th century, mapmakers often credited the original publisher with something along the lines of, \"After [the original cartographer]\" in the map's title or cartouche.\nModern period.\nIn cartography, technology has continually changed in order to meet the demands of new generations of mapmakers and map users. The first maps were produced manually, with brushes and parchment; so they varied in quality and were limited in distribution. The advent of magnetic devices, such as the compass and much later, magnetic storage devices, allowed for the creation of far more accurate maps and the ability to store and manipulate them digitally.\nAdvances in mechanical devices such as the printing press, quadrant, and vernier allowed the mass production of maps and the creation of accurate reproductions from more accurate data. Hartmann Schedel was one of the first cartographers to use the printing press to make maps more widely available. Optical technology, such as the telescope, sextant, and other devices that use telescopes, allowed accurate land surveys and allowed mapmakers and navigators to find their latitude by measuring angles to the North Star at night or the Sun at noon.\nAdvances in photochemical technology, such as the lithographic and photochemical processes, make possible maps with fine details, which do not distort in shape and which resist moisture and wear. This also eliminated the need for engraving, which further speeded up map production.\nIn the 20th century, aerial photography, satellite imagery, and remote sensing provided efficient, precise methods for mapping physical features, such as coastlines, roads, buildings, watersheds, and topography. The United States Geological Survey has devised multiple new map projections, notably the Space Oblique Mercator for interpreting satellite ground tracks for mapping the surface. The use of satellites and space telescopes now allows researchers to map other planets and moons in outer space. Advances in electronic technology ushered in another revolution in cartography: ready availability of computers and peripherals such as monitors, plotters, printers, scanners (remote and document) and analytic stereo plotters, along with computer programs for visualization, image processing, spatial analysis, and database management, have democratized and greatly expanded the making of maps. The ability to superimpose spatially located variables onto existing maps has created new uses for maps and new industries to explore and exploit these potentials. See also digital raster graphic.\nIn the early years of the new millennium, three key technological advances transformed cartography: the removal of Selective Availability in the Global Positioning System (GPS) in May 2000, which improved locational accuracy for consumer-grade GPS receivers to within a few metres; the invention of OpenStreetMap in 2004, a global digital counter-map that allowed anyone to contribute and use new spatial data without complex licensing agreements; and the launch of Google Earth in 2005 as a development of the virtual globe EarthViewer 3D (2004), which revolutionised access to satellite and aerial imagery. These advances brought more accuracy to geographical and location-based data and widened the range of applications for cartography, for example in the development of satnav devices.\nToday most commercial-quality maps are made using software of three main types: CAD, GIS and specialized illustration software. Spatial information can be stored in a database, from which it can be extracted on demand. These tools lead to increasingly dynamic, interactive maps that can be manipulated digitally.\nField-rugged computers, GPS, and laser rangefinders make it possible to create maps directly from measurements made on site.\nDeconstruction.\nThere are technical and cultural aspects to producing maps. In this sense, maps can sometimes be said to be biased. The study of bias, influence, and agenda in making a map is what comprise a map's deconstruction. A central tenet of deconstructionism is that maps have power. Other assertions are that maps are inherently biased and that we search for metaphor and rhetoric in maps.\nIt is claimed that the Europeans promoted an \"epistemological\" understanding of the map as early as the 17th century. An example of this understanding is that \"[European reproduction of terrain on maps] reality can be expressed in mathematical terms; that systematic observation and measurement offer the only route to cartographic truth\u2026\".\nA common belief is that science heads in a direction of progress, and thus leads to more accurate representations of maps. In this belief, European maps must be superior to others, which necessarily employed different map-making skills. \"There was a 'not cartography' land where lurked an army of inaccurate, heretical, subjective, valuative, and ideologically distorted images. Cartographers developed a 'sense of the other' in relation to nonconforming maps.\"\nDepictions of Africa are a common target of deconstructionism. According to deconstructionist models, cartography was used for strategic purposes associated with imperialism and as instruments and representations of power during the conquest of Africa. The depiction of Africa and the low latitudes in general on the Mercator projection has been interpreted as imperialistic and as symbolic of subjugation due to the diminished proportions of those regions compared to higher latitudes where the European powers were concentrated.\nMaps furthered imperialism and colonization of Africa in practical ways by showing basic information like roads, terrain, natural resources, settlements, and communities. Through this, maps made European commerce in Africa possible by showing potential commercial routes and made natural resource extraction possible by depicting locations of resources. Such maps also enabled military conquests and made them more efficient, and imperial nations further used them to put their conquests on display. These same maps were then used to cement territorial claims, such as at the Berlin Conference of 1884\u20131885.\nBefore 1749, maps of the African continent had African kingdoms drawn with assumed or contrived boundaries, with unknown or unexplored areas having drawings of animals, imaginary physical geographic features, and descriptive texts. In 1748, Jean B. B. d'Anville created the first map of the African continent that had blank spaces to represent the unknown territory.\nMap types.\nGeneral vs. thematic cartography.\nIn understanding basic maps, the field of cartography can be divided into two general categories: general cartography and thematic cartography. General cartography involves those maps that are constructed for a general audience and thus contain a variety of features. General maps exhibit many reference and location systems and often are produced in a series. For example, the 1:24,000 scale topographic maps of the United States Geological Survey (USGS) are a standard as compared to the 1:50,000 scale Canadian maps. The government of the UK produces the classic 1:50,000 (replacing the older 1\u00a0inch to 1 mile) \"Ordnance Survey\" maps of the entire UK and with a range of correlated larger- and smaller-scale maps of great detail. Many private mapping companies have also produced thematic map series.\nThematic cartography involves maps of specific geographic themes, oriented toward specific audiences. A couple of examples might be a dot map showing corn production in Indiana or a shaded area map of Ohio counties, divided into numerical choropleth classes. As the volume of geographic data has exploded over the last century, thematic cartography has become increasingly useful and necessary to interpret spatial, cultural and social data.\nA third type of map is known as an \"orienteering,\" or special purpose map. This type of map falls somewhere between thematic and general maps. They combine general map elements with thematic attributes in order to design a map with a specific audience in mind. Oftentimes, the type of audience an orienteering map is made for is in a particular industry or occupation. An example of this kind of map would be a municipal utility map.\nTopographic vs. topological.\nA topographic map is primarily concerned with the topographic description of a place, including (especially in the 20th and 21st centuries) the use of contour lines showing elevation. Terrain or relief can be shown in a variety of ways (see Cartographic relief depiction). In the present era, one of the most widespread and advanced methods used to form topographic maps is to use computer software to generate digital elevation models which show shaded relief. Before such software existed, cartographers had to draw shaded relief by hand. One cartographer who is respected as a master of hand-drawn shaded relief is the Swiss professor Eduard Imhof whose efforts in hill shading were so influential that his method became used around the world despite it being so labor-intensive.\nA topological map is a very general type of map, the kind one might sketch on a napkin. It often disregards scale and detail in the interest of clarity of communicating specific route or relational information. Beck's London Underground map is an iconic example. Although the most widely used map of \"The Tube,\" it preserves little of reality: it varies scale constantly and abruptly, it straightens curved tracks, and it contorts directions. The only topography on it is the River Thames, letting the reader know whether a station is north or south of the river. That and the topology of station order and interchanges between train lines are all that is left of the geographic space. Yet those are all a typical passenger wishes to know, so the map fulfills its purpose.\nMap design.\nModern technology, including advances in printing, the advent of Geographic information systems and Graphics software, and the Internet, has vastly simplified the process of map creation and increased the palette of design options available to cartographers. This has led to a decreased focus on production skill, and an increased focus on quality design, the attempt to craft maps that are both aesthetically pleasing and practically useful for their intended purposes.\nMap purpose and audience.\nA map has a purpose and an audience. Its purpose may be as broad as teaching the major physical and political features of the entire world, or as narrow as convincing a neighbor to move a fence. The audience may be as broad as the general public or as narrow as a single person. Mapmakers use design principles to guide them in constructing a map that is effective for its purpose and audience.\nCartographic process.\n The cartographic process spans many stages, starting from conceiving the need for a map and extending all the way through its consumption by an audience. Conception begins with a real or imagined environment. As the cartographer gathers information about the subject, they consider how that information is structured and how that structure should inform the map's design. Next, the cartographers experiment with generalization, symbolization, typography, and other map elements to find ways to portray the information so that the map reader can interpret the map as intended. Guided by these experiments, the cartographer settles on a design and creates the map, whether in physical or electronic form. Once finished, the map is delivered to its audience. The map reader interprets the symbols and patterns on the map to draw conclusions and perhaps to take action. By the spatial perspectives they provide, maps help shape how we view the world.\nAspects of map design.\nDesigning a map involves bringing together a number of elements and making a large number of decisions. The elements of design fall into several broad topics, each of which has its own theory, its own research agenda, and its own best practices. That said, there are synergistic effects between these elements, meaning that the overall design process is not just working on each element one at a time, but an iterative feedback process of adjusting each to achieve the desired gestalt.\nCartographic errors.\nSome maps contain deliberate errors or distortions, either as propaganda or as a \"watermark\" to help the copyright owner identify infringement if the error appears in competitors' maps. The latter often come in the form of nonexistent, misnamed, or misspelled \"trap streets\". Other names and forms for this are paper towns, fictitious entries, and copyright easter eggs.\nAnother motive for deliberate errors is cartographic \"vandalism\": a mapmaker wishing to leave their mark on the work. Mount Richard, for example, was a fictitious peak on the Rocky Mountains' continental divide that appeared on a Boulder County, Colorado map in the early 1970s. It is believed to be the work of draftsman Richard Ciacci. The fiction was not discovered until two years later.\nSandy Island in New Caledonia is an example of a fictitious location that stubbornly survives, reappearing on new maps copied from older maps while being deleted from other new editions.\nWith the emergence of the internet and Web mapping, technologies allow for the creation and distribution of maps by people without proper cartographic training are readily available. This has led to maps that ignore cartographic conventions and are potentially misleading.\nProfessional and learned societies.\nProfessional and learned societies include:\nAcademic journals.\nThe above societies publish a number of academic journals:\nOther journals related to cartography, as well as GIS and GISc, include:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nMapmaking\nHistory\nMeanings"}
{"id": "7295", "revid": "45079649", "url": "https://en.wikipedia.org/wiki?curid=7295", "title": "Consumption", "text": "Consumption may refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "7296", "revid": "45455215", "url": "https://en.wikipedia.org/wiki?curid=7296", "title": "Cardiac glycoside", "text": "Class of organic compounds\nCardiac glycosides are a class of organic compounds that increase the output force of the heart and decrease its rate of contractions by inhibiting the cellular sodium-potassium ATPase pump. Their beneficial medical uses are as treatments for congestive heart failure and cardiac arrhythmias; however, their relative toxicity prevents them from being widely used. Most commonly found as secondary metabolites in several plants such as foxglove plants, these compounds nevertheless have a diverse range of biochemical effects regarding cardiac cell function and have also been suggested for use in cancer treatment.\nClassification.\nGeneral structure.\nThe general structure of a cardiac glycoside consists of a steroid molecule attached to a sugar (glycoside) and an R group. The steroid nucleus consists of four fused rings to which other functional groups such as methyl, hydroxyl, and aldehyde groups can be attached to influence the overall molecule's biological activity. Cardiac glycosides also vary in the groups attached at either end of the steroid. Specifically, different sugar groups attached at the sugar end of the steroid can alter the molecule's solubility and kinetics; however, the lactone moiety at the R group end only serves a structural function.\nIn particular, the structure of the ring attached at the R end of the molecule allows it to be classified as either a cardenolide or bufadienolide. Cardenolides differ from bufadienolides due to the presence of an \"enolide,\" a five-membered ring with a single double bond, at the lactone end. Bufadienolides, on the other hand, contain a \"dienolide,\" a six-membered ring with two double bonds, at the lactone end. While compounds of both groups can be used to influence the cardiac output of the heart, cardenolides are more commonly used medicinally, primarily due to the widespread availability of the plants from which they are derived.\nClassification.\nCardiac glycosides can be more specifically categorized based on the plant they are derived from, as in the following list. For example, cardenolides have been primarily derived from the foxglove plants \"Digitalis purpurea\" and \"Digitalis lanata\", while bufadienolides have been derived from the venom of the cane toad \"Bufo marinus\", from which they receive the \"bufo\" portion of their name. Below is a list of organisms from which cardiac glycosides can be derived.\nMechanism of action.\nCardiac glycosides affect the sodium-potassium ATPase pump in cardiac muscle cells to alter their function. Normally, these sodium-potassium pumps move potassium ions in and sodium ions out. Cardiac glycosides, however, inhibit this pump by stabilizing it in the E2-P transition state, so that sodium cannot be extruded: intracellular sodium concentration therefore increases. With regard to potassium ion movement, because both cardiac glycosides and potassium compete for binding to the ATPase pump, changes in extracellular potassium concentration can potentially lead to altered drug efficacy. Nevertheless, by carefully controlling the dosage, such adverse effects can be avoided. Continuing on with the mechanism, raised intracellular sodium levels inhibit the function of a second membrane ion exchanger, NCX, which is responsible for pumping calcium ions out of the cell and sodium ions in at a ratio of 3Na+/Ca2+. Thus, calcium ions are also not extruded and will begin to build up inside the cell as well.\nThe disrupted calcium homeostasis and increased cytoplasmic calcium concentrations cause increased calcium uptake into the sarcoplasmic reticulum (SR) via the SERCA2 transporter. Raised calcium stores in the SR allow for greater calcium release on stimulation, so the myocyte can achieve faster and more powerful contraction by cross-bridge cycling. The refractory period of the AV node is increased, so cardiac glycosides also function to decrease heart rate. For example, the ingestion of digoxin leads to increased cardiac output and decreased heart rate without significant changes in blood pressure; this quality allows it to be widely used medicinally in the treatment of cardiac arrhythmias.\nNon-cardiac uses.\nCardiac glycosides were identified as senolytics: they can selectively eliminate senescent cells which are more sensitive to the ATPase-inhibiting action due to cell membrane changes.\nClinical significance.\nCardiac glycosides have long served as the main medical treatment to congestive heart failure and cardiac arrhythmia, due to their effects of increasing the force of muscle contraction while reducing heart rate. Heart failure is characterized by an inability to pump enough blood to support the body, possibly due to a decrease in the volume of the blood or its contractile force. Treatments for the condition thus focus on lowering blood pressure, so that the heart does not have to exert as much force to pump the blood, or directly increasing the heart's contractile force, so that the heart can overcome the higher blood pressure. Cardiac glycosides, such as the commonly used digoxin and digitoxin, deal with the latter, due to their positive inotropic activity. On the other hand, cardiac arrhythmia are changes in heart rate, whether faster (tachycardia) or slower (bradycardia). Medicinal treatments for this condition work primarily to counteract tachycardia or atrial fibrillation by slowing down heart rate, as done by cardiac glycosides.\nNevertheless, due to questions of toxicity and dosage, cardiac glycosides have been replaced with synthetic drugs such as ACE inhibitors and beta blockers and are no longer used as the primary medical treatment for such conditions. Depending on the severity of the condition, though, they may still be used in conjunction with other treatments.\nToxicity.\nFrom ancient times, humans have used cardiac-glycoside-containing plants and their crude extracts as arrow coatings, homicidal or suicidal aids, rat poisons, heart tonics, diuretics and emetics, primarily due to the toxic nature of these compounds. Thus, though cardiac glycosides have been used for their medicinal function, their toxicity must also be recognized. For example, in 2008 US poison centers reported 2,632 cases of digoxin toxicity, and 17 cases of digoxin-related deaths. Because cardiac glycosides affect the cardiovascular, neurologic, and gastrointestinal systems, these three systems can be used to determine the effects of toxicity. The effect of these compounds on the cardiovascular system presents a reason for concern, as they can directly affect the function of the heart through their inotropic and chronotropic effects. In terms of inotropic activity, excessive cardiac glycoside dosage results in cardiac contractions with greater force, as further calcium is released from the SR of cardiac muscle cells. Toxicity also results in changes to heart chronotropic activity, resulting in multiple kinds of dysrhythmia and potentially fatal ventricular tachycardia. These dysrhythmias are an effect of an influx of sodium and decrease of resting membrane potential threshold in cardiac muscle cells. When taken beyond a narrow dosage range specific to each particular cardiac glycoside, these compounds can rapidly become dangerous. In sum, they interfere with fundamental processes that regulate membrane potential. They are toxic to the heart, the brain, and the gut at doses that are not difficult to reach. In the heart, the most common negative effect is premature ventricular contraction.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7297", "revid": "40019", "url": "https://en.wikipedia.org/wiki?curid=7297", "title": "Ca plus plus antagonist", "text": ""}
{"id": "7298", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=7298", "title": "Cyclic AMP", "text": ""}
{"id": "7299", "revid": "39191556", "url": "https://en.wikipedia.org/wiki?curid=7299", "title": "Colonialism", "text": "Creation and maintenance of colonies by people from another area\nColonialism is a practice by which a country controls people or areas, often by establishing colonies, generally for strategic and economic advancement. There is no clear definition of colonialism; definitions may vary depending on the use and context.\nColonialism was first used to describe, and comes from the Roman term, for a farm, and later an outpost or the largest class of Roman city. It is formed by adding the -ism suffix, and has been associated with a variety of philosophies and structural understandings of colonies.\nThough colonialism has existed since ancient times, the concept is most strongly associated with the European colonial period starting with the 15th century when some European states established colonising empires. At first, European colonising countries followed policies of mercantilism, aiming to strengthen the home-country economy, so agreements usually restricted the colony to trading only with the metropole (mother country). By the mid-19th century, the British Empire gave up mercantilism and trade restrictions and adopted the principle of free trade, with few restrictions or tariffs.\nMissionaries were active in practically all of the European-controlled colonies because the metropoles were Christian. Historian Philip Hoffman calculated that by 1800, before the Industrial Revolution, Europeans already controlled at least 35% of the globe, and by 1914, they had gained control of 84% of the globe. In the aftermath of World War II colonial powers retreated between 1945 and 1975; over which time nearly all colonies gained independence, entering into changed colonial, so-called postcolonial and neocolonialist relations.\nPostcolonialism and neocolonialism have continued or shifted relations and ideologies of colonialism, justifying its continuation with concepts such as development and new frontiers, as in exploring outer space for colonization.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nDefinitions.\n\"Collins English Dictionary\" defines colonialism as \"the practice by which a powerful country directly controls less powerful countries and uses their resources to increase its own power and wealth\". \"Webster's Encyclopedic Dictionary\" defines colonialism as \"the system or policy of a nation seeking to extend or retain its authority over other people or territories\". The \"Merriam-Webster Dictionary\" offers four definitions, including \"something characteristic of a colony\" and \"control by one power over a dependent area or people\". Etymologically, the word \"colony\" comes from the Latin \"col\u014dnia\"\u00a0\u2013 \"a place for agriculture\".\nThe \"Stanford Encyclopedia of Philosophy\" uses the term \"to describe the process of European settlement and political control over the rest of the world, including the Americas, Australia, and parts of Africa and Asia\". It discusses the distinction between colonialism, imperialism and conquest and states that \"[t]he difficulty of defining colonialism stems from the fact that the term is often used as a synonym for imperialism. Both colonialism and imperialism were forms of conquest that were expected to benefit Europe economically and strategically,\" and continues \"given the difficulty of consistently distinguishing between the two terms, this entry will use \"colonialism\" broadly to refer to the project of European political domination from the sixteenth to the twentieth centuries that ended with the national liberation movements of the 1960s\".\nIn his preface to J\u00fcrgen Osterhammel's \"Colonialism: A Theoretical Overview\", Roger Tignor says \"For Osterhammel, the essence of colonialism is the existence of colonies, which are by definition governed differently from other territories such as protectorates or informal spheres of influence.\" In the book, Osterhammel asks, \"How can 'colonialism' be defined independently from 'colony?'\" He settles on a three-sentence definition:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Colonialism is a relationship between an indigenous (or forcibly imported) majority and a minority of foreign invaders. The fundamental decisions affecting the lives of the colonised people are made and implemented by the colonial rulers in pursuit of interests that are often defined in a distant metropolis. Rejecting cultural compromises with the colonised population, the colonisers are convinced of their own superiority and their ordained mandate to rule.\nAdditional definitions.\n\"The Times\" once quipped that there were three types of colonial empire: \"The English, which consists in making colonies with colonists; the German, which collects colonists without colonies; the French, which sets up colonies without colonists.\" Modern studies of colonialism have often distinguished between various overlapping categories of colonialism, broadly classified into four types: settler colonialism, exploitation colonialism, surrogate colonialism, and internal colonialism. Some historians have identified other forms of colonialism, including national and trade forms.\nSocio-cultural evolution.\nAs colonialism often played out in pre-populated areas, sociocultural evolution included the formation of various ethnically hybrid populations. Colonialism gave rise to culturally and ethnically mixed populations such as the mestizos of the Americas, as well as racially divided populations such as those found in French Algeria or in Southern Rhodesia. In fact, everywhere where colonial powers established a consistent and continued presence, hybrid communities existed.\nNotable examples in Asia include the Anglo-Burmese, Anglo-Indian, Burgher, Eurasian Singaporean, Filipino mestizo, Kristang, and Macanese peoples. In the Dutch East Indies (later Indonesia) the vast majority of \"Dutch\" settlers were in fact Eurasians known as Indo-Europeans, formally belonging to the European legal class in the colony (see also Indos in pre-colonial history and Indos in colonial history).\nHistory.\nAntiquity.\nActivity that could be called colonialism has a long history, starting at least as early as the ancient Egyptians. Phoenicians, Greeks, and Romans founded colonies in antiquity. Phoenicia had an enterprising maritime trading-culture that spread across the Mediterranean from 1550\u00a0BC to 300\u00a0BC; later the Persian Empire and various Greek city-states continued on this line of setting up colonies. The Romans would soon follow, setting up \"coloniae\" throughout the Mediterranean, in North Africa, and in Western Asia. Beginning in the 7th century, Arabs colonized a substantial portion of the Middle East, North Africa, and parts of Asia and Europe. From the 9th century Vikings (Norsemen) such as Leif Erikson established colonies in Britain, Ireland, Iceland, Greenland, North America, present-day Russia and Ukraine, France (Normandy) and Sicily. In the 9th century a new wave of Mediterranean colonisation began, with competitors such as the Venetians, Genovese and Amalfians infiltrating the wealthy previously Byzantine or Eastern Roman islands and lands. European Crusaders set up colonial regimes in Outremer (in the Levant, 1097\u20131291) and in the Baltic littoral (12th century onwards). Venice began to dominate Dalmatia and reached its greatest nominal colonial extent at the conclusion of the Fourth Crusade in 1204, with the declaration of the acquisition of three octaves of the Byzantine Empire.\nModernity.\nMore than a century before the first North American colonial Jamestown, Virginia settlement, early modern colonialism started with the Portuguese Prince Henry the Navigator (1394\u20131460), initiating the Age of Discovery and establishing African trading posts (1445 onwards).\nSpain (initially the Crown of Castile) soon after encountered the Americas (1492 onwards) through sea travel and built trading posts or conquered large extents of land. For some people, it is this building of colonies across oceans that differentiates colonialism from other types of expansionism. Madrid and Lisbon divided the areas of these \"new\" lands between the Spanish and Portuguese Empires in 1494; other would-be colonial powers paid little heed to the theoretical demarcation.\nThe 17th century saw the birth of the Dutch and French colonial empires, as well as the English overseas possessions, which later became the British Empire. It also saw the establishment of some Danish and Swedish overseas colonies.\nA first wave of independence movements started with the American Revolutionary War (1775\u20131783), initiating a new phase for the British Empire. The Spanish Empire largely collapsed in the Americas with the Spanish American wars of independence (c.\u20091808 onwards). Empire-builders established several new colonies after this time, including in the German and Belgian colonial empires. In the late 19th century, many European powers became involved in the Scramble for Africa.\nThe Austrian, Russian, and Ottoman Empires existed at the same time as the above empires but did not expand over oceans. Rather, these empires expanded through the more traditional route of the conquest of neighbouring territories. There was, though, some Russian colonisation of North America across the Bering Strait. From the 1860s, the Empire of Japan modelled itself on European colonial empires and expanded its territories in the Pacific and on the Asian mainland. Argentina and the Empire of Brazil fought for hegemony in South America. The United States gained overseas territories after the 1898 Spanish\u2013American War, hence, the coining of the term \"American Empire\".\n20th century.\nThe world's colonial population at the outbreak of the First World War (1914)\u00a0\u2013 a high point for colonialism\u00a0\u2013 totalled about 560\u00a0million people, of whom 70% lived in British possessions, 10% in French possessions, 9% in Dutch possessions, 4% in Japanese possessions, 2% in German possessions, 2% in American possessions, 3% in Portuguese possessions, 1% in Belgian possessions and 0.5% in Italian possessions. The domestic domains of the colonial powers had a total population of about 370\u00a0million people. Outside Europe, few areas had remained without coming under formal colonial tutorship \u2013 and even Siam, China, Japan, Nepal, Afghanistan, Persia, and Abyssinia had felt varying degrees of Western colonial-style influence\u00a0\u2013 concessions, unequal treaties, extraterritoriality and the like.\nAsking whether colonies paid, economic historian Grover Clark (1891\u20131938) argues an emphatic \"No!\" He reports that in every case the support cost, especially the military system necessary to support and defend colonies, outran the total trade they produced. Apart from the British Empire, they did not provide favoured destinations for the immigration of surplus metropole populations. The question of whether colonies paid is a complicated one when recognizing the multiplicity of interests involved. In some cases colonial powers paid a lot in military costs while private investors pocketed the benefits. In other cases the colonial powers managed to move the burden of administrative costs to the colonies themselves by imposing taxes.\nAfter World War I (1914\u20131918), the victorious Allies divided up the German colonial empire and much of the Ottoman Empire between themselves as League of Nations mandates, grouping these territories into three classes according to how quickly it was deemed that they could prepare for independence. The empires of Russia and Austria collapsed in 1917\u20131918. Nazi Germany set up short-lived colonial systems (\"Reichskommissariate\", \"Generalgouvernement\") in Eastern Europe in the early 1940s.\nAfter World War II (1939\u20131945), decolonisation progressed rapidly, due to a number of reasons. First, the Japanese victories in the Pacific War of 1941\u20131945 had showed Indians and other subject peoples that the colonial powers were not invincible. Second, World War II had significantly weakened all the overseas colonial powers economically.\nThe word \"neocolonialism\" originated from Jean-Paul Sartre in 1956, to refer to a variety of contexts since the decolonisation that took place after World War II. Generally it does not refer to a type of direct colonisation\u00a0\u2013 rather to colonialism or colonial-style exploitation by other means. Specifically, neocolonialism may refer to the theory that former or existing economic relationships, such as the General Agreement on Tariffs and Trade and the Central American Free Trade Agreement, or the operations of companies (such as Royal Dutch Shell in Nigeria and Brunei) fostered by former colonial powers were or are used to maintain control of former colonies and dependencies after the colonial independence movements of the post\u2013World War II period.\nThe term \"neocolonialism\" became popular in ex-colonies in the late 20th century.\nImpact.\nThe impacts of colonisation are immense and pervasive. Various effects, both immediate and protracted, include the spread of virulent diseases, unequal social relations, detribalization, exploitation, enslavement, medical advances, the creation of new institutions, abolitionism, improved infrastructure, and technological progress. Colonial practices also spur the spread of colonist languages, literature and cultural institutions, while endangering or obliterating those of native peoples. The native cultures of the colonised peoples can also have a powerful influence on the imperial country.\nEconomy, trade and commerce.\nEconomic expansion, sometimes described as the colonial surplus, has accompanied imperial expansion since ancient times. Greek trade networks spread throughout the Mediterranean region while Roman trade expanded with the primary goal of directing tribute from the colonised areas towards the Roman metropole. According to Strabo, by the time of emperor Augustus, up to 120 Roman ships would set sail every year from Myos Hormos in Roman Egypt to India. With the development of trade routes under the Ottoman Empire,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Gujari Hindus, Syrian Muslims, Jews, Armenians, Christians from south and central Europe operated trading routes that supplied Persian and Arab horses to the armies of all three empires, Mocha coffee to Delhi and Belgrade, Persian silk to India and Istanbul.\nAztec civilisation developed into an extensive empire that, much like the Roman Empire, had the goal of exacting tribute from the conquered colonial areas. For the Aztecs, a significant tribute was the acquisition of sacrificial victims for their religious rituals.\nOn the other hand, European colonial empires sometimes attempted to channel, restrict and impede trade involving their colonies, funneling activity through the metropole and taxing accordingly.\nDespite the general trend of economic expansion, the economic performance of former European colonies varies significantly. In \"Institutions as a Fundamental Cause of Long-run Growth\", economists Daron Acemoglu, Simon Johnson and James A. Robinson compare the economic influences of the European colonists on different colonies and study what could explain the huge discrepancies in previous European colonies, for example, between West African colonies like Sierra Leone and Hong Kong and Singapore.\nAccording to the paper, economic institutions are the determinant of the colonial success because they determine their financial performance and order for the distribution of resources. At the same time, these institutions are also consequences of political institutions \u2013 especially how de facto and de jure political power is allocated. To explain the different colonial cases, we thus need to look first into the political institutions that shaped the economic institutions.\nFor example, one interesting observation is \"the Reversal of Fortune\"\u00a0\u2013 the less developed civilisations in 1500, like North America, Australia, and New Zealand, are now much richer than those countries who used to be in the prosperous civilisations in 1500 before the colonists came, like the Mughals in India and the Incas in the Americas. One explanation offered by the paper focuses on the political institutions of the various colonies: it was less likely for European colonists to introduce economic institutions where they could benefit quickly from the extraction of resources in the area. Therefore, given a more developed civilisation and denser population, European colonists would rather keep the existing economic systems than introduce an entirely new system; while in places with little to extract, European colonists would rather establish new economic institutions to protect their interests. Political institutions thus gave rise to different types of economic systems, which determined the colonial economic performance.\nEuropean colonisation and development also changed gendered systems of power already in place around the world. In many pre-colonialist areas, women maintained power, prestige, or authority through reproductive or agricultural control. For example, in certain parts of sub-Saharan Africa women maintained farmland in which they had usage rights. While men would make political and communal decisions for a community, the women would control the village's food supply or their individual family's land. This allowed women to achieve power and autonomy, even in patrilineal and patriarchal societies.\nThrough the rise of European colonialism came a large push for development and industrialisation of most economic systems. When working to improve productivity, Europeans focused mostly on male workers. Foreign aid arrived in the form of loans, land, credit, and tools to speed up development, but were only allocated to men. In a more European fashion, women were expected to serve on a more domestic level. The result was a technologic, economic, and class-based gender gap that widened over time.\nWithin a colony, the presence of extractive colonial institutions in a given area has been found have effects on the modern day economic development, institutions and infrastructure of these areas.\nSlavery and indentured servitude.\nEuropean nations entered their imperial projects with the goal of enriching the European metropoles. Exploitation of non-Europeans and of other Europeans to support imperial goals was acceptable to the colonisers. Two outgrowths of this imperial agenda were the extension of slavery and indentured servitude. In the 17th century, nearly two-thirds of English settlers came to North America as indentured servants.\nEuropean slave traders brought large numbers of African slaves to the Americas by sail. Spain and Portugal had brought African slaves to work in African colonies such as Cape Verde and S\u00e3o Tom\u00e9 and Pr\u00edncipe, and then in Latin America, by the 16th century. The British, French and Dutch joined in the slave trade in subsequent centuries. The European colonial system took approximately 11\u00a0million Africans to the Caribbean and to North and South America as slaves.\nAbolitionists in Europe and Americas protested the inhumane treatment of African slaves, which led to the elimination of the slave trade (and later, of most forms of slavery) by the late 19th century. One (disputed) school of thought points to the role of abolitionism in the American Revolution: while the British colonial metropole started to move towards outlawing slavery, slave-owning elites in the Thirteen Colonies saw this as one of the reasons to fight for their post-colonial independence and for the right to develop and continue a largely slave-based economy.\nBritish colonising activity in New Zealand from the early 19th century played a part in ending slave-taking and slave-keeping among the indigenous M\u0101ori.\nOn the other hand, British colonial administration in Southern Africa, when it officially abolished slavery in the 1830s, caused rifts in society which arguably perpetuated slavery in the Boer Republics and fed into the philosophy of \"apartheid\".\nThe labour shortages that resulted from abolition inspired European colonisers in Queensland, British Guaiana and Fiji (for example) to develop new sources of labour, re-adopting a system of indentured servitude. Indentured servants consented to a contract with the European colonisers. Under their contract, the servant would work for an employer for a term of at least a year, while the employer agreed to pay for the servant's voyage to the colony, possibly pay for the return to the country of origin, and pay the employee a wage as well. The employees became \"indentured\" to the employer because they owed a debt back to the employer for their travel expense to the colony, which they were expected to pay through their wages. In practice, indentured servants were exploited through terrible working conditions and burdensome debts imposed by the employers, with whom the servants had no means of negotiating the debt once they arrived in the colony.\nIndia and China were the largest source of indentured servants during the colonial era. Indentured servants from India travelled to British colonies in Asia, Africa and the Caribbean, and also to French and Portuguese colonies, while Chinese servants travelled to British and Dutch colonies. Between 1830 and 1930, around 30\u00a0million indentured servants migrated from India, and 24\u00a0million returned to India. China sent more indentured servants to European colonies, and around the same proportion returned to China.\nFollowing the Scramble for Africa, an early but secondary focus for most colonial regimes was the suppression of slavery and the slave trade. By the end of the colonial period they were mostly successful in this aim, though slavery persists in Africa and in the world at large with much the same practices of \"de facto\" servility despite legislative prohibition.\nMilitary innovation.\nConquering forces have throughout history applied innovation in order to gain an advantage over the armies of the people they aim to conquer. Greeks developed the phalanx system, which enabled their military units to present themselves to their enemies as a wall, with foot soldiers using shields to cover one another during their advance on the battlefield. Under Philip II of Macedon, they were able to organise thousands of soldiers into a formidable battle force, bringing together carefully trained infantry and cavalry regiments. Alexander the Great exploited this military foundation further during his conquests.\nThe Spanish Empire held a major advantage over Mesoamerican warriors through the use of weapons made of stronger metal, predominantly iron, which was able to shatter the blades of axes used by the Aztec civilisation and others. The use of gunpowder weapons cemented the European military advantage over the peoples they sought to subjugate in the Americas and elsewhere.\nEnd of empire.\nThe populations of some colonial territories, such as Canada, enjoyed relative peace and prosperity as part of a European power, at least among the majority. Minority populations such as First Nations peoples and French-Canadians experienced marginalisation and resented colonial practices. Francophone residents of Quebec, for example, were vocal in opposing conscription into the armed services to fight on behalf of Britain during World War I, resulting in the Conscription crisis of 1917. Other European colonies had much more pronounced conflict between European settlers and the local population. Rebellions broke out in the later decades of the imperial era, such as India's Sepoy Rebellion of 1857.\nThe territorial boundaries imposed by European colonisers, notably in central Africa and South Asia, defied the existing boundaries of native populations that had previously interacted little with one another. European colonisers disregarded native political and cultural animosities, imposing peace upon people under their military control. Native populations were often relocated at the will of the colonial administrators.\nThe Partition of British India in August 1947 led to the Independence of India and the creation of Pakistan. These events also caused much bloodshed at the time of the migration of immigrants from the two countries. Muslims from India and Hindus and Sikhs from Pakistan migrated to the respective countries they sought independence for.\nPost-independence population movement.\nIn a reversal of the migration patterns experienced during the modern colonial era, post-independence era migration followed a route back towards the imperial country. In some cases, this was a movement of settlers of European origin returning to the land of their birth, or to an ancestral birthplace. 900,000 French colonists (known as the \"Pied-Noirs\") resettled in France following Algeria's independence in 1962. A significant number of these migrants were also of Algerian descent. 800,000 people of Portuguese origin migrated to Portugal after the independence of former colonies in Africa between 1974 and 1979; 300,000 settlers of Dutch origin migrated to the Netherlands from the Dutch West Indies after Dutch military control of the colony ended.\nAfter WWII 300,000 Dutchmen from the Dutch East Indies, of which the majority were people of Eurasian descent called Indo Europeans, repatriated to the Netherlands. A significant number later migrated to the US, Canada, Australia and New Zealand.\nGlobal travel and migration in general developed at an increasingly brisk pace throughout the era of European colonial expansion. Citizens of the former colonies of European countries may have a privileged status in some respects with regard to immigration rights when settling in the former European imperial nation. For example, rights to dual citizenship may be generous, or larger immigrant quotas may be extended to former colonies.\nIn some cases, the former European imperial nations continue to foster close political and economic ties with former colonies. The Commonwealth of Nations is an organisation that promotes cooperation between and among Britain and its former colonies, the Commonwealth members. A similar organisation exists for former colonies of France, the Francophonie; the Community of Portuguese Language Countries plays a similar role for former Portuguese colonies, and the Dutch Language Union is the equivalent for former colonies of the Netherlands.\nMigration from former colonies has proven to be problematic for European countries, where the majority population may express hostility to ethnic minorities who have immigrated from former colonies. Cultural and religious conflict have often erupted in France in recent decades, between immigrants from the Maghreb countries of north Africa and the majority population of France. Nonetheless, immigration has changed the ethnic composition of France; by the 1980s, 25% of the total population of \"inner Paris\" and 14% of the metropolitan region were of foreign origin, mainly Algerian.\nOn colonisers.\nIn his 1955 essay, \"Discourse on Colonialism\" (), French poet Aim\u00e9 C\u00e9saire evaluates the effects of racist, sexist, and capitalist attitudes and motivations on the civilisations that attempted to colonise other civilisations. In explaining his position, he says, \"I admit that it is a good thing to place different civilisations in contact with each other that it is an excellent thing to blend different worlds; that whatever its own particular genius may be, a civilisation that withdraws into itself atrophies; that for civilisations, exchange is oxygen.\"\nTo illustrate his point, he explains that colonisation relies on racist and xenophobic frameworks that dehumanise the targets of colonisation and justify their extreme and brutal mistreatment. Every time an immoral act perpetrated by colonisers onto the colonised is justified by racist, sexist, otherwise xenophobic, or capitalist motivations to subjugate a group of people, the colonising civilisation \"acquires another dead weight, a universal regression takes place, a gangrene sets in, a centre of infection begins to spread.\" C\u00e9saire argues the result of this process is that \"a poison [is] instilled into the veins of Europe and, slowly but surely, the continent proceeds toward \"savagery\".\"\nIntroduced diseases.\nEncounters between explorers and populations in the rest of the world often introduced new diseases, which sometimes caused local epidemics of extraordinary virulence. For example, smallpox, measles, malaria, yellow fever, and others were unknown in pre-Columbian America.\nHalf the native population of Hispaniola in 1518 was killed by smallpox. Smallpox also ravaged Mexico in the 1520s, killing 150,000 in Tenochtitlan alone, including the emperor, and Peru in the 1530s, aiding the European conquerors. Measles killed a further two million Mexican natives in the 17th century. In 1618\u20131619, smallpox wiped out 90% of the Massachusetts Bay Native Americans. Smallpox epidemics in 1780\u20131782 and 1837\u20131838 brought devastation and drastic depopulation among the Plains Indians. Some believe that the death of up to 95% of the Native American population of the New World was caused by Old World diseases. Over the centuries, the Europeans had developed high degrees of immunity to these diseases, while the indigenous peoples had no time to build such immunity.\nSmallpox decimated the native population of Australia, killing around 50% of indigenous Australians in the early years of British colonisation. It also killed many New Zealand M\u0101ori. As late as 1848\u201349, as many as 40,000 out of 150,000 Hawaiians are estimated to have died of measles, whooping cough and influenza. Introduced diseases, notably smallpox, nearly wiped out the native population of Easter Island. In 1875, measles killed over 40,000 Fijians, approximately one-third of the population. The Ainu population decreased drastically in the 19th century, due in large part\nto infectious diseases brought by Japanese settlers pouring into Hokkaido.\nConversely, researchers have hypothesised that a precursor to syphilis may have been carried from the New World to Europe after Columbus's voyages. The findings suggested Europeans could have carried the nonvenereal tropical bacteria home, where the organisms may have mutated into a more deadly form in the different conditions of Europe. The disease was more frequently fatal than it is today; syphilis was a major killer in Europe during the Renaissance. The first cholera pandemic began in Bengal, then spread across India by 1820. Ten thousand British troops and countless Indians died during this pandemic. Between 1736 and 1834 only some 10% of East India Company's officers survived to take the final voyage home. Waldemar Haffkine, who mainly worked in India, who developed and used vaccines against cholera and bubonic plague in the 1890s, is considered the first microbiologist.\nAccording to a 2021 study by J\u00f6rg Baten and Laura Maravall on the anthropometric influence of colonialism on Africans, the average height of Africans decreased by 1.1 centimetres upon colonization and later recovered and increased overall during colonial rule. The authors attributed the decrease to diseases, such as malaria and sleeping sickness, forced labor during the early decades of colonial rule, conflicts, land grabbing, and widespread cattle deaths from the rinderpest viral disease.\nCountering disease.\nAs early as 1803, the Spanish Crown organised a mission (the Balmis expedition) to transport the smallpox vaccine to the Spanish colonies, and establish mass vaccination programs there. By 1832, the federal government of the United States established a smallpox vaccination program for Native Americans. Under the direction of Mountstuart Elphinstone a program was launched to propagate smallpox vaccination in India. From the beginning of the 20th century onwards, the elimination or control of disease in tropical countries became a driving force for all colonial powers. The sleeping sickness epidemic in Africa was arrested due to mobile teams systematically screening millions of people at risk. In the 20th century, the world saw the biggest increase in its population in human history due to lessening of the mortality rate in many countries due to medical advances. The world population has grown from 1.6\u00a0billion in 1900 to over seven billion today.\nBotany.\nColonial botany refers to the body of works concerning the study, cultivation, marketing and naming of the new plants that were acquired or traded during the age of European colonialism. Notable examples of these plants included sugar, nutmeg, tobacco, cloves, cinnamon, Peruvian bark, peppers and tea. This work was a large part of securing financing for colonial ambitions, supporting European expansion and ensuring the profitability of such endeavors. Vasco de Gama and Christopher Columbus were seeking to establish routes to trade spices, dyes and silk from the Moluccas, India and China by sea that would be independent of the established routes controlled by Venetian and Middle Eastern merchants. Naturalists like Hendrik van Rheede, Georg Eberhard Rumphius, and Jacobus Bontius compiled data about eastern plants on behalf of the Europeans. Though Sweden did not possess an extensive colonial network, botanical research based on Carl Linnaeus identified and developed techniques to grow cinnamon, tea and rice locally as an alternative to costly imports.\nGeography.\nSettlers acted as the link between indigenous populations and the imperial hegemony, thus bridging the geographical, ideological and commercial gap between the colonisers and colonised. While the extent in which geography as an academic study is implicated in colonialism is contentious, geographical tools such as cartography, shipbuilding, navigation, mining and agricultural productivity were instrumental in European colonial expansion. Colonisers' awareness of the Earth's surface and abundance of practical skills provided colonisers with a knowledge that, in turn, created power.\nAnne Godlewska and Neil Smith argue that \"empire was 'quintessentially a geographical project'\". Historical geographical theories such as environmental determinism legitimised colonialism by positing the view that some parts of the world were underdeveloped, which created notions of skewed evolution. Geographers such as Ellen Churchill Semple and Ellsworth Huntington put forward the notion that northern climates bred vigour and intelligence as opposed to those indigenous to tropical climates (See The Tropics) viz a viz a combination of environmental determinism and Social Darwinism in their approach.\nPolitical geographers also maintain that colonial behaviour was reinforced by the physical mapping of the world, therefore creating a visual separation between \"them\" and \"us\". Geographers are primarily focused on the spaces of colonialism and imperialism; more specifically, the material and symbolic appropriation of space enabling colonialism.\nMaps played an extensive role in colonialism, as Bassett would put it \"by providing geographical information in a convenient and standardised format, cartographers helped open West Africa to European conquest, commerce, and colonisation\". Because the relationship between colonialism and geography was not scientifically objective, cartography was often manipulated during the colonial era. Social norms and values had an effect on the constructing of maps. During colonialism map-makers used rhetoric in their formation of boundaries and in their art. The rhetoric favoured the view of the conquering Europeans; this is evident in the fact that any map created by a non-European was instantly regarded as inaccurate. Furthermore, European cartographers were required to follow a set of rules which led to ethnocentrism; portraying one's own ethnicity in the centre of the map. As J.B. Harley put it, \"The steps in making a map\u00a0\u2013 selection, omission, simplification, classification, the creation of hierarchies, and 'symbolisation'\u00a0\u2013 are all inherently rhetorical.\"\nA common practice by the European cartographers of the time was to map unexplored areas as \"blank spaces\". This influenced the colonial powers as it sparked competition amongst them to explore and colonise these regions. Imperialists aggressively and passionately looked forward to filling these spaces for the glory of their respective countries. The \"Dictionary of Human Geography\" notes that cartography was used to empty 'undiscovered' lands of their Indigenous meaning and bring them into spatial existence via the imposition of \"Western place-names and borders, [therefore] priming 'virgin' (putatively empty land, 'wilderness') for colonisation (thus sexualising colonial landscapes as domains of male penetration), reconfiguring alien space as absolute, quantifiable and separable (as property).\"\nDavid Livingstone stresses \"that geography has meant different things at different times and in different places\" and that we should keep an open mind in regards to the relationship between geography and colonialism instead of identifying boundaries. Geography as a discipline was not and is not an objective science, Painter and Jeffrey argue, rather it is based on assumptions about the physical world. Comparison of exogeographical representations of ostensibly tropical environments in science fiction art support this conjecture, finding the notion of the tropics to be an artificial collection of ideas and beliefs that are independent of geography.\nMarxism.\nMarxism views colonialism as a form of capitalism, enforcing exploitation and social change. Marx thought that working within the global capitalist system, colonialism is closely associated with uneven development. It is an \"instrument of wholesale destruction, dependency and systematic exploitation producing distorted economies, socio-psychological disorientation, massive poverty and neocolonial dependency\". Colonies are constructed into modes of production. The search for raw materials and the current search for new investment opportunities is a result of inter-capitalist rivalry for capital accumulation. Lenin regarded colonialism as the root cause of imperialism, as imperialism was distinguished by monopoly capitalism via colonialism and as Lyal S. Sunga explains: \"Vladimir Lenin advocated forcefully the principle of self-determination of peoples in his \"Theses on the Socialist Revolution and the Right of Nations to Self-Determination\" as an integral plank in the programme of socialist internationalism\" and he quotes Lenin who contended that \"The right of nations to self-determination implies exclusively the right to independence in the political sense, the right to free political separation from the oppressor nation. Specifically, this demand for political democracy implies complete freedom to agitate for secession and for a referendum on secession by the seceding nation.\" Non Russian marxists within the RSFSR and later the USSR, like Sultan Galiev and Vasyl Shakhrai, meanwhile, between 1918 and 1923 and then after 1929, considered the Soviet Regime a renewed version of the Russian imperialism and colonialism.\nIn his critique of colonialism in Africa, the Guyanese historian and political activist Walter Rodney states:The decisiveness of the short period of colonialism and its negative consequences for Africa spring mainly from the fact that Africa lost power. Power is the ultimate determinant in human society, being basic to the relations within any group and between groups. It implies the ability to defend one's interests and if necessary to impose one's will by any means available\u00a0... When one society finds itself forced to relinquish power entirely to another society that in itself is a form of underdevelopment\u00a0... During the centuries of pre-colonial trade, some control over social political and economic life was retained in Africa, in spite of the disadvantageous commerce with Europeans. That little control over internal matters disappeared under colonialism. Colonialism went much further than trade. It meant a tendency towards direct appropriation by Europeans of the social institutions within Africa. Africans ceased to set indigenous cultural goals and standards, and lost full command of training young members of the society. Those were undoubtedly major steps backwards\u00a0... Colonialism was not merely a system of exploitation, but one whose essential purpose was to repatriate the profits to the so-called 'mother country'. From an African view-point, that amounted to consistent expatriation of surplus produced by African labour out of African resources. It meant the development of Europe as part of the same dialectical process in which Africa was underdeveloped. Colonial Africa fell within that part of the international capitalist economy from which surplus was drawn to feed the metropolitan sector. As seen earlier, exploitation of land and labour is essential for human social advance, but only on the assumption that the product is made available within the area where the exploitation takes place.According to Lenin, the new imperialism emphasised the transition of capitalism from free trade to a stage of monopoly capitalism to finance capital. He states it is, \"connected with the intensification of the struggle for the partition of the world\". As free trade thrives on exports of commodities, monopoly capitalism thrived on the export of capital amassed by profits from banks and industry. This, to Lenin, was the highest stage of capitalism. He goes on to state that this form of capitalism was doomed for war between the capitalists and the exploited nations with the former inevitably losing. War is stated to be the consequence of imperialism. As a continuation of this thought G.N. Uzoigwe states, \"But it is now clear from more serious investigations of African history in this period that imperialism was essentially economic in its fundamental impulses.\"\nLiberalism and capitalism.\nClassical liberals were generally in abstract opposition to colonialism and imperialism, including Adam Smith, Fr\u00e9d\u00e9ric Bastiat, Richard Cobden, John Bright, Henry Richard, Herbert Spencer, H.R. Fox Bourne, Edward Morel, Josephine Butler, W.J. Fox and William Ewart Gladstone. Their philosophies found the colonial enterprise, particularly mercantilism, in opposition to the principles of free trade and liberal policies. Adam Smith wrote in \"The Wealth of Nations\" that Britain should grant independence to all of its colonies and also argued that it would be economically beneficial for British people in the average, although the merchants having mercantilist privileges would lose out.\nRace and gender.\nDuring the colonial era, the global process of colonisation served to spread and synthesize the social and political belief systems of the \"mother-countries\" which often included a belief in a certain natural racial superiority of the race of the mother-country. Colonialism also acted to reinforce these same racial belief systems within the \"mother-countries\" themselves. Usually also included within the colonial belief systems was a certain belief in the inherent superiority of male over female. This particular belief was often pre-existing amongst the pre-colonial societies, prior to their colonisation.\nPopular political practices of the time reinforced colonial rule by legitimising European (and/ or Japanese) male authority, and also legitimising female and non-mother-country race inferiority through studies of craniology, comparative anatomy, and phrenology. Biologists, naturalists, anthropologists, and ethnologists of the 19th century were focused on the study of colonised indigenous women, as in the case of Georges Cuvier's study of Sarah Baartman. Such cases embraced a natural superiority and inferiority relationship between the races based on the observations of naturalists' from the mother-countries. European studies along these lines gave rise to the perception that African women's anatomy, and especially genitalia, resembled those of mandrills, baboons, and monkeys, thus differentiating colonised Africans from what were viewed as the features of the evolutionarily superior, and thus rightfully authoritarian, European woman.\nIn addition to what would now be viewed as pseudo-scientific studies of race, which tended to reinforce a belief in an inherent mother-country racial superiority, a new supposedly \"science-based\" ideology concerning gender roles also then emerged as an adjunct to the general body of beliefs of inherent superiority of the colonial era. Female inferiority across all cultures was emerging as an idea supposedly supported by craniology that led scientists to argue that the typical brain size of the female human was, on the average, slightly smaller than that of the male, thus inferring that therefore female humans must be less developed and less evolutionarily advanced than males. This finding of relative cranial size difference was later attributed to the general typical size difference of the human male body versus that of the typical human female body.\nWithin the former European colonies, non-Europeans and women sometimes faced invasive studies by the colonial powers in the interest of the then prevailing pro-colonial scientific ideology of the day. Such seemingly flawed studies of race and gender coincided with the era of colonialism and the initial introduction of foreign cultures, appearances, and gender roles into the now gradually widening world-views of the scholars of the mother-countries.\nOthering.\nOthering is the process of creating a separate entity to persons or groups who are labelled as different or non-normal due to the repetition of characteristics. Othering is the creation of those who discriminate, to distinguish, label, categorise those who do not fit in the societal norm. Several scholars in recent decades developed the notion of the \"other\" as an epistemological concept in social theory. For example, postcolonial scholars, believed that colonising powers explained an \"other\" who were there to dominate, civilise, and extract resources through colonisation of land.\nPolitical geographers explain how colonial/imperial powers \"othered\" places they wanted to dominate to legalise their exploitation of the land. During and after the rise of colonialism the Western powers perceived the East as the \"other\", being different and separate from their societal norm. This viewpoint and separation of culture had divided the Eastern and Western culture creating a dominant/subordinate dynamic, both being the \"other\" towards themselves.\nPost-colonialism.\nPost-colonialism (or post-colonial theory) can refer to a set of theories in philosophy and literature that grapple with the legacy of colonial rule. In this sense, one can regard post-colonial literature as a branch of postmodern literature concerned with the political and cultural independence of peoples formerly subjugated in colonial empires.\nMany practitioners take Edward Sa\u00efd's book \"Orientalism\" (1978) as the theory's founding work (although French theorists such as Aim\u00e9 C\u00e9saire (1913\u20132008) and Frantz Fanon (1925\u20131961) made similar claims decades before Sa\u00efd). Sa\u00efd analyzed the works of Balzac, Baudelaire and Lautr\u00e9amont, arguing that they helped to shape a societal fantasy of European racial superiority.\nWriters of post-colonial fiction interact with the traditional colonial discourse, but modify or subvert it; for instance by retelling a familiar story from the perspective of an oppressed minor character in the story. Gayatri Chakravorty Spivak's \"Can the Subaltern Speak?\" (1998) gave its name to Subaltern Studies.\nIn \"A Critique of Postcolonial Reason\" (1999), Spivak argued that major works of European metaphysics (such as those of Kant and Hegel) not only tend to exclude the subaltern from their discussions, but actively prevent non-Europeans from occupying positions as fully human subjects. Hegel's \"Phenomenology of Spirit\" (1807), famous for its explicit ethnocentrism, considers Western civilisation as the most accomplished of all, while Kant also had some traces of racialism in his work.\nThe 2014 YouGov survey found that British people are mostly proud of colonialism and the British Empire:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A new YouGov survey finds that most think the British Empire is more something to be proud of (59%) than to be ashamed of (19%). 23% don't know. Young people are least likely to feel pride over shame when it comes to the Empire, though about half (48%) of 18\u201324 year old's do. In comparison, about two-thirds (65%) of over 60's feel mostly proud. ... A third of British people (34%) also say they would like it if Britain still had an empire. Under half (45%) say they would not like the Empire to exist today. 20% don't know.\nColonistics.\nThe field of colonistics studies colonialism from such viewpoints as those of economics, sociology and psychology.\nMigrations.\nNations and regions outside Europe with significant populations of European ancestry\nNations and regions outside Northern China with significant populations of Han Chinese ancestry:\nNumbers of European settlers in the colonies (1500\u20131914).\nBy 1914, Europeans had migrated to the colonies in the millions. Some intended to remain in the colonies as temporary settlers, mainly as military personnel or on business. Others went to the colonies as immigrants. British people were by far the most numerous population to migrate to the colonies: 2.5\u00a0million settled in Canada; 1.5\u00a0million in Australia; 750,000 in New Zealand; 450,000 in the Union of South Africa; and 200,000 in India. French citizens also migrated in large numbers, mainly to the colonies in the north African Maghreb region: 1.3\u00a0million settled in Algeria; 200,000 in Morocco; 100,000 in Tunisia; while only 20,000 migrated to French Indochina. Dutch and German colonies saw relatively scarce European migration, since Dutch and German colonial expansion focused on commercial goals rather than settlement. Portugal sent 150,000 settlers to Angola, 80,000 to Mozambique, and 20,000 to Goa. During the Spanish Empire, approximately 550,000 Spanish settlers migrated to Latin America.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7300", "revid": "13127810", "url": "https://en.wikipedia.org/wiki?curid=7300", "title": "Colonial", "text": "Colonial or The Colonial may refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "7301", "revid": "46029902", "url": "https://en.wikipedia.org/wiki?curid=7301", "title": "Casablanca", "text": "Largest city in Morocco\nCasablanca, known by its Arabic name A-Dar al-Bayda (, ], lit.: \"White House\"; ) is the largest city in Morocco and the country's economic and business center. Located on the Atlantic coast of the Chaouia plain in the central-western part of Morocco, the city has a population of about 3.71 million in the urban area, and over 4.27 million in the Greater Casablanca, making it the most populous city in the Maghreb region, and the eighth-largest in the Arab world.\nCasablanca is Morocco's chief port, with the Port of Casablanca being one of the largest artificial ports in the world, and the second largest port in North Africa, after Tanger-Med ( east of Tangier). Casablanca also hosts the primary naval base for the Royal Moroccan Navy.\nCasablanca is considered a Global Financial Centre, ranking 54th globally in the Global Financial Centres Index rankings for the year 2022, outperforming many cities such as New Delhi, Jakarta, Istanbul, and Mexico City. Casablanca is ranked among the Emerging International contenders, and it is considered the largest financial center in Africa. The leading Moroccan companies and many of the larger American and European corporations doing business in the country have their headquarters and main industrial facilities in Casablanca. Recent industrial statistics show Casablanca is the primary industrial zone of the nation.\nEtymology.\nAnfa.\nBefore the 15th century, the settlement at what is now Casablanca had been called \"Anfa\", rendered in European sources variously as El-Anfa, Anafa or Anaffa, Anafe, Anife, Anafee, Nafe, and Nafee. Ibn Khaldun ascribed the name to the \"Anfa\u00e7a\", a branch of the Aur\u00e9ba tribe of the Maghreb, though the sociologist Andr\u00e9 Adam refuted this claim due to the absence of the third syllable. Nahum Slouschz gave a Hebrew etymology, citing the \"Lexicon\" of Gesenius: \"an\u00e2ph\u00e2h\" (a type of bird) or \"anaph\" (face, figure), though Adam refuted this arguing that even a Judaized population would still have spoken Tamazight. Adam also refuted an Arabic etymology, (\"anf\", \"nose\"), as the city predated the linguistic Arabization of the country, and the term \"anf\" was not used to describe geographic areas. Adam affirmed a Tamazight etymology\u2014from \"anfa\" \"hill,\" \"anfa\" \"promontory on the sea,\" \"ifni\" \"sandy beach,\" or \"anfa\" \"threshing floor\"\u2014although he determined the available information insufficient to establish exactly which. The name Anfa is now rendered in Neo-Tifinagh as \u2d30\u2d4f\u2d3c\u2d30.\nThe name \"Anfa\" was used in maps until around 1830\u2014in some until 1851\u2014which Adam attributes to the tendency of cartographers to replicate previous maps.\nCasablanca.\nWhen Sultan Mohammed ben Abdallah (c.\u20091710\u20131790) rebuilt the city after its destruction in the earthquake of 1755, it was renamed \"\"ad-D\u0101r al-Bay\u1e0d\u0101\u02be\" \" ( \"The White House\"), though in vernacular use it was pronounced \"Dar al-Bai\u1e0d\u0101\" ( literally \"House of the White,\" although in Moroccan Arabic vernacular it retains the original sense of \"The White House\").\nThe origins of the name \"Casablanca\" are unclear, although several theories have been suggested. Andr\u00e9 Adam mentions the legend of the Sufi saint and merchant Allal al-Qairawani, who supposedly came from Tunisia and settled in Casablanca with his wife Lalla al-Bai\u1e0d\u0101\u02be ( \"White Lady\"). The villagers of Mediouna would reportedly provision themselves at \"Dar al-Bai\u1e0d\u0101\u02be\" ( \"House of the White\").\nIn fact, on a low hill slightly inland above the ruins of Anfa and just to the west of today's city centre, it appears there was a white-washed structure, possibly a Sufi zawiya that acted as a landmark to sailors. The Portuguese cartographer Duarte Pacheco wrote in the early 16th century that the city could easily be identified by a tower, and nautical guides from the late 19th century still mentioned a \"white tower\" as a point of reference. The Portuguese mariners calqued the modern Arabic name to \"Casa Branca\" (] \"White House\") in place of Anfa. The name \"Casablanca\" was then a calque of the Portuguese name when the Spanish took over trade through the Iberian Union.\nDuring the French protectorate in Morocco, the name remained Casablanca (]). Today, Moroccans still call the city Casablanca or \"Casa\" for short, or by its Arabic name, pronounced in Moroccan Arabic or in Standard Arabic.\nHistory.\nEarly history.\nThe area which is today Casablanca was founded and settled by Berbers by at least the seventh century BC. It was used as a port by the Phoenicians and later the Romans. In his book \"Description of Africa\", Leo Africanus refers to ancient Casablanca as \"Anfa\", a great city founded in the Berber kingdom of Barghawata in 744\u00a0AD. He believed Anfa was the most \"prosperous city on the Atlantic Coast because of its fertile land.\" Barghawata rose as an independent state around this time, and continued until it was conquered by the Almoravids in 1068. Following the defeat of the Barghawata in the 12th century, Arab tribes of Hilal and Sulaym descent settled in the region, mixing with the local Berbers, which led to widespread Arabization. During the 14th century, under the Merinids, Anfa rose in importance as a port. The last of the Merinids were ousted by a popular revolt in 1465.\nPortuguese conquest and Spanish influence.\nIn the early 15th century, the town became an independent state once again, and emerged as a safe harbour for pirates and privateers, leading to it being targeted by the Portuguese, who bombarded the town which led to its destruction in 1468. The Portuguese used the ruins of Anfa to build a military fortress in 1515. The town that grew up around it was called Casa Branca, meaning \"white house\" in Portuguese.\nBetween 1580 and 1640, the Crown of Portugal was integrated into the Crown of Spain, so Casablanca and all other areas occupied by the Portuguese were under Spanish control, though maintaining an autonomous Portuguese administration. As Portugal broke ties with Spain in 1640, Casablanca came under full Portuguese control once again. The Europeans eventually abandoned the area completely in 1755 following an earthquake which destroyed most of the town.\nThe town was finally reconstructed between 1756 and 1790 by Sultan Mohammed ben Abdallah, the grandson of Moulay Ismail and an ally of George Washington, with the help of Spaniards from the nearby emporium. The town was called \"ad-D\u0101r al-Bay\u1e0d\u0101\u02bc\" (\u0627\u0644\u062f\u0627\u0631 \u0627\u0644\u0628\u064a\u0636\u0627\u0621), the Arabic translation of the Portuguese \"Casa Branca\".\nColonial struggle.\nIn the 19th century, the area's population began to grow as it became a major supplier of wool to the booming textile industry in Britain and shipping traffic increased (the British, in return, began importing gunpowder tea, used in Morocco's national drink, mint tea). By the 1860s, around 5,000 residents were there, and the population grew to around 10,000 by the late 1880s. Casablanca remained a modestly sized port, with a population reaching around 12,000 within a few years of the French conquest and arrival of French colonialists in 1906. By 1921, this rose to 110,000, largely through the development of shanty towns.\nBombardment of Casablanca.\nThe Treaty of Algeciras of 1906 formalized French preeminence in Morocco and included three measures that directly impacted Casablanca: that French officers would control operations at the customs office and seize revenue as collateral for loans given by France, that the French holding company \"La Compagnie Marocaine\" would develop the port of Casablanca, and that a French-and-Spanish-trained police force would be assembled to patrol the port.\nTo build the port's breakwater, narrow-gauge track was laid in June 1907 for a small Decauville locomotive to connect the port to a quarry in Roches Noires, passing through the sacred Sidi Belyout graveyard. In resistance to this and the measures of the 1906 Treaty of Algeciras, tribesmen of the Chaouia attacked the locomotive, killing 9 Compagnie Marocaine laborers\u20143 French, 3 Italians, and 3 Spanish.\nIn response, the French bombarded the city in August 1907 with multiple gunboats and landed troops inside the town, causing severe damage and killing between 600 and 3,000 Moroccans. Estimates for the total casualties are as high as 15,000 dead and wounded. In the immediate aftermath of the bombardment and the deployment of French troops, the European homes and the \"Mellah\", or Jewish quarter, were sacked, and the latter was also set ablaze.\nAs Oujda had already been occupied, the bombardment and military invasion of the city opened a western front to the French military conquest of Morocco. \nFrench rule and influence.\nFrench control of Casablanca was formalized March 1912 when the Treaty of Fes established the French \"Protectorat\". Under French imperial control, Casablanca became a port of colonial extraction.\nGeneral Hubert Lyautey assigned the planning of the new colonial port city to Henri Prost. As he did in other Moroccan cities, Prost designed a European \"ville nouvelle\" outside the walls of the medina. In Casablanca, he also designed a new \"ville indig\u00e8ne\" to house Moroccans arriving from other cities.\nEuropeans formed almost half the population of Casablanca.\nA 1937-1938 typhoid fever outbreak was exploited by colonial authorities to justify the appropriation of urban spaces in Casablanca. Moroccans residing in informal housing were cleared out of the center and displaced, notably to .\nWorld War II.\nAfter Philippe P\u00e9tain of France signed the armistice with the Nazis, he ordered French troops in France's colonial empire to defend French territory against any aggressors\u2014Allied or otherwise\u2014applying a policy of \"asymmetrical neutrality\" in favour of the Germans. French colonists in Morocco generally supported P\u00e9tain, while Moroccans tended to favour de Gaulle and the Allies.\nOperation Torch, which started on 8 November 1942, was the British-American invasion of French North Africa during the North African campaign of World War II. The Western Task Force, composed of American units led by Major General George S. Patton and Rear Admiral Henry Kent Hewitt, carried out the invasions of Mehdia, Fedhala, and Asfi. American forces captured Casablanca from Vichy control when France surrendered 11 November 1942, but the Naval Battle of Casablanca continued until American forces sank German submarine U-173 on 16 November.\nCasablanca was the site of the Nouasseur Air Base, a large American air base used as the staging area for all American aircraft for the European Theatre of Operations during World War II. The airfield has since become Mohammed V International Airport.\nAnfa Conference.\nCasablanca hosted the Anfa Conference (also called the Casablanca Conference) in January 1943. Prime Minister Winston Churchill and President Franklin D. Roosevelt discussed the progress of the war. Also in attendance were the Free France generals Charles de Gaulle and Henri Giraud, though they played minor roles and didn't participate in the military planning.\nIt was at this conference that the Allies adopted the doctrine of \"unconditional surrender,\" meaning that the Axis powers would be fought until their defeat. Roosevelt also met privately with Sultan Muhammad V and expressed his support for Moroccan independence after the war. This became a turning point, as Moroccan nationalists were emboldened to openly seek complete independence.\nToward independence.\nDuring the 1940s and 1950s, Casablanca was a major centre of anti-French rioting.\n7 April 1947, a massacre of working class Moroccans, carried out by Senegalese Tirailleurs in the service of the French colonial army, was instigated just as Sultan Muhammed V was due to make a speech in Tangier appealing for independence.\nRiots in Casablanca took place from 7\u20138 December 1952, in response to the assassination of the Tunisian labor unionist Farhat Hached by \"La Main Rouge\"\u2014the clandestine militant wing of French intelligence. Then, on 25 December 1953 (Christmas Day), Muhammad Zarqtuni orchestrated a bombing of Casablanca's Central Market in response to the forced exile of Sultan Muhammad V and the royal family on 20 August (Eid al-Adha) of that year.\nSince independence.\nMorocco gained independence from France in 1956.\nCasablanca Group.\n4\u20137 January 1961, the city hosted an ensemble of progressive African leaders during the Casablanca Conference of 1961. Among those received by King Muhammad V were Gamal Abd An-Nasser, Kwame Nkrumah, Modibo Ke\u00efta, and Ahmed S\u00e9kou Tour\u00e9, Ferhat Abbas.\nJewish emigration.\nCasablanca was a major departure point for Jews leaving Morocco through Operation Yachin, an operation conducted by Mossad to secretly migrate Moroccan Jews to Israel between November 1961 and spring 1964.\n1965 riots.\nThe 1965 student protests organized by the National Union of Popular Forces-affiliated National Union of Moroccan Students, which spread to cities around the country and devolved into riots, started on 22 March 1965, in front of Lyc\u00e9e Mohammed V in Casablanca. The protests started as a peaceful march to demand the right to public higher education for Morocco, but expanded to include concerns of labourers, the unemployed, and other marginalized segments of society, and devolved into vandalism and rioting. The riots were violently repressed by security forces with tanks and armoured vehicles; Moroccan authorities reported a dozen deaths while the UNFP reported more than 1,000.\nKing Hassan II blamed the events on teachers and parents, and declared in a speech to the nation on 30 March 1965: \"There is no greater danger to the State than a so-called intellectual. It would have been better if you were all illiterate.\"\n1981 riots.\nOn 6 June 1981, the Casablanca Bread Riots took place, which were sparked by a sharp increase in the price of necessities such as butter, sugar, wheat flour, and cooking oil following a period of severe drought. Hassan II appointed the French-trained interior minister Driss Basri as hardliner, who would later become a symbol of the Years of Lead, with quelling the protests. The government stated that 66 people were killed and 100 were injured, while opposition leaders put the number of dead at 637, saying that many of these were killed by police and army gunfire.\n\"Mudawana\".\nIn March 2000, more than 60 women's groups organized demonstrations in Casablanca proposing reforms to the legal status of women in the country. About 40,000 women attended, calling for a ban on polygamy and the introduction of divorce law (divorce being a purely religious procedure at that time). Although the counter-demonstration attracted half a million participants, the movement for change started in 2000 was influential on King Mohammed VI, and he enacted a new \"mudawana\", or family law, in early 2004, meeting some of the demands of women's rights activists.\nFurther history.\nOn 16 May 2003, 33 civilians were killed and more than 100 people were injured when Casablanca was hit by a multiple suicide bomb attack carried out by Moroccans and claimed by some to have been linked to al-Qaeda. Twelve suicide bombers struck five locations in the city.\nAnother series of suicide bombings struck the city in early 2007. These events illustrated some of the persistent challenges the city faces in addressing poverty and integrating disadvantaged neighborhoods and populations. One initiative to improve conditions in the city's disadvantaged neighborhoods was the creation of the Sidi Moumen Cultural Center.\nAs calls for reform spread through the Arab world in 2011, Moroccans joined in, but concessions by the ruler led to acceptance. However, in December, thousands of people demonstrated in several parts of the city, especially the city center near la Fontaine, desiring more significant political reforms.\nGeography.\nCasablanca is located on the Atlantic coast of the Chaouia Plains, which have historically been the breadbasket of Morocco. Apart from the Atlantic coast, the Bouskoura forest is the only natural attraction in the city. The forest was planted in the 20th century and consists mostly of eucalyptus, palm, and pine trees. It is located halfway to the city's international airport.\nThe only watercourse in Casablanca is \"oued Bouskoura\", a small seasonal creek that until 1912 reached the Atlantic Ocean near the actual port. Most of oued Bouskoura's bed has been covered due to urbanization and only the part south of El Jadida road can now be seen. The closest permanent river to Casablanca is Oum Rabia, to the south-east.\nClimate.\nCasablanca has a hot-summer Mediterranean climate (K\u00f6ppen climate classification \"Csa\"). The cool Canary Current off the Atlantic coast moderates temperature variation, which results in a climate remarkably similar to that of coastal Los Angeles, with similar temperature ranges. The city has an annual average of 72 days with significant precipitation, which amounts to per year. The highest and lowest temperatures ever recorded in the city are and , respectively. The highest amount of rainfall recorded in a single day is on 30 November 2010.\nClimate change.\nA 2019 paper published in PLOS One estimated that under Representative Concentration Pathway 4.5, a \"moderate\" scenario of climate change where global warming reaches ~ by 2100, the climate of Casablanca in the year 2050 would most closely resemble the current climate of Tripoli, Libya. The annual temperature would increase by , and the temperature of the warmest month by , while the temperature of the coldest month would actually decrease by . \nMoreover, according to the 2022 IPCC Sixth Assessment Report, Casablanca is one of 12 major African cities (Abidjan, Alexandria, Algiers, Cape Town, Casablanca, Dakar, Dar es Salaam, Durban, Lagos, Lom\u00e9, Luanda and Maputo) which would be the most severely affected by future sea level rise. It estimates that they would collectively sustain cumulative damages of USD 65 billion under RCP 4.5 and USD 86.5 billion for the high-emission scenario RCP 8.5 by the year 2050. Additionally, RCP 8.5 combined with the hypothetical impact from marine ice sheet instability at high levels of warming would involve up to 137.5 billion USD in damages, while the additional accounting for the \"low-probability, high-damage events\" may increase aggregate risks to USD 187 billion for the \"moderate\" RCP4.5, USD 206 billion for RCP8.5 and USD 397 billion under the high-end ice sheet instability scenario. Since sea level rise would continue for about 10,000 years under every scenario of climate change, future costs of sea level rise would only increase, especially without adaptation measures.\nEconomy.\nThe Grand Casablanca region is considered the locomotive of the development of the Moroccan economy. It attracts 32% of the country's production units and 56% of industrial labor. The region uses 30% of the national electricity production. With MAD 93\u00a0billion, the region contributes to 44% of the industrial production of the kingdom. About 33% of national industrial exports, MAD 27\u00a0billion, comes from the Grand Casablanca; 30% of the Moroccan banking network is concentrated in Casablanca.\nCasablanca is considered a global financial centre, ranking 53rd globally in the Global Financial Centres Index for the year 2021, outperforming many cities such as Mumbai, New Delhi, Berlin, Istanbul, Mexico City, Glasgow, Jakarta, Rio de Janeiro, S\u00e3o Paulo, Riyadh, Doha, Kuwait City, Cape Town, and Johannesburg. Casablanca is ranked among the emerging international contenders, and it is the largest financial center in Africa.\nOne of the most important Casablancan exports is phosphate. Other industries include fishing, fish canning, sawmills, furniture production, building materials, glass, textiles, electronics, leather work, processed food, spirits, soft drinks, and cigarettes.\nThe Casablanca and Mohammedia seaports activity represent 50% of the international commercial flows of Morocco. Almost the entire Casablanca waterfront is under development, mainly the construction of huge entertainment centres between the port and Hassan II Mosque, the Anfa Resort project near the business, entertainment and living centre of Megarama, the shopping and entertainment complex of Morocco Mall, as well as a complete renovation of the coastal walkway. The Sindbad park is planned to be totally renewed with rides, games and entertainment services.\nRoyal Air Maroc has its head office at the Casablanca-Anfa Airport. In 2004, it announced that it was moving its head office from Casablanca to a location in Province of Nouaceur, close to Mohammed V International Airport. The agreement to build the head office in Nouaceur was signed in 2009.\nThe largest CBD both in Casablanca and the Maghreb is in Sidi Maarouf, near the Hassan II Mosque.\nAdministrative divisions.\nCasablanca is a commune, part of the region of Casablanca-Settat. The commune is divided into eight districts or prefectures, which are themselves divided into 16 subdivisions or arrondissements and one municipality. The districts and their subdivisions are:\nNeighborhoods.\nThe list of neighborhoods is indicative and not complete:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nDemographics.\nThe commune of Casablanca recorded a population of 3,359,818 in the 2014 Moroccan census. About 98% live in urban areas. Around 25% of the population are under 15 years old, and 9% are over 60 years old. The population of the city is about 11% of the total population of Morocco. Grand Casablanca is also the largest urban area in the Maghreb. 99.9% of the population of Morocco are Arab and Berber Muslims. During the French protectorate in Morocco, European Christians formed almost half the population of Casablanca. Since Moroccan independence in 1956, the European population has decreased substantially. The city also is still home to a small community of Moroccan Christians, as well as a small group of foreign Roman Catholic and Protestant residents.\nJudaism in Casablanca.\nJews have a long history in Casablanca. A Sephardic Jewish community was in Anfa up to the destruction of the city by the Portuguese in 1468. Jews were slow to return to the town, but by 1750, the Rabbi Elijah synagogue was built as the first Jewish synagogue in Casablanca. It was destroyed along with much of the town in the 1755 Lisbon earthquake.\nIn the mid-19th century, with commercial development through European economic penetration, industrial imports from Europe drove traditional Jewish crafts out of the market, costing many Jews in the interior their traditional livelihoods. Moroccan Jews started migrating from the interior to coastal cities such as Essaouira, Mazagan, Asfi, and later Casablanca for economic opportunity, participating in trade with Europeans and the development of those cities.\nCasablanca's \"mellah\" was ravaged in the bombardment of Casablanca of 1907, the beginning of the French invasion of Morocco from the West.\nJean-Louis Cohen highlights the roll of Jewish patrons in the architecture and urban development of Casablanca, particularly in construction of the overwhelming majority of the city's tallest buildings during the interwar period.\"\" One notable example of this trend is the L\u00e9vy-Bendayan Building designed by Marius Boyer.\"\"\nApproximately 28,000 Moroccan Jews immigrated to the State of Israel between 1948 and 1951, many through Casablanca. Casablanca then became a departure point in Operation Yachin, the covert Mossad-organized migration operation from 1961 to 1964. In 2018 it was estimated that there were only 2,500 Moroccan Jews living in Casablanca, while according to the World Jewish Congress there were only 1,000 Moroccan Jews remaining.\nToday, the Jewish cemetery of Casablanca is one of the major cemeteries of the city, and many synagogues remain in service, but the city's Jewish community has dwindled. The Moroccan Jewish Museum is a museum established in the city in 1997.\nEducation.\nColleges and universities.\nPublic: University of Hassan II Casablanca\nPrivate:\nPrimary and secondary schools.\nInternational schools:\nPlaces of worship.\nMost of the city's places of worship are Muslim mosques. Some of the city's synagogues, such as Ettedgui Synagogue, also remain. There are also Christian churches; some remain in use \u2014 particularly by the West African migrant community \u2014 while many of the churches built during the colonial period have been repurposed, such as Church of the Sacred Heart.\nSports.\nAssociation football.\nCasablanca is home to two popular football clubs: Wydad Casablanca and Raja Casablanca\u2014which are rivals. Raja's symbol is an eagle and Wydad's symbol is a star and crescent, a symbol of Islam. These two popular clubs have produced some of Morocco's best players, such as: Salaheddine Bassir, Abdelmajid Dolmy, Baddou Zaki, Aziz Bouderbala, and Noureddine Naybet. Other football teams on top of these two major teams based in the city of Casablanca include Rachad Bernoussi, TAS de Casablanca, Majd Al Madina, and Racing Casablanca. \nRaja CA, founded in 1949, compete in Botola and play their home games at the Stade Mohammed V. The club is known for their supporters and is one of the most supported teams in Africa. Wydad AC, founded in 1937, also compete in Botola and play their home games at the Stade Mohammed V. Both have a strong reputation on continental competitions, having both won the CAF Champions League three times.\nCasablanca hosted eight African Champions League finals, all eight at the Stade Mohammed V. The Stade also hosted the 2018 CHAN Final (which Morocco won) and 1988 African Cup of Nations final.\nTennis.\nCasablanca hosts The Grand Prix Hassan II, a professional men's tennis tournament of the ATP tour. It first began in 1986, and is played on clay courts type at Complexe Al Amal.\nNotable winners of the Hassan II Grand-Prix are Thomas Muster in 1990, Hicham Arazi in 1997, Younes El Aynaoui in 2002, and Stanislas Wawrinka in 2010.\nHosting.\nCasablanca staged the 1961 Pan Arab Games, the 1983 Mediterranean Games, and games during the 1988 Africa Cup of Nations. Morocco was scheduled to host the 2015 African Nations Cup, but decided to decline due to Ebola fears. Morocco was expelled and the tournament was held in Equatorial Guinea.\nVenues.\nThe Grand Stade de Casablanca is the proposed title of the planned football stadium to be built in the city. Once completed in 2025, it will be used mostly for football matches and will serve as the home of Raja Casablanca, Wydad Casablanca, and the Morocco national football team. The stadium was designed with a capacity of 93,000 spectators, making it one of the highest-capacity stadiums in Africa. Once completed, it will replace the Stade Mohamed V. The initial idea of the stadium was for the 2010 FIFA World Cup, for which Morocco lost their bid to South Africa. Nevertheless, the Moroccan government supported the decision to go ahead with the plans. It will be completed in 2025. The idea of the stadium was also for the 2026 FIFA World Cup, for which Morocco lost their bid to Canada, Mexico and United States. It is now hoping for the 2030 FIFA World Cup which Morocco is co-bidding with two European nations Spain and Portugal.\nRoad Racing.\nThe city is host to the International Casablanca Marathon, a 26.2-mile road race that draws international competition. The race was founded in 2008 and is a member of the Association of International Marathons and Distance Races .\nCulture.\nMusic.\nHaja El Hamdaouia, one of the most iconic figures in aita music, was born in Casablanca. Nass El Ghiwane, led by Larbi Batma, came out of Hay Mohammadi in Casablanca. Naima Samih of Derb Sultan gained prominence through the program \"Mawahib\" (). Abdelhadi Belkhayat and Abdelwahab Doukkali are musicians specializing in traditional Moroccan Arabic popular music. Zina Daoudia, Abdelaziz Stati, Abdellah Daoudi, and Said Senhaji are notable Moroccan chaabi musicians.\nAbdelakabir Faradjallah founded Attarazat Addahabia, a Moroccan funk band, in 1968. Fadoul, another funk band, formed in the 1970s.\nHoba Hoba Spirit also formed in Casablanca, and is still based there. Casablanca has a thriving hiphop scene, with artists such as El Grande Toto, Don Big, 7liwa, and Issam Harris.\nCasablanca hosts numerous music festivals, such as Jazzablanca and L'Boulevard, as well as a museum dedicated to Andalusi music, \"Dar ul-Aala\".\nLiterature.\nFrancesco Cavalli's \"L'Ormindo\" is a 17th century Venetian opera set between Anfa and Fes.\nThe French writer Antoine de Saint-Exup\u00e9ry is associated with Casablanca.\nDriss Chra\u00efbi's novel \"The Simple Past\" takes place in Casablanca. Mohamed Zafzaf lived in Maarif while writing and teaching at a high school.\nLamalif, a radical leftist political and cultural magazine, was based in Casablanca.\nCasablanca's International Book Fair is held at the fair grounds opposite Hassan II Mosque annually in February.\nTheater.\nTayeb Saddiki, described as the father of Moroccan theater, grew up in Casablanca and made his career there. Hanane el-Fadili and Hassan El Fad are popular comedians from Casablanca. Gad Elmaleh is another comedian from Casablanca, though he has made his career abroad.\nVisual art.\nThe \u00c9cole des Beaux-Arts of Casablanca was founded in 1919 by a French Orientalist painter named \u00c9douard Brindeau de Jarny, who started his career teaching drawing at Lyc\u00e9e Lyautey. The Casablanca School\u2014a Modernist art movement and collective including artists such as Farid Belkahia, Mohamed Melehi, and Mohammed Chab\u00e2a\u2014developed out of the \u00c9cole des Beaux-Arts of Casablanca in the late 1960s.\nThe Academy of Traditional Arts, part of the Hassan II Mosque complex, was founded 31 October 2012.\nL'Uzine is a community-based art and culture space in Casablanca.\nRebel Spirit published \"The Casablanca Guide\" (, ) a comic book about life in Casablanca.\nSbagha Bagha is a street art festival during which murals are created on the sides of apartment buildings.\nPhotography.\nPostcard companies such as L\u00e9on &amp; L\u00e9vy were active in Casablanca. Gabriel Veyre also worked and eventually died in Casablanca.\nMarcelin Flandrin (1889-1957), a French military photographer, settled in Casablanca and recorded much of the early colonial period in Morocco with his photography. With his staged nude postcard photos taken in Casablanca's colonial brothel quarter, Flandrin was also responsible for disseminating the orientalist image of Moroccan women as sexual objects.\nCasablanca has a thriving street photography scene. Yoriyas is prominent among photographers capturing the economic capital's street scenes, and has attracted international attention.\nFilm.\nIn the first half of the 20th century, Casablanca had many movie theaters, such as Cinema Rialto, Cinema Lynx and Cinema Vox, the largest in Africa when it was built.\nThe 1942 American film \"Casablanca\" is set in Casablanca and has had a lasting impact on the city's image although it was filmed in the United States. \"Salut Casa!\" was a propaganda film brandishing France's purported colonial triumph in its \"mission civilisatrice\" in the city.\nMostafa Derkaoui's revolutionary independent film \"About Some Meaningless Events\" (1974) took place in Casablanca. It was the main subject of Ali Essafi's documentary \"Before the Dying of the Light\".\n\"Love in Casablanca\" (1991), starring Abdelkarim Derqaoui and Muna Fettou, is one of the first Moroccan films to deal with Morocco's complex realities and to depict life in Casablanca with verisimilitude. Nour-Eddine Lakhmari's \"Casanegra\" (2008) depicts the harsh realities of Casablanca's working classes. The films \"Ali Zaoua\" (2000), \"Horses of God\" (2012), and \"Razzia\" (2017) of Nabil Ayouch, a French director of Moroccan heritage, deal with street crime, terrorism and social issues in Casablanca, respectively. The events in Meryem Benm'Barek-Alo\u00efsi's 2018 film Sofia revolve around an illegitimate pregnancy in Casablanca. Ahmed El Maanouni, Hicham Lasri and Said Naciri are also from Casablanca.\nArchitecture.\nCasablanca's architecture and urban development are historically significant. The city is home to many notable buildings in a variety of styles, including traditional Moroccan architecture, various colonial architectural styles, Art Nouveau, Art Deco, Neo-Mauresque, Streamline Moderne, Modernism, Brutalism, and more. During the French Protectorate, the French government described Casablanca as a \"laboratory of urbanism.\"\nThe work of the \"Groupe des Architectes Modernes Marocains\" (GAMMA) on public housing projects\u2014such as Carri\u00e8res Centrales in Hay Mohammadi\u2014in a style described as vernacular modernism influenced modernist architecture around the world.\nCasam\u00e9moire and MAMMA. are two organizations dedicated to the preservation and appreciation of the city's architectural heritage.\nTransport.\nRapid transit.\nThe Casablanca Tramway is the rapid transit tram system in Casablanca. As of 2019, the network consists of two lines covering , with 71 stops; further lines (T3 and T4) are under construction.\nSince the 1970s, Casablanca had planned to build a metro system to offer some relief to the problems of traffic congestion and poor air quality. However, the city council voted to abandon the metro project in 2014 due to high costs, and decided to continue expanding the already operating tram system instead.\nAir.\nCasablanca's main airport is Mohammed V International Airport, Morocco's busiest airport. Regular domestic flights serve Marrakech, Rabat, Agadir, Oujda, Tangier, Al Hoceima, and Laayoune, as well as other cities.\nCasablanca is well-served by international flights to Europe, especially French and Spanish airports, and has regular connections to North American, Middle Eastern and sub-Saharan African destinations. New York City, Montreal, Paris, Washington D.C., London and Dubai are important primary destinations.\nThe older, smaller Casablanca-Anfa Airport to the west of the city, served certain destinations including Damascus and Tunis, and was largely closed to international civilian traffic in 2006. It was eventually demolished to make way for construction of the \"Casablanca Finance City\", the new heart of the city of Casablanca. Casablanca Tit Mellil Airport is located in the nearby community of Tit Mellil.\nCoach buses.\n\"Compagnie de Transports au Maroc\" (CTM) offers private intercity coach buses on various lines run servicing most notable Moroccan towns, as well as a number of European cities. These run from the CTM Bus Station on Leo Africanus Street near the Central Market in downtown Casablanca. Supratours, an affiliate of ONCF, also offers coach bus service at a slightly lower cost, departing from a station on Wilad Zian Street. There is another bus station farther down on the same street called the Wilad Zian Bus Station; this station is the country's largest bus station, serving over 800 buses daily, catering more to Morocco's lower income population.\nTaxis.\nRegistered taxis in Casablanca are coloured red and known as \"petits taxis\" (small taxis), or coloured white and known as \"grands taxis\" (big taxis). As is standard Moroccan practice, \"petits taxis\", typically small-four door Dacia Logan, Peugeot 207, or similar cars, provide metered cab service in the central metropolitan areas. \"Grands taxis\", generally older Mercedes-Benz sedans, provide shared mini-bus like service within the city on predefined routes, or shared intercity service. \"Grands taxis\" may also be hired for private service by the hour or day.\nTrains.\nCasablanca is served by three main railway stations run by the national rail service, the ONCF.\nCasa-Voyageurs is the main intercity station, from which trains run south to Marrakech or El Jadida and north to Mohammedia and Rabat, and then on either to Tangier or Meknes, Fes, Taza and Oujda/Nador. It also serves as the southern terminus of the Al-Boraq high speed line from Tangier. A dedicated airport shuttle service to Mohammed V International Airport also has its primary in-city stop at this station, for connections on to further destinations.\nCasa-Port serves primarily commuter trains such as the Train Navette Rapide (TNR or Aouita) operating on the Casablanca \u2013 Kenitra rail corridor, with some connecting trains running on to Gare de Casa-Voyageurs. The station provides a direct interchange between train and shipping services, and is located near several port-area hotels. It is the nearest station to the old town of Casablanca, and to the modern city centre, around the landmark Casablanca Twin Center. Casa-Port station is being rebuilt in a modern and enlarged configuration. During the construction, the station is still operational. From 2013, it will provide a close connection from the rail network to the city's new tram network.\nCasa-Oasis was originally a suburban commuter station which was fully redesigned and rebuilt in the early 21st century, and officially reopened in 2005 as a primary city rail station. Owing to its new status, all southern intercity train services to and from Casa-Voyageurs now call at Casa-Oasis. ONCF stated in 2005 that the refurbishment and upgrading of Casa-Oasis to intercity standards was intended to relieve passenger congestion at Casa-Voyageurs station.\nTourism.\nAlthough Mohammed V International Airport receives most international flights into Morocco, international tourism in Casablanca is not as developed as it is in cities such as Fes and Marrakech.\nThe Hassan II Mosque, which is the second largest mosque in Africa and the seventh largest in the world, is the city's main tourist attraction. Visitors also come to see the city's rich architectural heritage.\nPopular sites for national tourism include shopping centers such as Morocco Mall, Anfa Place, the Marina Shopping Center, and the Tachfine Center. Additional sites include the Corniche and the beach of Ain Diab, and parks such as the Arab League Park or the Sindibad theme park.\nTwin towns \u2013 sister cities.\nCasablanca is twinned with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCasablanca also has cooperation agreements with:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7303", "revid": "39552901", "url": "https://en.wikipedia.org/wiki?curid=7303", "title": "Cross", "text": "Geometrical figure\nA cross is a geometrical figure consisting of two intersecting lines or bars, usually perpendicular to each other. The lines usually run vertically and horizontally. A cross of oblique lines, in the shape of the Latin letter X, is termed a saltire in heraldic terminology.\nThe cross has been widely recognized as a symbol of Christianity from an early period. Before then, it was a pagan religious symbol throughout Europe and western Asia. The effigy of a man hanging on a cross was set up in the fields to protect the crops. It often appeared in conjunction with the female-genital circle or oval, to signify the sacred marriage, as in Egyptian amulet Nefer with male cross and female orb, considered as an amulet of blessedness, a charm of sexual harmony.\nName.\nThe word \"cross\" is recorded in 11th-century Old English as \"cros\", exclusively for the instrument of Christ's crucifixion, replacing the native Old English word \"rood\". The word's history is complicated; it appears to have entered English from Old Irish, possibly via Old Norse, ultimately from the Latin (or its accusative and its genitive ), \"stake, cross\". The English verb \"to cross\" arises from the noun c.\u20091200, first in the sense \"to make the sign of the cross\"; the generic meaning \"to intersect\" develops in the 15th century. The Latin word was influenced by popular etymology by a native Germanic word reconstructed as *\"krukjo\" (English \"crook\", Old English , Old Norse , Old High German ). This word, by conflation with Latin , gave rise to Old French (modern French ), the term for a shepherd's crook, adopted in English as \"crosier\".\nLatin referred to the gibbet where criminals were executed, a stake or pole, with or without transom, on which the condemned were impaled or hanged, but more particularly a cross or the pole of a carriage. The derived verb means \"to put to death on the cross\" or, more frequently, \"to put to the rack, to torture, torment\", especially in reference to mental troubles.\n In the Roman world, replaced as the name of some cross-like instruments for lethal and temporary punishment, ranging from a forked cross to a gibbet or gallows.\nThe field of etymology is of no help in any effort to trace a supposed original meaning of \"crux\". A \"crux\" can be of various shapes: from a single beam used for impaling or suspending () to the various composite kinds of cross () made from more beams than one. The latter shapes include not only the traditional \u2020-shaped cross (the ), but also the T-shaped cross (the or tau cross), which the descriptions in antiquity of the execution cross indicate as the normal form in use at that time, and the X-shaped cross (the \"crux decussata\" or saltire).\nThe Greek equivalent of Latin \"crux\" \"stake, gibbet\" is , found in texts of four centuries or more before the gospels and always in the plural number to indicate a stake or pole. From the first century BC, it is used to indicate an instrument used in executions. The Greek word is used in descriptions in antiquity of the execution cross, which indicate that its normal shape was similar to the Greek letter tau (\u03a4).\nHistory.\nPre-Christian.\nDue to the simplicity of the design (two intersecting lines), cross-shaped incisions make their appearance from deep prehistory; as petroglyphs in European cult caves, dating back to the beginning of the Upper Paleolithic, and throughout prehistory to the Iron Age.\nAlso of prehistoric age are numerous variants of the simple cross mark, including the \"crux gammata\" with curving or angular lines, and the Egyptian \"crux ansata\" with a loop.\nSpeculation has associated the cross symbol \u2013 even in the prehistoric period \u2013 with astronomical or cosmological symbology involving\n\"four elements\" (Chevalier, 1997) or the cardinal points, or the unity of a vertical axis mundi or celestial pole with the horizontal world (Koch, 1955). Speculation of this kind became especially popular in the mid- to late-19th century in the context of comparative mythology seeking to tie Christian mythology to ancient cosmological myths. Influential works in this vein included\nG. de Mortillet (1866), L. M\u00fcller (1865), W. W. Blake (1888), Ansault (1891), etc.\nIn the European Bronze Age the cross symbol appeared to carry a religious meaning, perhaps as a symbol of consecration, especially pertaining to burial.\nThe cross sign occurs trivially in tally marks, and develops into a number symbol independently in the Roman numerals (X \"ten\"), the Chinese rod numerals (\u5341 \"ten\") and the Brahmi numerals (\"four\", whence the numeral 4).\nIn the Phoenician alphabet and derived scripts, the cross symbol represented the phoneme /t/, i.e. the letter taw, which is the historical predecessor of Latin T. The letter name \"taw\" means \"mark\", presumably continuing the Egyptian hieroglyph \"two crossed sticks\" (Gardiner Z9).\nChristian.\nThe shape of the cross (\"crux\", \"stauros\" \"stake, gibbet\"), as represented by the letter T, came to be used as a \"seal\" or symbol of Early Christianity by the 2nd century. Clement of Alexandria in the early 3rd century calls it (\"the Lord's sign\") he repeats the idea, current as early as the Epistle of Barnabas, that the number 318 (in Greek numerals, \u03a4\u0399\u0397) in Genesis 14:14 was a foreshadowing (a \"type\") of the cross (the letter Tau) and of Jesus (the letters Iota Eta). Clement's contemporary Tertullian rejects the accusation that Christians are \"crucis religiosi\" (i.e. \"adorers of the gibbet\"), and returns the accusation by likening the worship of pagan idols to the worship of poles or stakes.\nIn his book \"De Corona\", written in 204, Tertullian tells how it was already a tradition for Christians to trace repeatedly on their foreheads the sign of the cross.\nWhile early Christians used the T-shape to represent the cross in writing and gesture, the use of the Greek cross and Latin cross, i.e. crosses with intersecting beams, appears in Christian art towards the end of Late Antiquity. An early example of the cruciform halo, used to identify Christ in paintings, is found in the \"Miracles of the Loaves and Fishes\" mosaic of Sant'Apollinare Nuovo, Ravenna (6th century). The Patriarchal cross, a Latin cross with an additional horizontal bar, first appears in the 10th century. A wide variation of cross symbols is introduced for the purposes of heraldry beginning in the age of the Crusades.\nMarks and graphemes.\nThe cross mark is used to mark a position, or as a check mark, but also to mark deletion.\nDerived from Greek Chi are the Latin letter X, Cyrillic Kha and possibly runic Gyfu.\nEgyptian hieroglyphs involving cross shapes include \"ankh\" \"life\", \"ndj\" \"protect\" and \"nfr\" \"good; pleasant, beautiful\".\nSumerian cuneiform had a simple cross-shaped character, consisting of a horizontal and a vertical wedge (\ud808\ude26), read as \"ma\u0161\" \"tax, yield, interest\"; the superposition of two diagonal wedges results in a decussate cross (\ud808\ude7d), read as \"pap\" \"first, pre-eminent\" (the superposition of these two types of crosses results in the eight-pointed star used as the sign for \"sky\" or \"deity\" (\ud808\udc2d), DINGIR). The cuneiform script has other, more complex, cruciform characters, consisting of an arrangement of boxes or the fourfold arrangement of other characters, including the archaic cuneiform characters LAK-210, LAK-276, LAK-278, LAK-617 and the classical sign EZEN (\ud808\udca1).\nPhoenician \"t\u0101w\" is still cross-shaped in Paleo-Hebrew alphabet and in some Old Italic scripts (Raetic and Lepontic), and its descendant T becomes again cross-shaped in the Latin minuscule t. The plus sign (+) is derived from Latin t via a simplification of a ligature for \"et\" \"and\" (introduced by Johannes Widmann in the late 15th century).\nThe letter Aleph is cross-shaped in Aramaic and paleo-Hebrew.\nEgyptian hieroglyphs with cross-shapes include Gardiner Z9 \u2013 Z11 (\"crossed sticks\", \"crossed planks\").\nOther, unrelated cross-shaped letters include Brahmi \"ka\" (predecessor of the Devanagari letter \u0915) and Old Turkic (Orkhon) \"d\u00b2\" and Old Hungarian \"b\", and Katakana \u30ca \"na\" and \u30e1\"me\".\nThe multiplication sign (\u00d7), often attributed to William Oughtred (who first used it in an appendix to the 1618 edition of John Napier's \"Descriptio\") apparently had been in occasional use since the mid 16th century.\nOther typographical symbols resembling crosses include the dagger or \"obelus\" (\u2020), the Chinese (\u5341, Kangxi radical 24) and Roman (X ten).\nUnicode has a variety of cross symbols in the \"Dingbat\" block (U+2700\u2013U+27BF):\nThe Miscellaneous Symbols block (U+2626 to U+262F) adds three specific Christian cross variants, viz. the Patriarchal cross (\u2626), Cross of Lorraine (\u2628) and Cross potent (\u2629, mistakenly labeled a \"Cross of Jerusalem\").\nEmblems.\nThe following is a list of cross symbols, \"except\" for variants of the Christian cross and Heraldic crosses, for which see the dedicated lists at Christian cross variants and Crosses in heraldry, respectively.\nPhysical gestures.\nCross shapes are made by a variety of physical gestures. Crossing the fingers of one hand is a common invocation of the symbol. The sign of the cross associated with Christian genuflection is made with one hand: in Eastern Orthodox tradition the sequence is head-heart-right shoulder-left shoulder, while in Oriental Orthodox, Catholic and Anglican tradition the sequence is head-heart-left-right.\nCrossing the index fingers of both hands represents and a charm against evil in European folklore. Other gestures involving more than one hand include the \"cross my heart\" movement associated with making a promise and the Tau shape of the referee's \"time out\" hand signal.\nIn Chinese-speaking cultures, crossed index fingers represent the number 10.\nUnicode.\nUnicode provides various cross symbol:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7304", "revid": "36440400", "url": "https://en.wikipedia.org/wiki?curid=7304", "title": "Coordination complex", "text": "Molecule or ion containing ligands datively bonded to a central metallic atom\nA coordination complex is a chemical compound consisting of a central atom or ion, which is usually metallic and is called the \"coordination centre\", and a surrounding array of bound molecules or ions, that are in turn known as \"ligands\" or complexing agents. Many metal-containing compounds, especially those that include transition metals (elements like titanium that belong to the periodic table's d-block), are coordination complexes.\nNomenclature and terminology.\nCoordination complexes are so pervasive that their structures and reactions are described in many ways, sometimes confusingly. The atom within a ligand that is bonded to the central metal atom or ion is called the donor atom. In a typical complex, a metal ion is bonded to several donor atoms, which can be the same or different. A polydentate (multiple bonded) ligand is a molecule or ion that bonds to the central atom through several of the ligand's atoms; ligands with 2, 3, 4 or even 6 bonds to the central atom are common. These complexes are called chelate complexes; the formation of such complexes is called chelation, complexation, and coordination.\nThe central atom or ion, together with all ligands, comprise the coordination sphere. The central atoms or ion and the donor atoms comprise the first coordination sphere.\nCoordination refers to the \"coordinate covalent bonds\" (dipolar bonds) between the ligands and the central atom. Originally, a complex implied a reversible association of molecules, atoms, or ions through such weak chemical bonds. As applied to coordination chemistry, this meaning has evolved. Some metal complexes are formed virtually irreversibly and many are bound together by bonds that are quite strong.\nThe number of donor atoms attached to the central atom or ion is called the coordination number. The most common coordination numbers are 2, 4, and especially 6. A hydrated ion is one kind of a complex ion (or simply a complex), a species formed between a central metal ion and one or more surrounding ligands, molecules or ions that contain at least one lone pair of electrons.\nIf all the ligands are monodentate, then the number of donor atoms equals the number of ligands. For example, the cobalt(II) hexahydrate ion or the hexaaquacobalt(II) ion\u00a0[Co(H2O)6]2+ is a hydrated-complex ion that consists of six water molecules attached to a metal ion\u00a0Co. The oxidation state and the coordination number reflect the number of bonds formed between the metal ion and the ligands in the complex ion. However, the coordination number of\u00a0Pt(en) is 4 (rather than 2) since it has two bidentate ligands, which contain four donor atoms in total.\nAny donor atom will give a pair of electrons. There are some donor atoms or groups which can offer more than one pair of electrons. Such are called bidentate (offers two pairs of electrons) or polydentate (offers more than two pairs of electrons). In some cases an atom or a group offers a pair of electrons to two similar or different central metal atoms or acceptors\u2014by division of the electron pair\u2014into a three-center two-electron bond. These are called bridging ligands.\nHistory.\nCoordination complexes have been known since the beginning of modern chemistry. Early well-known coordination complexes include dyes such as Prussian blue. Their properties were first well understood in the late 1800s, following the 1869 work of Christian Wilhelm Blomstrand. Blomstrand developed what has come to be known as the \"complex ion chain theory.\" In considering metal amine complexes, he theorized that the ammonia molecules compensated for the charge of the ion by forming chains of the type [(NH3)X]X+, where X is the coordination number of the metal ion. He compared his theoretical ammonia chains to hydrocarbons of the form (CH2)X.\nFollowing this theory, Danish scientist Sophus Mads J\u00f8rgensen made improvements to it. In his version of the theory, J\u00f8rgensen claimed that when a molecule dissociates in a solution there were two possible outcomes: the ions would bind via the ammonia chains Blomstrand had described or the ions would bind directly to the metal.\nIt was not until 1893 that the most widely accepted version of the theory today was published by Alfred Werner. Werner's work included two important changes to the Blomstrand theory. The first was that Werner described the two possibilities in terms of location in the coordination sphere. He claimed that if the ions were to form a chain, this would occur outside of the coordination sphere while the ions that bound directly to the metal would do so within the coordination sphere. In one of his most important discoveries however Werner disproved the majority of the chain theory. Werner discovered the spatial arrangements of the ligands that were involved in the formation of the complex hexacoordinate cobalt. His theory allows one to understand the difference between a coordinated ligand and a charge balancing ion in a compound, for example the chloride ion in the cobaltammine chlorides and to explain many of the previously inexplicable isomers.\nIn 1911, Werner first resolved the coordination complex hexol into optical isomers, overthrowing the theory that only carbon compounds could possess chirality.\nStructures.\nThe ions or molecules surrounding the central atom are called ligands. Ligands are classified as L or X (or a combination thereof), depending on how many electrons they provide for the bond between ligand and central atom. L ligands provide two electrons from a lone electron pair, resulting in a coordinate covalent bond. X ligands provide one electron, with the central atom providing the other electron, thus forming a regular covalent bond. The ligands are said to be coordinated to the atom. For alkenes, the pi bonds can coordinate to metal atoms. An example is ethylene in the complex (Zeise's salt).\nGeometry.\nIn coordination chemistry, a structure is first described by its coordination number, the number of ligands attached to the metal (more specifically, the number of donor atoms). Usually one can count the ligands attached, but sometimes even the counting can become ambiguous. Coordination numbers are normally between two and nine, but large numbers of ligands are not uncommon for the lanthanides and actinides. The number of bonds depends on the size, charge, and electron configuration of the metal ion and the ligands. Metal ions may have more than one coordination number.\nTypically the chemistry of transition metal complexes is dominated by interactions between s and p molecular orbitals of the donor-atoms in the ligands and the d orbitals of the metal ions. The s, p, and d orbitals of the metal can accommodate 18 electrons (see 18-Electron rule). The maximum coordination number for a certain metal is thus related to the electronic configuration of the metal ion (to be more specific, the number of empty orbitals) and to the ratio of the size of the ligands and the metal ion. Large metals and small ligands lead to high coordination numbers, e.g. . Small metals with large ligands lead to low coordination numbers, e.g. . Due to their large size, lanthanides, actinides, and early transition metals tend to have high coordination numbers.\nMost structures follow the points-on-a-sphere pattern (or, as if the central atom were in the middle of a polyhedron where the corners of that shape are the locations of the ligands), where orbital overlap (between ligand and metal orbitals) and ligand-ligand repulsions tend to lead to certain regular geometries. The most observed geometries are listed below, but there are many cases that deviate from a regular geometry, e.g. due to the use of ligands of diverse types (which results in irregular bond lengths; the coordination atoms do not follow a points-on-a-sphere pattern), due to the size of ligands, or due to electronic effects (see, e.g., Jahn\u2013Teller distortion):\nThe idealized descriptions of 5-, 7-, 8-, and 9- coordination are often indistinct geometrically from alternative structures with slightly differing L-M-L (ligand-metal-ligand) angles, e.g. the difference between square pyramidal and trigonal bipyramidal structures.\nTo distinguish between the alternative coordinations for five-coordinated complexes, the \u03c4 geometry index was invented by Addison et al. This index depends on angles by the coordination center and changes between 0 for the square pyramidal to 1 for trigonal bipyramidal structures, allowing to classify the cases in between. This system was later extended to four-coordinated complexes by Houser et al. and also Okuniewski et al.\nIn systems with low d electron count, due to special electronic effects such as (second-order) Jahn\u2013Teller stabilization, certain geometries (in which the coordination atoms do not follow a points-on-a-sphere pattern) are stabilized relative to the other possibilities, e.g. for some compounds the trigonal prismatic geometry is stabilized relative to octahedral structures for six-coordination.\nIsomerism.\nThe arrangement of the ligands is fixed for a given complex, but in some cases it is mutable by a reaction that forms another stable isomer.\nThere exist many kinds of isomerism in coordination complexes, just as in many other compounds.\nStereoisomerism.\nStereoisomerism occurs with the same bonds in distinct orientations. Stereoisomerism can be further classified into:\nCis\u2013trans isomerism and facial\u2013meridional isomerism.\nCis\u2013trans isomerism occurs in octahedral and square planar complexes (but not tetrahedral). When two ligands are adjacent they are said to be cis, when\nopposite each other, trans. When three identical ligands occupy one face of an octahedron, the isomer is said to be facial, or fac. In a \"fac\" isomer, any two identical ligands are adjacent or \"cis\" to each other. If these three ligands and the metal ion are in one plane, the isomer is said to be meridional, or mer. A \"mer\" isomer can be considered as a combination of a \"trans\" and a \"cis\", since it contains both trans and cis pairs of identical ligands.\nOptical isomerism.\nOptical isomerism occurs when a complex is not superimposable with its mirror image. It is so called because the two isomers are each optically active, that is, they rotate the plane of polarized light in opposite directions. In the first molecule shown, the symbol \u039b (\"lambda\") is used as a prefix to describe the left-handed propeller twist formed by three bidentate ligands. The second molecule is the mirror image of the first, with the symbol \u0394 (\"delta\") as a prefix for the right-handed propeller twist. The third and fourth molecules are a similar pair of \u039b and \u0394 isomers, in this case with two bidentate ligands and two identical monodentate ligands.\nStructural isomerism.\nStructural isomerism occurs when the bonds are themselves different. Four types of structural isomerism are recognized: ionisation isomerism, solvate or hydrate isomerism, linkage isomerism and coordination isomerism.\nElectronic properties.\nMany of the properties of transition metal complexes are dictated by their electronic structures. The electronic structure can be described by a relatively ionic model that ascribes formal charges to the metals and ligands. This approach is the essence of crystal field theory (CFT). Crystal field theory, introduced by Hans Bethe in 1929, gives a quantum mechanically based attempt at understanding complexes. But crystal field theory treats all interactions in a complex as ionic and assumes that the ligands can be approximated by negative point charges.\nMore sophisticated models embrace covalency, and this approach is described by ligand field theory (LFT) and Molecular orbital theory (MO). Ligand field theory, introduced in 1935 and built from molecular orbital theory, can handle a broader range of complexes and can explain complexes in which the interactions are covalent. The chemical applications of group theory can aid in the understanding of crystal or ligand field theory, by allowing simple, symmetry based solutions to the formal equations.\nChemists tend to employ the simplest model required to predict the properties of interest; for this reason, CFT has been a favorite for the discussions when possible. MO and LF theories are more complicated, but provide a more realistic perspective.\nThe electronic configuration of the complexes gives them some important properties:\nColor of transition metal complexes.\nTransition metal complexes often have spectacular colors caused by electronic transitions by the absorption of light. For this reason they are often applied as pigments. Most transitions that are related to colored metal complexes are either d\u2013d transitions or charge transfer bands. In a d\u2013d transition, an electron in a d\u00a0orbital on the metal is excited by a photon to another d orbital of higher energy, therefore d\u2013d transitions occur only for partially-filled d-orbital complexes (d1\u20139). For complexes having d0 or d10 configuration, charge transfer is still possible even though d\u2013d transitions are not. A charge transfer band entails promotion of an electron from a metal-based orbital into an empty ligand-based orbital (metal-to-ligand charge transfer or MLCT). The converse also occurs: excitation of an electron in a ligand-based orbital into an empty metal-based orbital (ligand-to-metal charge transfer or LMCT). These phenomena can be observed with the aid of electronic spectroscopy; also known as UV-Vis. For simple compounds with high symmetry, the d\u2013d transitions can be assigned using Tanabe\u2013Sugano diagrams. These assignments are gaining increased support with computational chemistry.\nColors of lanthanide complexes.\nSuperficially lanthanide complexes are similar to those of the transition metals in that some are colored. However, for the common Ln3+ ions (Ln = lanthanide) the colors are all pale, and hardly influenced by the nature of the ligand. The colors are due to 4f electron transitions. As the 4f orbitals in lanthanides are \"buried\" in the xenon core and shielded from the ligand by the 5s and 5p orbitals they are therefore not influenced by the ligands to any great extent leading to a much smaller crystal field splitting than in the transition metals. The absorption spectra of an Ln3+ ion approximates to that of the free ion where the electronic states are described by spin-orbit coupling. This contrasts to the transition metals where the ground state is split by the crystal field. Absorptions for Ln3+ are weak as electric dipole transitions are parity forbidden (Laporte forbidden) but can gain intensity due to the effect of a low-symmetry ligand field or mixing with higher electronic states (\"e.g.\" d orbitals). f-f absorption bands are extremely sharp which contrasts with those observed for transition metals which generally have broad bands. This can lead to extremely unusual effects, such as significant color changes under different forms of lighting.\nMagnetism.\nMetal complexes that have unpaired electrons are magnetic. Considering only monometallic complexes, unpaired electrons arise because the complex has an odd number of electrons or because electron pairing is destabilized. Thus, monomeric Ti(III) species have one \"d-electron\" and must be (para)magnetic, regardless of the geometry or the nature of the ligands. Ti(II), with two d-electrons, forms some complexes that have two unpaired electrons and others with none. This effect is illustrated by the compounds TiX2[(CH3)2PCH2CH2P(CH3)2]2: when X\u00a0=\u00a0Cl, the complex is paramagnetic (high-spin configuration), whereas when X\u00a0=\u00a0CH3, it is diamagnetic (low-spin configuration). It is important to realize that ligands provide an important means of adjusting the ground state properties.\nIn bi- and polymetallic complexes, in which the individual centres have an odd number of electrons or that are high-spin, the situation is more complicated. If there is interaction (either direct or through ligand) between the two (or more) metal centres, the electrons may couple (antiferromagnetic coupling, resulting in a diamagnetic compound), or they may enhance each other (ferromagnetic coupling). When there is no interaction, the two (or more) individual metal centers behave as if in two separate molecules.\nReactivity.\nComplexes show a variety of possible reactivities:\nIf the ligands around the metal are carefully chosen, the metal can aid in (stoichiometric or catalytic) transformations of molecules or be used as a sensor.\nClassification.\nMetal complexes, also known as coordination compounds, include virtually all metal compounds. The study of \"coordination chemistry\" is the study of \"inorganic chemistry\" of all alkali and alkaline earth metals, transition metals, lanthanides, actinides, and metalloids. Thus, coordination chemistry is the chemistry of the majority of the periodic table. Metals and metal ions exist, in the condensed phases at least, only surrounded by ligands.\nThe areas of coordination chemistry can be classified according to the nature of the ligands, in broad terms:\n Examples: [Co(EDTA)]\u2212, [Co(NH3)6]3+, [Fe(C2O4)3]3-\n Example: (C5H5)Fe(CO)2CH3\n Example: hemoglobin contains heme, a porphyrin complex of iron\n Example: chlorophyll contains a porphyrin complex of magnesium\n Many natural ligands are \"classical\" especially including water.\n Example Ru3(CO)12\nExample: [Fe4S4(Scysteinyl)4]2\u2212, in which a cluster is embedded in a biologically active species.\nMineralogy, materials science, and solid state chemistry\u00a0\u2013 as they apply to metal ions\u00a0\u2013 are subsets of coordination chemistry in the sense that the metals are surrounded by ligands. In many cases these ligands are oxides or sulfides, but the metals are coordinated nonetheless, and the principles and guidelines discussed below apply. In hydrates, at least some of the ligands are water molecules. It is true that the focus of mineralogy, materials science, and solid state chemistry differs from the usual focus of coordination or inorganic chemistry. The former are concerned primarily with polymeric structures, properties arising from a collective effects of many highly interconnected metals. In contrast, coordination chemistry focuses on reactivity and properties of complexes containing individual metal atoms or small ensembles of metal atoms.\nNomenclature of coordination complexes.\nThe basic procedure for naming a complex is:\nExamples:\n [Cd(CN)2(en)2] \u2192 dicyanidobis(ethylenediamine)cadmium(II)\n [CoCl(NH3)5]SO4 \u2192 pentaamminechloridocobalt(III) sulfate\n [Cu(H2O)6] 2+ \u2192 hexaaquacopper(II) ion\n [CuCl5NH3]3\u2212 \u2192 amminepentachloridocuprate(II) ion\n K4[Fe(CN)6] \u2192 potassium hexacyanidoferrate(II)\n [NiCl4]2\u2212 \u2192 tetrachloridonickelate(II) ion (The use of chloro- was removed from IUPAC naming convention)\nThe coordination number of ligands attached to more than one metal (bridging ligands) is indicated by a subscript to the Greek symbol \u03bc placed before the ligand name. Thus the dimer of aluminium trichloride is described by Al2Cl4(\u03bc2-Cl)2.\nAny anionic group can be electronically stabilized by any cation. An anionic complex can be stabilised by a hydrogen cation, becoming an acidic complex which can dissociate to release the cationic hydrogen. This kind of complex compound has a name with \"ic\" added after the central metal. For example, H2[Pt(CN)4] has the name tetracyanoplatinic (II) acid.\nStability constant.\nThe affinity of metal ions for ligands is described by a stability constant, also called the formation constant, and is represented by the symbol Kf. It is the equilibrium constant for its assembly from the constituent metal and ligands, and can be calculated accordingly, as in the following example for a simple case:\nxM (aq) + yL (aq) \u21cc zZ (aq)\nformula_1\nwhere : x, y, and z are the stoichiometric coefficients of each species. M stands for metal / metal ion , the L for Lewis bases , and finally Z for complex ions. Formation constants vary widely. Large values indicate that the metal has high affinity for the ligand, provided the system is at equilibrium.\nSometimes the stability constant will be in a different form known as the constant of destability. This constant is expressed as the inverse of the constant of formation and is denoted as Kd = 1/Kf . This constant represents the reverse reaction for the decomposition of a complex ion into its individual metal and ligand components. When comparing the values for Kd, the larger the value, the more unstable the complex ion is.\nAs a result of these complex ions forming in solutions they also can play a key role in solubility of other compounds. When a complex ion is formed it can alter the concentrations of its components in the solution. For example:\nAg + 2NH3 \u21cc Ag(NH3)\nAgCl(s) + H2O(l) \u21cc Ag + Cl\nIf these reactions both occurred in the same reaction vessel, the solubility of the silver chloride would be increased by the presence of NH4OH because formation of the Diammine argentum(I) complex consumes a significant portion of the free silver ions from the solution. By Le Chatelier's principle, this causes the equilibrium reaction for the dissolving of the silver chloride, which has silver ion as a product, to shift to the right.\nThis new solubility can be calculated given the values of Kf and Ksp for the original reactions. The solubility is found essentially by combining the two separate equilibria into one combined equilibrium reaction and this combined reaction is the one that determines the new solubility. So Kc, the new solubility constant, is denoted by:\nformula_2\nApplication of coordination compounds.\nAs metals only exist in solution as coordination complexes, it follows then that this class of compounds is useful in a wide variety of ways.\nBioinorganic chemistry.\nIn bioinorganic chemistry and bioorganometallic chemistry, coordination complexes serve either structural or catalytic functions. An estimated 30% of proteins contain metal ions. Examples include the intensely colored vitamin B12, the heme group in hemoglobin, the cytochromes, the chlorin group in chlorophyll, and carboxypeptidase, a hydrolytic enzyme important in digestion. Another complex ion enzyme is catalase, which decomposes the cell's waste hydrogen peroxide. Synthetic coordination compounds are also used to bind to proteins and especially nucleic acids (e.g. anticancer drug cisplatin).\nIndustry.\nHomogeneous catalysis is a major application of coordination compounds for the production of organic substances. Processes include hydrogenation, hydroformylation, oxidation. In one example, a combination of titanium trichloride and triethylaluminium gives rise to Ziegler\u2013Natta catalysts, used for the polymerization of ethylene and propylene to give polymers of great commercial importance as fibers, films, and plastics.\nNickel, cobalt, and copper can be extracted using hydrometallurgical processes involving complex ions. They are extracted from their ores as ammine complexes. Metals can also be separated using the selective precipitation and solubility of complex ions. Cyanide is used chiefly for extraction of gold and silver from their ores.\nPhthalocyanine complexes are an important class of pigments.\nAnalysis.\nAt one time, coordination compounds were used to identify the presence of metals in a sample. Qualitative inorganic analysis has largely been superseded by instrumental methods of analysis such as atomic absorption spectroscopy (AAS), inductively coupled plasma atomic emission spectroscopy (ICP-AES) and inductively coupled plasma mass spectrometry (ICP-MS).\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7305", "revid": "6855902", "url": "https://en.wikipedia.org/wiki?curid=7305", "title": "Coleco", "text": "American manufacturer of consumer electronics\nColeco Industries, Inc. () was an American company founded in 1932 by Maurice Greenberg as The Connecticut Leather Company. It was a successful toy company in the 1980s, mass-producing versions of Cabbage Patch Kids dolls and its video game consoles, the Coleco Telstar dedicated consoles and ColecoVision. While the company ceased operations in 1988 as a result of bankruptcy, the Coleco brand was revived in 2005, and remains active to this day.\nOverview.\nColeco Industries, Inc. began in 1932 as The Connecticut Leather Company. The business supplied leather and \"shoe findings\" (the supplies and paraphernalia of a shoe repair shop) to shoe repairers. In 1938, the company began selling rubber footwear. During World War II demand for the company's supplies increased and by the end of the war, the company was larger and had expanded into new and used shoe machinery, hat cleaning equipment and marble shoeshine stands.\nBy the early 1950s, and thanks to Maurice Greenberg's son, Leonard Greenberg, the company had diversified further and was making leather lacing and leathercraft kits. In 1954, at the New York Toy Fair, their leather moccasin kit was selected as a Child Guidance Prestige Toy, and Connecticut Leather Company decided to commit to the toy business. In 1956, Leonard read about the emerging technology of vacuum formed plastic; the company adopted this and it became increasingly successful, producing a wide variety of plastic toys and wading pools.\nIn 1961, the leather and shoe findings portion of the business was sold, and Connecticut Leather Company became Coleco Industries, Inc. On January 9, 1962, Coleco went public, offering 120,000 shares of stock at $5.00 a share.\nIn 1963, the company acquired the Kestral Corporation of Springfield, Massachusetts, a manufacturer of inflatable vinyl pools and toys. This led to Coleco becoming the largest manufacturer of above-ground swimming pools in the world.\nIn 1966, Leonard persuaded his brother Arnold Greenberg to join the company. Further acquisitions included Playtime Products (1966) and Eagle Toys of Canada (1968). By the end of the 1960s, Coleco operated ten manufacturing facilities and occupied a new corporate headquarters in Hartford, Connecticut.\nColeco experienced financial difficulty during the 1970s, even though sales had grown to $48.6 million in 1971. In 1972 Coleco entered the snowmobile market through acquisition. Lower than expected snowfall that year and market conditions led to very reduced sales and poor profits.\nDozens of companies rushed to introduce game systems in 1976 year after the release of Atari's successful \"Pong\" console and the company entered the video game console business with the Telstar. Nearly all of the new game systems were based on General Instrument's \"\"Pong\"-on-a-chip\". General Instrument had underestimated demand, resulting in severe shortages. However, Coleco was one of the first to place an order and therefore one of the few companies to receive the full order. Though dedicated game consoles did not last long on the market, their early order enabled Coleco to break even.\nColeco continued to perform well in electronics. The company transitioned into handheld electronic games, a market popularized by Mattel. An early success was \"Electronic Quarterback\". Coleco produced two popular lines of games, the \"head to head\" series of two player sports games (\"Football\", \"Baseball\", \"Basketball\", \"Soccer\", \"Hockey\", \"Boxing\") and the Mini-Arcade series of licensed video arcade titles such as \"Donkey Kong\" and \"Ms. Pac-Man\". A third line of educational handhelds was also produced and included the Electronic Learning Machine, \"Lil Genius\", \"Digits\", and a trivia game called \"Quiz Wiz\". Launched in 1982, their first four tabletop Mini-Arcades, for \"Pac-Man\", \"Galaxian\", \"Donkey Kong\", and \"Frogger\", sold approximately three million units within a year. Among these, 1.5 million units were sold for \"Pac-Man\" alone. In 1983, it released three more Mini-Arcades: for \"Ms. Pac-Man\", \"Donkey Kong Junior\", and \"Zaxxon\".\nColeco returned to the video game console market in 1982 with the launch of the ColecoVision. The system was quite popular, and came bundled with a copy of \"Donkey Kong\". The console sold 560,000 units in 1982. Coleco also hedged its bet on video games by introducing a line of ROM cartridges for the Atari 2600 and Intellivision, selling six million cartridges for both systems, along with two million sold for the ColecoVision for a total of eight million cartridges sold in 1982. It also introduced the Coleco Gemini, a clone of the popular Atari 2600, which came bundled with a copy of \"Donkey Kong\".\nWhen the video game business began to implode in 1983, it seemed clear that video game consoles were being supplanted by home computers. Bob Greenberg, son of Leonard Greenberg and nephew of Arnold Greenberg, left Microsoft where he had been working as a program developer at the time to assist in Coleco's entry into this market. Coleco's strategy was to introduce the Coleco Adam home computer, both as a stand-alone system and as an expansion module to the ColecoVision. The effort failed, in part because Adams were often unreliable due to being released with fatal bugs, and in part because the computer's release coincided with the home computer industry crashing. Coleco withdrew from electronics early in 1985.\nIn 1983, Coleco released the Cabbage Patch Kids series of dolls which were wildly successful. In the same year, Dr. Seuss signed a deal with Coleco to design a line of toys, including home video games based on his characters. Flush with success, Coleco purchased Leisure Dynamics (manufacturer of the board games \"Aggravation\" and \"Perfection\") and beleaguered Selchow and Righter, manufacturers of \"Scrabble\", \"Parcheesi\", and \"Trivial Pursuit\", in 1986. Sales of Selchow &amp; Righter games had plummeted, leaving them with warehouses full of unsold games. The purchase price for Selchow &amp; Righter was $75 million. That same year, Coleco introduced an ALF plush, based on the furry alien character who had his own television series at the time, as well as a talking version and a cassette-playing \"Storytelling ALF\" doll. The combination of the purchase of Selchow &amp; Righter, the disastrous Adam computer, and the public's waning infatuation with Cabbage Patch Dolls all contributed to Coleco's financial decline. In 1988, the company filed for Chapter 11 bankruptcy.\nThe reorganized Coleco sold off all of its North American assets and outsourced thousands of jobs to foreign countries, closing plants in Amsterdam, New York and other cities. In 1988, Canada based SLM Action Sports Inc. purchased Coleco's swimming pool and snow goods divisions. In 1989, Hasbro purchased most of Coleco's remaining product lines.\nBrand.\nColeco as a brand name has been owned by several entities since it was created in 1961 by Coleco Industries, Inc.\nIn 2005, River West Brands, now Dormitus Brands, a Chicago-based brand revitalization company, re-introduced the Coleco brand to the marketplace. In late 2006, the company introduced the Coleco Sonic, a handheld system containing twenty Master System and Game Gear games, including two from the \"Sonic the Hedgehog\" series. In 2014, River West Brands established the subsidiary Coleco Holdings for their Coleco-branded projects.\nIn December 2015, Coleco Holdings announced the development of the Coleco Chameleon, a new cartridge-based video game system; in actuality, a re-branding of the controversial Retro VGS console, whose Indiegogo campaign failed to secure funding when it ended in early November 2015, with only $63,546 raised of its $1.95 million goal. In the press release, it was established that the system would be able to play new and classic games in the 8, 16, and 32-bit styles. The release for the system was announced to be sometime in early 2016, with a demonstration at Toy Fair New York in February. However, some critics suggested that the prototype fell short of its developmental goals and was nothing more than the motherboard of a Super NES model SNS-101 inside an Atari Jaguar case. Later mock images of a prototype posted by AtariAge showed the device utilizing a CCTV capture card in place of a motherboard. After Retro VGS failed to produce a fully working prototype, Coleco Holdings pulled out of involvement with Retro VGS, terminating the project.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7306", "revid": "39872398", "url": "https://en.wikipedia.org/wiki?curid=7306", "title": "ColecoVision", "text": "Second-generation home video game console\nColecoVision is a second-generation home video-game console developed by Coleco and launched in North America in August 1982. It was released a year later in Europe by CBS Electronics as the CBS ColecoVision.\nThe console offered a closer experience to more powerful arcade video games compared to competitors such as the Atari 2600 and Intellivision. The initial catalog of twelve games on ROM cartridge included the first home version of Nintendo's \"Donkey Kong\" as the pack-in game. Approximately 136 games were published between 1982 and 1984, including Sega's \"Zaxxon\" and some ports of lesser known arcade games that found a larger audience on the console, such as \"Lady Bug\", \"Cosmic Avenger\", and \"Venture\".\nColeco released a series of hardware add-ons and special controllers to expand the capabilities of the console. \"Expansion Module #1\" allows the system to play Atari 2600 cartridges. A later module converts ColecoVision into the Coleco Adam home computer.\nColecoVision was discontinued in 1985 when Coleco withdrew from the video game market. Coleco had already contemplated shifting focus to their Cabbage Patch Kids success after the costly failure of their Coleco Adam computer.\nDevelopment.\nColeco entered the video game market in 1976 during the dedicated-game home console period with their line of Telstar consoles. When that market became oversaturated over the next few years, the company nearly went bankrupt, but found a successful product through handheld electronic games, with products that beat out those of the current market leader, Mattel. The company also developed a line of miniaturized tabletop arcade video games with licensed rights from arcade game makers including Sega, Bally, Midway, and Nintendo. Coleco was able to survive on sales of their electronic games through to 1982, but that market itself began to wane, and Greenberg was still interested in producing a home video game console.\nAccording to Eric Bromley, who led the engineering for the ColecoVision, Coleco president Arnold Greenberg had wanted to get into the programmable home console market with arcade-quality games, but the cost of components had been a limiting factor. As early as 1979, Bromley had drawn out specifications for a system using a Texas Instruments video and a General Instruments audio chip, but could not get the go-ahead due to the cost of RAM. Around 1981, Bromley saw an article in \"The Wall Street Journal\" that asserted the price of RAM had fallen and, after working the cost numbers, Bromley found the system cost fell within their cost margins. Within ten minutes of reporting this to Greenberg, they had established the working name \"ColecoVision\" for the console as they began a more thorough design, which the marketing department never was able to surpass.\nColeco recognized that licensed conversion of arcade conversions had worked for Atari in selling the Atari VCS, so they had approached Nintendo around 1981 for potential access to their arcade titles. Bromley described a tense set of meetings with Nintendo's president Hiroshi Yamauchi under typical Japanese customs where he sought to negotiate for game rights, though Yamauchi only offered seemingly obscure titles. After a meal with Yamauchi during one day, Bromley excused himself to the restroom and happened upon one of the first \"Donkey Kong\" cabinets, which had yet to be released to Western countries. Knowing this game would likely be a hit, Bromley arranged a meeting the following day with Yamauchi and requested the exclusive rights to \"Donkey Kong\"; Yamauchi offered them if only they could provide $ upfront by that day and gave them $ per unit sold. Greenberg agreed, though as in Japanese custom, Bromley did not have a formal contract from Nintendo on his return. By the time of that year's Consumer Electronics Show, which Yamauchi was attending, Bromley found out from Yamauchi's daughter and translator that he had apparently given the rights to Atari. With Yamauchi's daughter's help, Bromley was able to commit Yamauchi to sign a formal contract to affirm the rights to Coleco. Coleco's announcement that they would bundle \"Donkey Kong\" with the console was initially met with surprise and skepticism, with journalists and retailers questioning why they would give away their most anticipated home video game with the console.\nRelease.\nThe ColecoVision was released in August 1982. By Christmas 1982, Coleco had sold more than 500,000 units, in part on the strength of \"Donkey Kong\" as the bundled game. ColecoVision's main competitor was the less commercially successful Atari 5200. Sales quickly passed 1 million in early 1983.\nThe ColecoVision was distributed by CBS Electronics outside of North America and was branded the CBS ColecoVision. In Europe, the console was released in July 1983, nearly one year after the North American release.\nBy the beginning of 1984, quarterly sales of the ColecoVision had dramatically decreased.\nIn January 1985, Coleco discontinued the Adam, which was a home computer expansion for ColecoVision. By mid-1985, Coleco planned to withdraw from the video game market, and the ColecoVision was officially discontinued by October. Total sales are uncertain, but were ultimately in excess of 2 million consoles, with the console continuing to sell modestly up until its discontinuation. \nIn 1983, Spectravideo announced the SV-603 ColecoVision Video Game Adapter for its SV-318 computer. The company stated that the $70 product allowed users to \"enjoy the entire library of exciting ColecoVision video-game cartridges\".\nHardware.\nColecoVision is based around the Zilog Z80 CPU and a variant of the Texas Instruments TMS9918 video chip that was introduced in 1979.\nOn NTSC ColecoVision consoles, all first-party cartridges and most third-party software titles feature a 12.7 second pause before presenting the game select screen. CBS Electronics reduced this pause in the BIOS to 3.3 seconds for their PAL and SECAM ColecoVision consoles.\nExpansion Modules and accessories.\nFrom its introduction, Coleco touted the ColecoVision's hardware expandability by highlighting the \"Expansion Module Interface\" on the front of the unit. These hardware expansion modules and accessories were sold separately.\nAtari 2600 expansion.\n\"Expansion Module #1\" makes the ColecoVision compatible with Atari 2600 cartridges and controllers. It leveraged the fact that the 2600 used largely off-the-shelf components and was effectively a complete set of 2600 electronics, including a reverse-engineered equivalent of the 2600's sole custom chip, the TIA. The ColecoVision console did not do any translation or processing of the game code on the 2600 cartridges; it only provided power and clock input to and audio/video output from the expansion module, which was otherwise entirely self-contained and could be thought of as the first Atari 2600 clone console. Functionally, this gave the ColecoVision the largest software library of any console of its day. The expansion module prompted legal action from Atari. Coleco and Atari settled out of court, with Coleco becoming licensed under Atari's patents. The royalty-based license also applied to Coleco's Gemini game system, a stand-alone clone of the 2600.\nDriving controller.\n\"Expansion Module #2\" is a driving controller (steering wheel / gas pedal) that came packaged with the cartridge \"Turbo\". The gas pedal is merely a simple on/off switch. Although Coleco called the driving controller an expansion module, it actually plugs into the controller port, not the \"Expansion Module Interface\". The driving controller is also compatible with the cartridges \"Destructor\", \"Bump 'n' Jump\", \"Pitstop\", and \"The Dukes of Hazzard\".\nAdam computer expansion.\n\"Expansion Module #3\" converts the ColecoVision into the Adam computer, complete with keyboard, digital data pack (DDP) cassette drive, 64 KB RAM, and printer.\nRoller Controller.\nThe \"Roller Controller\" is a trackball that came packaged with the cartridge \"Slither\", a conversion of the arcade game. The roller controller uses a special power connector that is not compatible with Expansion Module #3 (the Adam computer). Coleco mailed an adapter to owners of both units who complained. The other cartridge programmed to use the roller controller is \"Victory\". A joystick mode switch on the roller controller allows it to be used with all cartridges including \"WarGames\", \"Omega Race\", and Atarisoft's \"Centipede\".\nSuper Action Controller.\nThe \"Super Action Controller Set\", available in September 1983, is a set of two handheld joystick controllers that came packaged with the cartridge \"Super Action Baseball\". Each controller has a ball-top joystick, four finger triggered action buttons, a 12-button numeric keypad, and a \"speed roller\". The cartridges \"Super Action Football\", \"Rocky\" \"Super Action Boxing\", and a conversion of the arcade game \"Front Line\" are also designed to be used with the \"Super Action Controller\".\nUnreleased.\nExpansion Module #3 was originally the Super Game Module. It was advertised for an August 1983 release but was ultimately cancelled and replaced with the Adam computer expansion. The Super Game Module added a tape drive known as the Exatron Stringy Floppy with 128KB capacity, and the additional RAM, said to be 30 KB, to load and execute programs from tape. Games could be distributed on tiny tapes, called \"wafers\", and be much larger than the 16KB or 32 KB ROM cartridges of the day. \"Super Donkey Kong\", with all screens and animations, \"Super Donkey Kong Jr\", and \"Super Smurf Rescue\" were demonstrated with the Super Game Module. The Adam computer expansion with its 256 KB tape drive and 64 KB RAM fulfilled the specifications promised by the Super Game Module.\nLegacy.\nMasayuki Uemura, head of Famicom development, stated that the ColecoVision set the bar that influenced how he approached the creation of the Famicom. During the creation of the Nintendo Entertainment System, Takao Sawano, chief manager of the project, brought a ColecoVision home to his family, who were impressed by the system's capability to produce smooth graphics, which contrasted with the flickering commonly seen on Atari 2600 games.\nIn 1986, Bit Corporation produced a ColecoVision clone called the Dina, which was sold in the United States by Telegames as the Telegames Personal Arcade.\nIGN named the ColecoVision their 12th-best video-game console out of their list of 25, citing \"its incredible accuracy in bringing current-generation arcade hits home.\"\nIn 1996, the first homebrew ColecoVision game was released: a \"Tetris\" clone titled \"Kevtris\".\nIn 1997, Telegames released \"Personal Arcade Vol. 1\", a collection of ColecoVision games for Microsoft Windows, and a 1998 follow-up, \"Colecovision Hits Volume One\".\nIn 2012, Opcode Games released their own Super Game Module expansion, which increases RAM from 1 KB to 32 KB and adds four additional sound channels. This expansion brings the ColecoVision close to the MSX architecture standard, allowing MSX software to be more easily ported.\nIn 2014, AtGames began producing the ColecoVision Flashback console that includes 60 games, but not the original pack-in game, \"Donkey Kong\".\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7309", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7309", "title": "Coleco Telstar series", "text": "Series of home video game consoles\nThe Coleco Telstar brand is a series of dedicated first-generation home video game consoles produced, released and marketed by Coleco from 1976 to 1978. Starting with Coleco Telstar \"Pong\" clone based video game console on General Instrument's AY-3-8500 chip in 1976, there were 14 consoles released in the Coleco Telstar series. About one million units of the first model called Coleco Telstar were sold.\nThe large product lineup and the impending fading out of the \"Pong\" machines led Coleco to face near-bankruptcy in 1980."}
{"id": "7310", "revid": "1139146959", "url": "https://en.wikipedia.org/wiki?curid=7310", "title": "Conventional warfare", "text": "War between two states in open confrontation\nConventional warfare is a form of warfare conducted by using conventional weapons and battlefield tactics between two or more states in open confrontation. The forces on each side are well-defined and fight by using weapons that target primarily the opponent's military. It is normally fought by using conventional weapons, not chemical, biological, radiological, or nuclear weapons.\nThe general purpose of conventional warfare is to weaken or destroy the opponent's military, which negates its ability to engage in conventional warfare. In forcing capitulation, however, one or both sides may eventually resort to unconventional warfare tactics.\nHistory.\nFormation of state.\nThe state was first advocated by Plato but found more acceptance in the consolidation of power under the Roman Catholic Church. European monarchs then gained power as the Catholic Church was stripped of temporal power and was replaced by the divine right of kings. In 1648, the powers of Europe signed the Treaty of Westphalia, which ended the religious violence for purely political governance and outlook, signifying the birth of the modern state.\nWithin the statist paradigm, only the state and its appointed representatives may bear arms and enter into war. In fact, war then became understood only as a conflict between sovereign states. Monarchs strengthened that idea and gave it the force of law. Any noble had been allowed start a war, but European monarchs had to consolidate military power in response to the Napoleonic Wars.\nClausewitzian paradigm.\nPrussia was one of the countries that tried to amass military power. Carl von Clausewitz, one of Prussia's officers, wrote \"On War\", a work rooted solely in the world of the state. All other forms of intrastate conflict, such as rebellion, are not accounted for because in theoretical terms, he could not account for warfare before the state. However, near the end of his life, he grew increasingly aware of the importance of non-state military actors, as is revealed in his conceptions of \"the people in arms\", which he noted arose from the same social and political sources as traditional interstate warfare.\nPractices such as raiding or blood feuds were then labeled criminal activities and stripped of legitimacy. That war paradigm reflected the view of most of the modernized world in the early 21st century, as is verified by examination of the conventional armies of the time: large, high-maintenance, and technologically-advanced armies designed to compete against similarly-designed forces.\nClausewitz also forwarded the issue of casus belli. Wars had been fought for social, religious, or even cultural reasons, and Clausewitz taught that war is merely \"a continuation of politics by other means.\" It is a rational calculation in which states fight for their interests (whether they are economic, security-related, or otherwise) once normal discourse has broken down.\nPrevalence.\nMost modern wars have been conducted using the means of conventional means. Confirmed use of biological warfare by a nation state has not occurred since 1945, and chemical warfare has been used only a few times (the latest known confrontation in which it was utilized being the Syrian Civil War). Nuclear warfare has only occurred once: the American bombing the Japanese cities of Hiroshima and Nagasaki in August 1945.\nSince World War II.\nThe state and Clausewitzian principles peaked in the World Wars, during the 20th century, but they also laid the groundwork for their dilapidation from nuclear proliferation. During the Cold War, the superpowers sought to avoid open conflict between their respective forces, as both sides recognized that such a clash could very easily escalate and quickly involve nuclear weapons. Instead, the superpowers fought each other through their involvement in proxy wars, military buildups, and diplomatic standoffs. Thus, no two nuclear powers have yet fought a conventional war directly except for two brief skirmishes between China and Soviet Union in the 1969 Sino-Soviet conflict and between India and Pakistan in the 1999 Kargil War.\nHowever, conventional wars have been fought since 1945 between countries without nuclear weapons, such as the Iran\u2013Iraq War and Eritrean\u2013Ethiopian War, or between a nuclear state and a weaker non-nuclear state, like the Gulf War and Russo-Ukrainian War.\nSee also.\nContrast:\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7312", "revid": "35841168", "url": "https://en.wikipedia.org/wiki?curid=7312", "title": "Chauvinism", "text": "Form of extreme patriotism and nationalism\nChauvinism is the unreasonable belief in the superiority or dominance of one's own group or people, who are seen as strong and virtuous, while others are considered weak, unworthy, or inferior. The \"Encyclopaedia Britannica\" describes it as a form of excessive and unreasonable patriotism and nationalism, a fervent faith in national excellence and glory.\nIn English, the word has also come to be used in some quarters as shorthand for male chauvinism, a trend reflected in \"Merriam-Webster's Dictionary\", which, as of 2018, began its first example of use of the term \"chauvinism\" with \"an attitude of superiority toward members of the opposite sex\".\nAs nationalism.\nAccording to legend, French soldier Nicolas Chauvin was badly wounded in the Napoleonic Wars and received a meager pension for his injuries. After Napoleon abdicated, Chauvin maintained his fanatical Bonapartist belief in the messianic mission of Imperial France, despite the unpopularity of this view under the Bourbon Restoration. His single-minded devotion to his cause, despite neglect by his faction and harassment by its enemies, started the use of the term.\n\"Chauvinism\" has extended from its original use to include fanatical devotion and undue partiality to any group or cause to which one belongs, especially when such partisanship includes prejudice against or hostility toward outsiders or rival groups and persists even in the face of overwhelming opposition. This French quality finds its parallel in the English-language term \"jingoism\", which has retained the meaning of \"chauvinism\" strictly in its original sense; that is, an attitude of belligerent nationalism.\nIn 1945, political theorist Hannah Arendt described the concept thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Chauvinism is an almost natural product of the national concept in so far as it springs directly from the old idea of the \"national mission\". ... [A] nation's mission might be interpreted precisely as bringing its light to other, less fortunate peoples that, for whatever reason, have miraculously been left by history without a national mission. As long as this concept did not develop into the ideology of chauvinism and remained in the rather vague realm of national or even nationalistic pride, it frequently resulted in a high sense of responsibility for the welfare of backward people.\nIn this sense, chauvinism is irrational, in that no one can claim their nation or ethnic group to be inherently superior to another. An example of a modern-day English nationalist extreme enough to be considered a chauvinist is Nigel Farage.\nMale chauvinism.\nMale chauvinism is the belief that men are superior to women. The first documented use of the phrase \"male chauvinism\" is in the 1935 Clifford Odets play \"Till the Day I Die\".\nIn the workplace.\nThe balance of the workforce changed during World War II. As men entered or were conscripted into the military to fight in the war, women started replacing them. After the war ended, men returned home to find jobs in the workplace now occupied by women, which \"threatened the self-esteem many men derive from their dominance over women in the family, the economy, and society at large.\" Consequently, male chauvinism was on the rise, according to Cynthia B. Lloyd.\nLloyd and Michael Korda have argued that as they integrated back into the workforce, men returned to predominate, holding positions of power while women worked as their secretaries, usually typing dictations and answering telephone calls. This division of labor was understood and expected, and women typically felt unable to challenge their position or male superiors, argue Korda and Lloyd.\nCauses.\nChauvinist assumptions are seen by some as a bias in the TAT psychological personality test. Through cross-examinations, the TAT exhibits a tendency toward chauvinistic stimuli for its questions and has the \"potential for unfavorable clinical evaluation\" for women.\nAn often cited study done in 1976 by Sherwyn Woods, Some Dynamics of Male Chauvinism, attempts to find the underlying causes of male chauvinism.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Male chauvinism was studied in the psychoanalytic therapy of 11 men. It refers to the maintenance of fixed beliefs and attitudes of male superiority, associated with overt or covert depreciation of women. Challenging chauvinist attitudes often results in anxiety or other symptoms. It is frequently not investigated in psychotherapy because it is ego-syntonic, parallels cultural attitudes, and because therapists often share similar bias or neurotic conflict. Chauvinism was found to represent an attempt to ward off anxiety and shame arising from one or more of four prime sources: unresolved infantile strivings and regressive wishes, hostile envy of women, oedipal anxiety, and power and dependency conflicts related to masculine self-esteem. Mothers were more important than fathers in the development of male chauvinism, and resolution was sometimes associated with decompensation in wives.Adam Jukes argues that a reason for male chauvinism is masculinity itself:For the vast majority of people all over the world, the mother is a primary carer...There's an asymmetry in the development of boys and girls. Infant boys have to learn how to be masculine. Girls don't. Masculinity is not in a state of crisis. Masculinity is a crisis. I don't believe misogyny is innate, but I believe it's inescapable because of the development of masculinity.\nFemale chauvinism.\nFemale chauvinism is the belief that women are superior to men. Second-wave feminist Betty Friedan observed that \"...the assumption that women have any moral or spiritual superiority as a class is [...] female chauvinism.\" Ariel Levy used the term in her book \"Female Chauvinist Pigs\", in which she argues that many young women in the United States and beyond are replicating male chauvinism and older misogynist stereotypes.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\nHuddy, Leonie; Del Ponte, Alessandro. (2019). \"National Identity, Pride, and Chauvinism\u2014their Origins and Consequences for Globalization Attitudes\". In \"Liberal Nationalism and Its Critics: Normative and Empirical Questions\" (eds) Gina Gustavsson, David Miller. Oxford: Oxford Academic, pp. 38\u201356. https://doi.org/10.1093/oso/9780198842545.003.0003\nTuchowski, Andrzej. (2017). \"Nationalism, Chauvinism and Racism as Reflected in European Musical Thought and in Compositions from the Interwar Period.\" Bern: Peter Lang. ISBN 9783631787274."}
{"id": "7314", "revid": "23645052", "url": "https://en.wikipedia.org/wiki?curid=7314", "title": "Colonized", "text": ""}
{"id": "7315", "revid": "2300502", "url": "https://en.wikipedia.org/wiki?curid=7315", "title": "Colonies", "text": ""}
{"id": "7316", "revid": "784330", "url": "https://en.wikipedia.org/wiki?curid=7316", "title": "Hypothetical types of biochemistry", "text": "Possible alternative biochemicals used by life forms\nHypothetical types of biochemistry are forms of biochemistry agreed to be scientifically viable but not proven to exist at this time. The kinds of living organisms currently known on Earth all use carbon compounds for basic structural and metabolic functions, water as a solvent, and DNA or RNA to define and control their form. If life exists on other planets or moons it may be chemically similar, though it is also possible that there are organisms with quite different chemistries\u00a0\u2013 for instance, involving other classes of carbon compounds, compounds of another element, or another solvent in place of water.\nThe possibility of life-forms being based on \"alternative\" biochemistries is the topic of an ongoing scientific discussion, informed by what is known about extraterrestrial environments and about the chemical behaviour of various elements and compounds. It is of interest in synthetic biology and is also a common subject in science fiction.\nThe element silicon has been much discussed as a hypothetical alternative to carbon. Silicon is in the same group as carbon on the periodic table and, like carbon, it is tetravalent. Hypothetical alternatives to water include ammonia, which, like water, is a polar molecule, and cosmically abundant; and non-polar hydrocarbon solvents such as methane and ethane, which are known to exist in liquid form on the surface of Titan.\nShadow biosphere.\nA shadow biosphere is a hypothetical microbial biosphere of Earth that uses radically different biochemical and molecular processes than currently known life. Although life on Earth is relatively well-studied, the shadow biosphere may still remain unnoticed because the exploration of the microbial world targets primarily the biochemistry of the macro-organisms.\nAlternative-chirality biomolecules.\nPerhaps the least unusual alternative biochemistry would be one with differing chirality of its biomolecules. In known Earth-based life, amino acids are almost universally of the L form and sugars are of the D form. Molecules using D amino acids or L sugars may be possible; molecules of such a chirality, however, would be incompatible with organisms using the opposing chirality molecules. Amino acids whose chirality is opposite to the norm are found on Earth, and these substances are generally thought to result from decay of organisms of normal chirality. However, physicist Paul Davies speculates that some of them might be products of \"anti-chiral\" life.\nIt is questionable, however, whether such a biochemistry would be truly alien. Although it would certainly be an alternative stereochemistry, molecules that are overwhelmingly found in one enantiomer throughout the vast majority of organisms can nonetheless often be found in another enantiomer in different (often basal) organisms such as in comparisons between members of Archaea and other domains, making it an open topic whether an alternative stereochemistry is truly novel.\nNon-carbon-based biochemistries.\nOn Earth, all known living things have a carbon-based structure and system. Scientists have speculated about the pros and cons of using atoms other than carbon to form the molecular structures necessary for life, but no one has proposed a theory employing such atoms to form all the necessary structures. However, as Carl Sagan argued, it is very difficult to be certain whether a statement that applies to all life on Earth will turn out to apply to all life throughout the universe. Sagan used the term \"carbon chauvinism\" for such an assumption. He regarded silicon and germanium as conceivable alternatives to carbon (other plausible elements include but are not limited to palladium and titanium); but, on the other hand, he noted that carbon does seem more chemically versatile and is more abundant in the cosmos). Norman Horowitz devised the experiments to determine whether life might exist on Mars that were carried out by the Viking Lander of 1976, the first U.S. mission to successfully land an uncrewed probe on the surface of Mars. Horowitz argued that the great versatility of the carbon atom makes it the element most likely to provide solutions, even exotic solutions, to the problems of survival on other planets. He considered that there was only a remote possibility that non-carbon life forms could exist with genetic information systems capable of self-replication and the ability to evolve and adapt.\nSilicon biochemistry.\nThe silicon atom has been much discussed as the basis for an alternative biochemical system, because silicon has many chemical properties similar to those of carbon and is in the same group of the periodic table, the carbon group. Like carbon, silicon can create molecules that are sufficiently large to carry biological information.\nHowever, silicon has several drawbacks as an alternative to carbon. Silicon, unlike carbon, lacks the ability to form chemical bonds with diverse types of atoms as is necessary for the chemical versatility required for metabolism, and yet this precise inability is what makes silicon less susceptible to bond with all sorts of impurities from which carbon, in comparison, is not shielded. Elements creating organic functional groups with carbon include hydrogen, oxygen, nitrogen, phosphorus, sulfur, and metals such as iron, magnesium, and zinc. Silicon, on the other hand, interacts with very few other types of atoms. Moreover, where it does interact with other atoms, silicon creates molecules that have been described as \"monotonous compared with the combinatorial universe of organic macromolecules\". This is because silicon atoms are much bigger, having a larger mass and atomic radius, and so have difficulty forming double bonds (the double-bonded carbon is part of the carbonyl group, a fundamental motif of carbon-based bio-organic chemistry).\nSilanes, which are chemical compounds of hydrogen and silicon that are analogous to the alkane hydrocarbons, are highly reactive with water, and long-chain silanes spontaneously decompose. Molecules incorporating polymers of alternating silicon and oxygen atoms instead of direct bonds between silicon, known collectively as silicones, are much more stable. It has been suggested that silicone-based chemicals would be more stable than equivalent hydrocarbons in a sulfuric-acid-rich environment, as is found in some extraterrestrial locations.\nOf the varieties of molecules identified in the interstellar medium as of 1998[ [update]], 84 are based on carbon, while only 8 are based on silicon. Moreover, of those 8 compounds, 4 also include carbon within them. The cosmic abundance of carbon to silicon is roughly 10 to 1. This may suggest a greater variety of complex carbon compounds throughout the cosmos, providing less of a foundation on which to build silicon-based biologies, at least under the conditions prevalent on the surface of planets. Also, even though Earth and other terrestrial planets are exceptionally silicon-rich and carbon-poor (the relative abundance of silicon to carbon in Earth's crust is roughly 925:1), terrestrial life is carbon-based. The fact that carbon is used instead of silicon may be evidence that silicon is poorly suited for biochemistry on Earth-like planets. Reasons for this may be that silicon is less versatile than carbon in forming compounds, that the compounds formed by silicon are unstable, and that it blocks the flow of heat.\nEven so, biogenic silica is used by some Earth life, such as the silicate skeletal structure of diatoms. According to the clay hypothesis of A. G. Cairns-Smith, silicate minerals in water played a crucial role in abiogenesis: they replicated their crystal structures, interacted with carbon compounds, and were the precursors of carbon-based life.\nAlthough not observed in nature, carbon\u2013silicon bonds have been added to biochemistry by using directed evolution (artificial selection). A heme containing cytochrome \"c\" protein from \"Rhodothermus marinus\" has been engineered using directed evolution to catalyze the formation of new carbon\u2013silicon bonds between hydrosilanes and diazo compounds.\nSilicon compounds may possibly be biologically useful under temperatures or pressures different from the surface of a terrestrial planet, either in conjunction with or in a role less directly analogous to carbon. Polysilanols, the silicon compounds corresponding to sugars, are soluble in liquid nitrogen, suggesting that they could play a role in very-low-temperature biochemistry.\nArsenic as an alternative to phosphorus.\nArsenic, which is chemically similar to phosphorus, while poisonous for most life forms on Earth, is incorporated into the biochemistry of some organisms. Some marine algae incorporate arsenic into complex organic molecules such as arsenosugars and arsenobetaines. Fungi and bacteria can produce volatile methylated arsenic compounds. Arsenate reduction and arsenite oxidation have been observed in microbes (\"Chrysiogenes arsenatis\"). Additionally, some prokaryotes can use arsenate as a terminal electron acceptor during anaerobic growth and some can utilize arsenite as an electron donor to generate energy.\nIt has been speculated that the earliest life forms on Earth may have used arsenic biochemistry in place of phosphorus in the structure of their DNA. A common objection to this scenario is that arsenate esters are so much less stable to hydrolysis than corresponding phosphate esters that arsenic is poorly suited for this function.\nThe authors of a 2010 geomicrobiology study, supported in part by NASA, have postulated that a bacterium, named GFAJ-1, collected in the sediments of Mono Lake in eastern California, can employ such 'arsenic DNA' when cultured without phosphorus. They proposed that the bacterium may employ high levels of poly-\u03b2-hydroxybutyrate or other means to reduce the effective concentration of water and stabilize its arsenate esters. This claim was heavily criticized almost immediately after publication for the perceived lack of appropriate controls. Science writer Carl Zimmer contacted several scientists for an assessment: \"I reached out to a dozen experts ... Almost unanimously, they think the NASA scientists have failed to make their case\".\nOther authors were unable to reproduce their results and showed that the study had issues with phosphate contamination, suggesting that the low amounts present could sustain extremophile lifeforms.\nAlternatively, it was suggested that GFAJ-1 cells grow by recycling phosphate from degraded ribosomes, rather than by replacing it with arsenate.\nNon-water solvents.\nIn addition to carbon compounds, all currently known terrestrial life also requires water as a solvent. This has led to discussions about whether water is the only liquid capable of filling that role. The idea that an extraterrestrial life-form might be based on a solvent other than water has been taken seriously in recent scientific literature by the biochemist Steven Benner, and by the astrobiological committee chaired by John A. Baross. Solvents discussed by the Baross committee include ammonia, sulfuric acid, formamide, hydrocarbons, and (at temperatures much lower than Earth's) liquid nitrogen, or hydrogen in the form of a supercritical fluid.\nCarl Sagan once described himself as both a carbon chauvinist and a water chauvinist; however, on another occasion he said that he was a carbon chauvinist but \"not that much of a water chauvinist\".\nHe speculated on hydrocarbons, hydrofluoric acid, and ammonia as possible alternatives to water.\nSome of the properties of water that are important for life processes include:\nWater as a compound is cosmically abundant, although much of it is in the form of vapour or ice. Subsurface liquid water is considered likely or possible on several of the outer moons: Enceladus (where geysers have been observed), Europa, Titan, and Ganymede. Earth and Titan are the only worlds currently known to have stable bodies of liquid on their surfaces.\nNot all properties of water are necessarily advantageous for life, however. For instance, water ice has a high albedo, meaning that it reflects a significant quantity of light and heat from the Sun. During ice ages, as reflective ice builds up over the surface of the water, the effects of global cooling are increased.\nThere are some properties that make certain compounds and elements much more favorable than others as solvents in a successful biosphere. The solvent must be able to exist in liquid equilibrium over a range of temperatures the planetary object would normally encounter. Because boiling points vary with the pressure, the question tends not to be \"does\" the prospective solvent remain liquid, but \"at what pressure\". For example, hydrogen cyanide has a narrow liquid-phase temperature range at 1\u00a0atmosphere, but in an atmosphere with the pressure of Venus, with of pressure, it can indeed exist in liquid form over a wide temperature range.\nAmmonia.\nThe ammonia molecule (NH3), like the water molecule, is abundant in the universe, being a compound of hydrogen (the simplest and most common element) with another very common element, nitrogen. The possible role of liquid ammonia as an alternative solvent for life is an idea that goes back at least to 1954, when J.\u00a0B.\u00a0S. Haldane raised the topic at a symposium about life's origin.\nNumerous chemical reactions are possible in an ammonia solution, and liquid ammonia has chemical similarities with water. Ammonia can dissolve most organic molecules at least as well as water does and, in addition, it is capable of dissolving many elemental metals. Haldane made the point that various common water-related organic compounds have ammonia-related analogs; for instance the ammonia-related amine group (\u2212NH2) is analogous to the water-related hydroxyl group (\u2212OH).\nAmmonia, like water, can either accept or donate an H+ ion. When ammonia accepts an H+, it forms the ammonium cation (NH4+), analogous to hydronium (H3O+). When it donates an H+ ion, it forms the amide anion (NH2\u2212), analogous to the hydroxide anion (OH\u2212). Compared to water, however, ammonia is more inclined to accept an H+ ion, and less inclined to donate one; it is a stronger nucleophile. Ammonia added to water functions as Arrhenius base: it increases the concentration of the anion hydroxide. Conversely, using a solvent system definition of acidity and basicity, water added to liquid ammonia functions as an acid, because it increases the concentration of the cation ammonium. The carbonyl group (C=O), which is much used in terrestrial biochemistry, would not be stable in ammonia solution, but the analogous imine group (C=NH) could be used instead.\nHowever, ammonia has some problems as a basis for life. The hydrogen bonds between ammonia molecules are weaker than those in water, causing ammonia's heat of vaporization to be half that of water, its surface tension to be a third, and reducing its ability to concentrate non-polar molecules through a hydrophobic effect. Gerald Feinberg and Robert Shapiro have questioned whether ammonia could hold prebiotic molecules together well enough to allow the emergence of a self-reproducing system. Ammonia is also flammable in oxygen and could not exist sustainably in an environment suitable for aerobic metabolism.\nA biosphere based on ammonia would likely exist at temperatures or air pressures that are extremely unusual in relation to life on Earth. Life on Earth usually exists within the melting point and boiling point of water, at a pressure designated as normal pressure, and between . When also held to normal pressure, ammonia's melting and boiling points are and respectively. Because chemical reactions generally proceed more slowly at lower temperatures, ammonia-based life existing in this set of conditions might metabolize more slowly and evolve more slowly than life on Earth. On the other hand, lower temperatures could also enable living systems to use chemical species that would be too unstable at Earth temperatures to be useful.\nAnother set of conditions where ammonia is liquid at Earth-like temperatures would involve it being at a much higher pressure. For example, at 60\u00a0atm ammonia melts at and boils at .\nAmmonia and ammonia\u2013water mixtures remain liquid at temperatures far below the freezing point of pure water, so such biochemistries might be well suited to planets and moons orbiting outside the water-based habitability zone. Such conditions could exist, for example, under the surface of Saturn's largest moon Titan.\nMethane and other hydrocarbons.\nMethane (CH4) is a simple hydrocarbon: that is, a compound of two of the most common elements in the cosmos: hydrogen and carbon. It has a cosmic abundance comparable with ammonia. Hydrocarbons could act as a solvent over a wide range of temperatures, but would lack polarity. Isaac Asimov, the biochemist and science fiction writer, suggested in 1981 that poly-lipids could form a substitute for proteins in a non-polar solvent such as methane. Lakes composed of a mixture of hydrocarbons, including methane and ethane, have been detected on the surface of Titan by the \"Cassini\" spacecraft.\nThere is debate about the effectiveness of methane and other hydrocarbons as a solvent for life compared to water or ammonia. Water is a stronger solvent than the hydrocarbons, enabling easier transport of substances in a cell. However, water is also more chemically reactive and can break down large organic molecules through hydrolysis. A life-form whose solvent was a hydrocarbon would not face the threat of its biomolecules being destroyed in this way. Also, the water molecule's tendency to form strong hydrogen bonds can interfere with internal hydrogen bonding in complex organic molecules. Life with a hydrocarbon solvent could make more use of hydrogen bonds within its biomolecules. Moreover, the strength of hydrogen bonds within biomolecules would be appropriate to a low-temperature biochemistry.\nAstrobiologist Chris McKay has argued, on thermodynamic grounds, that if life does exist on Titan's surface, using hydrocarbons as a solvent, it is likely also to use the more complex hydrocarbons as an energy source by reacting them with hydrogen, reducing ethane and acetylene to methane. Possible evidence for this form of life on Titan was identified in 2010 by Darrell Strobel of Johns Hopkins University; a greater abundance of molecular hydrogen in the upper atmospheric layers of Titan compared to the lower layers, arguing for a downward diffusion at a rate of roughly 1025 molecules per second and disappearance of hydrogen near Titan's surface. As Strobel noted, his findings were in line with the effects Chris McKay had predicted if methanogenic life-forms were present. The same year, another study showed low levels of acetylene on Titan's surface, which were interpreted by Chris McKay as consistent with the hypothesis of organisms reducing acetylene to methane. While restating the biological hypothesis, McKay cautioned that other explanations for the hydrogen and acetylene findings are to be considered more likely: the possibilities of yet unidentified physical or chemical processes (e.g. a non-living surface catalyst enabling acetylene to react with hydrogen), or flaws in the current models of material flow. He noted that even a non-biological catalyst effective at 95\u00a0K would in itself be a startling discovery.\nAzotosome.\nA hypothetical cell membrane termed an azotosome, capable of functioning in liquid methane in Titan conditions was computer-modeled in an article published in February 2015. Composed of acrylonitrile, a small molecule containing carbon, hydrogen, and nitrogen, it is predicted to have stability and flexibility in liquid methane comparable to that of a phospholipid bilayer (the type of cell membrane possessed by all life on Earth) in liquid water. An analysis of data obtained using the Atacama Large Millimeter / submillimeter Array (ALMA), completed in 2017, confirmed substantial amounts of acrylonitrile in Titan's atmosphere. Later studies questioned whether acrylonitrile would be able to self-assemble into azotozomes.\nHydrogen fluoride.\nHydrogen fluoride (HF), like water, is a polar molecule, and due to its polarity it can dissolve many ionic compounds. Its melting point is \u221284\u00a0\u00b0C, and its boiling point is 19.54\u00a0\u00b0C (at atmospheric pressure); the difference between the two is a little more than 100\u00a0K. HF also makes hydrogen bonds with its neighbor molecules, as do water and ammonia. It has been considered as a possible solvent for life by scientists such as Peter Sneath and Carl Sagan.\nHF is dangerous to the systems of molecules that Earth-life is made of, but certain other organic compounds, such as paraffin waxes, are stable with it. Like water and ammonia, liquid hydrogen fluoride supports an acid\u2013base chemistry. Using a solvent system definition of acidity and basicity, nitric acid functions as a base when it is added to liquid HF.\nHowever, hydrogen fluoride is cosmically rare, unlike water, ammonia, and methane.\nHydrogen sulfide.\nHydrogen sulfide is the closest chemical analog to water, but is less polar and is a weaker inorganic solvent. Hydrogen sulfide is quite plentiful on Jupiter's moon Io and may be in liquid form a short distance below the surface; astrobiologist Dirk Schulze-Makuch has suggested it as a possible solvent for life there. On a planet with hydrogen sulfide oceans, the source of the hydrogen sulfide could come from volcanoes, in which case it could be mixed in with a bit of hydrogen fluoride, which could help dissolve minerals. Hydrogen sulfide life might use a mixture of carbon monoxide and carbon dioxide as their carbon source. They might produce and live on sulfur monoxide, which is analogous to oxygen (O2). Hydrogen sulfide, like hydrogen cyanide and ammonia, suffers from the small temperature range where it is liquid, though that, like that of hydrogen cyanide and ammonia, increases with increasing pressure.\nSilicon dioxide and silicates.\nSilicon dioxide, also known as silica and quartz, is very abundant in the universe and has a large temperature range where it is liquid. However, its melting point is , so it would be impossible to make organic compounds in that temperature, because all of them would decompose. Silicates are similar to silicon dioxide and some have lower melting points than silica. Feinberg and Shapiro have suggested that molten silicate rock could serve as a liquid medium for organisms with a chemistry based on silicon, oxygen, and other elements such as aluminium.\nOther solvents or cosolvents.\nOther solvents sometimes proposed:\nSulfuric acid in liquid form is strongly polar. It remains liquid at higher temperatures than water, its liquid range being 10\u00a0\u00b0C to 337\u00a0\u00b0C at a pressure of 1\u00a0atm, although above 300\u00a0\u00b0C it slowly decomposes. Sulfuric acid is known to be abundant in the clouds of Venus, in the form of aerosol droplets. In a biochemistry that used sulfuric acid as a solvent, the alkene group (C=C), with two carbon atoms joined by a double bond, could function analogously to the carbonyl group (C=O) in water-based biochemistry.\nA proposal has been made that life on Mars may exist and be using a mixture of water and hydrogen peroxide as its solvent.\nA 61.2% (by mass) mix of water and hydrogen peroxide has a freezing point of \u221256.5\u00a0\u00b0C and tends to super-cool rather than crystallize. It is also hygroscopic, an advantage in a water-scarce environment.\nSupercritical carbon dioxide has been proposed as a candidate for alternative biochemistry due to its ability to selectively dissolve organic compounds and assist the functioning of enzymes and because \"super-Earth\"- or \"super-Venus\"-type planets with dense high-pressure atmospheres may be common.\nOther speculations.\nNon-green photosynthesizers.\nPhysicists have noted that, although photosynthesis on Earth generally involves green plants, a variety of other-colored plants could also support photosynthesis, essential for most life on Earth, and that other colors might be preferred in places that receive a different mix of stellar radiation than Earth.\nThese studies indicate that blue plants would be unlikely; however yellow or red plants may be relatively common.\nVariable environments.\nMany Earth plants and animals undergo major biochemical changes during their life cycles as a response to changing environmental conditions, for example, by having a spore or hibernation state that can be sustained for years or even millennia between more active life stages. Thus, it would be biochemically possible to sustain life in environments that are only periodically consistent with life as we know it.\nFor example, frogs in cold climates can survive for extended periods of time with most of their body water in a frozen state, whereas desert frogs in Australia can become inactive and dehydrate in dry periods, losing up to 75% of their fluids, yet return to life by rapidly rehydrating in wet periods. Either type of frog would appear biochemically inactive (i.e. not living) during dormant periods to anyone lacking a sensitive means of detecting low levels of metabolism.\nAlanine world and hypothetical alternatives.\nThe genetic code may have evolved during the transition from the RNA world to a protein world. The Alanine World Hypothesis postulates that the evolution of the genetic code (the so-called GC phase) started with only four basic amino acids: alanine, glycine, proline and ornithine (now arginine). The evolution of the genetic code ended with 20 proteinogenic amino acids. From a chemical point of view, most of them are Alanine-derivatives particularly suitable for the construction of \u03b1-helices and \u03b2-sheets\u00a0\u2013 basic secondary structural elements of modern proteins. Direct evidence of this is an experimental procedure in molecular biology known as alanine scanning.\nA hypothetical \"Proline World\" would create a possible alternative life with the genetic code based on the proline chemical scaffold as the protein backbone. Similarly, a \"Glycine World\" and \"Ornithine World\" are also conceivable, but nature has chosen none of them. Evolution of life with Proline, Glycine, or Ornithine as the basic structure for protein-like polymers (foldamers) would lead to parallel biological worlds. They would have morphologically radically different body plans and genetics from the living organisms of the known biosphere.\nNonplanetary life.\nDusty plasma-based.\nIn 2007, Vadim N. Tsytovich and colleagues proposed that lifelike behaviors could be exhibited by dust particles suspended in a plasma, under conditions that might exist in space. Computer models showed that, when the dust became charged, the particles could self-organize into microscopic helical structures, and the authors offer \"a rough sketch of a possible model of...helical grain structure reproduction\".\nCosmic necklace-based.\nIn 2020, Luis A. Anchordoqu and Eugene M. Chudnovsky of the City University of New York hypothesized that cosmic necklace-based life composed of magnetic monopoles connected by cosmic strings could evolve inside stars. This would be achieved by a stretching of cosmic strings due to the star's intense gravity, thus allowing it to take on more complex forms and potentially form structures similar to the RNA and DNA structures found within carbon-based life. As such, it's theoretically possible that such beings could eventually become intelligent and construct a civilization using the power generated by the star's nuclear fusion. Because such use would use up part of the star's energy output, the luminosity would also fall. For this reason, it's thought that such life might exist inside stars observed to be cooling faster or dimmer than current cosmological models predict.\nLife on a neutron star.\nFrank Drake suggested in 1973 that intelligent life could inhabit neutron stars. Physical models in 1973 implied that Drake's creatures would be microscopic. In 1980, Robert L Forward wrote the science fiction novel Dragon's Egg using Drake's suggestion as a thesis.\nScientists who have published on this topic.\nScientists who have considered possible alternatives to carbon-water biochemistry include:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7322", "revid": "46108001", "url": "https://en.wikipedia.org/wiki?curid=7322", "title": "Creation myth", "text": "Symbolic narrative of how the world began and how people first came to inhabit it\nA creation myth or cosmogonic myth is a type of cosmogony, a symbolic narrative of how the world began and how people first came to inhabit it. While in popular usage the term \"myth\" often refers to false or fanciful stories, members of cultures often ascribe varying degrees of truth to their creation myths. In the society in which it is told, a creation myth is usually regarded as conveying profound truths\u00a0\u2013 metaphorically, symbolically, historically, or literally. They are commonly, although not always, considered cosmogonical myths\u00a0\u2013 that is, they describe the ordering of the cosmos from a state of chaos or amorphousness.\nCreation myths often share several features. They often are considered sacred accounts and can be found in nearly all known religious traditions. They are all stories with a plot and characters who are either deities, human-like figures, or animals, who often speak and transform easily. They are often set in a dim and nonspecific past that historian of religion Mircea Eliade termed \"in illo tempore\" ('at that time'). Creation myths address questions deeply meaningful to the society that shares them, revealing their central worldview and the framework for the self-identity of the culture and individual in a universal context.\nCreation myths develop in oral traditions and therefore typically have multiple versions; found throughout human culture, they are the most common form of myth.\nDefinitions.\nCreation myth definitions from modern references:\nReligion professor Mircea Eliade defined the word \"myth\" in terms of creation: \nMyth narrates a sacred history; it relates an event that took place in primordial Time, the fabled time of the \"beginnings.\" In other words, myth tells how, through the deeds of Supernatural Beings, a reality came into existence, be it the whole of reality, the Cosmos, or only a fragment of reality \u2013 an island, a species of plant, a particular kind of human behavior, an institution.\nHistory.\nCreation myths have been around since ancient history and have served important societal roles. Over 100 \"distinct\" ones have been discovered, although this is a fraction of the amount that have been told.\nChinese creation myths.\nChina has had many more creation myths transmitted throughout history and written down than most other countries and successors to prior civilizations.\nMeaning and function.\nAll creation myths are in one sense etiological because they attempt to explain how the world formed and where humanity came from. Myths attempt to explain the unknown and sometimes teach a lesson.\nEthnologists and anthropologists who study origin myths say that in the modern context theologians try to discern humanity's meaning from revealed truths and scientists investigate cosmology with the tools of empiricism and rationality, but creation myths define human reality in very different terms. In the past, historians of religion and other students of myth thought of such stories as forms of primitive or early-stage science or religion and analyzed them in a literal or logical sense. Today, however, they are seen as symbolic narratives which must be understood in terms of their own cultural context. Charles Long writes: \"The beings referred to in the myth \u2013 gods, animals, plants \u2013 are forms of power grasped existentially. The myths should not be understood as attempts to work out a rational explanation of deity.\"\nWhile creation myths are not literal explications, they do serve to define an orientation of humanity in the world in terms of a birth story. They provide the basis of a worldview that reaffirms and guides how people relate to the natural world, to any assumed spiritual world, and to each other. A creation myth acts as a cornerstone for distinguishing primary reality from relative reality, the origin and nature of being from non-being. In this sense cosmogonic myths serve as a philosophy of life \u2013 but one expressed and conveyed through symbol rather than through systematic reason. And in this sense they go beyond etiological myths (which explain specific features in religious rites, natural phenomena or cultural life). Creation myths also help to orient human beings in the world, giving them a sense of their place in the world and the regard that they must have for humans and nature.\nHistorian David Christian has summarised issues common to multiple creation myths:\nEach beginning seems to presuppose an earlier beginning. ... Instead of meeting a single starting point, we encounter an infinity of them, each of which poses the same problem. ... There are no entirely satisfactory solutions to this dilemma. What we have to find is not a solution but some way of dealing with the mystery ... And we have to do so using words. The words we reach for, from \"God\" to \"gravity\", are inadequate to the task. So we have to use language poetically or symbolically; and such language, whether used by a scientist, a poet, or a shaman, can easily be misunderstood.\nClassification.\nMythologists have applied various schemes to classify creation myths found throughout human cultures. Eliade and his colleague Charles Long developed a classification based on some common motifs that reappear in stories the world over. The classification identifies five basic types:\nMarta Weigle further developed and refined this typology to highlight nine themes, adding elements such as \"deus faber\", a creation crafted by a deity, creation from the work of two creators working together or against each other, creation from sacrifice and creation from division/conjugation, accretion/conjunction, or secretion.\nAn alternative system based on six recurring narrative themes was designed by Raymond Van Over:\n\"Ex nihilo\".\nThe myth that God created the world out of nothing \u2013 \"ex nihilo\" \u2013 is central today to Judaism, Christianity and Islam, and the medieval Jewish philosopher Maimonides felt it was the only concept that the three religions shared. Nonetheless, the concept is not found in the entire Hebrew Bible. The authors of Genesis 1 were concerned not with the origins of matter (the material which God formed into the habitable cosmos), but with assigning roles so that the Cosmos should function. In the early 2nd century CE, early Christian scholars were beginning to see a tension between the idea of world-formation and the omnipotence of God, and by the beginning of the 3rd century creation \"ex nihilo\" had become a fundamental tenet of Christian theology.\n\"Ex nihilo\" creation is found in creation stories from ancient Egypt, the Rig Veda, and many animistic cultures in Africa, Asia, Oceania and North America. In most of these stories, the world is brought into being by the speech, dream, breath, or pure thought of a creator but creation ex nihilo may also take place through a creator's bodily secretions.\nThe literal translation of the phrase \"ex nihilo\" is \"from nothing\" but in many creation myths the line is blurred whether the creative act would be better classified as a creation \"ex nihilo\" or creation from chaos. In \"ex nihilo\" creation myths, the potential and the substance of creation springs from within the creator. Such a creator may or may not be existing in physical surroundings such as darkness or water, but does not create the world from them, whereas in creation from chaos the substance used for creation is pre-existing within the unformed void.\nCreation from chaos.\nIn creation from chaos myths, there is nothing initially but a formless, shapeless expanse. In these stories the word \"chaos\" means \"disorder\", and this formless expanse, which is also sometimes called a void or an abyss, contains the material with which the created world will be made. Chaos may be described as having the consistency of vapor or water, dimensionless, and sometimes salty or muddy. These myths associate chaos with evil and oblivion, in contrast to \"order\" (\"cosmos\") which is the good. The act of creation is the bringing of order from disorder, and in many of these cultures it is believed that at some point the forces preserving order and form will weaken and the world will once again be engulfed into the abyss. One example is the Genesis creation narrative from the first chapter of the Book of Genesis.\nWorld parent.\nThere are two types of world parent myths, both describing a separation or splitting of a primeval entity, the world parent or parents. One form describes the primeval state as an eternal union of two parents, and the creation takes place when the two are pulled apart. The two parents are commonly identified as Sky (usually male) and Earth (usually female), who were so tightly bound to each other in the primeval state that no offspring could emerge. These myths often depict creation as the result of a sexual union and serve as genealogical record of the deities born from it.\nIn the second form of world parent myths, creation itself springs from dismembered parts of the body of the primeval being. Often, in these stories, the limbs, hair, blood, bones, or organs of the primeval being are somehow severed or sacrificed to transform into sky, earth, animal or plant life, and other worldly features. These myths tend to emphasize creative forces as animistic in nature rather than sexual, and depict the sacred as the elemental and integral component of the natural world. One example of this is the Norse creation myth described in V\u00f6lusp\u00e1, the first poem of \"Gylfaginning\".\nEmergence.\nIn emergence myths, humanity emerges from another world into the one they currently inhabit. The previous world is often considered the womb of the earth mother, and the process of emergence is likened to the act of giving birth. The role of midwife is usually played by a female deity, like the spider woman of several mythologies of Indigenous peoples in the Americas. Male characters rarely figure into these stories, and scholars often consider them in counterpoint to male-oriented creation myths, like those of the \"ex nihilo\" variety.\nEmergence myths commonly describe the creation of people and/or supernatural beings as a staged ascent or metamorphosis from nascent forms through a series of subterranean worlds to arrive at their current place and form. Often the passage from one world or stage to the next is impelled by inner forces, a process of germination or gestation from earlier, embryonic forms. The genre is most commonly found in Native American cultures where the myths frequently link the final emergence of people from a hole opening to the underworld to stories about their subsequent migrations and eventual settlement in their current homelands.\nEarth-diver.\nThe earth-diver is a common character in various traditional creation myths. In these stories a supreme being usually sends an animal (most often, a type of bird, but also crustaceans, insects and fishes in some narratives) into the primal waters to find bits of sand or mud with which to build habitable land. Some scholars interpret these myths psychologically while others interpret them cosmogonically. In both cases emphasis is placed on beginnings emanating from the depths.\nMotif distribution.\nAccording to Gudmund Hatt and Tristram P. Coffin, Earth-diver myths are common in Native American folklore, among the following populations: Shoshone, Fox people, Blackfoot, Chipewyan, Newettee, Yokuts of California, Mandan, Hidatsa, Cheyenne, Arapaho, Ojibwe, Yuchi and Cherokee.\nAmerican anthropologist Gladys Reichard located the distribution of the motif across \"all parts of North America\", save for \"the extreme north, northeast, and southwest\". In a 1977 study, anthropologist Victor Barnouw surmised that the earth-diver motif appeared in \"hunting-gathering societies\", mainly among northerly groups such as the Hare, Dogrib, Kaska, Beaver, Carrier, Chippewyan, Sarsi, Cree and Montagnais.\nSimilar tales are also found among the Chukchi and Yukaghir, the Tatars and many Finnic traditions, as well as among the Buryat and the Samoyed. In addition, the earth-diver motif also exists in narratives from Eastern Europe, namely Romani, Romanian, Slavic (namely, Bulgarian, Polish, Ukrainian, and Belarusian) and Lithuanian mythological traditions.\nThe pattern of distribution of these stories suggest they have a common origin in the eastern Asiatic coastal region, spreading as peoples migrated west into Siberia and east to the North American continent. However, there are examples of this mytheme found well outside of this boreal distribution pattern, for example the West African Yoruba creation myth of Obatala and Oduduwa.\nNative American narrative.\nCharacteristic of many Native American myths, earth-diver creation stories begin as beings and potential forms linger asleep or suspended in the primordial realm. The earth-diver is among the first of them to awaken and lay the necessary groundwork by building suitable lands where the coming creation will be able to live. In many cases, these stories will describe a series of failed attempts to make land before the solution is found.\nAmong the indigenous peoples of the Americas, the earth diver cosmogony is attested in Iroquois mythology: a female sky deity falls from the heavens, and certain animals, the beaver, the otter, the duck and the muskrat dive in the waters to fetch mud to construct an island.\nIn a similar story from the Seneca, people lived in a sky realm. One day, the chief's daughter was afflicted with a mysterious illness, and the only cure recommended for her (revealed in a dream) was to lie beside a tree and to have it be dug up. The people do so, but a man complains that the tree was their livelihood, and kicks the girl through the hole. She ends up falling from the sky to a world of only water, but is rescued by waterfowl. A turtle offers to bear her on its shell, but asked where would be a definitive dwelling place for her. They decide to create land, and the toad dives into the depths of the primal sea to get pieces of soil. The toad puts it on the turtle's back, which grows larger with every deposit of soil.\nIn another version from the Wyandot, the Wyandot lived in heaven. The daughter of the Big Chief (or Mighty Ruler) was sick, so the medicine man recommends that they dig up the wild apple tree that stands next to the Lodge of the Mighty Ruler, because the remedy is to be found on its roots. However, as the tree has been dug out, the ground begins to sink away, and the treetops catch and carry down the sick daughter with it. As the girl falls from the skies, two swans rescue her on their backs. The birds decide to summon all the Swimmers and the Water Tribes. Many volunteer to dive into the Great Water to fetch bits of earth from the bottom of the sea, but only the toad (female, in the story) is the one successful.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nBibliography.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nOn the Earth-diver motif:"}
{"id": "7324", "revid": "33299132", "url": "https://en.wikipedia.org/wiki?curid=7324", "title": "Crucifix", "text": "Image of Jesus on the cross\nA crucifix (from the Latin meaning '(one) fixed to a cross') is a cross with an image of Jesus on it, as distinct from a bare cross. The representation of Jesus himself on the cross is referred to in English as the (Latin for 'body').\nThe crucifix is a principal symbol for many groups of Christians, and one of the most common forms of the Crucifixion in the arts. It is especially important in the Catholic Church, but is also used in the Eastern Orthodox Church, most Oriental Orthodox Churches (except the Armenian &amp; Syriac Church), Lutheranism, Moravianism, and Anglicanism. The symbol is less common in churches of other Protestant denominations, and in the Assyrian Church of the East and Armenian Apostolic Church, which prefer to use a cross without the figure of Jesus (the ). The crucifix emphasizes Jesus' sacrifice\u2014his death by crucifixion, which Christians believe brought about the redemption of humankind. Most crucifixes portray Jesus on a Latin cross, rather than any other shape, such as a Tau cross or a Coptic cross.\nRoman Catholics see the crucifix as the perfect fulfillment of that inferred by the serpent created by Moses in Numbers 21:8\u20149, called the Nehushtan. It was promised that those sinners who looked upon the Nehushtan would be healed. The section of Numbers about the Nehushtan is one of the readings on Exaltation of the Cross that occurs on September 14 in the Roman Catholic Church. It is paired with John 3:14\u201315 as the gospel reading. Taken together, these readings explain the striking front and center position of a large crucifix normally fixed above or behind a Catholic altar.\nWestern crucifixes usually have a three-dimensional , but in Eastern Orthodoxy Jesus' body is normally painted on the cross, or in low relief. Strictly speaking, to be a crucifix, the cross must be three-dimensional, but this distinction is not always observed. An entire painting of the crucifixion of Jesus including a landscape background and other figures is not a crucifix either.\nLarge crucifixes high across the central axis of a church are known by the Old English term rood. By the Late Middle Ages these were a near-universal feature of Western churches, but they are now very rare. Modern Roman Catholic churches and many Lutheran churches often have a crucifix above the altar on the wall; for the celebration of Mass, the Roman Rite of the Catholic Church requires that \"on or close to the altar there is to be a cross with a figure of Christ crucified\".\nDescription.\nThe standard, four-pointed Latin crucifix consists of an upright post or and a single crosspiece to which the sufferer's arms were nailed. There may also be a short projecting nameplate, showing the letters INRI (Greek: INBI). The Russian Orthodox crucifix usually has an additional third crossbar, to which the feet are nailed, and which is angled upward toward the penitent thief Saint Dismas (to the viewer's left) and downward toward the impenitent thief Gestas (to the viewer's right). The corpus of Eastern crucifixes is normally a two-dimensional or low relief icon that shows Jesus as already dead, his face peaceful and somber. They are rarely three-dimensional figures as in the Western tradition, although these may be found where Western influences are strong, but are more typically icons painted on a piece of wood shaped to include the double-barred cross and perhaps the edge of Christ's hips and halo, and no background. More sculptural small crucifixes in metal relief are also used in Orthodoxy (see gallery examples), including as pectoral crosses and blessing crosses.\nWestern crucifixes may show Christ dead or alive, the presence of the spear wound in his ribs traditionally indicating that he is dead. In either case his face very often shows his suffering. In the Eastern Orthodox tradition he has normally been shown as dead since around the end of the period of Byzantine Iconoclasm. Eastern crucifixes have Jesus' two feet nailed side by side, rather than crossed one above the other, as Western crucifixes have shown them since around the 13th century. The crown of thorns is also generally absent in Eastern crucifixes, since the emphasis is not on Christ's suffering, but on his triumph over sin and death. The \"S\"-shaped position of Jesus' body on the cross is a Byzantine innovation of the late 10th century, though also found in the German Gero Cross of the same date. Probably more from Byzantine influence, it spread elsewhere in the West, especially to Italy, by the Romanesque period, though it was more usual in painting than sculpted crucifixes. It was in Italy that the emphasis was put on Jesus' suffering and realistic details, during a process of general humanization of Christ favored by the Franciscan order. During the 13th century the suffering Italian model () triumphed over the traditional Byzantine one () anywhere in Europe also due to the works of artists such as Giunta Pisano and Cimabue. Since the Renaissance the \"S\"-shape is generally much less pronounced. Eastern Christian blessing crosses will often have the Crucifixion depicted on one side, and the Resurrection on the other, illustrating Eastern Orthodox theology's understanding of the Crucifixion and Resurrection as two intimately related aspects of the same act of salvation.\nAnother, symbolic, depiction shows a triumphant Christ (), clothed in robes, rather than stripped as for his execution, with arms raised, appearing to rise up from the cross, sometimes accompanied by \"rays of light\", or an aureole encircling his body. He may be robed as a prophet, crowned as a king, and vested in a stole as Great High Priest.\nOn some crucifixes a skull and crossbones are shown below the corpus, referring to Golgotha (Calvary), the site at which Jesus was crucified, which the Gospels say means in Hebrew \"the place of the skull.\" Medieval tradition held that it was the burial-place of Adam and Eve, and that the cross of Christ was raised directly over Adam's skull, so many crucifixes manufactured in Catholic countries still show the skull and crossbones below the corpus.\nVery large crucifixes have been built, the largest being the Cross in the Woods in Michigan, with a high statue.\nUsage.\nIn the early Church, many Christians hung a cross on the eastern wall of their house in order to indicate the eastward direction of prayer. Prayer in front of a crucifix, which is seen as a sacramental, is often part of devotion for Christians, especially those worshipping in a church, also privately. The person may sit, stand, or kneel in front of the crucifix, sometimes looking at it in contemplation, or merely in front of it with head bowed or eyes closed. During the Middle Ages small crucifixes, generally hung on a wall, became normal in the personal cells or living quarters first of monks, then all clergy, followed by the homes of the laity, spreading down from the top of society as these became cheap enough for the average person to afford. Most towns had a large crucifix erected as a monument, or some other shrine at the crossroads of the town. Building on the ancient custom, many Catholics, Lutherans and Anglicans hang a crucifix inside their homes and also use the crucifix as a focal point of a home altar. The wealthy erected proprietary chapels as they could afford to do this.\nCatholic (both Eastern and Western), Eastern Orthodox, Oriental Orthodox, Anglican and Lutheran Christians generally use the crucifix in public religious services. They believe use of the crucifix is in keeping with the statement by Paul the Apostle in 1 Corinthians: \"we preach Christ crucified, a stumbling block to Jews and folly to Gentiles, but to those who are called, both Jews and Greeks, Christ the power of God and the wisdom of God\".\nIn the West, altar crosses and processional crosses began to be crucifixes in the 11th century, which became general around the 14th century, as they became cheaper. The Roman Rite requires that \"either on the altar or near it, there is to be a cross, with the figure of Christ crucified upon it, a cross clearly visible to the assembled people. It is desirable that such a cross should remain near the altar even outside of liturgical celebrations, so as to call to mind for the faithful the saving Passion of the Lord.\" The requirement of the altar cross was also mentioned in pre-1970 editions of the Roman Missal, though not in the original 1570 Roman Missal of Pope Pius V. The Rite of Funerals says that the Gospel Book, the Bible, or a cross (which will generally be in crucifix form) may be placed on the coffin for a Requiem Mass, but a second standing cross is not to be placed near the coffin if the altar cross can be easily seen from the body of the church.\nEastern Christian liturgical processions called crucessions include a cross or crucifix at their head. In the Eastern Orthodox Church, the crucifix is often placed above the iconostasis in the church. In the Russian Orthodox Church a large crucifix (\"Golgotha\") is placed behind the Holy Table (altar). During Matins of Good Friday, a large crucifix is taken in procession to the center of the church, where it is venerated by the faithful. Sometimes the \"soma\" () is removable and is taken off the crucifix at Vespers that evening during the Gospel lesson describing the Descent from the Cross. The empty cross may then remain in the centre of the church until the Paschal vigil (local practices vary). The blessing cross which the priest uses to bless the faithful at the dismissal will often have the crucifix on one side and an icon of the Resurrection of Jesus on the other, the side with the Resurrection being used on Sundays and during Paschaltide, and the crucifix on other days.\nExorcist Gabriele Amorth has stated that the crucifix is one of the most effective means of averting or opposing demons. In folklore, it is believed to ward off vampires, incubi, succubi, and other evils.\nModern anti-Christians have used an inverted (upside-down) crucifix when showing disdain for Jesus Christ or the Catholic Church which believes in his divinity. According to Christian tradition, Saint Peter was martyred by being crucified upside-down.\nControversies.\nProtestant Reformation.\nIn the Moravian Church, Nicolaus Zinzendorf had an experience in which he believed he encountered Jesus. Seeing a painting of a crucifix, Zinzendorf fell on his knees vowing to glorify Jesus after contemplating on the wounds of Christ and an inscription that stated \"This is what I have done for you, what will you do for me?\".\nThe Lutheran Churches retained the use of the crucifix, \"justifying their continued use of medieval crucifixes with the same arguments employed since the Middle Ages, as is evident from the example of the altar of the Holy Cross in the Cistercian church of Doberan.\" Martin Luther did not object to them, and this was among his differences with Andreas Karlstadt as early as 1525. At the time of the Reformation, Luther retained the crucifix in the Lutheran Church and they remain the center of worship in Lutheran parishes across Europe. In the United States, however, Lutheranism came under the influence of Calvinism, and the plain cross came to be used in many churches. In contrast to the practice of the Moravian Church and Lutheran Churches, the early Reformed Churches rejected the use of the crucifix, and indeed the unadorned cross, along with other traditional religious imagery, as idolatrous. Calvin, considered to be the father of the Reformed Church, was violently opposed to both cross and crucifix. In England, the Royal Chapels of Elizabeth I were most unusual among local churches in retaining crucifixes, following the Queen's conservative tastes. These disappeared under her successor, James I, and their brief re-appearance in the early 1620s when James' heir was seeking a Spanish marriage was the subject of rumour and close observation by both Catholics and Protestants; when the match fell through they disappeared.\nModern.\nIn 2005, a mother accused her daughter's school in Derby, England, of discriminating against Christians after the teenager was suspended for refusing to take off a crucifix necklace.\nIn 2008, a chapel in a prison in England replaced its crucifix and static altar with a cross and portable altar when it was renovated as a multi-faith chapel. Right-leaning media reported that the crucifix had been removed \"in case it offends Muslims\".\nIn 2008 in Spain, a local judge ordered crucifixes removed from public schools to settle a decades-old dispute over whether crucifixes should be displayed in public buildings in a non-confessional state.\nOn 18 March 2011, the European Court of Human Rights ruled in the \"Lautsi v. Italy\" case, that the requirement in Italian law that crucifixes be displayed in classrooms of state schools does not violate the European Convention on Human Rights. Crucifixes are common in most other Italian official buildings, including courts of law.\nOn 24 March 2011, the Constitutional Court of Peru ruled that the presence of crucifixes in courts of law does not violate the secular nature of the state.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7325", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7325", "title": "COFDM", "text": ""}
{"id": "7326", "revid": "12070", "url": "https://en.wikipedia.org/wiki?curid=7326", "title": "Coded orthogonal frequency division modulation", "text": ""}
{"id": "7327", "revid": "36549015", "url": "https://en.wikipedia.org/wiki?curid=7327", "title": "Copernican principle", "text": "Principle that humans are not privileged observers of the universe\n&lt;templatestyles src=\"Unsolved/styles.css\" /&gt;\nUnsolved problem in physics:\nAre cosmological observations made from Earth representative of observations from the average position in the universe?\nIn physical cosmology, the Copernican principle states that humans, on the Earth or in the Solar System, are not privileged observers of the universe, that observations from the Earth are representative of observations from the average position in the universe. Named for Copernican heliocentrism, it is a working assumption that arises from a modified cosmological extension of Copernicus' argument of a moving Earth.\nOrigin and implications.\nHermann Bondi named the principle after Copernicus in the mid-20th century, although the principle itself dates back to the 16th-17th century paradigm shift away from the Ptolemaic system, which placed Earth at the center of the universe. Copernicus proposed that the motion of the planets could be explained by reference to an assumption that the Sun is centrally located and stationary in contrast to the geocentrism. He argued that the apparent retrograde motion of the planets is an illusion caused by Earth's movement around the Sun, which the Copernican model placed at the centre of the universe. Copernicus himself was mainly motivated by technical dissatisfaction with the earlier system and not by support for any mediocrity principle. In fact, although the Copernican heliocentric model is often described as \"demoting\" Earth from its central role it had in the Ptolemaic geocentric model, it was successors to Copernicus, notably the 16th century Giordano Bruno, who adopted this new perspective. The Earth's central position had been interpreted as being in the \"lowest and filthiest parts\". Instead, as Galileo said, the Earth is part of the \"dance of the stars\" rather than the \"sump where the universe's filth and ephemera collect\". In the late 20th Century, Carl Sagan asked, \"Who are we? We find that we live on an insignificant planet of a humdrum star lost in a galaxy tucked away in some forgotten corner of a universe in which there are far more galaxies than people.\"\nWhile the Copernican principle is derived from the negation of past assumptions, such as geocentrism, heliocentrism, or galactocentrism which state that humans are at the center of the universe, the Copernican principle is stronger than \"acentrism\", which merely states that humans are not at the center of the universe. The Copernican principle assumes acentrism and also states that human observers or observations from Earth are representative of observations from the average position in the universe. Michael Rowan-Robinson emphasizes the Copernican principle as the threshold test for modern thought, asserting that: \"It is evident that in the post-Copernican era of human history, no well-informed and rational person can imagine that the Earth occupies a unique position in the universe.\"\nMost modern cosmology is based on the assumption that the cosmological principle is almost, but not exactly, true on the largest scales. The Copernican principle represents the irreducible philosophical assumption needed to justify this, when combined with the observations. If one assumes the Copernican principle and observes that the universe appears isotropic or the same in all directions from the vantage point of Earth, then one can infer that the universe is generally homogeneous or the same everywhere (at any given time) and is also isotropic about any given point. These two conditions make up the cosmological principle.\nIn practice, astronomers observe that the universe has heterogeneous or non-uniform structures up to the scale of galactic superclusters, filaments and great voids. In the current Lambda-CDM model, the predominant model of cosmology in the modern era, the universe is predicted to become more and more homogeneous and isotropic when observed on larger and larger scales, with little detectable structure on scales of more than about 260 million parsecs. However, recent evidence from galaxy clusters, quasars, and type Ia supernovae suggests that isotropy is violated on large scales. Furthermore, various large-scale structures have been discovered, such as the Clowes\u2013Campusano LQG, the Sloan Great Wall, U1.11, the Huge-LQG, the Hercules\u2013Corona Borealis Great Wall, and the Giant Arc, all which indicate that homogeneity might be violated.\nOn scales comparable to the radius of the observable universe, we see systematic changes with distance from Earth. For instance, galaxies contain more young stars and are less clustered, and quasars appear more numerous. If the Copernican principle is assumed, then it follows that this is evidence for the evolution of the universe with time: this distant light has taken most of the age of the universe to reach Earth and shows the universe when it was young. The most distant light of all, cosmic microwave background radiation, is isotropic to at least one part in a thousand.\nBondi and Thomas Gold used the Copernican principle to argue for the perfect cosmological principle which maintains that the universe is also homogeneous in time, and is the basis for the steady-state cosmology. However, this strongly conflicts with the evidence for cosmological evolution mentioned earlier: the universe has progressed from extremely different conditions at the Big Bang, and will continue to progress toward extremely different conditions, particularly under the rising influence of dark energy, apparently toward the Big Freeze or Big Rip.\nSince the 1990s the term has been used (interchangeably with \"the Copernicus method\") for J. Richard Gott's Bayesian-inference-based prediction of duration of ongoing events, a generalized version of the Doomsday argument.\nTests of the principle.\nThe Copernican principle has never been proven, and in the most general sense cannot be proven, but it is implicit in many modern theories of physics. Cosmological models are often derived with reference to the cosmological principle, slightly more general than the Copernican principle, and many tests of these models can be considered tests of the Copernican principle.\nHistorical.\nBefore the term Copernican principle was even coined, past assumptions, such as geocentrism, heliocentrism, and galactocentrism, which state that Earth, the Solar System, or the Milky Way respectively were located at the center of the universe, were shown to be false. The Copernican Revolution dethroned Earth to just one of many planets orbiting the Sun. Proper motion was mentioned by Halley. William Herschel found that the Solar System is moving through space within our disk-shaped Milky Way galaxy. Edwin Hubble showed that the Milky Way galaxy is just one of many galaxies in the universe. Examination of the galaxy's position and motion in the universe led to the Big Bang theory and the whole of modern cosmology.\nModern tests.\nRecent and planned tests relevant to the cosmological and Copernican principles include:\nPhysics without the principle.\nThe standard model of cosmology, the Lambda-CDM model, assumes the Copernican principle and the more general cosmological principle. Some cosmologists and theoretical physicists have created models without the cosmological or Copernican principles to constrain the values of observational results, to address specific known issues in the Lambda-CDM model, and to propose tests to distinguish between current models and other possible models.\nA prominent example in this context is inhomogeneous cosmology, to model the observed accelerating universe and cosmological constant. Instead of using the current accepted idea of dark energy, this model proposes the universe is much more inhomogeneous than currently assumed, and instead, we are in an extremely large low-density void. To match observations we would have to be very close to the centre of this void, immediately contradicting the Copernican principle.\nWhile the Big Bang model in cosmology is sometimes said to derive from the Copernican principle in conjunction with redshift observations, the Big Bang model can still be assumed to be valid in absence of the Copernican principle, because the cosmic microwave background, primordial gas clouds, and the structure, evolution, and distribution of galaxies all provide evidence, independent of the Copernican principle, in favor of the Big Bang. However, the key tenets of the Big Bang model, such as the expansion of the universe, become assumptions themselves akin to the Copernican principle, rather than derived from the Copernican principle and observations.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7329", "revid": "44387811", "url": "https://en.wikipedia.org/wiki?curid=7329", "title": "Cyprinidae", "text": "Family of freshwater fish\nCyprinidae is a family of freshwater fish commonly called the carp or minnow family, including the carps, the true minnows, and their relatives the barbs and barbels, among others. Cyprinidae is the largest and most diverse fish family, and the largest vertebrate animal family overall, with about 3,000 species; only 1,270 of these remain extant, divided into about 370 genera. Cyprinids range from about 12\u00a0mm in size to the giant barb (\"Catlocarpio siamensis\"). By genus and species count, the family makes up more than two-thirds of the ostariophysian order Cypriniformes. The family name is derived from the Greek word ( 'carp').\nBiology and ecology.\nCyprinids are stomachless, or agastric, fish with toothless jaws. Even so, food can be effectively chewed by the gill rakers of the specialized last gill bow. These pharyngeal teeth allow the fish to make chewing motions against a chewing plate formed by a bony process of the skull. The pharyngeal teeth are unique to each species and are used by scientists to identify species. Strong pharyngeal teeth allow fish such as the common carp and ide to eat hard baits such as snails and bivalves.\nHearing is a well-developed sense in the cyprinids since they have the Weberian organ, three specialized vertebral processes that transfer motion of the gas bladder to the inner ear. The vertebral processes of the Weberian organ also permit a cyprinid to detect changes in motion of the gas bladder due to atmospheric conditions or depth changes. The cyprinids are considered physostomes because the pneumatic duct is retained in adult stages and the fish are able to gulp air to fill the gas bladder, or they can dispose of excess gas to the gut.\nCyprinids are native to North America, Africa, and Eurasia. The largest known cyprinid is the giant barb (\"Catlocarpio siamensis\"), which may grow up to in length and in weight. Other very large species that can surpass are the golden mahseer (\"Tor putitora\") and mangar (\"Luciobarbus esocinus\"). The largest North American species is the Colorado pikeminnow (\"Ptychocheilus lucius\"), which can reach up to in length. Conversely, many species are smaller than . The smallest known fish is \"Paedocypris progenetica\", reaching at the longest.\nAll fish in this family are egg-layers and most do not guard their eggs; however, a few species build nests and/or guard the eggs. The bitterlings of subfamily Acheilognathinae are notable for depositing their eggs in bivalve molluscs, where the young develop until able to fend for themselves.\nCyprinids contain the first and only known example of androgenesis in a vertebrate, in the Squalius alburnoides allopolyploid complex.\nMost cyprinids feed mainly on invertebrates and vegetation, probably due to the lack of teeth and stomach; however, some species, like the asp, are predators that specialize in fish. Many species, such as the ide and the common rudd, prey on small fish when individuals become large enough. Even small species, such as the moderlieschen, are opportunistic predators that will eat larvae of the common frog in artificial circumstances.\nSome cyprinids, such as the grass carp, are specialized herbivores; others, such as the common nase, eat algae and biofilms, while others, such as the black carp, specialize in snails, and some, such as the silver carp, are specialized filter feeders. For this reason, cyprinids are often introduced as a management tool to control various factors in the aquatic environment, such as aquatic vegetation and diseases transmitted by snails.\nUnlike most fish species, cyprinids generally increase in abundance in eutrophic lakes. Here, they contribute towards positive feedback as they are efficient at eating the zooplankton that would otherwise graze on the algae, reducing its abundance.\nRelationship with humans.\nFood.\nCyprinids are highly important food fish; they are fished and farmed across Eurasia. In land-locked countries in particular, cyprinids are often the major species of fish eaten because they make the largest part of biomass in most water types except for fast-flowing rivers. In Eastern Europe, they are often prepared with traditional methods such as drying and salting. The prevalence of inexpensive frozen fish products made this less important now than it was in earlier times. Nonetheless, in certain places, they remain popular for food, as well as recreational fishing, for ornamental use, and have been deliberately stocked in ponds and lakes for centuries for this reason.\nSport.\nCyprinids are popular for angling especially for match fishing (due to their dominance in biomass and numbers) and fishing for common carp because of its size and strength.\nAs pest control.\nSeveral cyprinids have been introduced to waters outside their natural ranges to provide food, sport, or biological control for some pest species. The common carp (\"Cyprinus carpio\") and the grass carp (\"Ctenopharyngodon idella\") are the most important of these, for example in Florida. \nAs a pest species.\nCarp in particular can stir up sediment, reducing the clarity of the water and making plant growth difficult.\nIn America and Australia, such as the Asian carp in the Mississippi Basin, they have become invasive species that compete with native fishes or disrupt the environment. \n\"Cyprinus carpio\" is a major pest species in Australia impacting freshwater environments, amenity, and the agricultural economy, devastating biodiversity by decimating native fish populations where they first became established as a major pest in the wild in the 1960s. In the major river system of eastern Australia, the Murray-Darling Basin, they constitute 80-90 per cent of fish biomass. \nIn 2016 the federal government announced A$15.2 million to fund the National Carp Control Plan to investigate using Cyprinid herpesvirus 3 (carp virus) as a biological control agent while minimising impacts on industry and environment should a carp virus release go ahead. Despite initial, favourable assessment, in 2020 this plan was found to be unlikely to work due to the high fecundity of the fish.\nAquarium fish.\nNumerous cyprinids have become popular and important within the aquarium and fishpond hobbies, most famously the goldfish, which was bred in China from the Prussian carp (\"Carassius (auratus) gibelio\"). First imported into Europe around 1728, it was originally much-fancied by the Chinese nobility as early as 1150AD and, after it arrived there in 1502, also in Japan. In addition to the goldfish, the common carp was bred in Japan into the colorful ornamental variety known as koi \u2014 or more accurately , as simply means \"common carp\" in Japanese \u2014 from the 18th century until today.\nOther popular aquarium cyprinids include danionins, rasborines and true barbs. Larger species are bred by the thousands in outdoor ponds, particularly in Southeast Asia, and trade in these aquarium fishes is of considerable commercial importance. The small rasborines and danionines are perhaps only rivalled by characids (tetras) and poecilid livebearers in their popularity for community aquaria. Some of the most popular cyprinids among aquarists, other than goldfish and koi, include the cherry barb, Harlequin rasbora, pearl danios, rainbow sharks, tiger barbs, and the White Cloud Mountain minnow.\nOne particular species of these small and undemanding danionines is the zebrafish (\"Danio rerio\"). It has become the standard model species for studying developmental genetics of vertebrates, in particular fish.\nThreatened families.\nHabitat destruction and other causes have reduced the wild stocks of several cyprinids to dangerously low levels; some are already entirely extinct. In particular, the cyprinids of the subfamily Leuciscinae from southwestern North America have been hit hard by pollution and unsustainable water use in the early to mid-20th century; most globally extinct cypriniform species are in fact leuciscinid cyprinids from the southwestern United States and northern Mexico.\nSystematics.\nThe massive diversity of cyprinids has so far made it difficult to resolve their phylogeny in sufficient detail to make assignment to subfamilies more than tentative in many cases. Some distinct lineages obviously exist \u2013 for example, the Cultrinae and Leuciscinae, regardless of their exact delimitation, are rather close relatives and stand apart from Cyprininae\u00a0\u2013\u00a0but the overall systematics and taxonomy of the Cyprinidae remain a subject of considerable debate. A large number of genera are \"incertae sedis\", too equivocal in their traits and/or too little-studied to permit assignment to a particular subfamily with any certainty.\nPart of the solution seems that the delicate rasborines are the core group, consisting of minor lineages that have not shifted far from their evolutionary niche, or have coevolved for millions of years. These are among the most basal lineages of living cyprinids. Other \"rasborines\" are apparently distributed across the diverse lineages of the family.\nThe validity and circumscription of proposed subfamilies like the Labeoninae or Squaliobarbinae also remain doubtful, although the latter do appear to correspond to a distinct lineage. The sometimes-seen grouping of the large-headed carps (Hypophthalmichthyinae) with \"Xenocypris\", though, seems quite in error. More likely, the latter are part of the Cultrinae.\nThe entirely paraphyletic \"Barbinae\" and the disputed Labeoninae might be better treated as part of the Cyprininae, forming a close-knit group whose internal relationships are still little known. The small African \"barbs\" do not belong in \"Barbus\" \"sensu stricto\" \u2013 indeed, they are as distant from the typical barbels and the typical carps (\"Cyprinus\") as these are from \"Garra\" (which is placed in the Labeoninae by most who accept the latter as distinct) and thus might form another as yet unnamed subfamily. However, as noted above, how various minor lineages tie into this has not yet been resolved; therefore, such a radical move, though reasonable, is probably premature.\nThe tench (\"Tinca tinca\"), a significant food species farmed in western Eurasia in large numbers, is unusual. It is most often grouped with the Leuciscinae, but even when these were rather loosely circumscribed, it always stood apart. A cladistic analysis of DNA sequence data of the S7 ribosomal protein intron1 supports the view that it is distinct enough to constitute a monotypic subfamily. It also suggests it may be closer to the small East Asian \"Aphyocypris\", \"Hemigrammocypris\", and \"Yaoshanicus\". They would have diverged roughly at the same time from cyprinids of east-central Asia, perhaps as a result of the Alpide orogeny that vastly changed the topography of that region in the late Paleogene, when their divergence presumably occurred.\nA DNA-based analysis of these fish places the Rasborinae as the basal lineage with the Cyprininae as a sister clade to the Leuciscinae. The subfamilies Acheilognathinae, Gobioninae, and Leuciscinae are monophyletic.\nSubfamilies and genera.\nThe 5th Edition of Fishes of the World sets out the following subfamilies:\n\"Incertae sedis\".\nWith such a large and diverse family the taxonomy and phylogenies are always being worked on so alternative classifications are being created as new information is discovered, for example:\nPhylogeny.\nSubfamily Probarbinae\nSubfamily Labeoninae\nSubfamily Torinae\nSubfamily Smiliogastrinae\nSubfamily Cyprininae [incl. Barbinae]\nSubfamily Danioninae\nSubfamily Leptobarbinae\nSubfamily Xenocypridinae [incl. Cultrinae &amp; Squaliobarbinae]\nSubfamily Tincinae\nSubfamily Acheilognathinae (bitterlings)\nSubfamily Gobioninae\nSubfamily Tanichthyinae\nSubfamily Leuciscinae [incl. Alburninae]\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n Media related to at Wikimedia Commons\n Data related to at Wikispecies"}
{"id": "7330", "revid": "4248138", "url": "https://en.wikipedia.org/wiki?curid=7330", "title": "Complementary DNA", "text": "Single-stranded DNA synthesized from RNA\nIn genetics, complementary DNA (cDNA) is DNA synthesized from a single-stranded RNA (e.g., messenger RNA (mRNA) or microRNA (miRNA)) template in a reaction catalyzed by the enzyme reverse transcriptase. cDNA is often used to express a specific protein in a cell that does not normally express that protein (i.e., heterologous expression), or to sequence or quantify mRNA molecules using DNA based methods (qPCR, RNA-seq). cDNA that codes for a specific protein can be transferred to a recipient cell for expression, often bacterial or yeast expression systems. cDNA is also generated to analyze transcriptomic profiles in bulk tissue, single cells, or single nuclei in assays such as microarrays, qPCR, and RNA-seq.\ncDNA is also produced naturally by retroviruses (such as HIV-1, HIV-2, simian immunodeficiency virus, etc.) and then integrated into the host's genome, where it creates a provirus. \nThe term \"cDNA\" is also used, typically in a bioinformatics context, to refer to an mRNA transcript's sequence, expressed as DNA bases (deoxy-GCAT) rather than RNA bases (GCAU).\nIn 2013 the US Supreme Court declared, that cDNA is patent-eligible, whereas isolated sequences of naturally occuring DNA are not (see Association for Molecular Pathology v. Myriad Genetics, Inc.)\nSynthesis.\nRNA serves as a template for cDNA synthesis. In cellular life, cDNA is generated by viruses and retrotransposons for integration of RNA into target genomic DNA. In molecular biology, RNA is purified from source material after genomic DNA, proteins and other cellular components are removed. cDNA is then synthesized through \"in vitro\" reverse transcription.\nRNA Purification.\nRNA is transcribed from genomic DNA in host cells and is extracted by first lysing cells then purifying RNA utilizing widely-used methods such as phenol-chloroform, silica column, and bead-based RNA extraction methods. Extraction methods vary depending on the source material. For example, extracting RNA from plant tissue requires additional reagents, such as polyvinylpyrrolidone (PVP), to remove phenolic compounds, carbohydrates, and other compounds that will otherwise render RNA unusable. To remove DNA and proteins, enzymes such as DNase and Proteinase K are used for degradation. Importantly, RNA integrity is maintained by inactivating RNases with chaotropic agents such as guanidinium isothiocyanate, sodium dodecyl sulphate (SDS), phenol or chloroform. Total RNA is then separated from other cellular components and precipitated with alcohol. Various commercial kits exist for simple and rapid RNA extractions for specific applications. Additional bead-based methods can be used to isolate specific sub-types of RNA ...(e.g. mRNA and microRNA) based on size or unique RNA regions.\nReverse Transcription.\nFirst-strand synthesis.\nUsing a reverse transcriptase enzyme and purified RNA templates, one strand of cDNA is produced (first-strand cDNA synthesis). The M-MLV reverse transcriptase from the Moloney murine leukemia virus is commonly used due to its reduced RNase H activity suited for transcription of longer RNAs. The AMV reverse transcriptase from the avian myeloblastosis virus may also be used for RNA templates with strong secondary structures (i.e. high melting temperature). cDNA is commonly generated from mRNA for gene expression analyses such as RT-qPCR and RNA-seq. mRNA is selectively reverse transcribed using oligo-dT primers that are the reverse complement of the poly-adenylated tail on the 3' end of all mRNA. An optimized mixture of oligo-dT and random hexamer primers increases the chance of obtaining full-length cDNA while reducing 5' or 3' bias. Ribosomal RNA may also be depleted to enrich both mRNA and non-poly-adenylated transcripts such as some non-coding RNA.\nSecond-strand synthesis.\nThe result of first-strand syntheses, RNA-DNA hybrids, can be processed through multiple second-strand synthesis methods or processed directly in downstream assays. An early method known as hairpin-primed synthesis relied on hairpin formation on the 3' end of the first-strand cDNA to prime second-strand synthesis. However, priming is random and hairpin hydrolysis leads to loss of information. The Gubler and Hoffman Procedure uses E. Coli RNase H to nick mRNA that is replaced with E. Coli DNA Polymerase I and sealed with E. Coli DNA Ligase. An optimization of this procedure relies on low RNase H activity of M-MLV to nick mRNA with remaining RNA later removed by adding RNase H after DNA Polymerase translation of the second-strand cDNA. This prevents lost sequence information at the 5' end of the mRNA.\nApplications.\nComplementary DNA is often used in gene cloning or as gene probes or in the creation of a cDNA library. When scientists transfer a gene from one cell into another cell in order to express the new genetic material as a protein in the recipient cell, the cDNA will be added to the recipient (rather than the entire gene), because the DNA for an entire gene may include DNA that does not code for the protein or that interrupts the coding sequence of the protein (e.g., introns). Partial sequences of cDNAs are often obtained as expressed sequence tags.\nWith amplification of DNA sequences via polymerase chain reaction (PCR) now commonplace, one will typically conduct reverse transcription as an initial step, followed by PCR to obtain an exact sequence of cDNA for intra-cellular expression. This is achieved by designing sequence-specific DNA primers that hybridize to the 5' and 3' ends of a cDNA region coding for a protein. Once amplified, the sequence can be cut at each end with nucleases and inserted into one of many small circular DNA sequences known as expression vectors. Such vectors allow for self-replication, inside the cells, and potentially integration in the host DNA. They typically also contain a strong promoter to drive transcription of the target cDNA into mRNA, which is then translated into protein.\ncDNA is also used to study gene expression via methods such as RNA-seq or RT-qPCR. For sequencing, RNA must be fragmented due to sequencing platform size limitations. Additionally, second-strand synthesized cDNA must be ligated with adapters that allow cDNA fragments to be PCR amplified and bind to sequencing flow cells. Gene-specific analysis methods commonly use microarrays and RT-qPCR to quantify cDNA levels via fluorometric and other methods.\nOn 13 June 2013, the United States Supreme Court ruled in the case of \"Association for Molecular Pathology v. Myriad Genetics\" that while naturally occurring genes cannot be patented, cDNA is patent-eligible because it does not occur naturally.\nViruses and retrotransposons.\nSome viruses also use cDNA to turn their viral RNA into mRNA (viral RNA \u2192 cDNA \u2192 mRNA). The mRNA is used to make viral proteins to take over the host cell.\nAn example of this first step from viral RNA to cDNA can be seen in the HIV cycle of infection. Here, the host cell membrane becomes attached to the virus\u2019 lipid envelope which allows the viral capsid with two copies of viral genome RNA to enter the host. The cDNA copy is then made through reverse transcription of the viral RNA, a process facilitated by the chaperone CypA and a viral capsid associated reverse transcriptase.\ncDNA is also generated by retrotransposons in eukaryotic genomes. Retrotransposons are mobile genetic elements that move themselves within, and sometimes between, genomes via RNA intermediates. This mechanism is shared with viruses with the exclusion of the generation of infectious particles.\nReferences.\nMark D. Adams et al. \u201cComplementary DNA Sequencing: Expressed Sequence Tags and Human Genome Project.\u201d \"Science (American Association for the Advancement of Science)\" 252.5013 (1991): 1651\u20131656. Web.\nPhilip M. Murphy, and H. Lee Tiffany. \u201cCloning of Complementary DNA Encoding a Functional Human Interleukin-8 Receptor.\u201d \"Science (American Association for the Advancement of Science)\" 253.5025 (1991): 1280\u20131283. Web.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7331", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=7331", "title": "Cellular digital packet data", "text": "Cellular Digital Packet Data (CDPD) was a wide-area mobile data service which used unused bandwidth normally used by Advanced Mobile Phone System (AMPS) mobile phones between 800 and 900\u00a0MHz to transfer data. Speeds up to 19.2 kbit/s were possible, though real world speeds seldom reached higher than 9.6 kbit/s. The service was discontinued in conjunction with the retirement of the parent AMPS service; it has been functionally replaced by faster services such as 1xRTT, Evolution-Data Optimized, and UMTS/High Speed Packet Access (HSPA).\nDeveloped in the early 1990s, CDPD was large on the horizon as a future technology. However, it had difficulty competing against existing slower but less expensive Mobitex and DataTAC systems, and never quite gained widespread acceptance before newer, faster standards such as General Packet Radio Service (GPRS) became dominant.\nCDPD had very limited consumer products. AT&amp;T Wireless first sold the technology in the United States under the PocketNet brand. It was one of the first products of wireless web service. Digital Ocean, Inc. an original equipment manufacturer licensee of the Apple Newton, sold the Seahorse product, which integrated the Newton handheld computer, an AMPS/CDPD handset/modem along with a web browser in 1996, winning the CTIA's hardware product of the year award as a smartphone, arguably the world's first. A company named OmniSky provided service for Palm V devices. OmniSky then filed for bankruptcy in 2001 then was picked up by EarthLink Wireless. The technician that developed the tech support for all of the wireless technology was a man by the name of Myron Feasel he was brought from company to company ending up at Palm. Sierra Wireless sold PCMCIA devices and Airlink sold a serial modem. \nBoth of these were used by police and fire departments for dispatch. Wirelesss later sold CDPD under the Wireless Internet brand (not to be confused with Wireless Internet Express, their brand for GPRS/EDGE data). PocketNet was generally considered a failure with competition from 2G services such as Sprint's Wireless Web. AT&amp;T Wireless sold four PocketNet Phone models to the public: the Samsung Duette and the Mitsubishi MobileAccess-120 were AMPS/CDPD PocketNet phones introduced in October 1997; and two IS-136/CDPD Digital PocketNet phones, the Mitsubishi T-250 and the Ericsson R289LX.\nDespite its limited success as a consumer offering, CDPD was adopted in a number of enterprise and government networks. It was particularly popular as a first-generation wireless data solution for telemetry devices (machine to machine communications) and for public safety mobile data terminals.\nIn 2004, major carriers in the United States announced plans to shut down CDPD service. In July 2005, the AT&amp;T Wireless and Cingular Wireless CDPD networks were shut down. Equipment for this service now has little to no residual value.\nCDPD Network and system.\nPrimary elements of a CDPD network are:\n1. End systems: physical &amp; logical end systems that exchange information\n2. Intermediate systems: CDPD infrastructure elements that store, forward &amp; route the information\nThere are 2 kinds of End systems\n1. Mobile end system: subscriber unit to access CDPD network over a wireless interface\n2. Fixed end system: common host/server that is connected to the CDPD backbone and providing access to specific application and data\nThere are 2 kinds of Intermediate systems\n1. Generic intermediate system: simple router with no knowledge of mobility issues\n2. mobile data intermediate system: specialized intermediate system that routes data based on its knowledge of the current location of Mobile end system. It is a set of hardware and software functions that provide switching, accounting, registration, authentication, encryption, and so on.\nThe design of CDPD was based on several design objectives that are often repeated in designing overlay networks or new networks. A lot of emphasis was laid on open architectures and reusing as much of the existing RF infrastructure as possible. The design goal of CDPD included location independence and independence fro, service provider, so that coverage could be maximized ; application transparency and multiprotocol support, interoperability between products from multiple vendors."}
{"id": "7333", "revid": "169132", "url": "https://en.wikipedia.org/wiki?curid=7333", "title": "Chimera", "text": "Chimera, Chimaera, or Chimaira (Greek for \"she-goat\") originally referred to:\nChimera, chimera, chim\u00e8re, Chimaira, etc. may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "7335", "revid": "109754", "url": "https://en.wikipedia.org/wiki?curid=7335", "title": "Creature of statute", "text": "Corporation or entity created by statute law\nA creature of statute (also known as creature of the state) is a legal entity, such as a corporation, created by statute. Creatures of statute may include municipalities and other artificial legal entities or relationships. Thus, when a statute in some fashion requires the formation of a corporate body\u2014often for governmental purposes\u2014such bodies when formed are known as \"creatures of statute.\" The same concept is also expressed with the phrase \"creature of the state.\"\nThe term \"creature of statute\" is most common to the United States. In the United Kingdom, these bodies are simply called statutory corporations (or statutory bodies) and generally have some governmental function. The United Kingdom Atomic Energy Authority is an example. In a wider sense, most companies in the UK are created under statute since the Companies Act 1985 specifies how a company may be created by a member of the public, but these companies are not called 'statutory corporations'. Often, in American legal and business documents that speak of governing bodies (\"e.g.\", a board that governs small businesses in China) these bodies are described as \"creatures of statute\" to inform readers of their origins and format although the national governments that created them may not term them as creatures of statute. Australia also uses the term \"creature of statute\" to describe some governmental bodies.\nThe importance of a corporate body, regardless of its exact function, when such a body is a creature of statute is that its active functions can only be within the scope detailed by the statute which created that corporation. Thereby, the creature of statute is the tangible manifestation of the functions or work described by a given statute. The jurisdiction of a body that is a creature of statute is also therefore limited to the functional scope written into the laws that created that body. Unlike most (private) corporate bodies, creatures of statute cannot expand their business interests into other diverse areas.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7336", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7336", "title": "CPGM", "text": ""}
{"id": "7337", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7337", "title": "Convention of the Metre", "text": ""}
{"id": "7339", "revid": "42727488", "url": "https://en.wikipedia.org/wiki?curid=7339", "title": "General Conference on Weights and Measures", "text": "International metrological authority\nThe General Conference on Weights and Measures (GCWM; ) is the supreme authority of the International Bureau of Weights and Measures (BIPM), the intergovernmental organization established in 1875 under the terms of the Metre Convention through which member states act together on matters related to measurement science and measurement standards. The CGPM is made up of delegates of the governments of the member states and observers from the Associates of the CGPM. Under its authority, the International Committee for Weights and Measures (ICWM; ) executes an exclusive direction and supervision of the BIPM.\nInitially the Metre Convention was only concerned with the kilogram and the metre, but in 1921 the scope of the treaty was extended to accommodate all physical measurements and hence all aspects of the metric system. In 1960 the 11th CGPM approved the International System of Units, usually known as \"SI\".\nThe General Conference receives the report of the CIPM on work accomplished; it discusses and examines the arrangements required to ensure the propagation and improvement of the International System of Units (SI); it endorses the results of new fundamental metrological determinations and various scientific resolutions of international scope; and it decides all major issues concerning the organization and development of the BIPM, including its financial endowment.\nThe CGPM meets in Paris, usually once every four years. The 25th meeting of the CGPM took place from 18 to 20 November 2014, the 26th meeting of the CGPM took place in Versailles from 13 to 16 November 2018, and the 27th meeting of the CGPM took place from 15 to 18 November 2022.\nEstablishment.\nOn 20 May 1875 an international treaty known as the \"Convention du M\u00e8tre\" (Metre Convention) was signed by 17 states. This treaty established an international organisation, the Bureau international des poids et mesures (BIPM), consisting of:\nThe CGPM acts on behalf of the governments of its members. In so doing, it appoints members to the CIPM, receives reports from the CIPM which it passes on to the governments and national laboratories on member states, examines and where appropriate approves proposals from the CIPM in respect of changes to the International System of Units (SI), approves the budget for the BIPM (over \u20ac13 million in 2018) and it decides all major issues concerning the organization and development of the BIPM.\nThe structure is analogous to that of a stock corporation. The BIPM is the organisation, the CGPM is the general meeting of the shareholders, the CIPM is the board of directors appointed by the CGPM, and the staff at the site in Saint-Cloud perform the day-to-day work.\nMembership criteria.\nThe CGPM recognises two classes of membership \u2013 full membership for those states that wish to participate in the activities of the BIPM and associate membership for those countries or economies that only wish to participate in the CIPM MRA program. Associate members have observer status at the CGPM. Since all formal liaison between the convention organisations and national governments is handled by the member state's ambassador to France, it is implicit that member states must have diplomatic relations with France, though during both world wars, nations that were at war with France retained their membership of the CGPM. CGPM meetings are chaired by the Pr\u00e9sident de l'Acad\u00e9mie des Sciences de Paris.\nOf the twenty countries that attended the Conference of the Metre in 1875, representatives of seventeen signed the convention on 20 May 1875. In April 1884, H. J. Chaney, Warden of Standards in London unofficially contacted the BIPM inquiring whether the BIPM would calibrate some metre standards that had been manufactured in the United Kingdom. Broch, director of the BIPM replied that he was not authorised to perform any such calibrations for non-member states. On 17 September 1884, the British Government signed the convention on behalf of the United Kingdom. This number grew to 21 in 1900, 32 in 1950, and 49 in 2001. As of 18\u00a0\u00a02022[ [update]], there are 64 Member States and 36 Associate States and Economies of the General Conference (with year of partnership in parentheses):\nMember states.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nArgentina (1877)&lt;br&gt;\nAustralia (1947)&lt;br&gt;\nAustria (1875)&lt;br&gt;\nBelarus (2020)&lt;br&gt;\nBelgium (1875)&lt;br&gt;\nBrazil (1921)&lt;br&gt;\nBulgaria (1911)&lt;br&gt;\nCanada (1907)&lt;br&gt;\nChile (1908)&lt;br&gt;\nChina (1977)&lt;br&gt;\nColombia (2012)&lt;br&gt;\nCosta Rica (2022)&lt;br&gt;\nCroatia (2008)&lt;br&gt;\nCzech Republic (1922)&lt;br&gt;\nDenmark (1875)&lt;br&gt;\nEcuador (2019)&lt;br&gt;\nEgypt (1962)&lt;br&gt;\nEstonia (2021)&lt;br&gt;\nFinland (1913)&lt;br&gt;\nFrance (1875)&lt;br&gt;\nGermany (1875)&lt;br&gt;\nGreece (2001)&lt;br&gt;\nHungary (1925)&lt;br&gt;\nIndia (1880)&lt;br&gt;\nIndonesia (1960)&lt;br&gt;\nIran (1975)&lt;br&gt;\nIraq (2013)&lt;br&gt;\nIreland (1925)&lt;br&gt;\nIsrael (1985)&lt;br&gt;\nItaly (1875)&lt;br&gt;\nJapan (1885)&lt;br&gt;\nKazakhstan (2008)&lt;br&gt;\nKenya (2010)&lt;br&gt;\nLithuania (2015)&lt;br&gt;\nMalaysia (2001)&lt;br&gt;\nMexico (1890)&lt;br&gt;\nMontenegro (2018)&lt;br&gt;\nMorocco (2019)&lt;br&gt;\nNetherlands (1929)&lt;br&gt;\nNew Zealand (1991)&lt;br&gt;\nNorway (1875)&lt;br&gt;\nPakistan (1973)&lt;br&gt;\nPoland (1925)&lt;br&gt;\nPortugal (1876)&lt;br&gt;\nRomania (1884)&lt;br&gt;\nRussia (1875)&lt;br&gt;\nSaudi Arabia (2011)&lt;br&gt;\nSerbia (2001)&lt;br&gt;\nSingapore (1994)&lt;br&gt;\nSlovakia (1922)&lt;br&gt;\nSlovenia (2016)&lt;br&gt;\nSouth Africa (1964)&lt;br&gt;\nSouth Korea (1959)&lt;br&gt;\nSpain (1875)&lt;br&gt;\nSweden (1875)&lt;br&gt;\nSwitzerland (1875)&lt;br&gt;\nThailand (1912)&lt;br&gt;\nTunisia (2012)&lt;br&gt;\nTurkey (1875)&lt;br&gt;\nUkraine (2018)&lt;br&gt;\nUnited Arab Emirates (2015)&lt;br&gt;\nUnited Kingdom (1884)&lt;br&gt;\nUnited States (1878)&lt;br&gt;\nUruguay (1908)&lt;br&gt;\nFormer members.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCameroon (1970\u20132012)&lt;br&gt;\nNorth Korea (1982\u20132012)&lt;br&gt;\nVenezuela (1879\u20131907, 1960\u20132018)\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nAssociates.\nAt the 21st meeting of the CGPM in October 1999, the category of \"associate\" was created for states not yet BIPM members and for economic unions.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nAlbania (2007)&lt;br&gt;\nAzerbaijan (2015)&lt;br&gt;\nBangladesh (2010)&lt;br&gt;\nBolivia (2008)&lt;br&gt;\nBosnia and Herzegovina (2011)&lt;br&gt;\nBotswana (2012)&lt;br&gt;\nCambodia (2021)&lt;br&gt;\nCaribbean Community (2005)&lt;br&gt;\nChinese Taipei (2002)&lt;br&gt;\nEthiopia (2018)&lt;br&gt;\nGeorgia (2008)&lt;br&gt;\nGhana (2009)&lt;br&gt;\nHong Kong (2000)&lt;br&gt;\nJamaica (2003)&lt;br&gt;\nKuwait (2018)&lt;br&gt;\nLatvia (2001)&lt;br&gt;\nLuxembourg (2014)&lt;br&gt;\nMalta (2001)&lt;br&gt;\nMauritius (2010)&lt;br&gt;\nMoldova (2007)&lt;br&gt;\nMongolia (2013)&lt;br&gt;\nNamibia (2012)&lt;br&gt;\nNorth Macedonia (2006)&lt;br&gt;\nOman (2012)&lt;br&gt;\nPanama (2003)&lt;br&gt;\nParaguay (2009)&lt;br&gt;\nPeru (2009)&lt;br&gt;\nPhilippines (2002)&lt;br&gt;\nQatar (2016)&lt;br&gt;\nSri Lanka (2007)&lt;br&gt;\nSyria (2012)&lt;br&gt;\nTanzania (2018)&lt;br&gt;\nUzbekistan (2018)&lt;br&gt;\nVietnam (2003)&lt;br&gt;\nZambia (2010)&lt;br&gt;\nZimbabwe (2010\u20132020, 2022)&lt;br&gt;\nFormer Associates.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCuba (2000\u20132021)&lt;br&gt;\nSeychelles (2010\u20132021)&lt;br&gt;\nSudan (2014\u20132021)&lt;br&gt;\nInternational Committee for Weights and Measures.\nThe International Committee for Weights and Measures consists of eighteen persons, each of a different nationality. elected by the General Conference on Weights and Measures (CGPM) whose principal task is to promote worldwide uniformity in units of measurement by taking direct action or by submitting proposals to the CGPM.\nThe CIPM meets every year (since 2011 in two sessions per year) at the Pavillon de Breteuil where, among other matters, it discusses reports presented to it by its Consultative Committees. Reports of the meetings of the CGPM, the CIPM, and all the Consultative Committees, are published by the BIPM.\nMission.\nThe secretariat is based in Saint-Cloud, Hauts-de-Seine, France.\nIn 1999 the CIPM has established the CIPM \"Arrangement de reconnaissance mutuelle\" (Mutual Recognition Arrangement, MRA) which serves as the framework for the mutual acceptance of national measurement standards and for recognition of the validity of calibration and measurement certificates issued by national metrology institutes.\nA recent focus area of the CIPM has been the revision of the SI.\nConsultative committees.\nThe CIPM has set up a number of consultative committees (CC) to assist it in its work. These committees are under the authority of the CIPM. The president of each committee, who is expected to take the chair at CC meetings, is usually a member of the CIPM. Apart from the CCU, membership of a CC is open to National Metrology Institutes (NMIs) of Member States that are recognized internationally as most expert in the field. NMIs from Member States that are active in the field, but lack the expertise to become Members, are able to attend CC meetings as observers.\nThese committees are:\nThe CCU's role is to advise on matters related to the development of the SI and the preparation of the SI brochure. It has liaison with other international bodies such as International Organization for Standardization (ISO), International Astronomical Union (IAU), International Union of Pure and Applied Chemistry (IUPAC), International Union of Pure and Applied Physics (IUPAP) and International Commission on Illumination (CIE).\nMajor reports.\nOfficial reports of the CIPM include:\nFrom time to time the CIPM has been charged by the CGPM to undertake major investigations related to activities affecting the CGPM or the BIPM. Reports produced include:\nThe Blevin Report.\nThe Blevin Report, published in 1998, examined the state of worldwide metrology. The report originated from a resolution passed at the 20th CGPM (October 1995) which committed the CIPM to &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;study and report on the long-term national and international needs relating to metrology, the appropriate international collaborations and the unique role of the BIPM to meet these needs, and the financial and other commitments that will be required from the Member States in the coming decades.\nThe report identified, amongst other things, a need for closer cooperation between the BIPM and other organisations such as International Organization of Legal Metrology (OIML) and International Laboratory Accreditation Cooperation (ILAC) with clearly defined boundaries and interfaces between the organisations. Another major finding was the need for cooperation between accreditation laboratories and the need to involve developing countries in the world of metrology.\nThe Kaarls Report.\nThe Kaarls Report published in 2003 examined the role of the BIPM in the evolving needs for metrology in trade, industry and society.\nSI Brochure.\nThe CIPM has responsibility for commissioning the SI brochure, which is the formal definition of the International system of units. The brochure is produced by the CCU in conjunction with a number of other international organisations. Initially the brochure was only in French \u2013 the official language of the metre convention, but recent versions have been published simultaneously in both English and French, with the French text being the official text. The 6th edition was published in 1991, the 7th edition was published in 1998, and the 8th, in 2006. The most recent edition is the 9th edition, originally published as version 1 in 2019 to include the 2019 redefinition of the SI base units (aka \"new SI\"); it was updated to version 2 in December 2022 to also include the new SI prefixes ronna-, quetta-, ronto- and quecto- introduced in November 2022.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7341", "revid": "17178580", "url": "https://en.wikipedia.org/wiki?curid=7341", "title": "Cowboy Bebop", "text": "1998 anime television series\n is a Japanese neo-noir science fiction anime television series, which originally ran from 1998 to 1999. It was created and animated by Sunrise, led by a production team of director Shinichir\u014d Watanabe, screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical designer Kimitoshi Yamane, and composer Yoko Kanno, who are collectively billed as Hajime Yatate.\nThe series, which ran for twenty-six episodes (dubbed \"sessions\"), is set in the year 2071, and follows the lives of a traveling bounty-hunting crew aboard a spaceship, the \"Bebop\". Although it incorporates a wide variety of genres, the series draws most heavily from science fiction, Western, and noir films. Its most prominent themes are existential boredom, loneliness, and the inability to escape one's past.\nThe series was dubbed into English by Animaze and ZRO Limit Productions, and was originally licensed in North America by Bandai Entertainment (and is now licensed by Crunchyroll) and in Britain by Beez Entertainment (now by Anime Limited); Madman Entertainment owns the license in Australia and New Zealand. In 2001, it became the first anime title to be broadcast on Adult Swim.\n\"Cowboy Bebop\" has been hailed as one of the greatest animated television series of all time. It was a critical and commercial success both in Japanese and international markets, most notably in the United States. It garnered several major anime and science-fiction awards upon its release, and received unanimous praise for its style, characters, story, voice acting, animation, and soundtrack. The English dub was particularly lauded, and is regarded as one of the best anime dubs. Credited with helping to introduce anime to a new wave of Western viewers in the early 2000s, \"Cowboy Bebop\" has also been called a gateway series for anime as a whole.\nPlot.\nIn 2071, roughly fifty years after an accident with a hyperspace gateway which made Earth almost uninhabitable, humanity has colonized most of the rocky planets and moons of the Solar System. Amid a rising crime rate, the Inter Solar System Police (ISSP) set up a legalized contract system, in which registered bounty hunters (also referred to as \"Cowboys\") chase criminals and bring them in alive in return for a reward. The series' protagonists are bounty-hunters working from the spaceship \"Bebop\". The original crew are Spike Spiegel, an exiled former hitman of the criminal Red Dragon Syndicate, and Jet Black, a former ISSP officer. They are later joined by Faye Valentine, an amnesiac con artist; Edward, an eccentric child, skilled in hacking; and Ein, a genetically-engineered Pembroke Welsh Corgi with human-like intelligence. Over the course of the series, the team get involved in disastrous mishaps leaving them without money, while often confronting faces and events from their past: these include Jet's reasons for leaving the ISSP, and Faye's past as a young woman from Earth injured in an accident and cryogenically frozen to save her life.\nWhile much of the show is episodic in nature, the main story arc focuses on Spike and his deadly rivalry with Vicious, an ambitious criminal affiliated with the Red Dragon Syndicate. Spike and Vicious were once partners and friends, but when Spike began an affair with Vicious's girlfriend Julia and resolved to leave the Syndicate with her, Vicious sought to eliminate Spike by blackmailing Julia into killing him. Julia goes into hiding to protect herself and Spike, while Spike fakes his death to escape the Syndicate. In the present, Julia comes out of hiding and reunites with Spike, intending to complete their plan. Vicious, having staged a \"coup d'\u00e9tat\" and taken over the Syndicate, sends hitmen after the pair. Julia is killed, leaving Spike alone. Spike leaves the \"Bebop\" after saying a final goodbye to Faye and Jet. Upon infiltrating the syndicate, he finds Vicious on the top floor of the building and confronts him after dispatching the remaining Red Dragon members. The final battle ends with Spike killing Vicious, only to be seriously wounded himself in the ensuing confrontation. Looking up to the sky, Spike sees Julia. The series concludes as Spike descends the main staircase of the building into the rising sun before eventually falling to the ground.\nGenre and themes.\nWatanabe created a special tagline for the series to promote it during its original presentation, calling it \"a new genre unto itself\". The line was inserted before and after commercial breaks during its Japanese and US broadcasts. Later, Watanabe called the phrase an \"exaggeration\". The show is a hybrid of multiple genres, including westerns and pulp fiction. One reviewer described it as \"space opera meets noir, meets comedy, meets cyberpunk\". It has also been called a \"genre-busting space Western\".\nThe musical style was emphasized in many of the episode titles. Multiple philosophical themes are explored using the characters, including existentialism, existential boredom, loneliness, and the effect of the past on the protagonists. Other concepts referenced include environmentalism and capitalism. The series also makes specific references to or pastiches multiple films, including the works of John Woo and Bruce Lee, \"Midnight Run\", \"\", and \"Alien\". The series also includes extensive references and elements from science fiction, bearing strong similarities to the cyberpunk fiction of William Gibson. Several planets and space stations in the series are made in Earth's image. The streets of celestial objects such as Ganymede resemble a modern port city, while Mars features shopping malls, theme parks, casinos and cities. \"Cowboy Bebop\"'s universe is filled with video players and hyperspace gates, eco-politics and fairgrounds, spaceships and Native American shamans. This setting has been described as \"one part Chinese diaspora and two parts wild west\".\nCharacters.\nThe characters were created by Watanabe and character designed by Toshihiro Kawamoto. Watanabe envisioned each character as an extension of his own personality, or as an opposite person to himself. Each character, from the main cast to supporting characters, were designed to be outlaws unable to fit into society. Kawamoto designed the characters so they were easily distinguished from one another. All the main cast are characterized by a deep sense of loneliness or resignation to their fate and past. From the perspective of Brian Camp and Julie Davis, the main characters resemble the main characters of the anime series \"Lupin III\", if only superficially, given their more troubled pasts and more complex personalities.\nThe show focuses on the character of Spike Spiegel (voiced by K\u014dichi Yamadera), an iconic space cowboy with green hair and often seen wearing a blue suit, with the overall theme of the series being Spike's past and its karmic effect on him. Spike was portrayed as someone who had lost his expectations for the future, having lost the woman he loved, and so was in a near-constant lethargy. Spike's artificial eye was included as Watanabe wanted his characters to have flaws. He was originally going to be given an eyepatch, but this decision was vetoed by producers.\nJet (voiced by Unsh\u014d Ishizuka) is shown as someone who lost confidence in his former life and has become cynical about the state of society. Spike and Jet were designed to be opposites, with Spike being thin and wearing smart attire, while Jet was bulky and wore more casual clothing. The clothing, which was dark in color, also reflected their states of mind. Faye Valentine, Edward Wong (voiced by Aoi Tada), and Ein joined the crew in later episodes. Their designs were intended to contrast against Spike. Faye was described by her voice actress Megumi Hayashibara as initially being an \"ugly\" woman, with her defining traits being her liveliness, sensuality and humanity. To emphasize her situation when first introduced, she was compared to Poker Alice, a famous Western figure.\nEdward and Ein were the only main characters to have real-life models. The former had her behavior based on the antics of Yoko Kanno as observed by Watanabe when he first met her. While generally portrayed as carefree and eccentric, Edward is motivated by a sense of loneliness after being abandoned by her father. Kawamoto initially based Ein's design on a friend's pet corgi, later getting one himself to use as a motion model.\nProduction.\n\"Cowboy Bebop\" was developed by animation studio Sunrise and created by Hajime Yatate, the well-known pseudonym for the collective contributions of Sunrise's animation staff. The leader of the series' creative team was director Shinichir\u014d Watanabe, most notable at the time for directing \"Macross Plus\" and \"\". Other leading members of Sunrise's creative team were screenwriter Keiko Nobumoto, character designer Toshihiro Kawamoto, mechanical art designer Kimitoshi Yamane, composer Yoko Kanno, and producers Masahiko Minami and Yoshiyuki Takei. Most of them had previously worked together, in addition to having credits on other popular anime titles. Nobumoto had scripted \"Macross Plus\", Kawamoto had designed the characters for \"Gundam\", and Kanno had composed the music for \"Macross Plus\" and \"The Vision of Escaflowne\". Yamane had not worked with Watanabe yet, but his credits in anime included \"Bubblegum Crisis\" and \"The Vision of Escaflowne\". Minami joined the project as he wanted to do something different from his previous work on mecha anime.\nConcept.\n\"Cowboy Bebop\" was Watanabe's first project as solo director, as he had been co-director in his previous works. His original concept was for a movie, and during production he treated each episode as a miniature movie. His main inspiration for \"Cowboy Bebop\" was of the anime \"Lupin III\", a crime drama focusing on the exploits of the series' titular character. When developing the series' story, Watanabe began by creating the characters first. He explained, \"the first image that occurred to me was one of Spike, and from there I tried to build a story around him, trying to make him cool.\" While the original dialogue of the series was kept clean to avoid any profanities, its level of sophistication was made appropriate to adults in a criminal environment. Watanabe described \"Cowboy Bebop\" as \"80% serious story and 20% humorous touch\". The comical episodes were harder for the team to write than the serious ones, and though several events in them seemed random, they were carefully planned in advance. Watanabe conceived the series' ending early on, and each episode involving Spike and Vicious was meant to foreshadow their final confrontation. Some of the staff were unhappy about this approach as a continuation of the series would be difficult. While he considered altering the ending, he eventually settled with his original idea. The reason for creating the ending was that Watanabe did not want the series to become like \"Star Trek\", with him being tied to doing it for years.\nDevelopment.\nThe project had initially originated with Bandai's toy division as a sponsor, with the goal of selling spacecraft toys. Watanabe recalled his only instruction was \"So long as there's a spaceship in it, you can do whatever you want.\" But upon viewing early footage, it became clear that Watanabe's vision for the series did not match Bandai's. Believing the series would never sell toy merchandise, Bandai pulled out of the project, leaving it in development hell until sister company Bandai Visual stepped in to sponsor it. Since there was no need to merchandise toys with the property any more, Watanabe had free rein in the development of the series. Watanabe wanted to design not just a space adventure series for adolescent boys but a program that would also appeal to sophisticated adults. During the making of \"Bebop\", Watanabe often attempted to rally the animation staff by telling them that the show would be something memorable up to three decades later. While some of them were doubtful of that at the time, Watanabe many years later expressed his happiness to have been proven right in retrospect. He joked that if Bandai Visual had not intervened then \"you might be seeing me working the supermarket checkout counter right now.\"\nThe city locations were generally inspired by the cities of New York and Hong Kong. The atmospheres of the planets and the ethnic groups in \"Cowboy Bebop\" mostly originated from Watanabe's ideas, with some collaboration from set designers Isamu Imakake, Shoji Kawamori, and Dai Sat\u014d. The animation staff established the particular planet atmospheres early in the production of the series before working on the ethnic groups. It was Watanabe who wanted to have several groups of ethnic diversity appear in the series. Mars was the planet most often used in \"Cowboy Bebop\"'s storylines, with Satoshi Toba, the cultural and setting producer, explaining that the other planets \"were unexpectedly difficult to use\". He stated that each planet in the series had unique features, and the producers had to take into account the characteristics of each planet in the story. For the final episode, Toba explained that it was not possible for the staff to have the dramatic rooftop scene occur on Venus, so the staff \"ended up normally falling back to Mars\". In creating the backstory, Watanabe envisioned a world that was \"multinational rather than stateless\". In spite of certain American influences in the series, he stipulated that the country had been destroyed decades prior to the story, later saying the notion of the United States as the center of the world repelled him.\nMusic.\nThe music for \"Cowboy Bebop\" was composed by Yoko Kanno. Kanno formed the blues and jazz band Seatbelts to perform the series' music. According to Kanno, the music was one of the first aspects of the series to begin production, before most of the characters, story, or animation had been finalized. The genres she used for its composition were western, opera, and jazz. Watanabe noted that Kanno did not score the music exactly the way he told her to. He stated, \"She gets inspired on her own, follows up on her own imagery, and comes to me saying 'this is the song we need for \"Cowboy Bebop\"', and composes something completely on her own.\" Kanno herself was sometimes surprised at how pieces of her music were used in scenes, sometimes wishing it had been used elsewhere, though she also felt that none of their uses were \"inappropriate\". She was pleased with the working environment, finding the team very relaxed in comparison with other teams she had worked with.\nWatanabe further explained that he would take inspiration from Kanno's music after listening to it and create new scenes for the story from it. These new scenes in turn would inspire Kanno and give her new ideas for the music and she would come to Watanabe with even more music. Watanabe cited as an example, \"some songs in the second half of the series, we didn't even ask her for those songs, she just made them and brought them to us.\" He commented that while Kanno's method was normally \"unforgivable and unacceptable\", it was ultimately a \"big hit\" with \"Cowboy Bebop\". Watanabe described his collaboration with Kanno as \"a game of catch between the two of us in developing the music and creating the TV series \"Cowboy Bebop\"\". Since the series' broadcast, Kanno and the Seatbelts have released seven original soundtrack albums, two singles and extended plays, and two compilations through label Victor Entertainment.\nWeapons.\nThe guns on the show were chosen by the director, Watanabe, and in discussion with set designer, Isamu Imakake, and mechanical designer, Kimitoshi Yamane. Setting producer, Satoshi Toba said, \"They talked about how they didn't want common guns, because that wouldn't be very interesting, and so they decided on these guns.\"\nDistribution.\nBroadcast.\n\"Cowboy Bebop\" debuted on TV Tokyo, one of the main broadcasters of anime in Japan, airing from April 3 until June 26, 1998. Due to its 6:00 PM timeslot and depictions of graphic violence, the show's first run only included episodes 2, 3, 7 to 15, 18 and a special. Later that year, the series was shown in its entirety from October 24 until April 24, 1999, on satellite network Wowow. The full series has also been broadcast across Japan by anime television network Animax, which has also aired the series via its respective networks across Southeast Asia, South Asia and East Asia.\nThe first non-Asian country to air Cowboy Bebop was Italy. There, it was first aired on October 21, 1999, on MTV, where it inaugurated the 9:00\u201310:30 PM \"Anime Night\" programming block.\nIn the United States, \"Cowboy Bebop\" was one of the programs shown when Cartoon Network's late night block Adult Swim debuted on September 2, 2001, being the first anime shown on the block that night at midnight ET. During its original run on Adult Swim, episodes 6, 8, and 22 were skipped due to their violent themes in wake of the September 11 attacks. By the third run of the series, all these episodes had premiered for the first time. \"Cowboy Bebop\" was successful enough to be broadcast repeatedly for four years. It has been run at least once every year since 2007, and HD remasters of the show began broadcasting in 2015. In the United Kingdom, it was first broadcast in 2002 on the adult-oriented channel CNX. From November 6, 2007, it was repeated on AnimeCentral until the channel's closure in August 2008. In Australia, \"Cowboy Bebop\" was first broadcast on pay television in 2002 on Adult Swim in Australia. It was broadcast on Sci-Fi Channel on Foxtel. In Australia, \"Cowboy Bebop\" was first broadcast on free-to-air-TV on ABC2 (the national digital public television channel) on January 2, 2007. It has been repeated several times, most recently starting in 2008. \"Cowboy Bebop: The Movie\" also aired again on February 23, 2009, on SBS (a hybrid-funded Australian public broadcasting television network). In Canada, \"Cowboy Bebop\" was first broadcast on December 24, 2006, on Razer.\nIn Latin America, the series was first broadcast on pay-TV in 2001 on Locomotion. It aired again on January 9, 2016 on I.Sat.\nHome media.\n\"Cowboy Bebop\" has been released in four separate editions in North America.\nThe first release was sold in VHS format either as a box set or as seven individual tapes. The tapes were sold through Anime Village, a division of Bandai.\nIn the late 1990s Manga Entertainment purchased the rights to \"Cowboy Bebop\" with plans to release the english-dubbed PAL version on VHS, however the release would never make it to market. \nThe second release was sold in 2000 individually, and featured uncut versions of the original 26 episodes. In 2001, these DVDs were collected in the special edition \"Perfect Sessions\" which included the first 6 DVDs, the first \"Cowboy Bebop\" soundtrack, and a collector's box. At the time of release, the art box from the Perfect Sessions was made available for purchase on The Right Stuff International as a solo item for collectors who already owned the series.\nThe third release, \"The Best Sessions\", was sold in 2002 and featured what Bandai considered to be the best 6 episodes of the series remastered in Dolby Digital 5.1 and DTS surround sound.\nThe fourth release, \"Cowboy Bebop Remix\", was also distributed on 6 discs and included the original 26 uncut episodes, with sound remastered in Dolby Digital 5.1 and video remastered under the supervision of Shinichiro Watanabe. This release also included various extras that were not present in the original release. \"Cowboy Bebop Remix\" was itself collected as the \"Cowboy Bebop Remix: The Complete Collection\" in 2008.\nIn Japan, Cowboy Bebop received no less than a 9 volume VHS, 10 volume Laserdisc (Session 0 + volumes 1-9), and 9 volume DVD release. (Sources: 16-page booklet included in 2008's HD box set. CDjapan.jp) An HD remastered DVD box set containing 7 discs, which features a new 2 Channel Dolby Digital Surround Sound mix, released on February 22, 2008, and also includes Session 0 (disc 1). (Source: CDjapan.jp) On December 21, 2012, a Blu-ray box, both in Standard (7 discs) and Limited Edition (8 discs) forms saw publication, and again uses new video. (Source: CDjapan.jp) Amazon(.jp) issued a Limited Edition Blu-ray box as well, same street date. (Source: Amazon.jp)\nIn December 2012, newly founded distributor Anime Limited announced via Facebook and Twitter that they had acquired the home video license for the United Kingdom. Part 1 of the Blu-ray collection was released on July 29, 2013, while Part 2 was released on October 14. The standard DVD Complete Collection was originally meant to be released on September 23, 2013 with Part 2 of the Blu-ray release but due to mastering and manufacturing errors, the Complete Collection was delayed until November 27. Following the closure of Bandai Entertainment in 2012, Funimation and Sunrise had announced that they rescued \"Cowboy Bebop\", along with a handful of other former Bandai Entertainment properties, for home video and digital release. Funimation released the series on Blu-ray and DVD on December 16, 2014. The series was released in four separate editions: standard DVD, standard Blu-ray, an Amazon.com exclusive Blu-ray/DVD combo, and a Funimation.com exclusive Blu-ray/DVD combo.\nCrunchyroll released a limited edition Blu-ray box set on April 4, 2023 for its 25th anniversary.\nStreaming.\nNetflix acquired the streaming rights to the original anime, with all 26 episodes available worldwide as of October 21, 2021. The series is also available on Hulu and Funimation in the United States. On March 1, 2022, the anime became available on Crunchyroll to consolidate both Funimation and Wakanim into the service.\nRelated media.\nManga.\nTwo \"Cowboy Bebop\" manga series adaptations have been released, both published by Kadokawa Shoten and serialized in \"Asuka Fantasy DX\".&lt;ref name=\"DX 10/1997\"&gt;&lt;/ref&gt;&lt;ref name=\"DX 11/1998\"&gt;&lt;/ref&gt; The first manga series, titled \"Cowboy Bebop: Shooting Star\" and illustrated by Cain Kuga, was serialized from October issue 1997, before the anime series' release, to July issue 1998. It was collected into two volumes in 1998, the first one in May and the second one in September. The second manga series, simply titled \"Cowboy Bebop\" and illustrated by Yutaka Nanten, was serialized from November issue 1998 to March issue 2000. It was collected into three volumes, the first two in April and October 1999 and the third one in April 2000. Both manga series were licensed by Tokyopop for release in North America.\nVideo games.\nA \"Cowboy Bebop\" video game, developed and published by Bandai, was released in Japan for the PlayStation on May 14, 1998. A PlayStation 2 video game, \"\", was released in Japan on August 25, 2005, and an English version had been set for release in North America. However, in January 2007, IGN reported that the release had likely been cancelled, speculating that it did not survive Bandai's merger with Namco to Bandai Namco Games.\nIn 2022, Cowboy Bebop made its premier in the mainline Bandai Namco crossover game Super Robot Wars T, which is traditionally focused on turn based mecha combat\nFilm.\nAn anime film titled known in English as \"Cowboy Bebop: The Movie\", was released in Japan in September 2001 and in the United States in August 2002.\nOn July 22, 2008, \"If\" published an article on its website regarding a rumor of a live-action \"Cowboy Bebop\" movie in development by 20th Century Fox. Producer Erwin Stoff said that the film's development was in the early stages, and that they had \"just signed it\". Keanu Reeves was to play the role of Spike Spiegel. \"Variety\" confirmed on January 15, 2009 that production company Sunrise Animation would be \"closely involved with the development of the English-language project\". The site also confirmed Kenji Uchida, Shinichir\u014d Watanabe and series writer Keiko Nobumoto as associate producers, series producer Masahiko Minami as a production consultant, and Peter Craig as screenwriter. This was lauded by various sources as a promising move for the potential quality of the film. At the time it was slated to release in 2011, but problems with the budget delayed its production. The submitted script was sent back for rewrite to reduce the cost and little has been heard about it since an interview with producer Joshua Long on October 15, 2010; the project currently languishes in development hell. On October 25, 2014, series director Watanabe was asked about the live-action film at the MCM London Comicon. He stated: \"I'm afraid I don't know what they're thinking in Hollywood. Apparently the project hasn't come to a stop but I don't know how it's going to progress from here on. I hear that there are a lot of 'Hollywood' problems.\"\nLive-action series.\nIn 2017, it was announced that an American live-action adaptation of the series was being developed by Tomorrow Studios, a partnership between Marty Adelstein and ITV Studios, with executive production by Sunrise Inc. Christopher Yost was to write the series, and Netflix announced that it would distribute it. On April 4, 2019, Variety reported that John Cho, Mustafa Shakir, Daniella Pineda and Alex Hassell had been cast. Production was shut down in October 2019 due to a knee injury sustained by Cho, setting production back by more than six months. On April 17, 2020, it was revealed that the episodes would be an hour long. On May 19, 2020, Adelstein revealed that there were three finished episodes and that they had shot at least six episodes before Cho's knee injury. In the same interview it was revealed that the director of the anime series, Shinichir\u014d Watanabe, had been hired as a creative consultant. Production in New Zealand resumed on September 30, 2020, following a COVID-19 lockdown in the country. The series was released on November 19, 2021 to mixed reviews. On December 9, 2021, it was announced that it would not be renewed for a second season, with Netflix cancelling it entirely.\nOther media.\nAn official side story titled \"Cowboy Bebop: UT\" tells the story of Ural and Victoria Terpsichore (V.T. from the session \"Heavy Metal Queen\") when they were bounty-hunters. The story was available in its own official site, however the website was closed and is currently available at the site mirror.\nA deck-building board game, \"Cowboy Bebop: Space Serenade\", was released in 2019.\nReception.\nCritical reception.\n\"Cowboy Bebop\" received unanimous acclaim, beginning at the time of its initial broadcast. Beginning in 1998, Japanese critic Keith Rhee highlighted the series as a standout in an otherwise \"run-of-the-mill\" season, praising its overall production values, and singling out Kanno's soundtrack as \"a much-welcome change from all the sugary J-pop tunes of most anime features\". Rhee also highlighted the show's Japanese \"all-star cast\", which his colleague Mark L. Johnson described as being filled with \"veteran voice talent\", turning in even greater performances than those of their \"above average\" US counterparts. In 1999, Australian magazine \"Hyper\" reviewed the anime and rated it 9.5 out of 10.\nAnime News Network's Mike Crandol gave the series an 'A+' rating for the dubbed version, and an 'A' rating for the subbed version. He characterized the series as \"one of the most popular and respected anime titles in history\", before adding that it was \"a unique television show which skillfully transcends all kinds of genres\". Crandol praised its characters as \"some of the most endearing characters to ever grace an anime\", and commended the voice acting, especially the \"flawless English cast\". He also complimented the series' \"movie-quality\" animation, \"sophisticated\" writing, and its \"incredible\" musical score. Crandol hailed \"Cowboy Bebop\" as a \"landmark\" anime \"that will be remembered long after many others have been forgotten\", and went on to call it \"one of the greatest anime titles ever\". Additionally, Michael Toole of Anime News Network named \"Cowboy Bebop\" as one of the most important anime of the 1990s.\nT.H.E.M. Anime Reviews gave the entire series a perfect score of 5 out of 5 stars, with reviewer Christina Carpenter believing \"Cowboy Bebop\" as \"one of the best [anime]\" and touting it as a masterpiece that \"puts most anime...and Hollywood, to shame\". She described it as a \"very stylish, beautifully crafted series that deserves much more attention than it gets\". Carpenter praised the animation as \"a rarity and a marvel to behold\" and that it was \"beyond superb\", and the plot and characterization as having \"a sophistication and subtlety that is practically one-of-a-kind\". She also praised the soundtrack, and hailed the opening theme as one of the best intro pieces she had ever heard. Carpenter went to say that \"Bebop\" was a \"must-have for any serious collector of Japanese animation\".\nIn his article \"Asteroid Blues: The Lasting Legacy of \"Cowboy Bebop\"\", \"The Atlantic\" writer Alex Suskind states, \"On paper, \"Cowboy Bebop\", the legendary cult anime series from Shinichir\u014d Watanabe, reads like something John Wayne, Elmore Leonard, and Philip K. Dick came up with during a wild, all-night whiskey bender.\" He goes on to write, \"The response from critics and fans may have sounded hyperbolic\u2014the word 'masterpiece' was thrown around a great deal\u2014but the praise was justified. First-time solo director Watanabe had created a gorgeous tale of morality, romance, and violence\u2013a dark look at the lives of outlaws that's shot like an independent film.\"\nIn January 2015, television writer Kyle Mills of DVD Talk awarded the series five stars upon review. He stated, \"Regardless of the medium, be it live action television, film, or animation, \"Cowboy Bebop\" is simply one of the finest examples of storytelling ever created.\" In his review, he describes the finale as \"one of the best in television history\", referring to it as a \"widely revered\" ending that \"still sparks fan conversation, resonating with viewers 15 years on\". He closes by writing, \"\"Cowboy Bebop\" ends with a bang.\"\nIn his 2018 review of the series, \"Paste\" critic John Maher wrote, \"It feels like a \"magnum opus\" produced at the pinnacle of a long career despite being, almost unbelievably, Watanabe's first series as a director. It is a masterwork that should justly rank among the best works of television of all time.\" It was also placed at #1 on the publication's list of the \"50 Best Anime Series of All Time\".\nOn review aggregator Rotten Tomatoes, the series has an approval rating of 100% based on 23 reviews, with an average rating of 8.00/10. The website's critical consensus reads, \"Blending a head-spinning array of genres and references, \"Cowboy Bebop\" is an anime television classic that must be experienced.\"\nIn an April 2019 interview with Diego Molano, creator of \"Victor &amp; Valentino\", he said that Cowboy Bebop was the first anime he \"obsessed over\", as he spent time tracking down VHS tapes of the show in high school. He also argued that this series showed him \"how cinematic and emotional animation can be\".\nAccolades.\nIn the 1999 Anime Grand Prix awards for the anime of 1998, \"Cowboy Bebop\" won two 1st place awards: Spike Spiegel was awarded the best male character; and Megumi Hayashibara was awarded the best voice actor for her role as Faye Valentine. \"Cowboy Bebop\" also received rankings in other categories: the series itself was awarded the 2nd best anime series; Faye Valentine and Ed were ranked the 5th and 9th best female characters respectively; \"Tank!\" and \"The Real Folk Blues\" were ranked the 3rd and 15th best songs respectively; and \"Ballad of Fallen Angels\", \"Speak Like a Child\", \"Jamming with Edward\" and \"Mish-Mash Blues\" were ranked the 2nd, 8th, 18th and 20th best episodes respectively.\nIn the 2000 Anime Grand Prix awards for the anime of 1999, \"Cowboy Bebop\" won the same two 1st place awards again: best male character for Spike Spiegel; and best voice actor for Megumi Hayashibara. Other rankings the series received are: 2nd best anime series; 6th best female character for Faye Valentine; 7th and 12th best song for \"Tank!\" and \"Blue\" respectively; and 3rd and 17th best episode for \"The Real Folk Blues (Part 2)\" and \"Hard Luck Woman\" respectively. In the 2000 Seiun Awards, Cowboy Bebop was awarded for Best Media of the Year.\nA 2004 poll in \"Newtype USA\", the US edition of the Japanese magazine \"Newtype\", asked its readers to vote the \"Top 25 Anime Titles of All Time\"; \"Cowboy Bebop\" ranked 2nd on the list (after \"Neon Genesis Evangelion\"), placing it as one of the most socially relevant and influential anime series ever created. During that same year, \"Cinefantastique\" listed the anime as one of the \"10 Essential Animations\", citing the series' \"gleeful mix of noir-style, culture-hopping inclusiveness and music\". In 2007, the American Anime magazine \"Anime Insider\" listed the \"50 Best Anime Ever\" by compiling lists of industry regulars and magazine staff, and ranked \"Cowboy Bebop\" as the #1 anime of all time. In 2012, Madman Entertainment compiled the votes of fans online for \"The Top 20 Madman Anime Titles\" and ranked \"Cowboy Bebop\" at #7.\n\"Cowboy Bebop\" has been featured in several lists published by IGN. In the 2009 \"Top 100 Animated TV Series\" list, \"Cowboy Bebop\", labelled as \"a very original\u00a0\u2013 and arguably one of the best\u00a0\u2013 anime\", was placed 14th, making it the second highest ranking anime on the list (after \"Evangelion\") and one of the most influential series of the 1990s. In 2011, \"Bebop\" was ranked 29th in the \"Top 50 Sci-Fi TV Shows\" list, once again being the second highest ranking anime on the list (after \"Evangelion\"). In 2006, \"Cowboy Bebop\"'s soundtrack was ranked #1 in \"Top Ten Anime Themes and Soundtracks of All-Time\" list, with the series being commented as \"one of the best anime ever and certainly is tops when it comes to music.\" Spike Spiegel was ranked 4th place in the \"Top 25 Anime Characters of All Time\" article. IGN Movies also placed \"Cowboy Bebop\" in their list of \"10 Cartoon Adaptations We'd Like to See\".\nAnalysis.\nThe series has been subject to study and analysis since its debut, with the main focus being on its style and mixture of genres. Miguel Douglas, describing the series style in a review, said that \"the series distinctly establishes itself outside the realm of conventional Japanese animation and instead chooses to forge its own path. With a setting within the realm of science fiction, the series wisely offers a world that seems entirely realistic considering our present time. Free from many of the elements that accompany science fiction in general\u2014whether that be space aliens, giant robots, or laser guns\u2014the series delegates itself towards presenting a world that is quite similar to our own albeit showcasing some technological advances. Certainly not as pristine a future we would see in other series or films, \"Cowboy Bebop\" decides to deliver a future that closely reflects that of our own time. This aspect of familiarity does wonders in terms of relating to the viewer, and it presents a world that certainly resembles our very own.\" Daryl Surat of \"Otaku USA\", commenting on the series' appeal, said that it was \"that rare breed of science-fiction: 'accessible'. Unlike many anime titles, viewers weren't expected to have knowledge of Japanese culture\u2014character names, signs, and the like were primarily in English to begin with\u2014or have seen any other anime series prior.\" Michelle Onley Pirkle, in her book \"Science Fiction Film, Television, and Adaptation: Across the Screens\", said that \"\"Cowboy Bebop\" is taking a new take on genre, not by creating unique images and sounds, but by playing 'freely' with, 'remixing', or adapting the images and sounds of other familiar genres in a dynamic way.\" Robert Baigent, writing for the \"Graduate Journal of Asia-Pacific Studies\", said that the series' appeal likely stemmed from the trend in anime to emulate Western fiction.\nLegacy.\nIn March 2009, the print and web editions of \"The Onion\"'s \"The A.V. Club\" called \"Cowboy Bebop\" \"rightly a huge hit\", and listed it as a gateway series to understanding the medium of anime as a whole. Suskind said: \"It was unlike anything the genre had seen before. It even approached its music differently. The show kicked off with a wormhole of a theme song, and the soundtrack moves so seamlessly through genres, from rock to country to pop to jazz to funk, it's shocking to learn that one set of musicians is behind it all\". In an interview, producer Sean Akins also states that the series \"created a whole new world\". \"It's hard for me to quantify the impact that I think it has had. It changed anime. I think people began to think about what shows would be cool. I think it redefined cool within animation, not only in Japan but in the States\". One of the series' main animators, Tensai Okamura, went on to create his own anime in 2007: \"Darker than Black\". Okamura used his experience from \"Cowboy Bebop\" to write the screenplay of \"Darker than Black\", leading to narratives composed of two episodes similar to Japanese dramas.\nAmerican film director, screenwriter, and producer Rian Johnson has cited \"Cowboy Bebop\" as a visual influence on his films, most notably \"Brick\". \"Ender's Game\" writer Orson Scott Card also praised the series. He states that the series is \"better than most sci-fi films out there\". He goes on to say that he \"found this series brilliant, but what held me was a combination of strong relationship-based storytelling, a moody visual style that never got old and really smart dialogue\".\nAfter the creation of the series, an interviewer asked Watanabe if he had any plans to create more \"Cowboy Bebop\" material. Watanabe responded by saying that he does not believe that he \"should just keep on making \"Cowboy Bebop\" sequels for the sake of it\". Watanabe added that ending production and \"to quit while we're ahead when people still want more\" is more \"in keeping with the \"Bebop\" spirit\". In a more recent interview from 2006 with \"The Daily Texan\", Watanabe was asked if there would ever be more \"Cowboy Bebop\". Watanabe's answer was \"someday...maybe, someday\".\nIn May 2020, composer Mason Lieberman, who has never actually seen Cowboy Bebop, partnered with Sunrise and Funimation to produce an official \"Cowboy Bebop\" charity track for COVID-19 relief. This track was released on vinyl and featured the return of original series composer Y\u014dko Kanno, original recording band The Seatbelts, and a collection of forty other special musical guests.\nNotes.\nExplanatory\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nJapanese\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7342", "revid": "35832826", "url": "https://en.wikipedia.org/wiki?curid=7342", "title": "Clement of Alexandria", "text": "Christian theologian (c.150 \u2013 c.215)\nTitus Flavius Clemens, also known as Clement of Alexandria (; c.\u2009150 \u2013 c.\u2009215 AD), was a Christian theologian and philosopher who taught at the Catechetical School of Alexandria. Among his pupils were Origen and Alexander of Jerusalem. A convert to Christianity, he was an educated man who was familiar with classical Greek philosophy and literature. As his three major works demonstrate, Clement was influenced by Hellenistic philosophy to a greater extent than any other Christian thinker of his time, and in particular, by Plato and the Stoics. His secret works, which exist only in fragments, suggest that he was familiar with pre-Christian Jewish esotericism and Gnosticism as well. In one of his works he argued that Greek philosophy had its origin among non-Greeks, claiming that both Plato and Pythagoras were taught by Egyptian scholars. \nClement is usually regarded as a Church Father. He is venerated as a saint in Coptic Christianity, Eastern Catholicism, Ethiopian Christianity, and Anglicanism. He was revered in Western Catholicism until 1586, when his name was removed from the Roman Martyrology by Pope Sixtus V on the advice of Baronius. The Eastern Orthodox Church officially stopped any veneration of Clement of Alexandria in the 10th century. Nonetheless, he is still often referred to as \"Saint Clement of Alexandria\" by both Eastern Orthodox and Roman Catholic authors.\nBiography.\nNeither Clement's birthdate or birthplace is known with any degree of certainty. It is speculated that he was born sometime around 150 AD. According to Epiphanius of Salamis, he was born in Athens, but there is also a tradition of an Alexandrian birth.\nHis parents were pagans and Clement was a convert to Christianity. In the \"Protrepticus\" he displays an extensive knowledge of Greek religion and mystery religions, which could only have arisen from the practice of his family's religion.\nHaving rejected paganism as a young man due to its perceived moral corruption, he travelled in Greece, Asia Minor, Palestine, and Egypt. Clement's journeys were primarily a religious undertaking. In Greece, he encountered an Ionian theologian, who has been identified as Athenagoras of Athens; while in the east, he was taught by an Assyrian, sometimes identified with Tatian, and a Jew, possibly Theophilus of Caesarea.\nIn around 180 AD, Clement reached Alexandria, where he met Pantaenus, who taught at the Catechetical School of Alexandria. Eusebius suggests that Pantaenus was the head of the school, but controversy exists about whether the institutions of the school were formalized in this way before the time of Origen. Clement studied under Pantaenus, and was ordained to the priesthood by Pope Julian before 189. Otherwise, virtually nothing is known of Clement's personal life in Alexandria. He may have been married, a conjecture supported by his writings.\nDuring the Severian persecution of 202\u2013203, Clement left Alexandria. In 211, Alexander of Jerusalem wrote a letter commending him to the Church of Antioch, which may imply that Clement was living in Cappadocia or Jerusalem at that time. He died c.\u2009215 AD at an unknown location.\nTheological works.\nTrilogy.\nThree of Clement's major works have survived in full and they are collectively referred to as a trilogy:\n\"Protrepticus\".\nThe \"Protrepticus\" (: \"Exhortation to the Greeks\") is, as its title suggests, an exhortation to the pagans of Greece to adopt Christianity. Within it, Clement demonstrates his extensive knowledge of pagan mythology and theology. It is chiefly important due to Clement's exposition of religion as an anthropological phenomenon. After a short philosophical discussion, it opens with a history of Greek religion in seven stages. Clement suggests that at first, humans mistakenly believed the Sun, the Moon, and other heavenly bodies to be deities. The next developmental stage was the worship of the products of agriculture, from which he contends the cults of Demeter and Dionysus arose. Humans then paid reverence to revenge and deified human feelings of love and fear, among others. In the following stage, the poets Hesiod and Homer attempt to enumerate the deities; Hesiod's Theogony giving the number of twelve. Finally, humans reached a stage when they proclaimed others, such as Asclepius and Heracles, as deities. Discussing idolatry, Clement contends that the objects of primitive religion were unshaped wood and stone, and idols thus arose when such natural items were carved. Following Plato, Clement is critical of all forms of visual art, suggesting that artworks are but illusions and \"deadly toys\".\nClement criticizes Greek paganism in the \"Protrepticus\" on the basis that its deities are both false and poor moral examples. He attacks the mystery religions for their ritualism and mysticism. In particular, the worshippers of Dionysus are ridiculed by him for their family-based rituals (such as the use of children's toys in ceremony). He suggests at some points that the pagan deities are based on humans, but at other times he suggests that they are misanthropic demons, and he cites several classical sources in support of this second hypothesis. Clement, like many pre-Nicene church fathers, writes favourably about Euhemerus and other rationalist philosophers, on the grounds that they at least saw the flaws in paganism. However, his greatest praise is reserved for Plato, whose apophatic views of God prefigure Christianity.\nThe figure of Orpheus is prominent throughout the Protrepticus narrative, and Clement contrasts the song of Orpheus, representing pagan superstition, with the divine Logos of Christ. According to Clement, through conversion to Christianity alone can one fully participate in the Logos, which is universal truth.\n\"Paedagogus\".\nThe title of \"Paedagogus\", translatable as \"tutor\", refers to Christ as the teacher of all humans, and it features an extended metaphor of Christians as children. It is not simply instructional: Clement intends to show how the Christian should respond to the Love of God authentically. Following Plato (Republic 4:441), he divides life into three elements: character, actions, and passions. The first having been dealt with in the \"Protrepticus\", he devotes the \"Paedagogus\" to reflections on Christ's role in teaching humans to act morally and to control their passions. Despite its explicitly Christian nature, Clement's work draws on Stoic philosophy and pagan literature; Homer, alone, is cited more than sixty times in the work.\nAlthough Christ, like a human, is made in the image of God, he alone shares the likeness of God the Father. Christ is both sinless and apathetic, and thus by striving to imitate Christ, one can achieve salvation. To Clement, sin is involuntary, and thus irrational (), removed only through the wisdom of the Logos. God's guidance away from sin is thus a manifestation of God's universal love for mankind. The word play on and is characteristic of Clement's writing, and may be rooted in the Epicurean belief that relationships between words are deeply reflective of relationships between the objects they signify.\nClement argues for the equality of sexes, on the grounds that salvation is extended to all humans equally. Unusually, he suggests that Christ is neither female nor male, and that God the Father has both female and male aspects: the eucharist is described as milk from the breast (Christ) of the Father. Clement is supportive of women playing an active role in the leadership of the church and he provides a list of women he considers inspirational, which includes both Biblical and Classical Greek figures. It has been suggested that Clement's progressive views on gender as set out in the \"Paedagogus\" were influenced by Gnosticism, however, later in the work, he argues against the Gnostics that faith, not esoteric knowledge (), is required for salvation. According to Clement, it is through faith in Christ that one is enlightened and comes to know God.\nIn the second book, Clement provides practical rules on living a Christian life. He argues against overindulgence in food and in favour of good table manners. While prohibiting drunkenness, he promotes the drinking of alcohol in moderation following 1 Timothy 5:23. Clement argues for a simple way of life in accordance with the innate simplicity of Christian monotheism. He condemns elaborate and expensive furnishings and clothing, and argues against overly passionate music and perfumes, but Clement does not believe in the abandonment of worldly pleasures and argues that the Christian should be able to express joy in God's creation through gaiety and partying. He opposes the wearing of garlands, because the picking of the flowers ultimately kills a beautiful creation of God, and the garland resembles the crown of thorns.\nClement treats sex at some length. He argues that both promiscuity and sexual abstinence are unnatural, and that the main goal of human sexuality is procreation. He argues that adultery, coitus with pregnant women, concubinage, homosexuality, and prostitution all should be avoided as they will not contribute toward the generation of legitimate offspring.\nIn his third book, Clement continues along a similar vein, condemning cosmetics on the grounds that it is one's soul, not the body, one should seek to beautify. Clement also opposes the dyeing of men's hair and male depilation as being effeminate. He advises choosing one's company carefully, to avoid being corrupted by immoral people, and while arguing that material wealth is no sin in itself, it is too likely to distract one from the infinitely more important spiritual wealth that is found in Christ. The work finishes with selections of scripture supporting Clement's argument, and following a prayer, the lyrics of a hymn.\n\"Stromata\".\nThe contents of the \"Stromata\", as its title suggests, are miscellaneous. Its place in the trilogy is disputed \u2013 Clement initially intended to write the \"Didasculus\", a work that would complement the practical guidance of the \"Paedagogus\" with a more intellectual schooling in theology. The \"Stromata\" is less systematic and ordered than Clement's other works, and it has been theorized by Andr\u00e9 M\u00e9hat that it was intended for a limited, esoteric readership. Although Eusebius wrote of the eight books of the work, only seven undoubtedly survive. Photius, writing in the 9th century, found various text appended to manuscripts of the seven canonical books, which led Daniel Heinsius to suggest that the original eighth book is lost, and he identified the text purported to be from the eighth book as fragments of the \"Hypotyposes\".\nThe first book starts on the topic of Greek philosophy. Consistent with his other writing, Clement affirms that philosophy had a propaedeutic role for the Greeks, similar to the function of the law for the Jews. He then embarks on a discussion of the origins of Greek culture and technology, arguing that most of the important figures in the Greek world were foreigners, and (erroneously) that Jewish culture was the most significant influence on Greece. In an attempt to demonstrate the primacy of Moses, Clement gives an extended chronology of the world, wherein he dates the birth of Christ to 25 April or May, 4\u20132 BC, and the creation of the world to 5592 BC. The books ends with a discussion on the origin of languages and the possibility of a Jewish influence on Plato.\nThe second book is largely devoted to the respective roles of faith and philosophical argument. Clement contends that while both are important, the fear of God is foremost, because through faith one receives divine wisdom. To Clement, scripture is an innately true primitive philosophy that is complemented by human reason through the Logos. Faith is voluntary, and the decision to believe is a crucial fundamental step in becoming closer to God. It is never irrational, as it is founded on the knowledge of the truth of the Logos, but all knowledge proceeds from faith, as first principles are unprovable outside a systematic structure.\nThe third book covers asceticism. He discusses marriage, which is treated similarly in the \"Paedagogus\". Clement rejects the Gnostic opposition to marriage, arguing that only men who are uninterested in women should remain celibate, and that sex is a positive good if performed within marriage for the purposes of procreation. He argues that this has not always been so: the Fall occurred because Adam and Eve succumbed to their desire for each other, and copulated before the allotted time. He argues against the idea that Christians should reject their family for an ascetic life, which stems from Luke, contending that Jesus would not have contradicted the precept to \"Honour thy Father and thy Mother\", one of the Ten Commandments. Clement concludes that asceticism will only be rewarded if the motivation is Christian in nature, and thus the asceticism of non-Christians such as the gymnosophists is pointless.\nClement begins the fourth book with a belated explanation of the disorganized nature of the work, and gives a brief description of his aims for the remaining three or four books. The fourth book focuses on martyrdom. While all good Christians should be unafraid of death, Clement condemns those who actively seek out a martyr's death, arguing that they do not have sufficient respect for God's gift of life. He is ambivalent about whether any believing Christians can become martyrs by virtue of the manner of their death, or whether martyrdom is reserved for those who have lived exceptional lives. Marcionites cannot become martyrs, because they do not believe in the divinity of God the Father, so their sufferings are in vain. There is then a digression to the subject of theological epistemology. According to Clement, there is no way of empirically testing the existence of God the Father, because the Logos has revelatory, not analysable meaning, although Christ was an object of the senses. God had no beginning, and is the universal first principle.\nThe fifth book returns to the subject of faith. Clement argues that truth, justice, and goodness can be seen only by the mind, not the eye; faith is a way of accessing the unseeable. He stresses that knowledge of God can only be achieved through faith once one's moral faults have been corrected. This parallels Clement's earlier insistence that martyrdom can only be achieved by those who practice their faith in Christ through good deeds, not those who simply profess their faith. God transcends matter entirely, and thus the materialist cannot truly come to know God. Although Christ was God incarnate, it is spiritual, not physical comprehension of him that is important.\nIn the beginning of the sixth book, Clement intends to demonstrate that the works of Greek poets were derived from the prophetic books of the Bible. In order to reinforce his position that the Greeks were inclined toward plagiarism, he cites numerous instances of such inappropriate appropriation by classical Greek writers, reported second-hand from \"On Plagiarism\", an anonymous 3rd-century BC work sometimes ascribed to Aretades. Clement then digresses to the subject of sin and hell, arguing that Adam was not perfect when created, but given the potential to achieve perfection. He espouses broadly universalist doctrine, holding that Christ's promise of salvation is available to all, even those condemned to hell.\nThe final extant book begins with a description of the nature of Christ, and that of the true Christian, who aims to be as similar as possible to both the Father and the Son. Clement then criticizes the simplistic anthropomorphism of most ancient religions, quoting Xenophanes' famous description of African, Thracian, and Egyptian deities. He indicates that the Greek deities may also have had their origins in the personification of material objects: Ares representing iron, and Dionysus wine. Prayer, and the relationship between love and knowledge are then discussed. Corinthians 13:8 seems to contradict the characterization of the true Christian as one who knows; but to Clement knowledge vanishes only in that it is subsumed by the universal love expressed by the Christian in reverence for the Creator. Following Socrates, he argues that vice arises from a state of ignorance, not from intention. The Christian is a \"laborer in God's vineyard\", responsible both for one's own path to salvation and that of one's neighbor. The work ends with an extended passage against the contemporary divisions and heresies within the church.\nOther works.\nBesides the great trilogy, Clement's only other extant work is the treatise \"Salvation for the Rich\", also known as \"Who is the Rich Man who is Saved?\" written c.\u2009203 AD Having begun with a scathing criticism of the corrupting effects of money and misguided servile attitudes toward the wealthy, Clement discusses the implications of Mark 10:25. The rich are either unconvinced by the promise of eternal life, or unaware of the conflict between the possession of material and spiritual wealth, and the good Christian has a duty to guide them toward a better life through the Gospel. Jesus' words are not to be taken literally \u2014 the supercelestial () meanings should be sought in which the true route to salvation is revealed. The holding of material wealth in itself is not a wrong, so long as it is used charitably, but Christians should be careful not to let their wealth dominate their spirit. It is more important to give up sinful passions than external wealth. If the rich are to be saved, all they must do is to follow the two commandments, and while material wealth is of no value to God, it can be used to alleviate the suffering of neighbors.\nOther known works exist in fragments alone, including the four eschatological works in the secret tradition: \"Hypotyposes\", \"Excerpta ex Theodoto\", \"Eclogae Propheticae\", and the \"Adumbraetiones\". These cover Clement's celestial hierarchy, a complex schema in which the universe is headed by the Face of God, below which lie seven \"protoctists\", followed by archangels, angels, and humans. According to Jean Dani\u00e9lou, this schema is inherited from a Judaeo-Christian esotericism, followed by the Apostles, which was only imparted orally to those Christians who could be trusted with such mysteries. The \"proctocists\" are the first beings created by God, and act as priests to the archangels. Clement identifies them both as the \"Eyes of the Lord\" and with the Thrones. Clement characterizes the celestial forms as entirely different from anything earthly, although he argues that members of each order only seem incorporeal to those of lower orders. According to the \"Eclogae Propheticae\", every thousand years every member of each order moves up a degree, and thus humans can become angels. Even the \"protoctists\" can be elevated, although their new position in the hierarchy is not clearly defined. The apparent contradiction between the fact that there can be only seven \"protoctists\" but also a vast number of archangels to be promoted to their order is problematical. One modern solution regards the story as an example of \"interiorized apocalypticism\": imagistic details are not to be taken literally, but as symbolizing interior transformation.\nThe titles of several lost works are known because of a list in Eusebius' \"Ecclesiastical History\", 6.13.1\u20133. They include the \"Outlines\", in eight books, and \"Against Judaizers\". Others are known only from mentions in Clement's own writings, including \"On Marriage\" and \"On Prophecy\", although few are attested by other writers and it is difficult to separate works that he intended to write from those that were completed.\nThe Mar Saba letter was attributed to Clement by Morton Smith, but there remains much debate today over whether it is an authentic letter from Clement, an ancient pseudepigraph, or a modern forgery. If authentic, its main significance would be in its relating that the Apostle Mark came to Alexandria from Rome and there, wrote a more spiritual Gospel, which he entrusted to the Church in Alexandria on his death; if genuine, the letter pushes back the tradition related by Eusebius connecting Mark with Alexandria by a century.\nLegacy.\nEusebius is the first writer to provide an account of Clement's life and works, in his \"Ecclesiastical History\", 5.11.1\u20135, 6.6.1 Eusebius provides a list of Clement's works, biographical information, and an extended quotation from the \"Stromata\".\nPhotios I of Constantinople writes against Clement's theology in the \"Bibliotheca\", although he is appreciative of Clement's learning and the literary merits of his work. In particular, he is highly critical of the \"Hypotyposes\", a work of biblical exegesis of which only a few fragments have survived. Photios compared Clement's treatise, which, like his other works, was highly syncretic, featuring ideas of Hellenistic, Jewish, and Gnostic origin, unfavorably against the prevailing orthodoxy of the 9th century. Amongst the particular ideas Photios deemed heretical were:\nAs one of the earliest of the Church fathers whose works have survived, he is the subject of a significant amount of recent academic work, focusing on, among other things, his exegesis of scripture, his Logos-theology and pneumatology, the relationship between his thought and non-Christian philosophy, and his influence on Origen.\nVeneration.\nUp until the 17th century Clement was venerated as a saint in the Roman Catholic Church. His name was to be found in the martyrologies, and his feast fell on the fourth of December, but when the Roman Martyrology was revised by Pope Clement VIII his name was dropped from the calendar on the advice of Cardinal Baronius. Benedict XIV maintained this decision of his predecessor on the grounds that Clement's life was little known, that he had never obtained public cultus in the Church, and that some of his doctrines were, if not erroneous, at least suspect.\nAlthough Clement is not widely venerated in Eastern Christianity, the Prologue of Ohrid repeatedly refers to him as a saint, as do various Orthodox authorities including the Greek Metropolitan Kallinikos of Edessa.\nThe Coptic tradition considers Clement a saint. Saint Clement Coptic Orthodox Christian Academy in Nashville, Tennessee, is specifically named after him. \nClement is commemorated in Anglicanism. The independent Universal Catholic Church's cathedral in Dallas is also dedicated to him.\nTheology.\nGnosis.\nClement taught that faith was the basis of salvation, however he also believed that faith was also the basis of \"gnosis\" which for him mean spiritual and mystical knowledge. Clement of Alexandria appropriated the word \"gnosis\" from what the Gnostics used, whom he opposed, but re-interpreted the word in a more Christian manner. Clement of Alexandria distinguished between two kinds of Christians, a pistic Christian who lives according to God's law, and the Christian gnostic who lives on the level of the gospel and responds by discipline and love. Clement's views of gnosis can be considered a forerunner of monasticism that began in Egypt after his death.\nPhilosophy.\nClement suggested that philosophy was a preparatory discipline to the Greek world that would lead them to accept Christianity, and often sought to harmonize insights of Greek philosophy with biblical teaching. He defined philosophy as \"the desire for true being and the studies which lead to it.\" Clement has been described as \"the founder of what was to become the great tradition of Christian philosophical theology.\" He also was a forerunner to some views of Augustine, including arguably the just war theory and the theory of the two cities.\nUniversalism.\nClement is often regarded by patristics scholars as one of the first Christian universalists, espousing belief in the eventual salvation of every person (though not with the level of systematic clarity of his disciple Origen). Clement understood divine punishment as corrective and remedial rather than merely retributive or destructive. He writes, \u201c[God] destroys no one but gives salvation to all.\u201d \u201cHe bestows salvation on all mankind.\u201d \u201cHe indeed saves all universally\u2014some as converted by punishments, others by voluntary submission with dignity of honor\u2014that to Him every knee shall bow, both of beings in heaven, and on earth, and under the earth; that is, angels, and men, and souls departed this life.\u201d \u201cGod\u2019s punishments are saving and disciplinary, leading to conversion; choosing rather the repentance than the death of a sinner.\u201d \u201cI will grant that He punishes the disobedient, for punishment is for the good and advantage of him who is punished, for it is the correction of a refractory subject.\u201d \u201cFor all things are arranged with a view to the salvation of the universe by the Lord of the universe, both generally and particularly.\u201d\nEducation.\nFor Clement, disciplining the body will help the Christian discipline his soul, which is why he gives detailed instructions on proper Christian conduct, decorum, and relationships in the second and third books of \"The Instructor\". Only once the passions are subject to the authority of the Word (or reason) can the Christian embark on an advanced course of philosophical study and contemplation.\nClement adopts a position that will give rise to a whole stream of later Christian thought: true philosophy and authentic human knowledge have their origin in the Logos, which is the unique source of all truth. He accepts the conception of \u03c0\u03b1\u03b9\u03b4\u03b5\u03af\u03b1 as he conducts the wisdom taught by the Logos through education in the sacred letters: on the one hand, the Greek \u03c0\u03b1\u03b9\u03b4\u03b5\u03af\u03b1 prepares the mind of the Christian to distinguish and defend the truth, and, on the other, the liberal arts help the new Christian to direct all his efforts towards the truly useful of each particular discipline, geometry, music, grammar and philosophy.\nNotably (considering the time period), Clement seemed to advocate for the equality of women and men in the area of education, at least within the context of Christian spirituality and ethics. He wrote, \"Let us recognize, too, that both men and women practice the same sort of virtue; surely, if there is but one God for both, then there is but one Educator for both.\"\nEconomics.\nClement opposed a literal interpretation of the command \"sell what you have and give to the poor,\" and argued that the Bible does not command every person to renounce all property, and that wealth can be used either for good or evil. Yet he seems to have done so tentatively (and perhaps reluctantly), to address the concerns of upper-class converts, while simultaneously warning of the dangers of wealth.\nCreation.\nClement believed that the days mentioned in Genesis are allegorical. Clement assumed a double creation, one of an invisible world and the second being material creation. He believed that formless matter existed before the creation of the world, being influenced by Plato. Clement tried to interpret Genesis 6 in harmony with the Book of Enoch.\nOthers.\nThe first person in church history to introduce a view of an invisible and a visible church is Clement of Alexandria. Because Clement saw the Protoevangelium of James as canonical, it could imply he believed in the perpetual virginity of Mary, though some have argued that he does not seem to believe in the sinlessness of Mary.\nClement of Alexandria interprets \"Fire of Wisdom\" which prevades the soul as by a baptism.\nClement of Alexandria used the word \"symbol\" to define the Eucharist, and interpreted \"John 6\" to be an allegory about faith, however his views on real presence are disputed.\nClement of Alexandria was apparently an amillennialist.\nWorks.\nEditions.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nTranslations.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7344", "revid": "40764242", "url": "https://en.wikipedia.org/wiki?curid=7344", "title": "Cogito, ergo sum", "text": "Philosophical statement made by Ren\u00e9 Descartes\nThe Latin , usually translated into English as \"I think, therefore I am\", is the \"first principle\" of Ren\u00e9 Descartes's philosophy. He originally published it in French as , in his 1637 \"Discourse on the Method\", so as to reach a wider audience than Latin would have allowed. It later appeared in Latin in his \"Principles of Philosophy\", and a similar phrase also featured prominently in his \"Meditations on First Philosophy\". The dictum is also sometimes referred to as the cogito. As Descartes explained in a margin note, \"we cannot doubt of our existence while we doubt.\" In the posthumously published \"The Search for Truth by Natural Light\", he expressed this insight as (\"I doubt, therefore I am \u2014 or what is the same \u2014 I think, therefore I am\"). Antoine L\u00e9onard Thomas, in a 1765 essay in honor of Descartes presented it as (\"I doubt, therefore I think, therefore I am\").\nDescartes's statement became a fundamental element of Western philosophy, as it purported to provide a certain foundation for knowledge in the face of radical doubt. While other knowledge could be a figment of imagination, deception, or mistake, Descartes asserted that the very act of doubting one's own existence served\u2014at minimum\u2014as proof of the reality of one's own mind; there must be a thinking entity\u2014in this case the self\u2014for there to be a thought.\nOne critique of the dictum, first suggested by Pierre Gassendi, is that it presupposes that there is an \"I\" which must be doing the thinking. According to this line of criticism, the most that Descartes was entitled to say was that \"thinking is occurring\", not that \"I am thinking\".\nIn Descartes's writings.\nDescartes first wrote the phrase in French in his 1637 \"Discourse on the Method\". He referred to it in Latin without explicitly stating the familiar form of the phrase in his 1641 \"Meditations on First Philosophy\". The earliest written record of the phrase in Latin is in his 1644 \"Principles of Philosophy\", where, in a margin note (see below), he provides a clear explanation of his intent: \"[W]e cannot doubt of our existence while we doubt\". Fuller forms of the phrase are attributable to other authors.\n\"Discourse on the Method\".\nThe phrase first appeared (in French) in Descartes's 1637 \"Discourse on the Method\" in the first paragraph of its fourth part:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\"Meditations on First Philosophy\".\nIn 1641, Descartes published (in Latin) \"Meditations on first philosophy\" in which he referred to the proposition, though not explicitly as \"cogito, ergo sum\" in Meditation II:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\"Principles of Philosophy\".\nIn 1644, Descartes published (in Latin) his \"Principles of Philosophy\" where the phrase \"ego cogito, ergo sum\" appears in Part 1, article 7:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\nDescartes's margin note for the above paragraph is:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\n\"The Search for Truth by Natural Light\".\nDescartes, in a lesser-known posthumously published work dated as written ca. 1647 and titled (\"The Search for Truth by Natural Light\"), provides his only known phrasing of the cogito as and admits that his insight is also expressible as dubito, ergo sum:\n&lt;templatestyles src=\"Verse translation/styles.css\" /&gt;\nOther forms.\nThe proposition is sometimes given as . This form was penned by the French literary critic, Antoine L\u00e9onard Thomas, in an award-winning 1765 essay in praise of Descartes, where it appeared as \"\" ('Since I doubt, I think; since I think, I exist'). With rearrangement and compaction, the passage translates to \"I doubt, therefore I think, therefore I am,\" or in Latin, \"\"dubito, ergo cogito, ergo sum\".\" This aptly captures Descartes's intent as expressed in his posthumously published \"La Recherche de la V\u00e9rit\u00e9 par La Lumiere Naturale\" as noted above: I doubt, therefore I am \u2014 or what is the same \u2014 I think, therefore I am.\nA further expansion, (\"\u2026\u2014a thinking thing\") extends the \"cogito\" with Descartes's statement in the subsequent \"Meditation\", (\"I am a thinking [conscious] thing, that is, a being who doubts, affirms, denies, knows a few objects, and is ignorant of many,-- who loves, hates, wills, refuses, who imagines likewise, and perceives\"). This has been referred to as \"the expanded \"cogito\".\"\nTranslation.\n\"I am thinking\" vs. \"I think\".\nWhile the Latin translation \"c\u014dgit\u014d\" may be translated rather easily as \"I think/ponder/visualize\", does not indicate whether the verb form corresponds to the English simple present or progressive aspect. Technically speaking, the French lemma \"pense\" by itself is actually the result of numerous different conjugations of the verb \"penser\" (to think) \u2013 it could mean \"I think... (something)\"/\"He thinks... (something)\", \"I think.\"/\"He thinks.\", or even \"You (must) think... (something).\", thereby necessitating the use of the wider context, or a pronoun, to understand the meaning. In the case of \"je pense\", a pronoun is already included, \"je\" or \"I\", but this still leaves the question of whether \"I think...\" or \"I think.\" is intended. Therefore, translation needs a larger context to determine aspect.\nFollowing John Lyons (1982), Vladimir \u017degarac notes, \"The temptation to use the simple present is said to arise from the lack of progressive forms in Latin and French, and from a misinterpretation of the meaning of \"cogito\" as habitual or generic\" (cf. gnomic aspect). Also following Lyons, Ann Banfield writes, \"In order for the statement on which Descartes's argument depends to represent certain knowledge,\u2026 its tense must be a true present\u2014in English, a progressive,\u2026 not as 'I think' but as 'I am thinking, in conformity with the general translation of the Latin or French present tense in such nongeneric, nonstative contexts.\" Or in the words of Simon Blackburn, \"Descartes's premise is not 'I think' in the sense of 'I ski', which can be true even if you are not at the moment skiing. It is supposed to be parallel to 'I am skiing'.\"\nThe similar translation \"I am thinking, therefore I exist\" of Descartes's correspondence in French (\", \") appears in \"The Philosophical Writings of Descartes\" by Cottingham et al. (1988).\nThe earliest known translation as \"I am thinking, therefore I am\" is from 1872 by Charles Porterfield Krauth.\nFumitaka Suzuki writes \"Taking consideration of Cartesian theory of continuous creation, which theory was developed especially in the Meditations and in the Principles, we would assure that 'I am thinking, therefore I am/exist' is the most appropriate English translation of 'ego cogito, ergo sum'.\"\n\"I exist\" vs. \"I am\".\nAlexis Deodato S. Itao notes that is \"literally 'I think, therefore I am'.\" Others differ: 1) \"[A] precise English translation will read as 'I am thinking, therefore I exist'.; and 2) \"[S]ince Descartes ... emphasized that existence is such an important 'notion,' a better translation is 'I am thinking, therefore I exist.'\"\nPunctuation.\nDescartes wrote this phrase as such only once, in the posthumously published lesser-known work noted above,\"The Search for Truth by Natural Light\". It appeared there mid-sentence, uncapitalized, and with a comma. (Commas were not used in Classical Latin but were a regular feature of scholastic Latin, the Latin Descartes \"had learned in a Jesuit college at La Fl\u00e8che.\") Most modern reference works show it with a comma, but it is often presented without a comma in academic work and in popular usage. In Descartes's \"Principia Philosophiae\", the proposition appears as ego cogito, ergo sum.\nInterpretation.\nAs put succinctly by Krauth (1872), \"That cannot doubt which does not think, and that cannot think which does not exist. I doubt, I think, I exist.\"\nThe phrase \"cogito, ergo sum\" is not used in Descartes's \"Meditations on First Philosophy\" but the term \"the \"cogito\"\" is used to refer to an argument from it. In the \"Meditations\", Descartes phrases the conclusion of the argument as \"that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind\" (\"Meditation\" II). George Henry Lewes says Descartes \"has told us that [his objective] was to find a starting point from which to reason\u2014to find an irreversible certainty. And where did he find this? In his own consciousness. Doubt as I may, I cannot doubt of my own existence, because my very doubts reveal to me a something which doubts. You may call this an assumption, if you will; I point out the fact as one above and beyond all logic; which logic can neither prove nor disprove; but which must always remain an irreversible certainty, and as such a fitting basis of philosophy.\"\nAt the beginning of the second meditation, having reached what he considers to be the ultimate level of doubt\u2014his argument from the existence of a deceiving god\u2014Descartes examines his beliefs to see if any have survived the doubt. In his belief in his own existence, he finds that it is impossible to doubt that he exists. Even if there were a deceiving god (or an evil demon), one's belief in their own existence would be secure, for there is no way one could be deceived unless one existed in order to be deceived.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;But I have convinced myself that there is absolutely nothing in the world, no sky, no earth, no minds, no bodies. Does it now follow that I, too, do not exist? No. If I convinced myself of something [or thought anything at all], then I certainly existed. But there is a deceiver of supreme power and cunning who deliberately and constantly deceives me. In that case, I, too, undoubtedly exist, if he deceives me; and let him deceive me as much as he can, he will never bring it about that I am nothing, so long as I think that I am something. So, after considering everything very thoroughly, I must finally conclude that the proposition, \"I am, I exist,\" is necessarily true whenever it is put forward by me or conceived in my mind. (AT VII 25; CSM II 16\u201317)\nThere are three important notes to keep in mind here. First, he claims only the certainty of \"his own\" existence from the first-person point of view \u2014 he has not proved the existence of other minds at this point. This is something that has to be thought through by each of us for ourselves, as we follow the course of the meditations. Second, he does not say that his existence is necessary; he says that \"if he thinks\", then necessarily he exists (see the instantiation principle). Third, this proposition \"I am, I exist\" is held true not based on a deduction (as mentioned above) or on empirical induction but on the clarity and self-evidence of the proposition. Descartes does not use this first certainty, the \"cogito\", as a foundation upon which to build further knowledge; rather, it is the firm ground upon which he can stand as he works to discover further truths. As he puts it:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Archimedes used to demand just one firm and immovable point in order to shift the entire earth; so I too can hope for great things if I manage to find just one thing, however slight, that is certain and unshakable. (AT VII 24; CSM II 16)\nAccording to many Descartes specialists, including \u00c9tienne Gilson, the goal of Descartes in establishing this first truth is to demonstrate the capacity of his criterion \u2014 the immediate clarity and distinctiveness of self-evident propositions \u2014 to establish true and justified propositions despite having adopted a method of generalized doubt. As a consequence of this demonstration, Descartes considers science and mathematics to be justified to the extent that their proposals are established on a similarly immediate clarity, distinctiveness, and self-evidence that presents itself to the mind. The originality of Descartes's thinking, therefore, is not so much in expressing the \"cogito\"\u2014a feat accomplished by other predecessors, as we shall see\u2014but on using the \"cogito\" as demonstrating the most fundamental epistemological principle, that science and mathematics are justified by relying on clarity, distinctiveness, and self-evidence.\nBaruch Spinoza in \"Principia philosophiae cartesianae\" at its \"Prolegomenon\" identified \"cogito ergo sum\" the \"ego sum cogitans\" (I am a thinking being) as the thinking substance with his ontological interpretation.\nPredecessors.\nAlthough the idea expressed in \"cogito, ergo sum\" is widely attributed to Descartes, he was not the first to mention it. Plato spoke about the \"knowledge of knowledge\" (Greek: \u03bd\u03cc\u03b7\u03c3\u03b9\u03c2 \u03bd\u03bf\u03ae\u03c3\u03b5\u03c9\u03c2, \"n\u00f3esis no\u00e9seos\") and Aristotle explains the idea in full length:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;But if life itself is good and pleasant\u2026and if one who sees is conscious that he sees, one who hears that he hears, one who walks that he walks and similarly for all the other human activities there is a faculty that is conscious of their exercise, so that whenever we perceive, we are conscious that we perceive, and whenever we think, we are conscious that we think, and to be conscious that we are perceiving or thinking is to be conscious that we exist... (\"Nicomachean Ethics\", 1170a 25 ff.)\nThe Cartesian statement was interpreted to be an Aristotelian syllogism where the premise that all thinkers are also beings is not made explicit.\nIn the late sixth or early fifth century BC, Parmenides is quoted as saying \"For to be aware and to be are the same\". (Fragment B3)\nIn the early fifth century AD, Augustine of Hippo in \"De Civitate Dei\" (book XI, 26) affirmed his certain knowledge of his own existence, and added: \"So far as these truths are concerned, I do not at all fear the arguments of the Academics when they say, What if you are mistaken? For if I am mistaken, I exist.\" This formulation () is sometimes called the Augustinian . In 1640, Descartes wrote to thank Andreas Colvius (a friend of Descartes's mentor, Isaac Beeckman) for drawing his attention to Augustine:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I am obliged to you for drawing my attention to the passage of St Augustine relevant to my \"I am thinking, therefore I exist\". I went today to the library of this town to read it, and I do indeed find that he does use it to prove the certainty of our existence. He goes on to show that there is a certain likeness of the Trinity in us, in that we exist, we know that we exist, and we love the existence and the knowledge we have. I, on the other hand, use the argument to show that this \"I\" which is thinking is an immaterial substance with no bodily element. These are two very different things. In itself it is such a simple and natural thing to infer that one exists from the fact that one is doubting that it could have occurred to any writer. But I am very glad to find myself in agreement with St Augustine, if only to hush the little minds who have tried to find fault with the principle.\nAnother predecessor was Avicenna's \"Floating Man\" thought experiment on human self-awareness and self-consciousness.\nThe 8th century Hindu philosopher Adi Shankara wrote, in a similar fashion, that no one thinks 'I am not', arguing that one's existence cannot be doubted, as there must be someone there to doubt. The central idea of \"cogito, ergo sum\" is also the topic of \"Mandukya Upanishad\".\nSpanish philosopher G\u00f3mez Pereira in his 1554 work \"De Inmortalitate Animae\", wrote \"nosco me aliquid noscere, &amp; quidquid noscit, est, ergo ego sum\" ('I know that I know something, anyone who knows is, therefore I am').\nCritique.\nUse of \"I\".\nIn \"Descartes, The Project of Pure Enquiry\", Bernard Williams provides a history and full evaluation of this issue. The first to raise the \"I\" problem was Pierre Gassendi, who in his , as noted by Saul Fisher \"points out that recognition that one has a set of thoughts does not imply that one is a particular thinker or another. \u2026[T]he only claim that is indubitable here is the agent-independent claim that there is cognitive activity present.\"\nThe objection, as presented by Georg Lichtenberg, is that rather than supposing an entity that is thinking, Descartes should have said: \"thinking is occurring.\" That is, whatever the force of the \"cogito\", Descartes draws too much from it; the existence of a thinking thing, the reference of the \"I,\" is more than the \"cogito\" can justify. Friedrich Nietzsche criticized the phrase in that it presupposes that there is an \"I\", that there is such an activity as \"thinking\", and that \"I\" know what \"thinking\" is. He suggested a more appropriate phrase would be \"it thinks\" wherein the \"it\" could be an impersonal subject as in the sentence \"It is raining.\"\nKierkegaard.\nThe Danish philosopher S\u00f8ren Kierkegaard calls the phrase a tautology in his \"Concluding Unscientific Postscript\". He argues that the \"cogito\" already presupposes the existence of \"I\", and therefore concluding with existence is logically trivial. Kierkegaard's argument can be made clearer if one extracts the premise \"I think\" into the premises \"'x' thinks\" and \"I am that 'x'\", where \"x\" is used as a placeholder in order to disambiguate the \"I\" from the thinking thing.\nHere, the \"cogito\" has already assumed the \"I\"'s existence as that which thinks. For Kierkegaard, Descartes is merely \"developing the content of a concept\", namely that the \"I\", which already exists, thinks. As Kierkegaard argues, the proper logical flow of argument is that existence is already assumed or presupposed in order for thinking to occur, not that existence is concluded from that thinking.\nWilliams.\nBernard Williams claims that what we are dealing with when we talk of thought, or when we say \"I am thinking,\" is something conceivable from a third-person perspective\u2014namely objective \"thought-events\" in the former case, and an objective thinker in the latter. He argues, first, that it is impossible to make sense of \"there is thinking\" without relativizing it to \"something.\" However, this something cannot be Cartesian egos, because it is impossible to differentiate objectively between things just on the basis of the pure content of consciousness. The obvious problem is that, through introspection, or our experience of consciousness, we have no way of moving to conclude the existence of any third-personal fact, to conceive of which would require something above and beyond just the purely subjective contents of the mind.\nHeidegger.\nAs a critic of Cartesian subjectivity, Heidegger sought to ground human subjectivity in death as that certainty which individualizes and authenticates our being. As he wrote in 1925 in \"History of the Concept of Time\":&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;This certainty, that \"I myself am in that I will die,\" is the basic certainty of Dasein itself. It is a genuine statement of Dasein, while \"cogito sum\" is only the semblance of such a statement. If such pointed formulations mean anything at all, then the appropriate statement pertaining to Dasein in its being would have to be \"sum moribundus\" [I am in dying], \"moribundus\" not as someone gravely ill or wounded, but insofar as I am, I am \"moribundus\". The \"MORIBUNDUS\" first gives the \"SUM\" its sense.\nJohn Macmurray.\nThe Scottish philosopher John Macmurray rejects the \"cogito\" outright in order to place action at the center of a philosophical system he entitles the Form of the Personal. \"We must reject this, both as standpoint and as method. If this be philosophy, then philosophy is a bubble floating in an atmosphere of unreality.\" The reliance on thought creates an irreconcilable dualism between thought and action in which the unity of experience is lost, thus dissolving the integrity of our selves, and destroying any connection with reality. In order to formulate a more adequate \"cogito\", Macmurray proposes the substitution of \"I do\" for \"I think,\" ultimately leading to a belief in God as an agent to whom all persons stand in relation.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7345", "revid": "30052052", "url": "https://en.wikipedia.org/wiki?curid=7345", "title": "Carl Barks", "text": "American cartoonist (1901\u20132000)\nCarl Barks (March 27, 1901 \u2013 August 25, 2000) was an American cartoonist, author, and painter. He is best known for his work in Disney comic books, as the writer and artist of the first Donald Duck stories and as the creator of Scrooge McDuck. He worked anonymously until late in his career; fans dubbed him The Duck Man and The Good Duck Artist. In 1987, Barks was one of the three inaugural inductees of the Will Eisner Comic Book Hall of Fame.\nBarks worked for the Disney Studio and Western Publishing where he created Duckburg and many of its inhabitants, such as Scrooge McDuck (1947), Gladstone Gander (1948), the Beagle Boys (1951), The Junior Woodchucks (1951), Gyro Gearloose (1952), Cornelius Coot (1952), Flintheart Glomgold (1956), John D. Rockerduck (1961) and Magica De Spell (1961).\nHe has been named by animation historian Leonard Maltin as \"the most popular and widely read artist-writer in the world\". Will Eisner called him \"the Hans Christian Andersen of comic books.\" Beginning especially in the 1980s, Barks' artistic contributions would be a primary source for animated adaptations such as \"DuckTales\" and its 2017 remake.\nBiography.\nBarks was born in Merrill, Oregon, to William Barks and his wife, Arminta Johnson. He had an older brother named Clyde. His paternal grandparents were David Barks and his wife Ruth Shrum. Barks' maternal grandparents were Carl Johnson and his wife, Suzanna Massey, but little else is known about his ancestors. Barks was the descendant of Jacob Barks, who came to Missouri from North Carolina c. 1800. They lived in Marble Hill in Bollinger County. Jacob Barks' son Isaac was the father of the David Barks noted above.\nChildhood.\nAccording to Barks's description of his childhood, he was a rather lonely child. His parents owned of land that served as their farm. The nearest neighbor lived away, but he was more an acquaintance to Barks's parents than a friend. The closest school was about away and Barks had to walk that distance every day. The rural area had few children, though, and Barks later remembered that his school had only about eight or ten students including him. He had high praise for the quality of the education he received in that small school. \"Schools were good in those days\", he used to say.\nThe lessons lasted from nine o'clock in the morning to four o'clock in the afternoon and then he had to return to the farm. There he remembered not having anybody to talk to, as his parents were busy and he had little in common with his brother.\nIn 1908, William Barks (in an attempt to increase the family income) moved with his family to Midland, Oregon, some miles north of Merrill, to be closer to the new railway lines. He established a new stock-breeding farm and sold his produce to the local slaughterhouses.\nNine-year-old Clyde and seven-year-old Carl worked long hours there. But Carl later remembered that the crowd which gathered at Midland's market place made a strong impression on him. This was expected, as he was not used to crowds up until then. According to Barks, his attention was mostly drawn to the cowboys that frequented the market with their revolvers, strange nicknames for each other and sense of humor.\nBy 1911, they had been successful enough to move to Santa Rosa, California. There they started cultivating vegetables and set up some orchards. Unfortunately, the profits were not as high as William expected and they started having financial difficulties. William's anxiety over them was probably what caused his first nervous breakdown.\nAs soon as William recovered, he made the decision to move back to Merrill. The year was 1913, and Barks was already 12-years-old; but, due to the constant moving, he had not yet managed to complete grade school. He resumed his education at this point and finally managed to graduate in 1916.\n1916 served as a turning point in Barks's life for various reasons. First, Arminta, his mother, died in this year. Second, his hearing problems, which had already appeared earlier, had at the time become severe enough for him to have difficulties listening to his teachers talking. His hearing would continue to get worse later, but at that point he had not yet acquired a hearing aid. Later in life, he couldn't do without one. Third, the closest high school to their farm was away and even if he did enroll in it, his bad hearing was likely to contribute to his learning problems. He had to decide to stop his school education, much to his disappointment.\nFrom job to job.\nBarks started taking various jobs but had little success in such occupations as a farmer, woodcutter, turner, mule driver, cowboy and printer. From his jobs he learned, he later averred, how eccentric, stubborn and unpredictable men, animals and machines can be. At the same time he interacted with colleagues, fellow breadwinners who had satirical disposition towards even their worst troubles. Barks later declared that he was sure that if not for a little humor in their troubled lives, they would certainly go insane. It was an attitude towards life that Barks would adopt. Later he would say it was natural for him to satirize the secret yearnings and desires, the pompous style and the disappointments of his characters. According to Barks, this period of his life would later influence his best known fictional characters: Walt Disney's Donald Duck and his own Scrooge McDuck.\nDonald's drifting from job to job was reportedly inspired by Barks's own experiences. So was his usual lack of success. And even in those that he was successful this would be temporary, just until a mistake or chance event caused another failure, another disappointment for the frustrated duck. Barks also reported that this was another thing he was familiar with.\nScrooge's main difference to Donald, according to Barks, was that he too had faced the same difficulties in his past but through intelligence, determination and hard work, he was able to overcome them. Or, as Scrooge himself would say to Huey, Dewey, and Louie: by being \"tougher than the toughies and smarter than the smarties.\" In Barks's stories Scrooge would work to solve his many problems, even though the stories would often point out that his constant efforts seemed futile at the end.\nThrough both characters Barks would often exhibit his rather sarcastic sense of humor. It seems that this difficult period for the artist helped shape many of his later views in life that were expressed through his characters.\nProfessional artist.\nAt the same time Barks had started thinking about turning a hobby that he always enjoyed into a profession: that of drawing. Since his early childhood he spent his free time by drawing on any material he could find. He had attempted to improve his style by copying the drawings of his favorite comic strip artists from the newspapers where he could find them. As he later said, he wanted to create his own facial expressions, figures and comical situations in his drawings but wanted to study the master comic artists' use of the pen and their use of color and shading.\nAmong his early favorites were Winsor McCay (mostly known for \"Little Nemo\") and Frederick Burr Opper (mostly known for \"Happy Hooligan\") but he would later study any style that managed to draw his attention.\nAt age 16, he was mostly self-taught but at this point he decided to take some lessons through correspondence. He only followed the first four lessons and then had to stop because his working left him with little free time. But as he later said, the lessons proved very useful in improving his style.\nBy December 1918, he left his father's home to attempt to find a job in San Francisco, California. He worked for a while in a small publishing house while attempting to sell his drawings to newspapers and other printed material with little success.\nFirst and second marriages.\nWhile he continued drifting through various jobs, he met Pearl Turner (1904\u20131987). In 1921 they married and had two daughters:\nIn 1923 he returned to his paternal farm in Merrill in an attempt to return to the life of a farmer, but that ended soon. He continued searching for a job while attempting to sell his drawings. He soon managed to sell some of them to \"Judge\" magazine and then started having success submitting to the Minneapolis-based \"Calgary Eye-Opener\", a racy men's cartoon magazine of the era. He was eventually hired as editor and scripted and drew most of the contents while continuing to sell occasional work to other magazines. His salary of $90 per month was considered respectable enough for the time. A facsimile of one of the racy magazines he did cartoons for in this period, \"Coo Coo\" #1, was published by Hamilton Comics in 1997.\nMeanwhile, he had his first divorce. He and Pearl were separated in 1929 and divorced in 1930. After he moved to Minneapolis, Minnesota, where \"Calgary-Eye-Opener\" had its offices he met Clara Balken, who in 1938 became his second wife.\nDisney.\nIn November 1935, when he learned that Walt Disney was seeking more artists for his studio, Barks decided to apply. He was approved for a try-out which entailed a move to Los Angeles, California. He was one of two in his class of trainees who was hired. His starting salary was 20 dollars a week. He started at Disney Studios in 1935, more than a year after the debut of Donald Duck on June 9, 1934, in the short animated film \"The Wise Little Hen\".\nBarks initially worked as an inbetweener. This involved being teamed and supervised by one of the head animators who did the key poses of character action (often known as extremes) for which the inbetweeners did the drawings between the extremes to create the illusion of movement. While an inbetweener, Barks submitted gag ideas for cartoon story lines being developed and showed such a knack for creating comical situations that by 1937 he was transferred to the story department. His first story sale was the climax of \"Modern Inventions\", for a sequence where a robot barber chair gives Donald Duck a haircut on his bottom.\nIn 1937, when Donald Duck became the star of his own series of cartoons instead of co-starring with Mickey Mouse and Goofy as previously, a new unit of storymen and animators was created devoted solely to this series. Though he originally just contributed gag ideas to some duck cartoons by 1937 Barks was (principally with partner Jack Hannah) originating story ideas that were storyboarded and (if approved by Walt) put into production. He collaborated on such cartoons as \"Donald's Nephews\" (1938), \"Donald's Cousin Gus\" (1939), \"Mr. Duck Steps Out\" (1940), \"Timber\" (1941), \"The Vanishing Private\" (1942) and \"The Plastics Inventor\" (1944).\nThe Good Duck Artist.\nUnhappy at the emerging wartime working conditions at Disney, and bothered by ongoing sinus problems caused by the studio's air conditioning, Barks quit in 1942. Shortly before quitting, he moonlighted as a comic book artist, contributing half the artwork for a one-shot comic book (the other half of the art being done by story partner Jack Hannah) titled \"Donald Duck Finds Pirate Gold\". This 64-page story was adapted by Donald Duck comic strip writer Bob Karp from an unproduced feature, and published in October 1942 in Dell Comics \"Four Color Comics\" #9. It was the first Donald Duck story originally produced for an American comic book and also the first involving Donald and his nephews in a treasure hunting expedition, in this case for the treasure of Henry Morgan. Barks would later use the treasure hunting theme in many of his stories. This actually was not his first work in comics, as earlier the same year Barks along with Hannah and fellow storyman Nick George scripted \"Pluto Saves the Ship\", which was among the first original Disney comic book stories published in the United States.\nAfter quitting the Disney Studio, Barks relocated to the Hemet/San Jacinto area in the semi-desert Inland Empire region east of Los Angeles where he hoped to start a chicken farm.\nWhen asked which of his stories was a favorite in several interviews Barks cited the ten-pager in \"Walt Disney's Comics and Stories\" #146 (Nov. 1952) in which Donald tells the story of the chain of unfortunate events that took place when he owned a chicken farm in a town which subsequently was renamed Omelet. Likely one reason it was a favorite is that it was inspired by Barks' own experiences in the poultry business.\nBut to earn a living in the meantime he inquired whether Western Publishing, which had published \"Pirate Gold\", had any need for artists for Donald Duck comic book stories. He was immediately assigned to illustrate the script for a ten-page Donald Duck story for the monthly \"Walt Disney's Comics and Stories\". At the publisher's invitation he revised the storyline and the improvements impressed the editor sufficiently to invite Barks to try his hand at contributing both the script and the artwork of his follow-up story. This set the pattern for Barks' career in that (with rare exceptions) he provided art (pencil, inking, solid blacks and lettering) and scripting for his stories.\n\"The Victory Garden\", that initial ten-page story published in April, 1943 was the first of about 500 stories featuring the Disney ducks Barks would produce for Western Publishing over the next three decades, well into his purported retirement. These can be mostly divided into three categories:\nBarks' artistic growth during his first decade in comics saw a transformation from rather rudimentary storytelling derived from his years as an animation artist and storyman into a virtuoso creator of complex narratives, notably in his longer adventure tales. According to critic Geoffrey Blum, the process that saw its beginnings in 1942's Pirate Gold first bore its full fruit in 1950's \"Vacation Time\", which he describes as \"a visual primer for reading comics and understanding\u00a0... the form\".\nHe surrounded Donald Duck and nephews Huey, Dewey, and Louie with a cast of eccentric and colorful characters, such as the aforementioned Scrooge McDuck, the wealthiest duck in the world; Gladstone Gander, Donald's obscenely lucky cousin; inventor Gyro Gearloose; the persistent Beagle Boys; the sorceress Magica De Spell; Scrooge's rivals Flintheart Glomgold and John D. Rockerduck; Daisy's nieces April, May and June; Donald's neighbor Jones, and The Junior Woodchucks organization.\nBarks's stories (whether humorous adventures or domestic comedies) often exhibited a wry, dark irony born of hard experience. The ten-pagers showcased Donald as everyman, struggling against the cruel bumps and bruises of everyday life with the nephews often acting as a Greek chorus commenting on the unfolding disasters Donald wrought upon himself. Yet while seemingly defeatist in tone, the humanity of the characters shines through in their persistence despite the obstacles. These stories found popularity not only among young children but adults as well. Despite the fact that Barks had done little traveling his adventure stories often had the duck clan globe-trotting to the most remote or spectacular of places. This allowed Barks to indulge his penchant for elaborate backgrounds that hinted at his thwarted ambitions of doing realistic stories in the vein of Hal Foster's \"Prince Valiant\".\nThird marriage.\nAs Barks blossomed creatively, his marriage to Clara deteriorated. This is the period referred to in Barks' famed quip that he could feel his creative juices flowing while the whiskey bottles hurled at him by a tipsy Clara flew by his head. They were divorced in 1951, his second and last divorce. In this period Barks dabbled in fine art, exhibiting paintings at local art shows. It was at one of these in 1952 he became acquainted with fellow exhibitor Margaret Wynnfred Williams (1917 \u2013 March 10, 1993), nicknamed Gar\u00e9. She was an accomplished landscape artist, some of whose paintings are in the collection of the Leanin' Tree Museum of Western Art. During her lifetime, and to this day, note cards of her paintings are available from Leanin' Tree. Her nickname appears as a store name in the story \"Christmas in Duckburg\", featured on page 1 of Walt Disney's Christmas Parade #9, published in 1958. Soon after they met, she started assisting Barks, handling the solid blacks and lettering, both of which he had found onerous. They married in 1954 and the union lasted until her death.\nNo longer anonymous.\nPeople who worked for Disney (and its comic book licensees) generally did so in relative anonymity; stories would only carry Walt Disney's name and (sometimes) a short identification number. Prior to 1960 Barks' identity remained a mystery to his readers. However, many readers recognized Barks' work and drawing style and began to call him the Good Duck Artist, a label that stuck even after his true identity was discovered by fans in the late 1950s. Malcolm Willits was the first person to learn Barks's name and address, but two brothers named John and Bill Spicer became the first fans to contact Barks after independently discovering the same information. After Barks received a 1960 visit from the Spicer brothers and Ron Leonard, he was no longer anonymous, as word of his identity spread through the emerging network of comic book fandom fanzines and conventions.\nLater life.\nCarl Barks retired in 1966, but was persuaded by editor Chase Craig to continue to script stories for Western. The last new comic book story drawn by Carl Barks was a Daisy Duck tale (\"The Dainty Daredevil\") published in \"Walt Disney Comics Digest\" issue 5 (Nov. 1968). When bibliographer Michael Barrier asked Barks why he drew it, Barks' vague recollection was no one was available and he was asked to do it as a favor by Craig.\nHe wrote one Uncle Scrooge story, and three Donald Duck stories. From 1970 to 1974, Barks was the main writer for the Junior Woodchucks comic book (issues 6 through 25). The latter included environmental themes that Barks first explored in 1957 [\"Land of the Pygmy Indians\", \"Uncle Scrooge\" #18]. Barks also sold a few sketches to Western that were redrawn as covers. For a time the Barkses lived in Goleta, California, before returning to the Inland Empire by moving to Temecula.\nTo make a little extra money beyond what his pension and scripting earnings brought in, Barks started doing oil paintings to sell at the local art shows where he and Gar\u00e9 exhibited. Subjects included humorous depictions of life on the farm and portraits of Native American princesses. These skillfully rendered paintings encouraged fan Glenn Bray to ask Barks if he could commission a painting of the ducks (\"A Tall Ship and a Star to Steer Her By\", taken from the cover of \"Walt Disney's Comics and Stories\" #108 by Barks). This prompted Barks to contact George Sherman at Disney's Publications Department to request permission to produce and sell oil paintings of scenes from his stories. In July 1971 Barks was granted a royalty-free license by Disney. When word spread that Barks was taking commissions from those interested in purchasing an oil of the ducks, much to his astonishment the response quickly outstripped what he reasonably could produce in the next few years.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nOde to the Disney Ducks\n&lt;poem&gt;They ride tall ships to the far away,\nand see the long ago.\nThey walk where fabled people trod,\nand Yetis trod the snow.\nThey meet the folks who live on stars,\nand find them much like us,\nWith food and love and happiness\nthe things they most discuss.\nThe world is full of clans and cults\nabuzz as angry bees,\nAnd Junior Woodchucks snapping jeers\nat Littlest Chickadees.\nThe ducks show us that part of life\nis to forgive a slight.\nThat black eyes given in revenge\nkeep hatred burning bright.\nSo when our walks in sun or shade\npass graveyards filled by wars,\nIt's nice to stop and read of ducks\nwhose battles leave no scars.\nTo read of ducks who parody\nour vain attempts at glory,\nThey don't exist, but somehow leave\nus glad we bought their story.&lt;/poem&gt;\n\u2014Carl Barks, 1999\nWhen Barks expressed dismay at coping with the backlog of orders he faced, fan/dealers Bruce Hamilton and Russ Cochran suggested Barks instead auction his paintings at conventions and via Cochran's catalog \"Graphic Gallery\". By September 1974 Barks had discontinued taking commissions.\nAt Boston's NewCon convention, in October 1975, the first Carl Barks oil painting auctioned at a comic book convention (\"She Was Spangled and Flashy\") sold for $2,500. Subsequent offerings saw an escalation in the prices realized.\nIn 1976, Barks and Gar\u00e9 went to Boston for the NewCon show, their first comic convention appearance. Among the other attendees was famed \"Little Lulu\" comic book scripter John Stanley; despite both having worked for Western Publishing this was the first time they met. The highlight of the convention was the auctioning of what was to that time the largest duck oil painting Barks had done, \"July Fourth in Duckburg\", which included depictions of several prominent Barks fans and collectors. It sold for a then record high amount: $6,400.\nSoon thereafter a fan sold unauthorized prints of some of the Scrooge McDuck paintings, leading Disney to withdraw permission for further paintings. To meet demand for new work Barks embarked on a series of paintings of non-Disney ducks and fantasy subjects such as Beowulf and Xerxes. These were eventually collected in the limited-edition book \"Animal Quackers\".\nAs the result of heroic efforts by \"Star Wars\" producer Gary Kurtz and screenwriter Edward Summer, Disney relented and, in 1981, allowed Barks to do a now seminal oil painting called \"Wanderers of Wonderlands\" for a breakthrough limited edition book entitled \"Uncle Scrooge McDuck: His Life and Times\". The book collected 11 classic Barks stories of Uncle Scrooge colored by artist Peter Ledger along with a new Scrooge story by Barks done storybook style with watercolor illustrations, \"Go Slowly, Sands of Time\". After being turned down by every major publisher in New York City, Kurtz and Summer published the book through Celestial Arts, which Kurtz acquired partly for this purpose. The book went on to become the model for virtually every important collection of comic book stories. It was the first book of its kind ever reviewed in \"Time\" magazine and subsequently in \"Newsweek\", and the first book review in \"Time\" with large color illustrations.\nIn 1977 and 1982, Barks attended the San Diego Comic-Con. As with his appearance in Boston, the response to his presence was overwhelming, with long lines of fans waiting to meet Barks and get his autograph.\nIn 1981, Bruce Hamilton and Russ Cochran, two long-time Disney comics fans, decided to combine forces to bring greater recognition to the works of Carl Barks. Their first efforts went into establishing Another Rainbow Publishing, the banner under which they produced and issued the award-winning book \"The Fine Art of Walt Disney's Donald Duck by Carl Barks\", a comprehensive collection of the Disney duck paintings of this artist and storyteller. Not long after, the company began producing fine art lithographs of many of these paintings, in strictly limited editions, all signed by Barks, who eventually produced many original works for the series.\nIn 1983, Barks relocated one last time to Grants Pass, Oregon, near where he grew up, partly at the urging of friend and \"Broom Hilda\" artist Russell Myers, who lived in the area. The move also was motivated, Barks stated in another famous quip, by Temecula being too close to Disneyland and thus facilitating a growing torrent of drop-in visits by vacationing fans. In this period Barks made only one public appearance, at a comic book shop near Grants Pass.\nIn 1983, Another Rainbow took up the daunting task of collecting the entire Disney comic book oeuvre of Barks\u2014over 500 stories in all\u2014in the ten-set, thirty-volume \"Carl Barks Library\". These oversized hardbound volumes reproduced Barks' pages in pristine black and white line art, as close as possible to the way he would originally draw them, and included mountains of special features, articles, reminiscences, interviews, storyboards, critiques, and more than a few surprises. This monumental project was finally completed in mid-1990.\nIn 1985, a new division was founded, Gladstone Publishing, which took up the then-dormant Disney comic book license. Gladstone introduced a new generation of Disney comic book readers to the storytelling of Barks, Paul Murry, and Floyd Gottfredson, as well as presenting the first works of modern Disney comics artists Don Rosa and William Van Horn. Seven years after Gladstone's founding, the \"Carl Barks Library\" was revived as the \"Carl Barks Library in Color\", as full-color, high-quality squarebound comic albums (including the first-ever Carl Barks trading cards).\nFrom 1993 to 1998, Barks' career was managed by the \"Carl Barks Studio\" (Bill Grandey and Kathy Morby\u2014they had sold Barks original art since 1979). This involved numerous art projects and activities, including a tour of 11 European countries in 1994, Iceland being the first foreign country he ever visited. Barks appeared at the first of many Disneyana conventions in 1993. Silk screen prints of paintings along with high-end art objects (such as original water colors, bronze figurines and ceramic tiles) were produced based on designs by Barks.\nDuring the summer of 1994 and until his death, Barks and his studio personally assigned Peter Reichelt, a museum exhibition producer from Mannheim, Germany, as his agent for Europe. Publisher \"Edition 313\" put out numerous lithographs. In 1997, tensions between Barks and the Studio eventually resulted in a lawsuit that was settled with an agreement that included the disbanding of the Studio. Barks never traveled to make another Disney appearance. He was represented by Ed Bergen, as he completed a final project. Gerry Tank and Jim Mitchell were to assist Barks in his final years.\nDuring his Carl Barks Studio years, Barks created two more stories: the script for the final Uncle Scrooge story \"Horsing Around with History\", which was first published in Denmark in 1994 with Bill Van Horn art. The outlines for Barks' final Donald Duck story \"Somewhere in Nowhere\", were first published in 1997, in Italy, with art by Pat Block.\nAustrian artist Gottfried Helnwein curated and organized the first solo museum-exhibition of Barks. Between 1994 and 1998 the retrospective was shown in ten European museums and seen by more than 400,000 visitors.\nAt the same time in spring 1994, Reichelt and Ina Brockmann designed a special museum exhibition tour about Barks' life and work. Also represented for the first time at this exhibition were Disney artists Al Taliaferro and Floyd Gottfredson. Since 1995, more than 500,000 visitors have attended the shows in Europe.\nReichelt also translated Michael Barrier's biography of Barks into German and published it in 1994.\nFinal days and death.\nBarks spent his final years in a new home in Grants Pass, Oregon, which he and Gar\u00e9, who died in 1993, had built next door to their original home. In July 1999, he was diagnosed with chronic lymphocytic leukemia, a form of cancer arising from the white blood cells in the bone marrow, for which he received oral chemotherapy. However, as the disease progressed, causing him great discomfort, the ailing Barks decided to stop receiving treatment in June 2000. In spite of his terminal condition, Barks remained, according to caregiver Serene Hunicke, \"funny up to the end\".\nThe year before, Barks had told the university professor Donald Ault:\nI have no apprehension, no fear of death. I do not believe in an afterlife.\u00a0... I think of death as total peace. You're beyond the clutches of all those who would crush you.\nOn August 25, 2000, shortly after midnight, Carl Barks died quietly in his sleep at the age of 99. He was interred in Hillcrest Memorial Cemetery in Grants Pass, beside Gar\u00e9's grave.\nInfluence.\n\"(A)n asteroid was named after the Duck Man in 1983 --- 2730 Barks, a carbonaceous C-type asteroid with a diameter of between 10 and 16 kilometers, an ordital period of six years and four months, and a rotation period of just over six hours.\" In a 1983 interview, Barks says that \"Island\nin the Sky,\" a story about the Ducks traveling to the asteroid belt to find a place Uncle Scrooge can store his money, was his favorite story.\nBarks' Donald Duck stories were rated #7 on \"The Comics Journal\" list of 100 top comics; his Uncle Scrooge stories were rated #20.\nSteven Spielberg and George Lucas have acknowledged that the rolling-boulder booby trap in the opening scene of \"Raiders of the Lost Ark\" was inspired by the 1954 Carl Barks Uncle Scrooge adventure \"The Seven Cities of Cibola\" (\"Uncle Scrooge\" #7). Lucas and Spielberg have also said that some of Barks' stories about space travel and the depiction of aliens had an influence on them. Lucas wrote the foreword to the 1982 \"Uncle Scrooge McDuck: His Life and Times\". In it he calls Barks' stories \"cinematic\" and \"a priceless part of our literary heritage\".\nThe Walt Disney Treasures DVD set \"\" includes a salute to Barks.\nCarl Barks has an asteroid named after him, 2730 Barks.\nIn Almere, Netherlands, a street was named after him: Carl Barksweg. The same neighborhood also includes a Donald Ducklaan and a Goofystraat.\nJapanese animator and cartoonist Osamu Tezuka, who created manga such as \"Astro Boy\" and \"Black Jack\", was a fan of Barks' work. \"New Treasure Island\", one of Tezuka's first works, was partly influenced by \"Donald Duck Finds Pirate Gold\".\nA 1949 Donald Duck ten-pager features Donald raising a yacht from the ocean floor by filling it with ping pong balls. In December 1965 Karl Kr\u00f8yer, a Dane, lifted the sunken freight vessel \"Al Kuwait\" in the Kuwait Harbor by filling the hull with 27 million tiny inflatable balls of polystyrene. Kr\u00f8yer denies having been inspired by this Barks story. Some sources claim Kr\u00f8yer was denied a Dutch patent registration (application number NL 6514306) for his invention on the grounds that the Barks story was a prior publication of the invention. Kr\u00f8yer later successfully raised another ship off Greenland using the same method, and several other sunken vessels worldwide have since been raised by modified versions of this concept. The television show \"MythBusters\" also tested this method and was able to raise a small boat.\nDon Rosa, one of the most popular living Disney artists, and possibly the one who has been most keen on connecting the various stories into a coherent universe and chronology, considers (with few exceptions) all Barks' duck stories as canon, and all others as apocryphal. Rosa has said that a number of novelists and movie-makers cite Carl Barks as their 'major influence and inspiration'.\nWhen the news of Barks' passing was hardly covered by the press in America, \"in Europe the sad news was flashed instantly across the airwaves and every newspaper\u00a0\u2014 they realized the world had lost one of the most beloved, influential and well-known creators in international culture.\"\nIn 2010 Oregon Cartoon Institute produced a video about the influence of Carl Barks and Basil Wolverton on Robert Crumb.\nThe video game \"\" is dedicated to the memory of Carl Barks.\nCarl Barks drew an early Andy Panda comic book story published in \"New Funnies\" #76, 1943. It is one of his few stories to feature humans interacting with talking animal characters (another is \"Dangerous Disguise\", \"Four Color\" #308, 1951). See List of Fictional Pandas.\nThe life story of Carl Barks, largely drawing upon his relationship with Disney and the phonetic similarity of his name to Karl Marx, serves as a loose inspiration to one of the subplots in \"The Last Song of Manuel Sendero\" by Ariel Dorfman.\nThe first image ever to be displayed on an Apple Macintosh was a scan of Carl Barks' Scrooge McDuck.\nFilmography.\nFilms where Barks served as storyman or story director include:\nArt materials.\nBarks was an enthusiastic user of Esterbrook pens, and used a N\u00ba 356 model to ink and letter his Donald Duck comic-book pages.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I used a #356 Esterbrook art and drafting pen which could do everything from thin \"fadeaways\" to broad accented curve sweeps on foreground circles such as the ducks' forms. The trick of breaking in a new pen, I discovered, is to soak it for several minutes in the ink bottle. Then wipe off the ink and the pen's varnish. For some weird reason most new pens then start out flexible and free-flowing\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7346", "revid": "12331483", "url": "https://en.wikipedia.org/wiki?curid=7346", "title": "Centimetre\u2013gram\u2013second system of units", "text": "Physical system of measurement that uses the centimetre, gram, and second as base units\nThe centimetre\u2013gram\u2013second system of units (abbreviated CGS or cgs) is a variant of the metric system based on the centimetre as the unit of length, the gram as the unit of mass, and the second as the unit of time. All CGS mechanical units are unambiguously derived from these three base units, but there are several different ways in which the CGS system was extended to cover electromagnetism.\nThe CGS system has been largely supplanted by the MKS system based on the metre, kilogram, and second, which was in turn extended and replaced by the International System of Units (SI). In many fields of science and engineering, SI is the only system of units in use, but there remain certain subfields where CGS is prevalent.\nIn measurements of purely mechanical systems (involving units of length, mass, force, energy, pressure, and so on), the differences between CGS and SI are straightforward and rather trivial; the unit-conversion factors are all powers of 10 as 100 cm = 1 m and 1000 g = 1 kg. For example, the CGS unit of force is the dyne, which is defined as , so the SI unit of force, the newton (), is equal to .\nOn the other hand, in measurements of electromagnetic phenomena (involving units of charge, electric and magnetic fields, voltage, and so on), converting between CGS and SI is more subtle. Formulas for physical laws of electromagnetism (such as Maxwell's equations) take a form that depends on which system of units is being used, because the electromagnetic quantities are defined differently in SI and in CGS. Furthermore, within CGS, there are several plausible ways to define electromagnetic quantities, leading to different \"sub-systems\", including Gaussian units, \"ESU\", \"EMU\", and Heaviside\u2013Lorentz units. Among these choices, Gaussian units are the most common today, and \"CGS units\" is often intended to refer to CGS-Gaussian units.\nHistory.\nThe CGS system goes back to a proposal in 1832 by the German mathematician Carl Friedrich Gauss to base a system of absolute units on the three fundamental units of length, mass and time. Gauss chose the units of millimetre, milligram and second. In 1873, a committee of the British Association for the Advancement of Science, including physicists James Clerk Maxwell and William Thomson recommended the general adoption of centimetre, gram and second as fundamental units, and to express all derived electromagnetic units in these fundamental units, using the prefix \"C.G.S. unit of ...\".\nThe sizes of many CGS units turned out to be inconvenient for practical purposes. For example, many everyday objects are hundreds or thousands of centimetres long, such as humans, rooms and buildings. Thus the CGS system never gained wide use outside the field of science. Starting in the 1880s, and more significantly by the mid-20th century, CGS was gradually superseded internationally for scientific purposes by the MKS (metre\u2013kilogram\u2013second) system, which in turn developed into the modern SI standard.\nSince the international adoption of the MKS standard in the 1940s and the SI standard in the 1960s, the technical use of CGS units has gradually declined worldwide. SI units are predominantly used in engineering applications and physics education, while Gaussian CGS units are commonly used in theoretical physics, describing microscopic systems, relativistic electrodynamics, and astrophysics. CGS units are today no longer accepted by the house styles of most scientific journals, textbook publishers, or standards bodies, although they are commonly used in astronomical journals such as \"The Astrophysical Journal\". The continued usage of CGS units is prevalent in magnetism and related fields because the B and H fields have the same units in free space and there is potential for confusion when converting published measurements from CGS to MKS.\nThe units gram and centimetre remain useful as noncoherent units within the SI system, as with any other prefixed SI units.\nDefinition of CGS units in mechanics.\nIn mechanics, the quantities in the CGS and SI systems are defined identically. The two systems differ only in the scale of the three base units (centimetre versus metre and gram versus kilogram, respectively), with the third unit (second) being the same in both systems.\nThere is a direct correspondence between the base units of mechanics in CGS and SI. Since the formulae expressing the laws of mechanics are the same in both systems and since both systems are coherent, the definitions of all coherent derived units in terms of the base units are the same in both systems, and there is an unambiguous correspondence of derived units:\nThus, for example, the CGS unit of pressure, barye, is related to the CGS base units of length, mass, and time in the same way as the SI unit of pressure, pascal, is related to the SI base units of length, mass, and time:\n 1\u00a0unit of pressure = 1\u00a0unit of force/(1\u00a0unit of length)2 = 1\u00a0unit of mass/(1\u00a0unit of length\u22c5(1\u00a0unit of time)2)\n 1\u00a0Ba = 1\u00a0g/(cm\u22c5s2)\n 1\u00a0Pa = 1\u00a0kg/(m\u22c5s2).\nExpressing a CGS derived unit in terms of the SI base units, or vice versa, requires combining the scale factors that relate the two systems:\n 1\u00a0Ba = 1\u00a0g/(cm\u22c5s2) = 10\u22123\u00a0kg / (10\u22122\u00a0m\u22c5s2) = 10\u22121\u00a0kg/(m\u22c5s2) = 10\u22121\u00a0Pa.\nDerivation of CGS units in electromagnetism.\nCGS approach to electromagnetic units.\nThe conversion factors relating electromagnetic units in the CGS and SI systems are made more complex by the differences in the formulae expressing physical laws of electromagnetism as assumed by each system of units, specifically in the nature of the constants that appear in these formulae. This illustrates the fundamental difference in the ways the two systems are built: \nAlternative derivations of CGS units in electromagnetism.\nElectromagnetic relationships to length, time and mass may be derived by several equally appealing methods. Two of them rely on the forces observed on charges. Two fundamental laws relate (seemingly independently of each other) the electric charge or its rate of change (electric current) to a mechanical quantity such as force. They can be written in system-independent form as follows:\nMaxwell's theory of electromagnetism relates these two laws to each other. It states that the ratio of proportionality constants formula_10 and formula_14 must obey formula_17, where \"c\" is the speed of light in vacuum. Therefore, if one derives the unit of charge from the Coulomb's law by setting formula_18 then Amp\u00e8re's force law will contain a factor formula_19. Alternatively, deriving the unit of current, and therefore the unit of charge, from the Amp\u00e8re's force law by setting formula_20 or formula_21, will lead to a constant factor in the Coulomb's law.\nIndeed, both of these mutually exclusive approaches have been practiced by the users of CGS system, leading to the two independent and mutually exclusive branches of CGS, described in the subsections below. However, the freedom of choice in deriving electromagnetic units from the units of length, mass, and time is not limited to the definition of charge. While the electric field can be related to the work performed by it on a moving electric charge, the magnetic force is always perpendicular to the velocity of the moving charge, and thus the work performed by the magnetic field on any charge is always zero. This leads to a choice between two laws of magnetism, each relating magnetic field to mechanical quantities and electric charge:\nThese two laws can be used to derive Amp\u00e8re's force law above, resulting in the relationship: formula_25. Therefore, if the unit of charge is based on the Amp\u00e8re's force law such that formula_26, it is natural to derive the unit of magnetic field by setting formula_27. However, if it is not the case, a choice has to be made as to which of the two laws above is a more convenient basis for deriving the unit of magnetic field.\nFurthermore, if we wish to describe the electric displacement field D and the magnetic field H in a medium other than vacuum, we need to also define the constants \"\u03b5\"0 and \"\u03bc\"0, which are the vacuum permittivity and permeability, respectively. Then we have (generally) formula_28 and formula_29, where P and M are polarization density and magnetization vectors. The units of P and M are usually so chosen that the factors \"\u03bb\" and \"\u03bb\"\u2032 are equal to the \"rationalization constants\" formula_30 and formula_31, respectively. If the rationalization constants are equal, then formula_32. If they are equal to one, then the system is said to be \"rationalized\": the laws for systems of spherical geometry contain factors of 4\"\u03c0\" (for example, point charges), those of cylindrical geometry \u2013 factors of 2\u03c0 (for example, wires), and those of planar geometry contain no factors of \"\u03c0\" (for example, parallel-plate capacitors). However, the original CGS system used \"\u03bb\" = \"\u03bb\"\u2032 = 4\"\u03c0\", or, equivalently, formula_33. Therefore, Gaussian, ESU, and EMU subsystems of CGS (described below) are not rationalized.\nVarious extensions of the CGS system to electromagnetism.\nThe table below shows the values of the above constants used in some common CGS subsystems:\nAlso, note the following correspondence of the above constants to those in Jackson and Leung:\nformula_34\nOf these variants, only in Gaussian and Heaviside\u2013Lorentz systems formula_35 equals formula_36 rather than 1. As a result, vectors formula_37 and formula_38 of an electromagnetic wave propagating in vacuum have the same units and are equal in magnitude in these two variants of CGS.\nIn each of these systems the quantities called \"charge\" etc. may be a different quantity; they are distinguished here by a superscript. The corresponding quantities of each system are related through a proportionality constant.\nMaxwell's equations can be written in each of these systems as:\nElectrostatic units (ESU).\nIn the electrostatic units variant of the CGS system, (CGS-ESU), charge is defined as the quantity that obeys a form of Coulomb's law without a multiplying constant (and current is then defined as charge per unit time):\nformula_39\nThe ESU unit of charge, franklin (Fr), also known as statcoulomb or esu charge, is therefore defined as follows: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;two equal point charges spaced 1 centimetre apart are said to be of 1 franklin each if the electrostatic force between them is 1 dyne. Therefore, in CGS-ESU, a franklin is equal to a centimetre times square root of dyne:\n formula_40\nThe unit of current is defined as:\n formula_41\nIn the CGS-ESU system, charge \"q\" is therefore has the dimension to M1/2L3/2T\u22121.\nOther units in the CGS-ESU system include the statampere (1\u00a0statC/s) and statvolt (1\u00a0erg/statC).\nIn CGS-ESU, all electric and magnetic quantities are dimensionally expressible in terms of length, mass, and time, and none has an independent dimension. Such a system of units of electromagnetism, in which the dimensions of all electric and magnetic quantities are expressible in terms of the mechanical dimensions of mass, length, and time, is traditionally called an 'absolute system'.:3\nUnit symbols.\nAll electromagnetic units in the CGS-ESU system that have not been given names of their own are named as the corresponding SI name with an attached prefix \"stat\" or with a separate abbreviation \"esu\", and similarly with the corresponding symbols.\nElectromagnetic units (EMU).\nIn another variant of the CGS system, electromagnetic units (EMU), current is defined via the force existing between two thin, parallel, infinitely long wires carrying it, and charge is then defined as current multiplied by time. (This approach was eventually used to define the SI unit of ampere as well). In the EMU CGS subsystem, this is done by setting the Ampere force constant formula_26, so that Amp\u00e8re's force law simply contains 2 as an explicit factor.\nThe EMU unit of current, biot (Bi), also known as abampere or emu current, is therefore defined as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The biot is that constant current which, if maintained in two straight parallel conductors of infinite length, of negligible circular cross-section, and placed one centimetre apart in vacuum, would produce between these conductors a force equal to two dynes per centimetre of length. Therefore, in electromagnetic CGS units, a biot is equal to a square root of dyne:\n formula_43.\nThe unit of charge in CGS EMU is:\n formula_44.\nDimensionally in the CGS-EMU system, charge \"q\" is therefore equivalent to M1/2L1/2. Hence, neither charge nor current is an independent physical quantity in the CGS-EMU system.\nEMU notation.\nAll electromagnetic units in the CGS-EMU system that do not have proper names are denoted by a corresponding SI name with an attached prefix \"ab\" or with a separate abbreviation \"emu\".\nRelations between ESU and EMU units.\nThe ESU and EMU subsystems of CGS are connected by the fundamental relationship formula_17 (see above), where \"c\" = \u2248 is the speed of light in vacuum in centimetres per second. Therefore, the ratio of the corresponding \"primary\" electrical and magnetic units (e.g. current, charge, voltage, etc. \u2013 quantities proportional to those that enter directly into Coulomb's law or Amp\u00e8re's force law) is equal either to \"c\"\u22121 or \"c\":\nformula_46\nand\nformula_47.\nUnits derived from these may have ratios equal to higher powers of \"c\", for example:\nformula_48.\nPractical CGS units.\nThe practical CGS system is a hybrid system that uses the volt and the ampere as the units of voltage and current respectively. Doing this avoids the inconveniently large and small electrical units that arise in the esu and emu systems. This system was at one time widely used by electrical engineers because the volt and ampere had been adopted as international standard units by the International Electrical Congress of 1881. As well as the volt and ampere, the farad (capacitance), ohm (resistance), coulomb (electric charge), and henry (inductance) are consequently also used in the practical system and are the same as the SI units. The magnetic units are those of the emu system.\nThe electrical units, other than the volt and ampere, are determined by the requirement that any equation involving only electrical and kinematical quantities that is valid in SI should also be valid in the system. For example, since electric field strength is voltage per unit length, its unit is the volt per centimetre, which is one hundred times the SI unit.\nThe system is electrically rationalized and magnetically unrationalized; i.e., \"\u03bb\" = 1 and \"\u03bb\"\u2032 = 4\u03c0, but the above formula for \"\u03bb\" is invalid. A closely related system is the International System of Electric and Magnetic Units, which has a different unit of mass so that the formula for \"\u03bb\"\u2032 is invalid. The unit of mass was chosen to remove powers of ten from contexts in which they were considered to be objectionable (e.g., \"P\" = \"VI\" and \"F\" = \"qE\"). Inevitably, the powers of ten reappeared in other contexts, but the effect was to make the familiar joule and watt the units of work and power respectively.\nThe ampere-turn system is constructed in a similar way by considering magnetomotive force and magnetic field strength to be electrical quantities and rationalizing the system by dividing the units of magnetic pole strength and magnetization by 4\u03c0. The units of the first two quantities are the ampere and the ampere per centimetre respectively. The unit of magnetic permeability is that of the emu system, and the magnetic constitutive equations are B = (4\"\u03c0\"/10)\"\u03bc\"H and B = (4\"\u03c0\"/10)\"\u03bc\"0H + \"\u03bc\"0M. Magnetic reluctance is given a hybrid unit to ensure the validity of Ohm's law for magnetic circuits.\nOther variants.\nThere were at various points in time about half a dozen systems of electromagnetic units in use, most based on the CGS system. These include the Gaussian units and the Heaviside\u2013Lorentz units.\nElectromagnetic units in various CGS systems.\nIn this table, \"c\" = is the dimensionless numeric value of the speed of light in vacuum when expressed in units of centimetres per second. The symbol \"\u2258\" is used instead of \"=\" as a reminder that the quantities are \"corresponding\" but not in general \"equal\", even between CGS variants. For example, according to the next-to-last row of the table, if a capacitor has a capacitance of 1\u00a0F in SI, then it has a capacitance of (10\u22129\u00a0\"c\"2) cm in ESU; \"but\" it is incorrect to replace \"1\u00a0F\" with \"(10\u22129\u00a0\"c\"2)\u00a0cm\" within an equation or formula. (This warning is a special aspect of electromagnetism units in CGS. By contrast, for example, it is \"always\" correct to replace \"1\u00a0m\" with \"100\u00a0cm\" within an equation or formula.)\nOne can think of the SI value of the Coulomb constant \"k\"C as:\nformula_49\nThis explains why SI to ESU conversions involving factors of \"c\"2 lead to significant simplifications of the ESU units, such as 1\u00a0statF = 1\u00a0cm and 1\u00a0stat\u03a9 = 1\u00a0s/cm: this is the consequence of the fact that in ESU system \"k\"C = 1. For example, a centimetre of capacitance is the capacitance of a sphere of radius 1\u00a0cm in vacuum. The capacitance \"C\" between two concentric spheres of radii \"R\" and \"r\" in ESU CGS system is:\n formula_50.\nBy taking the limit as \"R\" goes to infinity we see \"C\" equals \"r\".\nAdvantages and disadvantages.\nWhile the absence of constant coefficients in the formulae expressing some relation between the quantities in some CGS subsystems simplifies some calculations, it has the disadvantage that sometimes the units in CGS are hard to define through experiment. Also, lack of unique unit names leads to a great confusion: thus \"15 emu\" may mean either 15 abvolts, or 15 emu units of electric dipole moment, or 15 emu units of magnetic susceptibility, sometimes (but not always) per gram, or per mole. On the other hand, SI starts with a unit of current, the ampere, that is easier to determine through experiment, but which requires extra coefficients in the electromagnetic equations. With its system of uniquely named units, the SI also removes any confusion in usage: 1 ampere is a fixed value of a specified quantity, and so are 1 henry, 1\u00a0ohm, and 1\u00a0volt.\nAn advantage of the CGS-Gaussian system is that electric and magnetic fields have the same units, 4\"\u03c0\u03b5\"0 is replaced by 1, and the only dimensional constant appearing in the Maxwell equations is \"c\", the speed of light. The Heaviside\u2013Lorentz system has these properties as well (with \"\u03b5\"0 equaling 1), but it is a \"rationalized\" system (as is SI) in which the charges and fields are defined in such a way that there are fewer factors of 4\"\u03c0\" appearing in the formulas, and it is in Heaviside\u2013Lorentz units that the Maxwell equations take their simplest form.\nIn SI, and other rationalized systems (for example, Heaviside\u2013Lorentz), the unit of current was chosen such that electromagnetic equations concerning charged spheres contain 4\"\u03c0\", those concerning coils of current and straight wires contain 2\"\u03c0\" and those dealing with charged surfaces lack \"\u03c0\" entirely, which was the most convenient choice for applications in electrical engineering. However, modern hand calculators and personal computers have eliminated this \"advantage\". In some fields where formulas concerning spheres are common (for example, in astrophysics), it has been argued that the nonrationalized CGS system can be somewhat more convenient notationally.\nSpecialized unit systems are used to simplify formulas even further than \"either\" SI \"or\" CGS, by eliminating constants through some system of natural units. For example, in particle physics a system is in use where every quantity is expressed by only one unit of energy, the electronvolt, with lengths, times, and so on all converted into electronvolts by inserting factors of speed of light \"c\" and the reduced Planck constant \"\u0127\". This unit system is convenient for calculations in particle physics, but it would be considered impractical in other contexts."}
{"id": "7355", "revid": "31599192", "url": "https://en.wikipedia.org/wiki?curid=7355", "title": "Christology", "text": "Theological study of Jesus Christ\nIn Christianity, Christology (from the Greek and ), translated from Greek as 'the study of Christ', is a branch of theology that concerns Jesus. Different denominations have different opinions on questions such as whether Jesus was human, divine, or both, and as a messiah what his role would be in the freeing of the Jewish people from foreign rulers or in the prophesied Kingdom of God, and in the salvation from what would otherwise be the consequences of sin.\nThe earliest Christian writings gave several titles to Jesus, such as Son of Man, Son of God, Messiah, and , which were all derived from Hebrew scripture. These terms centered around two opposing themes, namely \"Jesus as a preexistent figure who becomes human and then returns to God\", versus adoptionism \u2013 that Jesus was human who was \"adopted\" by God at his baptism, crucifixion, or resurrection.\nFrom the second to the fifth centuries, the relation of the human and divine nature of Christ was a major focus of debates in the early church and at the first seven ecumenical councils. The Council of Chalcedon in 451 issued a formulation of the hypostatic union of the two natures of Christ, one human and one divine, \"united with neither confusion nor division\". Most of the major branches of Western Christianity and Eastern Orthodoxy subscribe to this formulation, while many branches of Oriental Orthodox Churches reject it, subscribing to miaphysitism.\nDefinition and approaches.\n\"Christology\" (from the Greek and ), literally 'the understanding of Christ', is the study of the nature (person) and work (role in salvation) of Jesus Christ. It studies Jesus Christ's humanity and divinity, and the relation between these two aspects; and the role he plays in salvation.\n\"Ontological Christology\" analyzes the nature or being of Jesus Christ. \"Functional Christology\" analyzes the works of Jesus Christ, while \"soteriological Christology\" analyzes the \"salvific\" standpoints of Christology.\nSeveral approaches can be distinguished within Christology. The term \"Christology from above\" or \"high Christology\" refers to approaches that include aspects of divinity, such as Lord and Son of God, and the idea of the pre-existence of Christ as the \"Logos\" ('the Word'), as expressed in the . These approaches interpret the works of Christ in terms of his divinity. According to Pannenberg, Christology from above \"was far more common in the ancient Church, beginning with Ignatius of Antioch and the second century Apologists.\" The term \"Christology from below\" or \"low Christology\" refers to approaches that begin with the human aspects and the ministry of Jesus (including the miracles, parables, etc.) and move towards his divinity and the mystery of incarnation.\nPerson of Christ.\nA basic Christological teaching is that the person of Jesus Christ is both human and divine. The human and divine natures of Jesus Christ apparently (\"prosopic\") form a duality, as they coexist within one person (\"hypostasis\"). There are no direct discussions in the New Testament regarding the dual nature of the Person of Christ as both divine and human, and since the early days of Christianity, theologians have debated various approaches to the understanding of these natures, at times resulting in ecumenical councils, and schisms.\nSome historical christological doctrines gained broad support:\nInfluential Christologies which were broadly condemned as heretical are:\nVarious church councils, mainly in the 4th and 5th centuries, resolved most of these controversies, making the doctrine of the Trinity orthodox in nearly all branches of Christianity. Among them, only the Dyophysite doctrine was recognized as true and not heretical, belonging to the Christian orthodoxy and deposit of faith.\nSalvation.\nIn Christian theology, atonement is the method by which human beings can be reconciled to God through Christ's sacrificial suffering and death. Atonement is the forgiving or pardoning of sin in general and original sin in particular through the suffering, death and resurrection of Jesus, enabling the reconciliation between God and his creation. Due to the influence of Gustaf Aul\u00e8n's (1879\u20131978) (1931), the various theories or paradigmata of atonement are often grouped as \"classical paradigm\", \"objective paradigm\", and the \"subjective paradigm\":\nOther theories are the \"embracement theory\" and the \"shared atonement\" theory.\nEarly Christologies (1st century).\nEarly notions of Christ.\nThe earliest christological reflections were shaped by both the Jewish background of the earliest Christians, and by the Greek world of the eastern Mediterranean in which they operated. The earliest Christian writings give several titles to Jesus, such as Son of Man, Son of God, Messiah, and \"Kyrios\", which were all derived from Hebrew scripture. According to Matt Stefon and Hans J. Hillerbrand:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Until the middle of the 2nd century, such terms emphasized two themes: that of Jesus as a preexistent figure who becomes human and then returns to God and that of Jesus as a creature elected and \"adopted\" by God. The first theme makes use of concepts drawn from Classical antiquity, whereas the second relies on concepts characteristic of ancient Jewish thought. The second theme subsequently became the basis of \"adoptionist Christology\" (see adoptionism), which viewed Jesus' baptism as a crucial event in his adoption by God.\nHistorically in the Alexandrian school of thought (fashioned on the Gospel of John), Jesus Christ is the eternal \"Logos\" who already possesses unity with the Father before the act of Incarnation. In contrast, the Antiochian school viewed Christ as a single, unified human person apart from his relationship to the divine.\nPre-existence.\nThe notion of pre-existence is deeply rooted in Jewish thought, and can be found in apocalyptic thought and among the rabbis of Paul's time, but Paul was most influenced by Jewish-Hellenistic wisdom literature, where \"'Wisdom' is extolled as something existing before the world and already working in creation. According to Witherington, Paul \"subscribed to the christological notion that Christ existed prior to taking on human flesh[,] founding the story of Christ[...] on the story of divine Wisdom\".\n\"Kyrios\".\nThe title \"Kyrios\" for Jesus is central to the development of New Testament Christology. In the Septuagint it translates the Tetragrammaton, the holy Name of God. As such, it closely links Jesus with God \u2013 in the same way a verse such as Matthew 28:19, \"The Name (singular) of the Father, the Son, and the Holy Spirit\".\n\"Kyrios\" is also conjectured to be the Greek translation of Aramaic , which in everyday Aramaic usage was a very respectful form of polite address, which means more than just 'teacher' and was somewhat similar to 'rabbi'. While the term expressed the relationship between Jesus and his disciples during his life, the Greek \"Kyrios\" came to represent his lordship over the world.\nThe early Christians placed \"Kyrios\" at the center of their understanding, and from that center attempted to understand the other issues related to the Christian mysteries. The question of the deity of Christ in the New Testament is inherently related to the \"Kyrios\" title of Jesus used in the early Christian writings and its implications for the absolute lordship of Jesus. In early Christian belief, the concept of \"Kyrios\" included the pre-existence of Christ, for they believed if Christ is one with God, he must have been united with God from the very beginning.\nDevelopment of \"low Christology\" and \"high Christology\".\nTwo fundamentally different Christologies developed in the early Church, namely a \"low\" or adoptionist Christology, and a \"high\" or \"incarnation\" Christology. The chronology of the development of these early Christologies is a matter of debate within contemporary scholarship.\nThe \"low Christology\" or \"adoptionist Christology\" is the belief \"that God exalted Jesus to be his Son by raising him from the dead\", thereby raising him to \"divine status\". According to the \"evolutionary model\" or evolutionary theories, the Christological understanding of Jesus developed over time, as witnessed in the Gospels, with the earliest Christians believing that Jesus was a human who was exalted, or else adopted as God's Son, when he was resurrected. Later beliefs shifted the exaltation to his baptism, birth, and subsequently to the idea of his pre-existence, as witnessed in the Gospel of John. This \"evolutionary model\" was proposed by proponents of the , especially Wilhelm Bousset's influential \"Kyrios Christos\" (1913). This evolutionary model was very influential, and the \"low Christology\" has long been regarded as the oldest Christology.\nThe other early Christology is \"high Christology\", which is \"the view that Jesus was a pre-existent divine being who became a human, did the Father's will on earth, and then was taken back up into heaven whence he had originally come\", and from where he appeared on earth. According to Bousset, this \"high Christology\" developed at the time of Paul's writing, under the influence of Gentile Christians, who brought their pagan Hellenistic traditions to the early Christian communities, introducing divine honours to Jesus. According to Casey and Dunn, this \"high Christology\" developed after the time of Paul, at the end of the first century CE when the Gospel of John was written.\nSince the 1970s, these late datings for the development of a \"high Christology\" have been contested, and a majority of scholars argue that this \"high Christology\" existed already before the writings of Paul. According to the \"New \", or the Early High Christology Club, which includes Martin Hengel, Larry Hurtado, N. T. Wright, and Richard Bauckham, this \"incarnation Christology\" or \"high Christology\" did not evolve over a longer time, but was a \"big bang\" of ideas which were already present at the start of Christianity, and took further shape in the first few decades of the church, as witnessed in the writings of Paul. Some 'Early High Christology' proponents scholars argue that this \"high Christology\" may go back to Jesus himself.\nThere is a controversy regarding whether Jesus himself claimed to be divine. In \"Honest to God\", then-Bishop of Woolwich, John A. T. Robinson, questioned the idea. John Hick, writing in 1993, mentioned changes in New Testament studies, citing \"broad agreement\" that scholars do not today support the view that Jesus claimed to be God, quoting as examples Michael Ramsey (1980), C. F. D. Moule (1977), James Dunn (1980), Brian Hebblethwaite (1985) and David Brown (1985). Larry Hurtado, who argues that the followers of Jesus within a very short period developed an exceedingly high level of devotional reverence to Jesus, at the same time rejects the view that Jesus made a claim to messiahship or divinity to his disciples during his life as \"naive and ahistorical\". According to Gerd L\u00fcdemann, the broad consensus among modern New Testament scholars is that the proclamation of the divinity of Jesus was a development within the earliest Christian communities. N. T. Wright points out that arguments over the claims of Jesus regarding divinity have been passed over by more recent scholarship, which sees a more complex understanding of the idea of God in first century Judaism. However, Andrew Loke argues that if Jesus did not claim and show himself to be truly divine and rise from the dead, the earliest Christian leaders who were devout ancient monotheistic Jews would have regarded Jesus as merely a teacher or a prophet; they would not have come to the widespread agreement that he was truly divine, which they did.\nNew Testament writings.\nThe study of the various Christologies of the Apostolic Age is based on early Christian documents.\nPaul.\nThe oldest Christian sources are the writings of Paul. The central Christology of Paul conveys the notion of Christ's pre-existence and the identification of Christ as \"Kyrios\". Both notions already existed before him in the early Christian communities, and Paul deepened them and used them for preaching in the Hellenistic communities.\nWhat exactly Paul believed about the nature of Jesus cannot be determined decisively. In Philippians 2, Paul states that Jesus was preexistent and came to Earth \"by taking the form of a servant, being made in human likeness\". This sounds like an incarnation Christology. In Romans 1:4, however, Paul states that Jesus \"was declared with power to be the Son of God by his resurrection from the dead\", which sounds like an adoptionistic Christology, where Jesus was a human being who was \"adopted\" after his death. Different views would be debated for centuries by Christians and finally settled on the idea that he was both fully human and fully divine by the middle of the 5th century in the Council of Ephesus. Paul's thoughts on Jesus' teachings, versus his nature and being, are more defined, in that Paul believed Jesus was sent as an atonement for the sins of everyone.\nThe Pauline epistles use \"Kyrios\" to identify Jesus almost 230 times, and express the theme that the true mark of a Christian is the confession of Jesus as the true Lord. Paul viewed the superiority of the Christian revelation over all other divine manifestations as a consequence of the fact that Christ is the Son of God.\nThe Pauline epistles also advanced the \"cosmic Christology\" later developed in the Gospel of John, elaborating the cosmic implications of Jesus' existence as the Son of God: \"Therefore, if anyone is in Christ, he is a new creation. The old has passed away; behold, the new has come.\" Paul writes that Christ came to draw all back to God: \"Through him God was pleased to reconcile to himself all things, whether on earth or in heaven\" (Colossians 1:20); in the same epistle, he writes that \"He is the image of the invisible God, the firstborn of all creation\" (Colossians 1:15).\nThe Gospels.\nThe synoptic Gospels date from after the writings of Paul. They provide episodes from the life of Jesus and some of his works, but the authors of the New Testament show little interest in an absolute chronology of Jesus or in synchronizing the episodes of his life, and as in , the Gospels do not claim to be an exhaustive list of his works.\nChristologies that can be gleaned from the three Synoptic Gospels generally emphasize the humanity of Jesus, his sayings, his parables, and his miracles. The Gospel of John provides a different perspective that focuses on his divinity. The first 14 verses of the Gospel of John are devoted to the divinity of Jesus as the \"Logos\", usually translated as \"Word\", along with his pre-existence, and they emphasize the cosmic significance of Christ, e.g.: \"All things were made through him, and without him was not any thing made that was made.\" In the context of these verses, the Word made flesh is identical with the Word who was in the beginning with God, being exegetically equated with Jesus.\nControversies and ecumenical councils (2nd\u20138th century).\nPost-Apostolic controversies.\nFollowing the Apostolic Age, from the second century onwards, a number of controversies developed about how the human and divine are related within the person of Jesus. As of the second century, a number of different and opposing approaches developed among various groups. In contrast to prevailing monoprosopic views on the Person of Christ, alternative dyoprosopic notions were also promoted by some theologians, but such views were rejected by the ecumenical councils. For example, Arianism did not endorse divinity, Ebionism argued Jesus was an ordinary mortal, while Gnosticism held docetic views which argued Christ was a spiritual being who only appeared to have a physical body. The resulting tensions led to schisms within the church in the second and third centuries, and ecumenical councils were convened in the fourth and fifth centuries to deal with the issues.\nAlthough some of the debates may seem to various modern students to be over a theological iota, they took place in controversial political circumstances, reflecting the relations of temporal powers and divine authority, and certainly resulted in schisms, among others that separated the Church of the East from the Church of the Roman Empire.\nFirst Council of Nicaea (325) and First Council of Constantinople (381).\nIn 325, the First Council of Nicaea defined the persons of the Godhead and their relationship with one another, decisions which were ratified at the First Council of Constantinople in 381. The language used was that the one God exists in three persons (Father, Son, and Holy Spirit); in particular, it was affirmed that the Son was \"homoousios\" (of the same being) as the Father. The Nicene Creed declared the full divinity and full humanity of Jesus. After the First Council of Nicaea in 325 the \"Logos\" and the second Person of the Trinity were being used interchangeably.\nFirst Council of Ephesus (431).\nIn 431, the First Council of Ephesus was initially called to address the views of Nestorius on Mariology, but the problems soon extended to Christology, and schisms followed. The 431 council was called because in defense of his loyal priest Anastasius, Nestorius had denied the \"Theotokos\" title for Mary and later contradicted Proclus during a sermon in Constantinople. Pope Celestine I (who was already upset with Nestorius due to other matters) wrote about this to Cyril of Alexandria, who orchestrated the council. During the council, Nestorius defended his position by arguing there must be two persons of Christ, one human, the other divine, and Mary had given birth only to a human, hence could not be called the \"Theotokos\", i.e. \"the one who gives birth to God\". The debate about the single or dual nature of Christ ensued in Ephesus.\nThe First Council of Ephesus debated miaphysitism (two natures united as one after the hypostatic union) versus dyophysitism (coexisting natures after the hypostatic union) versus monophysitism (only one nature) versus Nestorianism (two hypostases). From the Christological viewpoint, the council adopted ('but being made one', ) \u2013 Council of Ephesus, Epistle of Cyril to Nestorius, i.e. 'one nature of the Word of God incarnate' (, ). In 451, the Council of Chalcedon affirmed dyophysitism. The Oriental Orthodox rejected this and subsequent councils and continued to consider themselves as \"miaphysite\" according to the faith put forth at the Councils of Nicaea and Ephesus. The council also confirmed the \"Theotokos\" title and excommunicated Nestorius.\nCouncil of Chalcedon (451).\nThe 451 Council of Chalcedon was highly influential, and marked a key turning point in the christological debates. It is the last council which many Lutherans, Anglicans and other Protestants consider ecumenical.\nThe Council of Chalcedon fully promulgated the Western dyophysite understanding put forth by Pope Leo I of Rome of the \"hypostatic union\", the proposition that Christ has one human nature \"(physis)\" and one divine nature \"(physis)\", each distinct and complete, and united with neither confusion nor division. Most of the major branches of Western Christianity (Roman Catholicism, Anglicanism, Lutheranism, and Reformed), Church of the East, Eastern Catholicism and Eastern Orthodoxy subscribe to the Chalcedonian Christological formulation, while many branches of Oriental Orthodox Churches (Syrian Orthodoxy, Coptic Orthodoxy, Ethiopian Orthodoxy, and Armenian Apostolicism) reject it.\nAlthough the Chalcedonian Creed did not put an end to all christological debate, it did clarify the terms used and became a point of reference for many future Christologies. But it also broke apart the church of the Eastern Roman Empire in the fifth century, and unquestionably established the primacy of Rome in the East over those who accepted the Council of Chalcedon. This was reaffirmed in 519, when the Eastern Chalcedonians accepted the Formula of Hormisdas, anathematizing all of their own Eastern Chalcedonian hierarchy, who died out of communion with Rome from 482 to 519.\nFifth\u2013Seventh Ecumenical Council (553, 681, 787).\nThe Second Council of Constantinople in 553 interpreted the decrees of Chalcedon, and further explained the relationship of the two natures of Jesus. It also condemned the alleged teachings of Origen on the pre-existence of the soul, and other topics.\nThe Third Council of Constantinople in 681 declared that Christ has two wills of his two natures, human and divine, contrary to the teachings of the Monothelites, with the divine will having precedence, leading and guiding the human will.\nThe Second Council of Nicaea was called under the Empress Regent Irene of Athens in 787, known as the second of Nicaea. It supports the veneration of icons while forbidding their worship. It is often referred to as \"The Triumph of Orthodoxy\".\nWestern medieval Christology.\nThe Franciscan piety of the 12th and 13th centuries led to \"popular Christology\". Systematic approaches by theologians, such as Thomas Aquinas, are called \"scholastic Christology\".\nIn the 13th century, Thomas Aquinas provided the first systematic Christology that consistently resolved a number of the existing issues. In his Christology from above, Aquinas also championed the principle of perfection of Christ's human attributes.\nThe Middle Ages also witnessed the emergence of the \"tender image of Jesus\" as a friend and a living source of love and comfort, rather than just the \"Kyrios\" image.\nReformation.\nJohn Calvin maintained there was no human element in the Person of Christ which could be separated from the Person of The Word. Calvin also emphasized the importance of the \"Work of Christ\" in any attempt at understanding the Person of Christ and cautioned against ignoring the Works of Jesus during his ministry.\nModern developments.\nLiberal Protestant theology.\nThe 19th century saw the rise of Liberal Protestant theology, which questioned the dogmatic foundations of Christianity, and approached the Bible with critical-historical tools. The divinity of Jesus was problematized, and replaced with an emphasis on the ethical aspects of his teachings.\nRoman Catholicism.\nCatholic theologian Karl Rahner sees the purpose of modern Christology as to formulate the Christian belief that \"God became man and that God-made-man is the individual Jesus Christ\" in a manner that this statement can be understood consistently, without the confusions of past debates and mythologies. Rahner pointed out the coincidence between the Person of Christ and the Word of God, referring to and which state whoever is ashamed of the words of Jesus is ashamed of the Lord himself.\nHans von Balthasar argued the union of the human and divine natures of Christ was achieved not by the \"absorption\" of human attributes, but by their \"assumption\". Thus, in his view, the divine nature of Christ was not affected by the human attributes and remained forever divine.\nTopics.\nNativity and the Holy Name.\nThe Nativity of Jesus impacted the Christological issues about his person from the earliest days of Christianity. Luke's Christology centers on the dialectics of the dual natures of the earthly and heavenly manifestations of existence of the Christ, while Matthew's Christology focuses on the mission of Jesus and his role as the savior. The salvific emphasis of later impacted the theological issues and the devotions to Holy Name of Jesus.\n provides a key to the \"Emmanuel Christology\" of Matthew. Beginning with 1:23, the Gospel of Matthew shows a clear interest in identifying Jesus as \"God with us\" and in later developing the Emmanuel characterization of Jesus at key points throughout the rest of the Gospel. The name 'Emmanuel' does not appear elsewhere in the New Testament, but Matthew builds on it in (\"I am with you always, even unto the end of the world\") to indicate Jesus will be with the faithful to the end of the age. According to Ulrich Luz, the Emmanuel motif brackets the entire Gospel of Matthew between 1:23 and 28:20, appearing explicitly and implicitly in several other passages.\nCrucifixion and resurrection.\nThe accounts of the crucifixion and subsequent resurrection of Jesus provides a rich background for christological analysis, from the canonical Gospels to the Pauline Epistles.\nA central element in the christology presented in the Acts of the Apostles is the affirmation of the belief that the death of Jesus by crucifixion happened \"with the foreknowledge of God, according to a definite plan\". In this view, as in , the cross is not viewed as a scandal, for the crucifixion of Jesus \"at the hands of the lawless\" is viewed as the fulfilment of the plan of God.\nPaul's Christology has a specific focus on the death and resurrection of Jesus. For Paul, the crucifixion of Jesus is directly related to his resurrection and the term \"the cross of Christ\" used in Galatians 6:12 may be viewed as his abbreviation of the message of the Gospels. For Paul, the crucifixion of Jesus was not an isolated event in history, but a cosmic event with significant eschatological consequences, as in 1 Corinthians 2:8. In the Pauline view, Jesus, obedient to the point of death (Philippians 2:8), died \"at the right time\" (Romans 5:6) based on the plan of God. For Paul, the \"power of the cross\" is not separable from the resurrection of Jesus.\nThreefold office.\nThe threefold office (Latin ) of Jesus Christ is a Christian doctrine based upon the teachings of the Old Testament. It was described by Eusebius and more fully developed by John Calvin. It states that Jesus Christ performed three functions (or \"offices\") in his earthly ministry \u2013 those of prophet, priest, and king. In the Old Testament, the appointment of someone to any of these three positions could be indicated by anointing him or her by pouring oil over the head. Thus, the term \"messiah\", meaning \"anointed one\", is associated with the concept of the threefold office. While the office of king is that most frequently associated with the Messiah, the role of Jesus as priest is also prominent in the New Testament, being most fully explained in chapters 7 to 10 of the Book of Hebrews.\nMariology.\nSome Christians, notably Roman Catholics, view Mariology as a key component of Christology. In this view, not only is Mariology a logical and necessary consequence of Christology, but without it, Christology is incomplete, since the figure of Mary contributes to a fuller understanding of who Christ is and what he did.\nProtestants have criticized Mariology because many of its assertions lack any Biblical foundation. Strong Protestant reaction against Roman Catholic Marian devotion and teaching has been a significant issue for ecumenical dialogue.\nJoseph Cardinal Ratzinger (later Pope Benedict XVI) expressed this sentiment about Roman Catholic Mariology when in two separate occasions he stated, \"The appearance of a truly Marian awareness serves as the touchstone indicating whether or not the christological substance is fully present\" and \"It is necessary to go back to Mary, if we want to return to the truth about Jesus Christ.\"\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7357", "revid": "28979433", "url": "https://en.wikipedia.org/wiki?curid=7357", "title": "Complaint", "text": "Legal document, the filing of which initiates a lawsuit\nIn legal terminology, a complaint is any formal legal document that sets out the facts and legal reasons (see: cause of action) that the filing party or parties (the plaintiff(s)) believes are sufficient to support a claim against the party or parties against whom the claim is brought (the defendant(s)) that entitles the plaintiff(s) to a remedy (either money damages or injunctive relief). For example, the Federal Rules of Civil Procedure (FRCP) that govern civil litigation in United States courts provide that a civil action is commenced with the filing or service of a pleading called a complaint. Civil court rules in states that have incorporated the Federal Rules of Civil Procedure use the same term for the same pleading. \nIn Civil Law, a \"complaint\" is the first formal action taken to officially begin a lawsuit. This written document contains the allegations against the defense, the specific laws violated, the facts that led to the dispute, and any demands made by the plaintiff to restore justice.\nIn some jurisdictions, specific types of criminal cases may also be commenced by the filing of a complaint, also sometimes called a criminal complaint or felony complaint. Most criminal cases are prosecuted in the name of the governmental authority that promulgates criminal statutes and enforces the police power of the state with the goal of seeking criminal sanctions, such as the State (also sometimes called the People) or Crown (in Commonwealth realms). In the United States, the complaint is often associated with misdemeanor criminal charges presented by the prosecutor without the grand jury process. In most U.S. jurisdictions, the charging instrument presented to and authorized by a grand jury is referred to as an indictment.\nUnited States.\nVirtually every U.S. state has some forms available on the web for most common complaints for lawyers and self-representing litigants; if a petitioner cannot find an appropriate form in their state, they often can modify a form from another state to fit his or her request. Several United States federal courts publish general guidelines for the petitioners and Civil Rights complaint forms.\nA complaint generally has the following structural elements:\nAfter the complaint has been filed with the court, it has to be properly served to the opposite parties, but usually petitioners are not allowed to serve the complaint personally. The court also can issue a summons \u2013 an official summary document which the plaintiff needs to have served together with the complaint. The defendants have limited time to respond, depending on the State or Federal rules. A defendant's failure to answer a complaint can result in a default judgment in favor of the petitioner.\nFor example, in United States federal courts, any person who is at least 18 years old and not a party may serve a summons and complaint in a civil case. The defendant must submit an answer within 21 days after being served with the summons and complaint, or request a waiver, according to FRCP Rule 12. After the civil complaint has been served to the defendants, the plaintiff must, as soon as practicable initiate a conference between the parties to plan for the rest of the discovery process and then the parties should submit a proposed discovery plan to the judge within 14 days after the conference.\nIn many U.S. jurisdictions, a complaint submitted to a court must be accompanied by a Case Information Statement, which sets forth specific key information about the case and the lawyers representing the parties. This allows the judge to make determinations about which deadlines to set for different phases of the case, as it moves through the court system.\nThere are also freely accessible web search engines to assist parties in finding court decisions that can be cited in the complaint as an example or analogy to resolve similar questions of law. Google Scholar is the biggest database of full text state and federal courts decisions that can be accessed without charge. These web search engines often allow one to select specific state courts to search.\nFederal courts created the Public Access to Court Electronic Records (PACER) system to obtain case and docket information from the United States district courts, United States courts of appeals, and United States bankruptcy courts. The system is managed by the Administrative Office of the United States Courts; it allows lawyers and self-represented clients to obtain documents entered in the case much faster than regular mail.\nFiling and privacy.\nIn addition to Federal Rules of Civil Procedure, many of the U.S. district courts have developed their own requirements included in Local Rules for filing with the Court. Local Rules can set up a limit on the number of pages, establish deadlines for motions and responses, explain whether it is acceptable to combine a motion petition with a response, specify if a judge needs an additional copy of the documents (called \"judge\u2019s copy\"), etc. Local Rules can define page layout elements like: margins, text font/size, distance between lines, mandatory footer text, page numbering, and provide directions on how the pages need to be bound together \u2013 i.e. acceptable fasteners, number and location of fastening holes, etc. If the filed motion does not comply with the Local Rules then the judge can choose to strike the motion completely, or order the party to re-file its motion, or grant a special exception to the Local Rules.\nAccording to Federal Rules of Civil Procedure (FRCP) 5.2, sensitive text like Social Security number, Taxpayer Identification Number, birthday, bank accounts and children\u2019s names, should be redacted from the filings made with the court and accompanying exhibits, (exhibits normally do not need to be attached to the original complaint, but should be presented to Court after the discovery). The redacted text can be erased with black-out or white-out, and the page should have an indication that it was redacted - most often by stamping word \"redacted\" on the bottom. Alternately, the filing party may ask the court\u2019s permission to file some exhibits completely under seal. A minor's name of the petitions should be replaced with initials.\nA person making a redacted filing can file an unredacted copy under seal, or the Court can choose to order later that an additional filing be made under seal without redaction. Copies of both redacted and unredacted documents filed with court should be provided to the other parties in the case. Some courts also require that an additional electronic courtesy copy be emailed to the other parties.\nAttorney fees.\nBefore filing the complaint, it is important for plaintiff(s) to remember that Federal courts can impose liability for the prevailing party's attorney fees to the losing party, if the judge considers the case frivolous or for purposes of harassment, even when the case was voluntarily dismissed. In the case of \"Fox v. Vice\", the U.S. Supreme Court held that reasonable attorneys' fees could be awarded to the defendant under 42 U.S.C. Sec. 1988, but only for costs that the defendant would not have incurred \"but for the frivolous claims.\" Even when there is no actual trial or judgment, if there is only pre-trial motion practice such as motions to dismiss, attorney fee shifting still can be awarded under FRCP Rule 11 when the opposing party files a Motion for Sanctions and the court issue an order identifying the sanctioned conduct and the basis for the sanction. The losing party has a right to appeal any order for sanctions in the higher court. In the state courts, each party is generally responsible only for its own attorney fees, with certain exceptions.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7360", "revid": "2092487", "url": "https://en.wikipedia.org/wiki?curid=7360", "title": "Cray Research", "text": ""}
{"id": "7362", "revid": "27823944", "url": "https://en.wikipedia.org/wiki?curid=7362", "title": "Casimir III the Great", "text": "Casimir III the Great (; 30 April 1310 \u2013 5 November 1370) reigned as the King of Poland from 1333 to 1370. He also later became King of Ruthenia in 1340, and fought to retain the title in the Galicia-Volhynia Wars. He was the last Polish king from the Piast dynasty.\nCasimir inherited a kingdom weakened by war and made it prosperous and wealthy. He reformed the Polish army and doubled the size of the kingdom. He reformed the judicial system and introduced a legal code, gaining the title \"the Polish Justinian\". Casimir built extensively and founded the Jagiellonian University (back then simply called the University of Krakow), the oldest Polish university and one of the oldest in the world. He also confirmed privileges and protections previously granted to Jews and encouraged them to settle in Poland in great numbers.\nCasimir left no legitimate sons. When he died in 1370 from an injury received while hunting, his nephew, King Louis I of Hungary, succeeded him as king of Poland in personal union with Hungary.\nThe Great King.\nCasimir was born on 30 April 1310 in Kowal, Kuyavia, the third son of Ladislaus the Short and Jadwiga of Kalisz. He had two brothers who died in infancy and three sisters: Kunegunda, El\u017cbieta, and Jadwiga. When Casimir attained the throne in 1333, his position was in danger, as his neighbours did not recognise his title and instead called him \"king of Krak\u00f3w\". The kingdom was depopulated and exhausted by war, and the economy was ruined. In 1335, in the Treaty of Trentschin, Casimir was forced to relinquish his claims to Silesia \"in perpetuity\".\nCasimir began to rebuild the country and strengthen its defenses. During his reign, nearly 30 towns were supplied with fortification walls and some 50 castles were constructed, including castles along the Trail of the Eagle's Nests. These achievements are still celebrated today, in a commonly-known ditty that translates as follows: \"inherited wooden towns and left them fortified with stone and brick\" (Kazimierz Wielki zasta\u0142 Polsk\u0119 drewnian\u0105, a zostawi\u0142 murowan\u0105).\nHe organized a meeting of kings in Krak\u00f3w in 1364 at which he exhibited the wealth of the Polish kingdom. Casimir is the only king in Polish history to both receive and retain the title of \"Great\", as Boles\u0142aw I is more commonly known as \"the Brave\".\nReforms.\nCasimir ensured stability and great prospects for the future of the country. He established the Corona Regni Poloniae \u2013 the Crown of the Polish Kingdom, which certified the existence of the Polish lands independently from the monarch. Prior to that, the lands were only the property of the Piast dynasty.\nAt the Sejm in Wi\u015blica, on 11 March 1347, Casimir introduced reforms to the Polish judicial system and sanctioned civil and criminal codes for Great and Lesser Poland, earning the title \"the Polish Justinian\". In 1364, having received permission from Pope Urban V, Casimir established the University of Krak\u00f3w, now the oldest university in Poland. It was regarded as a rare distinction, since it was only the second university founded in Central Europe, after the Charles University in Prague.\nPolitics and Expansion.\nCasimir demonstrated competence in foreign diplomacy and managed to double the size of his kingdom. He neutralized relations with potential enemies to the west and north, and began to expand his territory eastward. He conquered the Ruthenian kingdom of Halych and Volodymyr (a territory in the modern-day Ukraine), known in Polish history as Red Ruthenia and Volhynia. By extending the borders far south-east, the Polish kingdom gained access to the lucrative Black Sea trade.\nSuccession.\nIn 1355, in Buda, Casimir designated his nephew Louis I of Hungary as his successor should he produce no male heir, just as his father had with Charles I of Hungary to gain help against Bohemia. In exchange Casimir gained a favourable Hungarian attitude, needed in disputes with the hostile Teutonic Order and the Kingdom of Bohemia. At the time Casimir was 45 years old, and so producing a son did not seem unreasonable.\nCasimir left no legal son, however, begetting five daughters instead. He tried to adopt his grandson, Casimir IV, Duke of Pomerania, in his last will. The child had been born to his eldest daughter, Elisabeth, Duchess of Pomerania, in 1351. This part of the testament was invalidated by Louis I of Hungary, however, who had traveled to Krak\u00f3w quickly after Casimir died (in 1370) and bribed the nobles with future privileges. Casimir III also had a son-in-law, Louis VI of Bavaria, Margrave and Prince-elector of Brandenburg, who was considered a possible successor, but he was deemed ineligible as his wife, Casimir's daughter Cunigunde, had died in 1357 without issue.\nThus King Louis I of Hungary became successor in Poland. Louis was proclaimed king upon Casimir's death in 1370, though Casimir's sister Elisabeth (Louis's mother) held much of the real power until her death in 1380.\nSociety under the reign of Casimir.\nCasimir was facetiously named \"the Peasants' King\". He introduced the codes of law of Greater and Lesser Poland as an attempt to end the overwhelming superiority of the nobility. During his reign all three major classes \u2014 the nobility, priesthood, and bourgeoisie \u2014 were more or less counterbalanced, allowing Casimir to strengthen his monarchic position. He was known for siding with the weak when the law did not protect them from nobles and clergymen. He reportedly even supported a peasant whose house had been demolished by his own mistress, after she had ordered it to be pulled down because it disturbed her enjoyment of the beautiful landscape.\nHis popularity with the peasants helped to rebuild the country, as part of the reconstruction program was funded by a land tax paid by the lower social class.\nRelationship with Jews.\nOn 9 October 1334, Casimir confirmed the privileges granted to Jews in 1264 by Boles\u0142aw V the Chaste. Under penalty of death, he prohibited the kidnapping of Jewish children for the purpose of enforced Christian baptism, and he inflicted heavy punishment for the desecration of Jewish cemeteries. While Jews had lived in Poland since before his reign, Casimir allowed them to settle in Poland in great numbers and protected them as \"people of the king\". About 70 percent of the world's European Jews, or Ashkenazi, can trace their ancestry to Poland due to Casimir\u2019s reforms. Casimir's legendary Jewish mistress Esterka remains unconfirmed by direct historical evidence.\nRelationships and children.\nCasimir III was married four times:\nAldona of Lithuania.\nOn 30 April or 16 October 1325, Casimir married Aldona of Lithuania, daughter of Grand Duke Gediminas of Lithuania and Jewna. They had:\nAldona died on 26 May 1339. Casimir remained a widower for two years.\nAdelaide of Hesse.\nOn 29 September 1341, Casimir married his second wife, Adelaide of Hesse. She was a daughter of Henry II, Landgrave of Hesse, and Elizabeth of Meissen. They had no children. Casimir started living separately from Adelaide soon after the marriage. Their loveless marriage lasted until 1356, when he declared himself divorced.\nChristina Rokiczana.\nAfter Casimir \"divorced\" Adelaide he married his mistress Christina Rokiczana, the widow of Miklusz Rokiczani, a wealthy merchant. Her own origins are unknown. Following the death of her first husband she had entered the court of Bohemia in Prague as a lady-in-waiting. Casimir brought her with him from Prague and convinced the abbot of the Benedictine abbey of Tyniec to marry them. The marriage was held in a secret ceremony but soon became known. Queen Adelaide renounced it as bigamous and returned to Hesse. Casimir continued living with Christine despite complaints by Pope Innocent VI on behalf of Queen Adelaide. This marriage lasted until 1363\u201364 when Casimir again declared himself divorced. They had no children.\nHedwig of \u017baga\u0144.\nIn about 1365, Casimir married his fourth wife Hedwig of \u017baga\u0144. She was a daughter of Henry V of Iron, Duke of \u017baga\u0144 and Anna of Mazovia. They had three children:\nAs Adelheid was still alive (and possibly Christina as well), the marriage to Hedwig was also considered bigamous. Because of this, the legitimacy of his three young daughters was disputed. Casimir managed to have Anna and Kunigunde legitimated by Pope Urban V on 5 December 1369. Jadwiga the younger was legitimated by Pope Gregory XI on 11 October 1371 (after Casimir's death).\nTitle and style.\nCasimir's full title was: \"Casimir by the grace of God king of Poland and Rus' (Ruthenia), lord and heir of the land of Krak\u00f3w, Sandomierz, Sieradz, \u0141\u0119czyca, Kuyavia, Pomerania (Pomerelia)\". The title in Latin was: \"Kazimirus, Dei gratia rex Polonie et Russie, nec non Cracovie, Sandomirie, Siradie, Lancicie, Cuiavie, et Pomeranieque Terrarum et Ducatuum Dominus et Heres.\"\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7363", "revid": "45922833", "url": "https://en.wikipedia.org/wiki?curid=7363", "title": "Complexity", "text": "Properties of systems that cannot be simply described or modeled\nComplexity characterises the behaviour of a system or model whose components interact in multiple ways and follow local rules, leading to nonlinearity, randomness, collective dynamics, hierarchy, and emergence.\nThe term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.\nThe intuitive criterion of complexity can be formulated as follows: a system would be more complex if more parts could be distinguished, and if more connections between them existed.\nAs of 2010[ [update]], a number of approaches to characterizing complexity have been used in science; Zayed \"et al.\"\nreflect many of these. Neil Johnson states that \"even among scientists, there is no unique definition of complexity \u2013 and the scientific notion has traditionally been conveyed using particular examples...\" Ultimately Johnson adopts the definition of \"complexity science\" as \"the study of the phenomena which emerge from a collection of interacting objects\".\nOverview.\nDefinitions of complexity often depend on the concept of a \"system\" \u2013 a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.\nWarren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.\nPhenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront \"dealing simultaneously with a sizable number of factors which are interrelated into an organic whole\". Weaver's 1948 paper has influenced subsequent thinking about complexity.\nThe approaches that embody concepts of systems, multiple elements, multiple relational regimes, and state spaces might be summarized as implying that complexity arises from the number of distinguishable relational regimes (and their associated state spaces) in a defined system.\nSome definitions relate to the algorithmic basis for the expression of a complex phenomenon or model or mathematical expression, as later set out herein.\nDisorganized vs. organized.\nOne of the problems in addressing complexity issues has been formalizing the intuitive conceptual distinction between the large number of variances in relationships extant in random collections, and the sometimes large, but smaller, number of relationships between elements in systems where constraints (related to correlation of otherwise independent elements) simultaneously reduce the variations from element independence and create distinguishable regimes of more-uniform, or correlated, relationships, or interactions.\nWeaver perceived and addressed this problem, in at least a preliminary way, in drawing a distinction between \"disorganized complexity\" and \"organized complexity\".\nIn Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.\nA prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits \u2013 the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.\nOrganized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis-a-vis to other systems than the subject system can be said to \"emerge,\" without any \"guiding hand\".\nThe number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.\nSources and factors.\nThere are generally rules which can be invoked to explain the origin of complexity in a given system.\nThe source of disorganized complexity is the large number of parts in the system of interest, and the lack of correlation between elements in the system.\nIn the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.\nComplexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.\nVaried meanings.\nIn several scientific fields, \"complexity\" has a precise meaning:\nOther fields introduce less precisely defined notions of complexity:\nStudy.\nComplexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex \u2013 displaying variation without being random \u2013 is most worthy of interest given the rewards found in the depths of exploration.\nThe use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting \"stovepipes\" and effective \"integrated\" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.\nWhile this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or economic systems, social systems. One such interdisciplinary group of fields is relational order theories.\nTopics.\nBehaviour.\nThe behavior of a complex system is often said to be due to emergence and self-organization. Chaos theory has investigated the sensitivity of systems to variations in initial conditions as one cause of complex behaviour.\nMechanisms.\nRecent developments in artificial life, evolutionary computation and genetic algorithms have led to an increasing emphasis on complexity and complex adaptive systems.\nSimulations.\nIn social science, the study on the emergence of macro-properties from the micro-properties, also known as macro-micro view in sociology. The topic is commonly recognized as social complexity that is often related to the use of computer simulation in social science, i.e.: computational sociology.\nSystems.\nSystems theory has long been concerned with the study of complex systems (in recent times, \"complexity theory\" and \"complex systems\" have also been used as names of the field). These systems are present in the research of a variety disciplines, including biology, economics, social studies and technology. Recently, complexity has become a natural domain of interest of real world socio-cognitive systems and emerging systemics research. Complex systems tend to be high-dimensional, non-linear, and difficult to model. In specific circumstances, they may exhibit low-dimensional behaviour.\nData.\nIn information theory, algorithmic information theory is concerned with the complexity of strings of data.\nComplex strings are harder to compress. While intuition tells us that this may depend on the codec used to compress a string (a codec could be theoretically created in any arbitrary language, including one in which the very small command \"X\" could cause the computer to output a very complicated string like \"18995316\"), any two Turing-complete languages can be implemented in each other, meaning that the length of two encodings in different languages will vary by at most the length of the \"translation\" language \u2013 which will end up being negligible for sufficiently large data strings.\nThese algorithmic measures of complexity tend to assign high values to random noise. However, those studying complex systems would not consider randomness as complexity.\nInformation entropy is also sometimes used in information theory as indicative of complexity, but entropy is also high for randomness. Information fluctuation complexity, fluctuations of information about entropy, does not consider randomness to be complex and has been useful in many applications.\nRecent work in machine learning has examined the complexity of the data as it affects the performance of supervised classification algorithms. Ho and Basu present a set of complexity measures for binary classification problems.\nThe complexity measures broadly cover:\nInstance hardness is a bottom-up approach that first seeks to identify instances that are likely to be misclassified (or, in other words, which instances are the most complex). The characteristics of the instances that are likely to be misclassified are then measured based on the output from a set of hardness measures. The hardness measures are based on several supervised learning techniques such as measuring the number of disagreeing neighbors or the likelihood of the assigned class label given the input features. The information provided by the complexity measures has been examined for use in meta-learning to determine for which data sets filtering (or removing suspected noisy instances from the training set) is the most beneficial and could be expanded to other areas.\nIn molecular recognition.\nA recent study based on molecular simulations and compliance constants describes molecular recognition as a phenomenon of organisation.\nEven for small molecules like carbohydrates, the recognition process can not be predicted or designed even assuming that each individual hydrogen bond's strength is exactly known.\nThe law of requisite complexity.\nDriving from the law of requisite variety, Boisot and McKelvey formulated the \u2018Law of Requisite Complexity\u2019, that holds that, in order to be efficaciously adaptive, the internal complexity of a system must match the external complexity it confronts.\nPositive, appropriate and negative complexity.\nThe application in project management of the Law of Requisite Complexity, as proposed by Stefan Morcov, is the analysis of positive, appropriate and negative complexity.\nIn project management.\nProject complexity is the property of a project which makes it difficult to understand, foresee, and keep under control its overall behavior, even when given reasonably complete information about the project system.\nIn systems engineering.\nMaik Maurer considers complexity as a reality in engineering. He proposed a methodology for managing complexity in systems engineering :\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a01. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Define the system.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a02. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Identify the type of complexity.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a03. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Determine the strategy.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a04. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Determine the method.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a05. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Model the system.\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a06. \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 Implement the method.\nApplications.\nComputational complexity theory is the study of the complexity of problems \u2013 that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm \u2013 usually a computer program \u2013 to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved in time formula_1 (where \"n\" is the size of the network to visit \u2013 the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.\nEven though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.\nThere exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.\nThere is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.\nEmerging applications in other fields.\nThe concept of complexity is being increasingly used in the study of Cosmology, Big History, and Cultural Evolution with increasing granularity, as well as increasing quantification.\nApplication in cosmology.\nEric Chaisson has advanced a cosmoglogical complexity metric which he terms Energy Rate Density. This approach has been expanded in various works, most recently applied to measuring evolving complexity of nation-states and their growing cities.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7365", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7365", "title": "Catholic church", "text": ""}
{"id": "7366", "revid": "39088972", "url": "https://en.wikipedia.org/wiki?curid=7366", "title": "Chastity", "text": "Ethic concept of temperance related to sexuality\nChastity, also known as purity, is a virtue related to temperance. Someone who is \"chaste\" refrains either from sexual activity considered immoral or any sexual activity, according to their state of life. In some contexts, for example when making a vow of chastity, chastity means the same as celibacy.\nEtymology.\nThe words \"chaste\" and \"chastity\" stem from the Latin adjective (\"cut off\", \"separated\", \"pure\"). The words entered the English language around the middle of the 13th century. \"Chaste\" meant \"virtuous\", \"pure from unlawful sexual intercourse\") or (from the early 14th century on) as a noun, a virgin, while \"chastity\" meant \"(sexual) purity\".\nThomas Aquinas links \"(chastity)\" to the Latin verb (\"chastise, reprimand, correct\"), with a reference to Aristotle's \"Nicomachean Ethics\": \"Chastity takes its name from the fact that reason 'chastises' concupiscence, which, like a child, needs curbing, as the Philosopher states\".\nIn Abrahamic religions.\nFor many Jews, Christians, and Muslims, acts of sexual nature are restricted to marriage. For unmarried persons, chastity is equivalent to sexual abstinence. Sexual acts outside of or apart from marriage, such as adultery, fornication, masturbation, and prostitution, are considered immoral due to lust.\nChristianity.\nTraditions.\nIn many Christian traditions, chastity is synonymous with purity. The Catholic Church teaches that chastity involves, in the words of cardinal bishop Alfonso L\u00f3pez Trujillo, \"the successful integration of sexuality within the person and thus the inner unity of man in his bodily and spiritual being\", which according to one's marital status requires either having no sexual relationship, or only having sexual relations with one's spouse. In Western Christian morality, chastity is placed opposite the deadly sin of lust, and is classified as one of seven virtues. The moderation of sexual desires is required to be virtuous. Reason, will, and desire can harmoniously work together to do what is good.\nAs an emblem of inward chastity, some Christians choose to wear a cord, girdle or a cincture of one of the several Confraternities of the Cord or a purity ring. The cord is worn as a symbol of chastity in honour to a chaste saint who is asked for intercession. The purity ring is worn before holy matrimony by those who marry or for the rest of their lives by those who stay single.\nMarital chastity.\nIn marriage, the spouses commit to a lifelong relationship that excludes sexual intimacy with other persons. A third form of chastity, often called \"vidual chastity\", is expected of a woman for a period after her husband dies. For example, Anglican Bishop Jeremy Taylor defined 5 rules in \"Holy Living\" (1650), including abstaining from marrying \"so long as she is with child by her former husband\" and \"within the year of mourning\".\nCelibacy.\nIn the Roman Catholic Church, celibacy is vowed or promised as one of the evangelical counsels by the persons of the consecrated life. Furthermore, in 306, the Synod of Elvira proscribed clergy from marrying. This was unevenly enforced until the Second Lateran Council in 1139 and found its way into canon law. Unmarried deacons promise celibacy to their local bishop when ordained.\nEastern Catholic priests are permitted to marry, provided they do so before ordination and outside monastic life.\nVows of chastity.\n\"Vows of chastity\" can be taken either as part of an organised religious life (such as Roman Catholic Beguines and Beghards in the past) or on an individual basis: as a voluntary act of devotion, or as part of an ascetic lifestyle (often devoted to contemplation), or both. Some Protestant religious communities, such as the Bruderhof, take vows of chastity as part of the church membership process.\nTeaching by denomination.\nCatholicism.\nChastity is a central and pivotal concept in Roman Catholic praxis. Chastity's importance in traditional Roman Catholic teaching stems from the fact that it is regarded as essential in maintaining and cultivating the unity of body with spirit and thus the integrity of the human being. It is also regarded as fundamental to the practise of the Catholic life because it involves an \"apprenticeship in self-mastery\". By attaining mastery over one's passions, reason, will, and desire can harmoniously work together to do what is good.\nLutheranism.\nThe theology of the body of the Lutheran Churches emphasizes the role of the Holy Spirit, who has sanctified the bodies of Christians to be God's temple.\nMany Lutheran monks and Lutheran nuns practice celibacy, though in other Lutheran religious orders it is not compulsory.\nThe Church of Jesus Christ of Latter-Day Saints.\nIn the Church of Jesus Christ of Latter-day Saints chastity is very important:\n\"Physical intimacy between husband and wife is a beautiful and sacred part of God's plan for His children. It is an expression of love within marriage and allows husband and wife to participate in the creation of life. God has commanded that this sacred power be expressed only between a man and a woman who are legally married. The law of chastity applies to both men and women. It includes strict abstinence from sexual relations before marriage and complete fidelity and loyalty to one's spouse after marriage.\"\n\"The law of chastity requires that sexual relations be reserved for marriage between a man and a woman.\n\"In addition to reserving sexual intimacy for marriage, we obey the law of chastity by controlling our thoughts, words, and actions. Jesus Christ taught, \"Ye have heard that it was said by them of old time, Thou shalt not commit adultery: but I say unto you, That whosoever looketh on a woman to lust after her hath committed adultery with her already in his heart\" (Matthew 5:27\u201328).\"\nTeachings of the Church of Jesus Christ of Latter-Day Saints also include that sexual expression within marriage is an important dimension of spousal bonding apart from but not necessarily avoiding its procreative result.\nIslam.\nQuran.\nThe most famous personal example of chastity in the Quran is the Virgin Mary (Mariam):\n\"And [mention] the one who guarded her chastity, so We blew into her [garment] through Our angel [Gabriel], and We made her and her son a sign for the worlds.\" (21 :91)\n\"And she took, in seclusion from them, a screen. Then We sent to her Our Angel, and he represented himself to her as a well-proportioned man. She said, \"Indeed, I seek refuge in the Most Merciful from you, [so leave me], if you should be fearing of Allah.\" He said, \"I am only the messenger of your Lord to give you [news of] a pure boy.\" She said, \"How can I have a boy while no man has touched me and I have not been unchaste?\"\" (19:17\u201320)\nExtramarital sex is forbidden. The Quran says:\n\"And do not approach unlawful sexual intercourse. Indeed, it is ever an immorality and is evil as a way.\" (17:32)\n\"And those who do not invoke with Allah another deity or kill the soul which Allah has forbidden [to be killed], except by right, and do not commit unlawful sexual intercourse And whoever should do that will meet a penalty. Multiplied for him is the punishment on the Day of Resurrection, and he will abide therein humiliated \u2013 Except for those who repent, believe and do righteous work. For them Allah will replace their evil deeds with good. And ever is Allah Forgiving and Merciful.\" (25:68\u201370)\nIn a list of commendable deeds the Quran says:\n\"Indeed, the Muslim men and Muslim women, the believing men and believing women, the obedient men and obedient women, the truthful men and truthful women, the patient men and patient women, the humble men and humble women, the charitable men and charitable women, the fasting men and fasting women, the men who guard their private parts and the women who do so, and the men who remember Allah often and the women who do so \u2013 for them Allah has prepared forgiveness and a great reward.\" (33:35)\nBecause the sex desire is usually attained before a man is financially capable of marriage, the love to God and mindfulness of Him should be sufficient motive for chastity:\n\"But let them who find not [the means for] marriage abstain [from sexual relations] until Allah enriches them from His bounty. And those who seek a contract [for eventual emancipation] from among whom your right hands possess \u2013 then make a contract with them if you know there is within them goodness and give them from the wealth of Allah which He has given you. And do not compel your slave girls to prostitution, if they desire chastity, to seek [thereby] the temporary interests of worldly life. And if someone should compel them, then indeed, Allah is [to them], after their compulsion, Forgiving and Merciful.\" (24:33)\nSharia (Law).\nChastity is mandatory in Islam. Sex outside legitimacy is prohibited, for both men and women whether married or unmarried. The injunctions and forbiddings in Islam apply equally to men and women. The legal punishment for adultery is equal for men and women. Social hypocrisy in many societies over history had led to a double standard when considering sin committed by men versus sin committed by women. Society tended to be more lenient and permissive towards men forgiving men for sins not forgivable when women do them.\nThe prophet's prescription to the youth was:\nThose of you who own the means should marry for this should keep their eyes uncraving and their chastity secure. Those who don't, may practise fasting for it curbs desire. \" (Ibn Massoud)\nChastity is an attitude and a way of life. In Islam it is both a personal and a social value. A Muslim society should not condone relations entailing or conducive to sexual license. Social patterns and practices calculated to inflame sexual desire are frowned upon by Islam, such incitements to immorality including permissive ideologies, titillating works of art and the failure to inculcate sound moral principles in the young. At the heart of such a view of human sexuality lies the conviction that the notion of personal freedom should never be misconstrued as the freedom to flout God's laws by overstepping the bounds which, in His infinite wisdom, He has set upon the relations of the sexes.\nBah\u00e1\u02bc\u00ed Faith.\nChastity is highly prized in the Bah\u00e1\u02bc\u00ed Faith. Similar to other Abrahamic religions, Bah\u00e1\u02bc\u00ed teachings call for the restriction of sexual activity to that between a wife and husband in Bah\u00e1\u02bc\u00ed marriage, and discourage members from using pornography or engaging in sexually explicit recreational activities. The concept of chastity is extended to include avoidance of alcohol and mind-altering drugs, profanity, and gaudy or immodest attire.\nIn Eastern religions.\nHinduism.\nHinduism's view on premarital sex is rooted in its concept of Ashrama (stage) or the stages of life. The first of these stages, known as \"Brahmacharya,\" roughly translates as chastity. Celibacy and chastity are considered the appropriate behavior for both male and female students during this stage, which precedes the stage of the married householder (Grihastha). Sanyasis and Hindu monks or Sadhus are also celibate as part of their ascetic discipline.\nSikhism.\nIn Sikhism, premarital or extramarital sex is strictly forbidden. However, it is encouraged to marry and live as a family unit to provide and nurture children for the perpetual benefit of creation (as opposed to Sannyasa or living as a monk, which was, and remains, a common spiritual practice in India). A Sikh is encouraged not to live as a recluse, beggar, monk, nun, celibate, or in any similar vein.\nJainism.\nThe Jain ethical code contains the vow of brahmacarya (meaning \"pure conduct\"), which prescribes the expectations for Jains concerning sexual activity. Brahmacarya is one of the five major and minor vows of Jainism, prescribing slightly different expectations for ascetics and laypeople, respectively.\nComplete celibacy is expected only of Jain ascetics (who are also referred to as monks and nuns). For laypeople, chastity is expected, with extramarital sex and adultery being prohibited.\nBuddhism.\nThe teachings of Buddhism include the Noble Eightfold Path, comprising a division called right action. Under the Five Precepts ethical code, Up\u0101saka and Up\u0101sik\u0101 lay followers should abstain from sexual misconduct, while Bhikkhu and Bhikkhuni monastics should practice strict chastity.\nTaoism.\nThe Five Precepts of the Taoist religion include No Sexual Misconduct, which is interpreted as prohibiting extramarital sex for lay practitioners and marriage or sexual intercourse for monks and nuns.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7372", "revid": "18048", "url": "https://en.wikipedia.org/wiki?curid=7372", "title": "Causes of heterosexuality", "text": ""}
{"id": "7375", "revid": "1754504", "url": "https://en.wikipedia.org/wiki?curid=7375", "title": "Causes of sexual orientation", "text": ""}
{"id": "7376", "revid": "212526", "url": "https://en.wikipedia.org/wiki?curid=7376", "title": "Cosmic microwave background", "text": "Electromagnetic radiation as a remnant from an early stage of the universe in Big Bang cosmology\nThe cosmic microwave background (CMB, CMBR) is microwave radiation that fills all space in the observable universe. It is a remnant that provides an important source of data on the primordial universe. With a standard optical telescope, the background space between stars and galaxies is almost completely dark. However, a sufficiently sensitive radio telescope detects a faint background glow that is almost uniform and is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of the CMB in 1965 by American radio astronomers Arno Penzias and Robert Wilson was the culmination of work initiated in the 1940s.\nCMB is landmark evidence of the Big Bang theory for the origin of the universe. In the Big Bang cosmological models, during the earliest periods, the universe was filled with an opaque fog of dense, hot plasma of sub-atomic particles. As the universe expanded, this plasma cooled to the point where protons and electrons combined to form neutral atoms of mostly hydrogen. Unlike the plasma, these atoms could not scatter thermal radiation by Thomson scattering, and so the universe became transparent. Known as the recombination epoch, this decoupling event released photons to travel freely through space \u2013 sometimes referred to as \"relic radiation\". However, the photons have grown less energetic, since the expansion of space causes their wavelength to increase. The \"surface of last scattering\" refers to a shell at the right distance in space so photons are now received that were originally emitted at the time of decoupling.\nThe CMB is not completely smooth and uniform, showing a faint anisotropy that can be mapped by sensitive detectors. Ground and space-based experiments such as COBE and WMAP have been used to measure these temperature inhomogeneties. The anisotropy structure is determined by various interactions of matter and photons up to the point of decoupling, which results in a characteristic lumpy pattern that varies with angular scale. The distribution of the anisotropy across the sky has frequency components that can be represented by a power spectrum displaying a sequence of peaks and valleys. The peak values of this spectrum hold important information about the physical properties of the early universe: the first peak determines the overall curvature of the universe, while the second and third peak detail the density of normal matter and so-called dark matter, respectively. Extracting fine details from the CMB data can be challenging, since the emission has undergone modification by foreground features such as galaxy clusters.\nImportance of precise measurement.\nPrecise measurements of the CMB are critical to cosmology, since any proposed model of the universe must explain this radiation. The CMB has a thermal black body spectrum at a temperature of . The spectral radiance \"dE\"\"\u03bd\"/\"d\u03bd\" peaks at 160.23\u00a0GHz, in the microwave range of frequencies, corresponding to a photon energy of about . Alternatively, if spectral radiance is defined as \"dE\"\"\u03bb\"/\"d\u03bb\", then the peak wavelength is 1.063\u00a0mm (282\u00a0GHz, photons). The glow is very nearly uniform in all directions, but the tiny residual variations show a very specific pattern, the same as that expected of a fairly uniformly distributed hot gas that has expanded to the current size of the universe. In particular, the spectral radiance at different angles of observation in the sky contains small anisotropies, or irregularities, which vary with the size of the region examined. They have been measured in detail, and match what would be expected if small thermal variations, generated by quantum fluctuations of matter in a very tiny space, had expanded to the size of the observable universe we see today. This is a very active field of study, with scientists seeking both better data (for example, the Planck spacecraft) and better interpretations of the initial conditions of expansion. Although many different processes might produce the general form of a black body spectrum, no model other than the Big Bang has yet explained the fluctuations. As a result, most cosmologists consider the Big Bang model of the universe to be the best explanation for the CMB.\nThe high degree of uniformity throughout the observable universe and its faint but measured anisotropy lend strong support for the Big Bang model in general and the \u039bCDM (\"Lambda Cold Dark Matter\") model in particular. Moreover, the fluctuations are coherent on angular scales that are larger than the apparent cosmological horizon at recombination. Either such coherence is acausally fine-tuned, or cosmic inflation occurred.&lt;ref name=\"hep-ph/0309057\"&gt;&lt;/ref&gt;\nOther than the temperature and polarization anisotropy, the CMB frequency spectrum is expected to feature tiny departures from the black-body law known as spectral distortions. These are also at the focus of an active research effort with the hope of a first measurement within the forthcoming decades, as they contain a wealth of information about the primordial universe and the formation of structures at late time.\nFeatures.\nThe cosmic microwave background radiation is an emission of uniform, black body thermal energy coming from all parts of the sky. The radiation is isotropic to roughly one part in 100,000: the root mean square variations are only 18\u00a0\u03bcK, after subtracting out a dipole anisotropy from the Doppler shift of the background radiation. The latter is caused by the peculiar velocity of the Sun relative to the comoving cosmic rest frame as it moves at 369.82\u00a0\u00b1\u00a00.11\u00a0km/s towards the constellation Leo (galactic longitude 264.021\u00a0\u00b1\u00a00.011, galactic latitude 48.253\u00a0\u00b1\u00a00.005). The CMB dipole and aberration at higher multipoles have been measured, consistent with galactic motion.\nIn the Big Bang model for the formation of the universe, inflationary cosmology predicts that after about 10\u221237 seconds the nascent universe underwent exponential growth that smoothed out nearly all irregularities. The remaining irregularities were caused by quantum fluctuations in the inflation field that caused the inflation event. Long before the formation of stars and planets, the early universe was smaller, much hotter and, starting 10\u22126 seconds after the Big Bang, filled with a uniform glow from its white-hot fog of interacting plasma of photons, electrons, and baryons.\nAs the universe expanded, adiabatic cooling caused the energy density of the plasma to decrease until it became favorable for electrons to combine with protons, forming hydrogen atoms. This recombination event happened when the temperature was around 3000\u00a0K or when the universe was approximately 379,000\u00a0years old. As photons did not interact with these electrically neutral atoms, the former began to travel freely through space, resulting in the decoupling of matter and radiation.\nThe color temperature of the ensemble of decoupled photons has continued to diminish ever since; now down to , it will continue to drop as the universe expands. The intensity of the radiation corresponds to black-body radiation at 2.726\u00a0K because red-shifted black-body radiation is just like black-body radiation at a lower temperature. According to the Big Bang model, the radiation from the sky we measure today comes from a spherical surface called \"the surface of last scattering\". This represents the set of locations in space at which the decoupling event is estimated to have occurred and at a point in time such that the photons from that distance have just reached observers. Most of the radiation energy in the universe is in the cosmic microwave background, making up a fraction of roughly of the total density of the universe.\nTwo of the greatest successes of the Big Bang theory are its prediction of the almost perfect black body spectrum and its detailed prediction of the anisotropies in the cosmic microwave background. The CMB spectrum has become the most precisely measured black body spectrum in nature.\nThe energy density of the CMB is which yields about 411 photons/cm3.\nHistory.\nThe cosmic microwave background was first predicted in 1948 by Ralph Alpher and Robert Herman, in close relation to work performed by Alpher's PhD advisor George Gamow. Alpher and Herman were able to estimate the temperature of the cosmic microwave background to be 5\u00a0K, though two years later they re-estimated it at 28\u00a0K. This high estimate was due to a misestimate of the Hubble constant by Alfred Behr, which could not be replicated and was later abandoned for the earlier estimate. Although there were several previous estimates of the temperature of space, these estimates had two flaws. First, they were measurements of the effective temperature of space and did not suggest that space was filled with a thermal Planck spectrum. Next, they depend on our being at a special spot at the edge of the Milky Way galaxy and they did not suggest the radiation is isotropic. The estimates would yield very different predictions if Earth happened to be located elsewhere in the universe.\nThe 1948 results of Alpher and Herman were discussed in many physics settings through about 1955, when both left the Applied Physics Laboratory at Johns Hopkins University. The mainstream astronomical community, however, was not intrigued at the time by cosmology. Alpher and Herman's prediction was rediscovered by Yakov Zel'dovich in the early 1960s, and independently predicted by Robert Dicke at the same time. The first published recognition of the CMB radiation as a detectable phenomenon appeared in a brief paper by Soviet astrophysicists A. G. Doroshkevich and Igor Novikov, in the spring of 1964. In 1964, David Todd Wilkinson and Peter Roll, Dicke's colleagues at Princeton University, began constructing a Dicke radiometer to measure the cosmic microwave background. In 1964, Arno Penzias and Robert Woodrow Wilson at the Crawford Hill location of Bell Telephone Laboratories in nearby Holmdel Township, New Jersey had built a Dicke radiometer that they intended to use for radio astronomy and satellite communication experiments. On 20\u00a0May 1964 they made their first measurement clearly showing the presence of the microwave background, with their instrument having an excess 4.2K antenna temperature which they could not account for. After receiving a telephone call from Crawford Hill, Dicke said \"Boys, we've been scooped.\" A meeting between the Princeton and Crawford Hill groups determined that the antenna temperature was indeed due to the microwave background. Penzias and Wilson received the 1978 Nobel Prize in Physics for their discovery.\nThe interpretation of the cosmic microwave background was a controversial issue in the 1960s with some proponents of the steady state theory arguing that the microwave background was the result of scattered starlight from distant galaxies. Using this model, and based on the study of narrow absorption line features in the spectra of stars, the astronomer Andrew McKellar wrote in 1941: \"It can be calculated that the 'rotational temperature' of interstellar space is 2\u00a0K.\" However, during the 1970s the consensus was established that the cosmic microwave background is a remnant of the big bang. This was largely because new measurements at a range of frequencies showed that the spectrum was a thermal, black body spectrum, a result that the steady state model was unable to reproduce.\nHarrison, Peebles, Yu and Zel'dovich realized that the early universe would require inhomogeneities at the level of 10\u22124 or 10\u22125. Rashid Sunyaev later calculated the observable imprint that these inhomogeneities would have on the cosmic microwave background. Increasingly stringent limits on the anisotropy of the cosmic microwave background were set by ground-based experiments during the 1980s. RELIKT-1, a Soviet cosmic microwave background anisotropy experiment on board the Prognoz 9 satellite (launched 1 July 1983) gave upper limits on the large-scale anisotropy. The NASA COBE mission clearly confirmed the primary anisotropy with the Differential Microwave Radiometer instrument, publishing their findings in 1992. The team received the Nobel Prize in physics for 2006 for this discovery.\nInspired by the COBE results, a series of ground and balloon-based experiments measured cosmic microwave background anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the scale of the first acoustic peak, which COBE did not have sufficient resolution to resolve. This peak corresponds to large scale density variations in the early universe that are created by gravitational instabilities, resulting in acoustical oscillations in the plasma. The first peak in the anisotropy was tentatively detected by the Toco experiment and the result was confirmed by the BOOMERanG and MAXIMA experiments. These measurements demonstrated that the geometry of the universe is approximately flat, rather than curved. They ruled out cosmic strings as a major component of cosmic structure formation and suggested cosmic inflation was the right theory of structure formation.\nThe second peak was tentatively detected by several experiments before being definitively detected by WMAP, which has tentatively detected the third peak. As of 2010, several experiments to improve measurements of the polarization and the microwave background on small angular scales are ongoing. These include DASI, WMAP, BOOMERanG, QUaD, Planck spacecraft, Atacama Cosmology Telescope, South Pole Telescope and the QUIET telescope.\nRelationship to the Big Bang.\nThe cosmic microwave background radiation and the cosmological redshift-distance relation are together regarded as the best available evidence for the Big Bang event. Measurements of the CMB have made the inflationary Big Bang model the Standard Cosmological Model. The discovery of the CMB in the mid-1960s curtailed interest in alternatives such as the steady state theory.\nIn the late 1940s Alpher and Herman reasoned that if there was a Big Bang, the expansion of the universe would have stretched the high-energy radiation of the very early universe into the microwave region of the electromagnetic spectrum, and down to a temperature of about 5\u00a0K. They were slightly off with their estimate, but they had the right idea. They predicted the CMB. It took another 15 years for Penzias and Wilson to discover that the microwave background was actually there.\nAccording to standard cosmology, the CMB gives a snapshot of the hot early universe at the point in time when the temperature dropped enough to allow electrons and protons to form hydrogen atoms. This event made the universe nearly transparent to radiation because light was no longer being scattered off free electrons. When this occurred some 380,000 years after the Big Bang, the temperature of the universe was about 3,000\u00a0K. This corresponds to an ambient energy of about , which is much less than the ionization energy of hydrogen. This epoch is generally known as the \"time of last scattering\" or the period of recombination or decoupling.\nSince decoupling, the color temperature of the background radiation has dropped by an average factor of 1,089 due to the expansion of the universe. As the universe expands, the CMB photons are redshifted, causing them to decrease in energy. The color temperature of this radiation stays inversely proportional to a parameter that describes the relative expansion of the universe over time, known as the scale length. The color temperature \"T\"r of the CMB as a function of redshift, \"z\", can be shown to be proportional to the color temperature of the CMB as observed in the present day (2.725\u00a0K or 0.2348\u00a0meV):\n\"T\"r = 2.725\u00a0K \u00d7 (1 + \"z\")\nFor details about the reasoning that the radiation is evidence for the Big Bang, see Cosmic background radiation of the Big Bang.\nPrimary anisotropy.\nThe anisotropy, or directional dependency, of the cosmic microwave background is divided into two types: primary anisotropy, due to effects that occur at the surface of last scattering and before; and secondary anisotropy, due to effects such as interactions of the background radiation with intervening hot gas or gravitational potentials, which occur between the last scattering surface and the observer.\nThe structure of the cosmic microwave background anisotropies is principally determined by two effects: acoustic oscillations and diffusion damping (also called collisionless damping or Silk damping). The acoustic oscillations arise because of a conflict in the photon\u2013baryon plasma in the early universe. The pressure of the photons tends to erase anisotropies, whereas the gravitational attraction of the baryons, moving at speeds much slower than light, makes them tend to collapse to form overdensities. These two effects compete to create acoustic oscillations, which give the microwave background its characteristic peak structure. The peaks correspond, roughly, to resonances in which the photons decouple when a particular mode is at its peak amplitude.\nThe peaks contain interesting physical signatures. The angular scale of the first peak determines the curvature of the universe (but not the topology of the universe). The next peak\u2014ratio of the odd peaks to the even peaks\u2014determines the reduced baryon density. The third peak can be used to get information about the dark-matter density.\nThe locations of the peaks give important information about the nature of the primordial density perturbations. There are two fundamental types of density perturbations called \"adiabatic\" and \"isocurvature\". A general density perturbation is a mixture of both, and different theories that purport to explain the primordial density perturbation spectrum predict different mixtures.\nThe CMB spectrum can distinguish between these two because these two types of perturbations produce different peak locations. Isocurvature density perturbations produce a series of peaks whose angular scales (\"\u2113\" values of the peaks) are roughly in the ratio 1\u00a0:\u00a03\u00a0:\u00a05\u00a0:\u00a0..., while adiabatic density perturbations produce peaks whose locations are in the ratio 1\u00a0:\u00a02\u00a0:\u00a03\u00a0:\u00a0... Observations are consistent with the primordial density perturbations being entirely adiabatic, providing key support for inflation, and ruling out many models of structure formation involving, for example, cosmic strings.\nCollisionless damping is caused by two effects, when the treatment of the primordial plasma as fluid begins to break down:\nThese effects contribute about equally to the suppression of anisotropies at small scales and give rise to the characteristic exponential damping tail seen in the very small angular scale anisotropies.\nThe depth of the LSS refers to the fact that the decoupling of the photons and baryons does not happen instantaneously, but instead requires an appreciable fraction of the age of the universe up to that era. One method of quantifying how long this process took uses the \"photon visibility function\" (PVF). This function is defined so that, denoting the PVF by \"P\"(\"t\"), the probability that a CMB photon last scattered between time \"t\" and \"t\" + \"dt\" is given by \"P\"(\"t\")\u2009\"dt\".\nThe maximum of the PVF (the time when it is most likely that a given CMB photon last scattered) is known quite precisely. The first-year WMAP results put the time at which \"P\"(\"t\") has a maximum as 372,000 years. This is often taken as the \"time\" at which the CMB formed. However, to figure out how long it took the photons and baryons to decouple, we need a measure of the width of the PVF. The WMAP team finds that the PVF is greater than half of its maximal value (the \"full width at half maximum\", or FWHM) over an interval of 115,000 years. By this measure, decoupling took place over roughly 115,000 years, and when it was complete, the universe was roughly 487,000 years old.\nLate time anisotropy.\nSince the CMB came into existence, it has apparently been modified by several subsequent physical processes, which are collectively referred to as late-time anisotropy, or secondary anisotropy. When the CMB photons became free to travel unimpeded, ordinary matter in the universe was mostly in the form of neutral hydrogen and helium atoms. However, observations of galaxies today seem to indicate that most of the volume of the intergalactic medium (IGM) consists of ionized material (since there are few absorption lines due to hydrogen atoms). This implies a period of reionization during which some of the material of the universe was broken into hydrogen ions.\nThe CMB photons are scattered by free charges such as electrons that are not bound in atoms. In an ionized universe, such charged particles have been liberated from neutral atoms by ionizing (ultraviolet) radiation. Today these free charges are at sufficiently low density in most of the volume of the universe that they do not measurably affect the CMB. However, if the IGM was ionized at very early times when the universe was still denser, then there are two main effects on the CMB:\nBoth of these effects have been observed by the WMAP spacecraft, providing evidence that the universe was ionized at very early times, at a redshift more than 17. The detailed provenance of this early ionizing radiation is still a matter of scientific debate. It may have included starlight from the very first population of stars (population III stars), supernovae when these first stars reached the end of their lives, or the ionizing radiation produced by the accretion disks of massive black holes.\nThe time following the emission of the cosmic microwave background\u2014and before the observation of the first stars\u2014is semi-humorously referred to by cosmologists as the Dark Age, and is a period which is under intense study by astronomers (see 21 centimeter radiation).\nTwo other effects which occurred between reionization and our observations of the cosmic microwave background, and which appear to cause anisotropies, are the Sunyaev\u2013Zeldovich effect, where a cloud of high-energy electrons scatters the radiation, transferring some of its energy to the CMB photons, and the Sachs\u2013Wolfe effect, which causes photons from the Cosmic Microwave Background to be gravitationally redshifted or blueshifted due to changing gravitational fields.\nPolarization.\nThe cosmic microwave background is polarized at the level of a few microkelvin. There are two types of polarization, called E-modes and B-modes. This is in analogy to electrostatics, in which the electric field (\"E\"-field) has a vanishing curl and the magnetic field (\"B\"-field) has a vanishing divergence. The E-modes arise naturally from Thomson scattering in a heterogeneous plasma. The B-modes are not produced by standard scalar type perturbations. Instead they can be created by two mechanisms: the first one is by gravitational lensing of E-modes, which has been measured by the South Pole Telescope in 2013; the second one is from gravitational waves arising from cosmic inflation. Detecting the B-modes is extremely difficult, particularly as the degree of foreground contamination is unknown, and the weak gravitational lensing signal mixes the relatively strong E-mode signal with the B-mode signal.\nE-modes.\nE-modes were first seen in 2002 by the Degree Angular Scale Interferometer (DASI).\nB-modes.\nCosmologists predict two types of B-modes, the first generated during cosmic inflation shortly after the big bang, and the second generated by gravitational lensing at later times.\nPrimordial gravitational waves.\nPrimordial gravitational waves are gravitational waves that could be observed in the polarisation of the cosmic microwave background and having their origin in the early universe. Models of cosmic inflation predict that such gravitational waves should appear; thus, their detection supports the theory of inflation, and their strength can confirm and exclude different models of inflation. It is the result of three things: inflationary expansion of space itself, reheating after inflation, and turbulent fluid mixing of matter and radiation.\nOn 17 March 2014, it was announced that the BICEP2 instrument had detected the first type of B-modes, consistent with inflation and gravitational waves in the early universe at the level of \"r\" =, which is the amount of power present in gravitational waves compared to the amount of power present in other scalar density perturbations in the very early universe. Had this been confirmed it would have provided strong evidence for cosmic inflation and the Big Bang and against the ekpyrotic model of Paul Steinhardt and Neil Turok. However, on 19 June 2014, considerably lowered confidence in confirming the findings was reported\nand on 19 September 2014, new results of the Planck experiment reported that the results of BICEP2 can be fully attributed to cosmic dust.\nGravitational lensing.\nThe second type of B-modes was discovered in 2013 using the South Pole Telescope with help from the Herschel Space Observatory. In October 2014, a measurement of the B-mode polarization at 150\u00a0GHz was published by the POLARBEAR experiment. Compared to BICEP2, POLARBEAR focuses on a smaller patch of the sky and is less susceptible to dust effects. The team reported that POLARBEAR's measured B-mode polarization was of cosmological origin (and not just due to dust) at a 97.2% confidence level.\nMicrowave background observations.\nSubsequent to the discovery of the CMB, hundreds of cosmic microwave background experiments have been conducted to measure and characterize the signatures of the radiation. The most famous experiment is probably the NASA Cosmic Background Explorer (COBE) satellite that orbited in 1989\u20131996 and which detected and quantified the large scale anisotropies at the limit of its detection capabilities. Inspired by the initial COBE results of an extremely isotropic and homogeneous background, a series of ground- and balloon-based experiments quantified CMB anisotropies on smaller angular scales over the next decade. The primary goal of these experiments was to measure the angular scale of the first acoustic peak, for which COBE did not have sufficient resolution. These measurements were able to rule out cosmic strings as the leading theory of cosmic structure formation, and suggested cosmic inflation was the right theory.\nDuring the 1990s, the first peak was measured with increasing sensitivity and by 2000 the BOOMERanG experiment reported that the highest power fluctuations occur at scales of approximately one degree. Together with other cosmological data, these results implied that the geometry of the universe is flat. A number of ground-based interferometers provided measurements of the fluctuations with higher accuracy over the next three years, including the Very Small Array, Degree Angular Scale Interferometer (DASI), and the Cosmic Background Imager (CBI). DASI made the first detection of the polarization of the CMB and the CBI provided the first E-mode polarization spectrum with compelling evidence that it is out of phase with the T-mode spectrum.\nIn June 2001, NASA launched a second CMB space mission, WMAP, to make much more precise measurements of the large scale anisotropies over the full sky. WMAP used symmetric, rapid-multi-modulated scanning, rapid switching radiometers to minimize non-sky signal noise. The first results from this mission, disclosed in 2003, were detailed measurements of the angular power spectrum at a scale of less than one degree, tightly constraining various cosmological parameters. The results are broadly consistent with those expected from cosmic inflation as well as various other competing theories, and are available in detail at NASA's data bank for Cosmic Microwave Background (CMB) (see links below). Although WMAP provided very accurate measurements of the large scale angular fluctuations in the CMB (structures about as broad in the sky as the moon), it did not have the angular resolution to measure the smaller scale fluctuations which had been observed by former ground-based interferometers.\nA third space mission, the ESA (European Space Agency) Planck Surveyor, was launched in May 2009 and performed an even more detailed investigation until it was shut down in October 2013. Planck employed both HEMT radiometers and bolometer technology and measured the CMB at a smaller scale than WMAP. Its detectors were trialled in the Antarctic Viper telescope as ACBAR (Arcminute Cosmology Bolometer Array Receiver) experiment\u2014which has produced the most precise measurements at small angular scales to date\u2014and in the Archeops balloon telescope.\nOn 21 March 2013, the European-led research team behind the Planck cosmology probe released the mission's all-sky map (565x318 jpeg, 3600x1800 jpeg) of the cosmic microwave background. The map suggests the universe is slightly older than researchers expected. According to the map, subtle fluctuations in temperature were imprinted on the deep sky when the cosmos was about years old. The imprint reflects ripples that arose as early, in the existence of the universe, as the first nonillionth of a second. Apparently, these ripples gave rise to the present vast cosmic web of galaxy clusters and dark matter. Based on the 2013 data, the universe contains 4.9% ordinary matter, 26.8% dark matter and 68.3% dark energy. On 5 February 2015, new data was released by the Planck mission, according to which the age of the universe is billion years old and the Hubble constant was measured to be .\nAdditional ground-based instruments such as the South Pole Telescope in Antarctica and the proposed Clover Project, Atacama Cosmology Telescope and the QUIET telescope in Chile will provide additional data not available from satellite observations, possibly including the B-mode polarization.\nData reduction and analysis.\nRaw CMBR data, even from space vehicles such as WMAP or Planck, contain foreground effects that completely obscure the fine-scale structure of the cosmic microwave background. The fine-scale structure is superimposed on the raw CMBR data but is too small to be seen at the scale of the raw data. The most prominent of the foreground effects is the dipole anisotropy caused by the Sun's motion relative to the CMBR background. The dipole anisotropy and others due to Earth's annual motion relative to the Sun and numerous microwave sources in the galactic plane and elsewhere must be subtracted out to reveal the extremely tiny variations characterizing the fine-scale structure of the CMBR background.\nThe detailed analysis of CMBR data to produce maps, an angular power spectrum, and ultimately cosmological parameters is a complicated, computationally difficult problem. Although computing a power spectrum from a map is in principle a simple Fourier transform, decomposing the map of the sky into spherical harmonics,\nformula_1\nwhere the formula_2 term measures the mean temperature and formula_3 term accounts for the fluctuation, where the formula_4 refers to a spherical harmonic, and \"\u2113\" is the multipole number while \"m\" is the azimuthal number.\nBy applying the angular correlation function, the sum can be reduced to an expression that only involves \"\u2113\" and power spectrum term\u00a0formula_5 The angled brackets indicate the average with respect to all observers in the universe; since the universe is homogeneous and isotropic, therefore there is an absence of preferred observing direction. Thus, \"C\" is independent of \"m\". Different choices of \"\u2113\" correspond to multipole moments of CMB.\nIn practice it is hard to take the effects of noise and foreground sources into account. In particular, these foregrounds are dominated by galactic emissions such as Bremsstrahlung, synchrotron, and dust that emit in the microwave band; in practice, the galaxy has to be removed, resulting in a CMB map that is not a full-sky map. In addition, point sources like galaxies and clusters represent another source of foreground which must be removed so as not to distort the short scale structure of the CMB power spectrum.\nConstraints on many cosmological parameters can be obtained from their effects on the power spectrum, and results are often calculated using Markov chain Monte Carlo sampling techniques.\nCMBR monopole term (\"\u2113\" = 0).\nWhen \"\u2113\" = 0, the formula_3 term reduced to 1, and what we have left here is just the mean temperature of the CMB. This \"mean\" is called CMB monopole, and it is observed to have an average temperature of about \"T\"\"\u03b3\" = with one standard deviation confidence. The accuracy of this mean temperature may be impaired by the diverse measurements done by different mapping measurements. Such measurements demand absolute temperature devices, such as the FIRAS instrument on the COBE satellite. The measured \"kT\u03b3\" is equivalent to 0.234\u00a0meV or . The photon number density of a blackbody having such temperature is formula_7. Its energy density is formula_8,}} and the ratio to the critical density is \u03a9\"\u03b3\" = 5.38\u00a0\u00d7\u00a010\u22125.\nCMBR dipole anisotropy (\"\u2113\" = 1).\nCMB dipole represents the largest anisotropy, which is in the first spherical harmonic (\"\u2113\" = 1). When \"\u2113\" = 1, the formula_3 term reduces to one cosine function and thus encodes amplitude fluctuation. The amplitude of CMB dipole is around . Since the universe is presumed to be homogeneous and isotropic, an observer should see the blackbody spectrum with temperature \"T\" at every point in the sky. The spectrum of the dipole has been confirmed to be the differential of a blackbody spectrum.\nCMB dipole is frame-dependent. The CMB dipole moment could also be interpreted as the peculiar motion of the Earth toward the CMB. Its amplitude depends on the time due to the Earth's orbit about the barycenter of the solar system. This enables us to add a time-dependent term to the dipole expression. The modulation of this term is 1 year, which fits the observation done by COBE FIRAS. The dipole moment does not encode any primordial information.\nFrom the CMB data, it is seen that the Sun appears to be moving at relative to the reference frame of the CMB (also called the CMB rest frame, or the frame of reference in which there is no motion through the CMB). The Local Group\u200a\u2014\u200athe galaxy group that includes our own Milky Way galaxy\u200a\u2014\u200aappears to be moving at in the direction of galactic longitude \"\u2113\" =, \"b\" =. This motion results in an anisotropy of the data (CMB appearing slightly warmer in the direction of movement than in the opposite direction). The standard interpretation of this temperature variation is a simple velocity redshift and blueshift due to motion relative to the CMB, but alternative cosmological models can explain some fraction of the observed dipole temperature distribution in the CMB.\nA 2021 study of Wide-field Infrared Survey Explorer questions the kinematic interpretation of CMB anisotropy with high statistical confidence.\nMultipole (\"\u2113\" \u2265 2).\nThe temperature variation in the CMB temperature maps at higher multipoles, or \"\u2113\" \u2265 2, is considered to be the result of perturbations of the density in the early Universe, before the recombination epoch. Before recombination, the Universe consisted of a hot, dense plasma of electrons and baryons. In such a hot dense environment, electrons and protons could not form any neutral atoms. The baryons in such early Universe remained highly ionized and so were tightly coupled with photons through the effect of Thompson scattering. These phenomena caused the pressure and gravitational effects to act against each other, and triggered fluctuations in the photon-baryon plasma. Quickly after the recombination epoch, the rapid expansion of the universe caused the plasma to cool down and these fluctuations are \"frozen into\" the CMB maps we observe today. The said procedure happened at a redshift of around\u00a0\"z\" \u22cd 1100.\nOther anomalies.\nWith the increasingly precise data provided by WMAP, there have been a number of claims that the CMB exhibits anomalies, such as very large scale anisotropies, anomalous alignments, and non-Gaussian distributions.&lt;ref name=\"arXiv:astro-ph/0511666\"&gt;&lt;/ref&gt;&lt;ref name=\"arXiv:astro-ph/0503213\"&gt;&lt;/ref&gt; The most longstanding of these is the low-\"\u2113\" multipole controversy. Even in the COBE map, it was observed that the quadrupole (\"\u2113\" = 2, spherical harmonic) has a low amplitude compared to the predictions of the Big Bang. In particular, the quadrupole and octupole (\"\u2113\" = 3) modes appear to have an unexplained alignment with each other and with both the ecliptic plane and equinoxes. A number of groups have suggested that this could be the signature of new physics at the greatest observable scales; other groups suspect systematic errors in the data.\nUltimately, due to the foregrounds and the cosmic variance problem, the greatest modes will never be as well measured as the small angular scale modes. The analyses were performed on two maps that have had the foregrounds removed as far as possible: the \"internal linear combination\" map of the WMAP collaboration and a similar map prepared by Max Tegmark and others. Later analyses have pointed out that these are the modes most susceptible to foreground contamination from synchrotron, dust, and Bremsstrahlung emission, and from experimental uncertainty in the monopole and dipole.\nA full Bayesian analysis of the WMAP power spectrum demonstrates that the quadrupole prediction of Lambda-CDM cosmology is consistent with the data at the 10% level and that the observed octupole is not remarkable. Carefully accounting for the procedure used to remove the foregrounds from the full sky map further reduces the significance of the alignment by ~5%.\nRecent observations with the Planck telescope, which is very much more sensitive than WMAP and has a larger angular resolution, record the same anomaly, and so instrumental error (but not foreground contamination) appears to be ruled out. Coincidence is a possible explanation, chief scientist from WMAP, Charles L. Bennett suggested coincidence and human psychology were involved, \"I do think there is a bit of a psychological effect; people want to find unusual things.\"\nFuture evolution.\nAssuming the universe keeps expanding and it does not suffer a Big Crunch, a Big Rip, or another similar fate, the cosmic microwave background will continue redshifting until it will no longer be detectable, and will be superseded first by the one produced by starlight, and perhaps, later by the background radiation fields of processes that may take place in the far future of the universe such as proton decay, evaporation of black holes, and positronium decay.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7378", "revid": "43537120", "url": "https://en.wikipedia.org/wiki?curid=7378", "title": "Comparative law", "text": "Study of relationship between legal systems\nComparative law is the study of differences and similarities between the law (legal systems) of different countries. More specifically, it involves the study of the different legal \"systems\" (or \"families\") in existence in the world, including the common law, the civil law, socialist law, Canon law, Jewish Law, Islamic law, Hindu law, and Chinese law. It includes the description and analysis of foreign legal systems, even where no explicit comparison is undertaken. The importance of comparative law has increased enormously in the present age of internationalism, economic globalization, and democratization.\nHistory.\nThe origins of modern Comparative Law can be traced back to Gottfried Wilhelm Leibniz in 1667 in his Latin-language book (New Methods of Studying and Teaching Jurisprudence). Chapter 7 (Presentation of Law as the Project for all Nations, Lands and Times) introduces the idea of classifying Legal Systems into several families. A few years later, Leibniz introduced an idea of Language families.\nAlthough every Legal System is unique, Comparative Law through studies of their similarities and differences allows for classification of Legal Systems, wherein Law Families is the basic level of the classification. The main differences between Law Families are found in the source(s) of Law, the role of court precedents, the origin and development of the Legal System. Montesquieu is generally regarded as an early founding figure of comparative law. His comparative approach is obvious in the following excerpt from Chapter III of Book I of his masterpiece, \"De l'esprit des lois\" (1748; first translated by Thomas Nugent, 1750):&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;[T]he political and civil laws of each nation ... should be adapted in such a manner to the people for whom they are framed that it should be a great chance if those of one nation suit another.\nThey should be in relation to the nature and principle of each government: whether they form it, as may be said of politic laws; or whether they support it, as in the case of civil institutions.\nThey should be in relation to the climate of each country, to the quality of its soil, to its situation and extent, to the principal occupation of the natives, whether husbandmen, huntsmen, or shepherds: they should have relation to the degree of liberty which the constitution will bear; to the religion of the inhabitants, to their inclinations, riches, numbers, commerce, manners, and customs.\nAlso, in Chapter XI (entitled 'How to compare two different Systems of Laws') of Book XXIX, discussing the French and English systems for punishment of false witnesses, he advises that \"to determine which of those systems is most agreeable to reason, we must take them each as a whole and compare them in their entirety.\" Yet another place where Montesquieu's comparative approach is evident is the following, from Chapter XIII of Book XXIX:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;As the civil laws depend on the political institutions, because they are made for the same society, whenever there is a design of adopting the civil law of another nation, it would be proper to examine beforehand whether they have both the same institutions and the same political law.\nThe modern founding figure of comparative and anthropological jurisprudence was Sir Henry Maine, a British jurist and legal historian. In his 1861 work \"Ancient Law: Its Connection with the Early History of Society, and Its Relation to Modern Ideas\", he set out his views on the development of legal institutions in primitive societies and engaged in a comparative discussion of Eastern and Western legal traditions. This work placed comparative law in its historical context and was widely read and influential.\nThe first university course on the subject was established at the University of Oxford in 1869, with Maine taking up the position of professor.\nComparative law in the US was brought by a legal scholar fleeing persecution in Germany, Rudolf Schlesinger. Schlesinger eventually became professor of comparative law at Cornell Law School helping to spread the discipline throughout the US.\nPurpose.\nComparative law is an academic discipline that involves the study of legal systems, including their constitutive elements and how they differ, and how their elements combine into a system.\nSeveral disciplines have developed as separate branches of comparative law, including comparative constitutional law, comparative administrative law, comparative civil law (in the sense of the law of torts, contracts, property and obligations), comparative commercial law (in the sense of business organisations and trade), and comparative criminal law. Studies of these specific areas may be viewed as micro- or macro-comparative legal analysis, i.e. detailed comparisons of two countries, or broad-ranging studies of several countries. Comparative civil law studies, for instance, show how the law of private relations is organised, interpreted and used in different systems or countries. The purposes of comparative law are:\nRelationship with other legal subjects.\nComparative law is different from general jurisprudence (i.e. legal theory) and from public and private international law. However, it helps inform all of these areas of normativity.\nFor example, comparative law can help international legal institutions, such as those of the United Nations System, in analyzing the laws of different countries regarding their treaty obligations. Comparative law would be applicable to private international law when developing an approach to interpretation in a conflicts analysis. Comparative law may contribute to legal theory by creating categories and concepts of general application. Comparative law may also provide insights into the question of legal transplants, i.e. the transplanting of law and legal institutions from one system to another. The notion of legal transplants was coined by Alan Watson, one of the world's renowned legal scholars specializing in comparative law.\nAlso, the usefulness of comparative law for sociology of law and law and economics (and vice versa) is very large. The comparative study of the various legal systems may show how different legal regulations for the same problem function in practice. Conversely, sociology of law and law &amp; economics may help comparative law answer questions, such as: \nClassifications of legal systems.\nDavid.\nRen\u00e9 David proposed the classification of legal systems, according to the different ideology inspiring each one, into five groups or families:\nEspecially with respect to the aggregating by David of the Civil and Common laws into a single family, David argued that the antithesis between the Common law and Civil law systems, is of a technical rather than of an ideological nature. Of a different kind is, for instance, the antithesis between, say, Italian and American laws, and of a different kind than between the Soviet, Muslim, Hindu, or Chinese laws. According to David, the Civil law legal systems included those countries where legal science was formulated according to Roman law, whereas Common law countries are those dominated by judge-made law. The characteristics that he believed uniquely differentiate the Western legal family from the other four are:\nArminjon, Nolde, and Wolff.\nArminjon, Nolde, and Wolff believed that, for purposes of classifying the (then) contemporary legal systems of the world, it was required that those systems \"per se\" get studied, irrespective of external factors, such as geographical ones. They proposed the classification of legal system into seven groups, or so-called 'families', in particular the:\nZweigert and K\u00f6tz.\nKonrad Zweigert and Hein K\u00f6tz propose a different, multidimensional methodology for categorizing laws, i.e. for ordering families of laws. They maintain that, to determine such families, five criteria should be taken into account, in particular: the historical background, the characteristic way of thought, the different institutions, the recognized sources of law, and the dominant ideology. Using the aforementioned criteria, they classify the legal systems of the world into six families:\nUp to the second German edition of their introduction to comparative law, Zweigert and K\u00f6tz also used to mention Soviet or socialist law as another family of laws.\nGlenn.\nH. Patrick Glenn proposed the classification of legal systems places national laws in the broader context of major legal tradition:\nReferences.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7379", "revid": "843981015", "url": "https://en.wikipedia.org/wiki?curid=7379", "title": "Classical liberal", "text": ""}
{"id": "7380", "revid": "4233019", "url": "https://en.wikipedia.org/wiki?curid=7380", "title": "CD (disambiguation)", "text": "A CD or compact disc is a thin plastic silvery disc for audio recordings.\nCD or cd may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "7381", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=7381", "title": "Cyberspace", "text": "Concept describing a widespread, interconnected digital technology\nCyberspace is a concept describing a widespread interconnected digital technology. \"[The expression dates] from the first decade of the diffusion of the internet. It refers to the online world as a world 'apart,' as distinct from everyday reality. In cyberspace people can hide behind fake identities, as in the famous \"The New Yorker\" cartoon ...\" The term entered popular culture from science fiction and the arts but is now used by technology strategists, security professionals, governments, military and industry leaders and entrepreneurs to describe the domain of the global technology environment, commonly defined as standing for the global network of interdependent information technology infrastructures, telecommunications networks and computer processing systems. Others consider cyberspace to be just a notional environment in which communication over computer networks occurs. The word became popular in the 1990s when the use of the Internet, networking, and digital communication were all growing dramatically; the term \"cyberspace\" was able to represent the many new ideas and phenomena that were emerging.\nAs a social experience, individuals can interact, exchange ideas, share information, provide social support, conduct business, direct actions, create artistic media, play games, engage in political discussion, and so on, using this global network. They are sometimes referred to as \"cybernauts\". The term \"cyberspace\" has become a conventional means to describe anything associated with the Internet and the diverse Internet culture. The United States government recognizes the interconnected information technology and the interdependent network of information technology infrastructures operating across this medium as part of the US national critical infrastructure. Amongst individuals on cyberspace, there is believed to be a code of shared rules and ethics mutually beneficial for all to follow, referred to as cyberethics. Many view the right to privacy as most important to a functional code of cyberethics. Such moral responsibilities go hand in hand when working online with global networks, specifically, when opinions are involved with online social experiences.\nAccording to Chip Morningstar and F. Randall Farmer, cyberspace is defined more by the social interactions involved rather than its technical implementation. In their view, the computational medium in cyberspace is an augmentation of the communication channel between real people; the core characteristic of cyberspace is that it offers an environment that consists of many participants with the ability to affect and influence each other. They derive this concept from the observation that people seek richness, complexity, and depth within a virtual world.\nOrigins of the term.\nThe term \"cyberspace\" first appeared in the visual arts in the late 1960s, when Danish artist Susanne Ussing (1940-1998) and her partner architect Carsten Hoff (b. 1934) constituted themselves as Atelier Cyberspace. Under this name the two made a series of installations and images entitled \"sensory spaces\" that were based on the principle of open systems adaptable to various influences, such as human movement and the behaviour of new materials.\nAtelier Cyberspace worked at a time when the Internet did not exist and computers were more or less off-limit to artists and creative engagement. In a 2015-interview with Scandinavian art magazine Kunstkritikk, Carsten Hoff recollects, that although Atelier Cyberspace did try to implement computers, they had no interest in the virtual space as such:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;To us, \"cyberspace\" was simply about managing spaces. There was nothing esoteric about it. Nothing digital, either. It was just a tool. The space was concrete, physical.\nAnd in the same interview Hoff continues:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Our shared point of departure was that we were working with physical settings, and we were both frustrated and displeased with the architecture from the period, particularly when it came to spaces for living. We felt that there was a need to loosen up the rigid confines of urban planning, giving back the gift of creativity to individual human beings and allowing them to shape and design their houses or dwellings themselves \u2013 instead of having some clever architect pop up, telling you how you should live. We were thinking in terms of open-ended systems where things could grow and evolve as required.\nFor instance, we imagined a kind of mobile production unit, but unfortunately the drawings have been lost. It was a kind of truck with a nozzle at the back. Like a bee building its hive. The nozzle would emit and apply material that grew to form amorphous mushrooms or whatever you might imagine. It was supposed to be computer-controlled, allowing you to create interesting shapes and sequences of spaces. It was a merging of organic and technological systems, a new way of structuring the world. And a response that counteracted industrial uniformity. We had this idea that sophisticated software might enable us to mimic the way in which nature creates products \u2013 where things that belong to the same family can take different forms. All oak trees are oak trees, but no two oak trees are exactly alike. And then a whole new material \u2013 polystyrene foam \u2013 arrived on the scene. It behaved like nature in the sense that it grew when its two component parts were mixed. Almost like a fungal growth. This made it an obvious choice for our work in Atelier Cyberspace.\nThe works of Atelier Cyberspace were originally shown at a number of Copenhagen venues and have later been exhibited at The National Gallery of Denmark in Copenhagen as part of the exhibition \"What's Happening?\"\nThe term \"cyberspace\" first appeared in fiction in the 1980s in the work of cyberpunk science fiction author William Gibson, first in his 1982 short story \"Burning Chrome\" and later in his 1984 novel \"Neuromancer\". In the next few years, the word became prominently identified with online computer networks. The portion of \"Neuromancer\" cited in this respect is usually the following:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Cyberspace. A consensual hallucination experienced daily by billions of legitimate operators, in every nation, by children being taught mathematical concepts... A graphic representation of data abstracted from the banks of every computer in the human system. Unthinkable complexity. Lines of light ranged in the nonspace of the mind, clusters and constellations of data. Like city lights, receding.\nNow widely used, the term has since been criticized by Gibson, who commented on the origin of the term in the 2000 documentary \"No Maps for These Territories\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;All I knew about the word \"cyberspace\" when I coined it, was that it seemed like an effective buzzword. It seemed evocative and essentially meaningless. It was suggestive of something, but had no real semantic meaning, even for me, as I saw it emerge on the page.\nMetaphorical.\nDon Slater uses a metaphor to define cyberspace, describing the \"sense of a social setting that exists purely within a space of representation and communication ... it exists entirely within a computer space, distributed across increasingly complex and fluid networks.\" The term \"Cyberspace\" started to become a de facto synonym for the Internet, and later the World Wide Web, during the 1990s, especially in academic circles and activist communities. Author Bruce Sterling, who popularized this meaning, credits John Perry Barlow as the first to use it to refer to \"the present-day nexus of computer and telecommunications networks\". Barlow describes it thus in his essay to announce the formation of the Electronic Frontier Foundation (note the spatial metaphor) in June 1990:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In this silent world, all conversation is typed. To enter it, one forsakes both body and place and becomes a thing of words alone. You can see what your neighbors are saying (or recently said), but not what either they or their physical surroundings look like. Town meetings are continuous and discussions rage on everything from sexual kinks to depreciation schedules.\nWhether by one telephonic tendril or millions, they are all connected to one another. Collectively, they form what their inhabitants call the Net. It extends across that immense region of electron states, microwaves, magnetic fields, light pulses and thought which sci-fi writer William Gibson named Cyberspace.\nAs Barlow, and the EFF continued public education efforts to promote the idea of \"digital rights\", the term was increasingly used during the Internet boom of the late 1990s.\nVirtual environments.\nAlthough the present-day, loose use of the term \"cyberspace\" no longer implies or suggests immersion in a virtual reality, current technology allows the integration of a number of capabilities (sensors, signals, connections, transmissions, processors, and controllers) sufficient to generate a virtual interactive experience that is accessible regardless of a geographic location. It is for these reasons cyberspace has been described as the ultimate tax haven.\nIn 1989, Autodesk, an American multinational corporation that focuses on 2D and 3D design software, developed a virtual design system called Cyberspace.\nRecent definitions of Cyberspace.\nAlthough several definitions of cyberspace can be found both in scientific literature and in official governmental sources, there is no fully agreed official definition yet. According to F. D. Kramer there are 28 different definitions of the term cyberspace. See in particular the following links: \"Cyberpower and National Security: Policy Recommendations for a Strategic Framework,\" in Cyberpower and National Security, FD Kramer, S. Starr, L.K. Wentz (ed.), National Defense University Press, Washington (DC) 2009; see also Mayer, M., Chiarugi, I., De Scalzi, N., https://www.academia.edu/14336129/International_Politics_in_the_Digital_Age.\nThe most recent draft definition is the following:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Cyberspace is a global and dynamic domain (subject to constant change) characterized by the combined use of electrons and the electromagnetic spectrum, whose purpose is to create, store, modify, exchange, share, and extract, use, eliminate information and disrupt physical resources. Cyberspace includes: a) physical infrastructures and telecommunications devices that allow for the connection of technological and communication system networks, understood in the broadest sense (SCADA devices, smartphones/tablets, computers, servers, etc.); b) computer systems (see point a) and the related (sometimes embedded) software that guarantee the domain's basic operational functioning and connectivity; c) networks between computer systems; d) networks of networks that connect computer systems (the distinction between networks and networks of networks is mainly organizational); e) the access nodes of users and intermediaries routing nodes; f) constituent data (or resident data). Often, in common parlance (and sometimes in commercial language), networks of networks are called the Internet (with a lowercase i), while networks between computers are called intranet. Internet (with a capital I, in journalistic language sometimes called the Net) can be considered a part of the system a). A distinctive and constitutive feature of cyberspace is that no central entity exercises control over all the networks that make up this new domain.\nJust as in the real world there is no world government, cyberspace lacks an institutionally predefined hierarchical center. To cyberspace, a domain without a hierarchical ordering principle, we can, therefore, extend the definition of international politics coined by Kenneth Waltz: as being \"with no system of law enforceable.\" This does not mean that the dimension of power in cyberspace is absent, nor that power is dispersed and scattered into a thousand invisible streams, nor that it is evenly spread across myriad people and organizations, as some scholars had predicted. On the contrary, cyberspace is characterized by a precise structuring of hierarchies of power.\nThe Joint Chiefs of Staff of the United States Department of Defense define cyberspace as one of five interdependent domains, the remaining four being land, air, maritime, and space. \"See United States Cyber Command\"\nCyberspace as an Internet metaphor.\nWhile cyberspace should not be confused with the Internet, the term is often used to refer to objects and identities that exist largely within the communication network itself, so that a website, for example, might be metaphorically said to \"exist in cyberspace\". According to this interpretation, events taking place on the Internet are not happening in the locations where participants or servers are physically located, but \"in cyberspace\". The philosopher Michel Foucault used the term heterotopias, to describe such spaces which are simultaneously physical and mental.\nFirstly, cyberspace describes the flow of digital data through the network of interconnected computers: it is at once not \"real\", since one could not spatially locate it as a tangible object, and clearly \"real\" in its effects. There have been several attempts to create a concise model about how cyberspace works since it is not a physical thing that can be looked at. Secondly, cyberspace is the site of computer-mediated communication (CMC), in which online relationships and alternative forms of online identity were enacted, raising important questions about the social psychology of Internet use, the relationship between \"online\" and \"offline\" forms of life and interaction, and the relationship between the \"real\" and the virtual. Cyberspace draws attention to remediation of culture through new media technologies: it is not just a communication tool but a social destination and is culturally significant in its own right. Finally, cyberspace can be seen as providing new opportunities to reshape society and culture through \"hidden\" identities, or it can be seen as borderless communication and culture.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Cyberspace is the \"place\" where a telephone conversation appears to occur. Not inside your actual phone, the plastic device on your desk. Not inside the other person's phone, in some other city. The place between the phones. [...] in the past twenty years, this electrical \"space,\" which was once thin and dark and one-dimensional\u2014little more than a narrow speaking-tube, stretching from phone to phone\u2014has flung itself open like a gigantic jack-in-the-box. Light has flooded upon it, the eerie light of the glowing computer screen. This dark electric netherworld has become a vast flowering electronic landscape. Since the 1960s, the world of the telephone has cross-bred itself with computers and television, and though there is still no substance to cyberspace, nothing you can handle, it has a strange kind of physicality now. It makes good sense today to talk of cyberspace as a place all its own.\nThe \"space\" in cyberspace has more in common with the abstract, mathematical meanings of the term (see space) than physical space. It does not have the duality of positive and negative volume (while in physical space, for example, a room has the negative volume of usable space delineated by positive volume of walls, Internet users cannot enter the screen and explore the unknown part of the Internet as an extension of the space they are in), but spatial meaning can be attributed to the relationship between different pages (of books as well as web servers), considering the unturned pages to be somewhere \"out there.\" The concept of cyberspace, therefore, refers not to the content being presented to the surfer, but rather to the possibility of surfing among different sites, with feedback loops between the user and the rest of the system creating the potential to always encounter something unknown or unexpected.\nVideo games differ from text-based communication in that on-screen images are meant to be figures that actually occupy a space and the animation shows the movement of those figures. Images are supposed to form the positive volume that delineates the empty space. A game adopts the cyberspace metaphor by engaging more players in the game, and then figuratively representing them on the screen as avatars. Games do not have to stop at the avatar-player level, but current implementations aiming for more immersive playing space (i.e. Laser tag) take the form of augmented reality rather than cyberspace, fully immersive virtual realities remaining impractical.\nAlthough the more radical consequences of the global communication network predicted by some cyberspace proponents (i.e. the diminishing of state influence envisioned by John Perry Barlow) failed to materialize and the word lost some of its novelty appeal, it remains current as of 2006[ [update]].\nSome virtual communities explicitly refer to the concept of cyberspace, for example Linden Lab calling their customers \"Residents\" of Second Life, while all such communities can be positioned \"in cyberspace\" for explanatory and comparative purposes (as did Sterling in \"The Hacker Crackdown\", followed by many journalists), integrating the metaphor into a wider cyber-culture.\nThe metaphor has been useful in helping a new generation of thought leaders to reason through new military strategies around the world, led largely by the US Department of Defense (DoD). The use of cyberspace as a metaphor has had its limits, however, especially in areas where the metaphor becomes confused with physical infrastructure. It has also been critiqued as being unhelpful for falsely employing a spatial metaphor to describe what is inherently a network.\nAlternate realities in philosophy and art.\nPredating computers.\nA forerunner of the modern ideas of cyberspace is the Cartesian notion that people might be deceived by an evil demon that feeds them a false reality. This argument is the direct predecessor of modern ideas of a brain in a vat and many popular conceptions of cyberspace take Descartes's ideas as their starting point.\nVisual arts have a tradition, stretching back to antiquity, of artifacts meant to fool the eye and be mistaken for reality. This questioning of reality occasionally led some philosophers and especially theologians to distrust art as deceiving people into entering a world which was not real (see Aniconism). The artistic challenge was resurrected with increasing ambition as art became more and more realistic with the invention of photography, film (see \"Arrival of a Train at La Ciotat\"), and immersive computer simulations.\nInfluenced by computers.\nPhilosophy.\nAmerican counterculture exponents like William S. Burroughs (whose literary influence on Gibson and cyberpunk in general is widely acknowledged) and Timothy Leary were among the first to extol the potential of computers and computer networks for individual empowerment.\nSome contemporary philosophers and scientists (e.g. David Deutsch in \"The Fabric of Reality\") employ virtual reality in various thought experiments. For example, Philip Zhai in \"Get Real: A Philosophical Adventure in Virtual Reality\" connects cyberspace to the Platonic tradition:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Let us imagine a nation in which everyone is hooked up to a network of VR infrastructure. They have been so hooked up since they left their mother's wombs. Immersed in cyberspace and maintaining their life by teleoperation, they have never imagined that life could be any different from that. The first person that thinks of the possibility of an alternative world like ours would be ridiculed by the majority of these citizens, just like the few enlightened ones in Plato's allegory of the cave.\nNote that this brain-in-a-vat argument conflates cyberspace with reality, while the more common descriptions of cyberspace contrast it with the \"real world\".\nCyber-Geography.\nThe \u201cGeography of Notopia\u201d (Papadimitriou, 2006) theorizes about the complex interplay of cyber-cultures and the geographical space. This interplay has several philosophical and psychological facets (Papadimitriou, 2009).\nA New Communication Model.\nThe technological convergence of the mass media is the result of a long adaptation process of their communicative resources to the evolutionary changes of each historical moment. Thus, the new media became (plurally) an extension of the traditional media in cyberspace, allowing to the public access information in a wide range of digital devices. In other words, it is a cultural virtualization of human reality as a result of the migration from physical to virtual space (mediated by the ICTs), ruled by codes, signs and particular social relationships. Forwards, arise instant ways of communication, interaction and possible quick access to information, in which we are no longer mere senders, but also producers, reproducers, co-workers and providers. New technologies also help to \"connect\" people from different cultures outside the virtual space, which was unthinkable fifty years ago. In this giant relationships web, we mutually absorb each other's beliefs, customs, values, laws and habits, cultural legacies perpetuated by a physical-virtual dynamics in constant metamorphosis (ibidem). In this sense, Professor Doctor Marcelo Mendon\u00e7a Teixeira created, in 2013, a new model of communication to the virtual universe, based in Claude Elwood Shannon (1948) article \"A Mathematical Theory of Communication\".\nArt.\nHaving originated among writers, the concept of cyberspace remains most popular in literature and film. Although artists working with other media have expressed interest in the concept, such as Roy Ascott, \"cyberspace\" in digital art is mostly used as a synonym for immersive virtual reality and remains more discussed than enacted.\nComputer crime.\nCyberspace also brings together every service and facility imaginable to expedite money laundering. One can purchase anonymous credit cards, bank accounts, encrypted global mobile telephones, and false passports. From there one can pay professional advisors to set up IBCs (International Business Corporations, or corporations with anonymous ownership) or similar structures in OFCs (Offshore Financial Centers). Such advisors are loath to ask any penetrating questions about the wealth and activities of their clients, since the average fees criminals pay them to launder their money can be as much as 20 percent.\n5-level model.\nIn 2010, a five-level model was designed in France. According to this model, cyberspace is composed of five layers based on information discoveries: 1) language, 2) writing, 3) printing, 4) Internet, 5) Etc., i.e. the rest, e.g. noosphere, artificial life, artificial intelligence, etc., etc. This original model links the world of information to telecommunication technologies.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7382", "revid": "2108405", "url": "https://en.wikipedia.org/wiki?curid=7382", "title": "The Maritimes", "text": "Region of Atlantic Canada\nThe Maritimes, also called the Maritime provinces, is a region of Eastern Canada consisting of three provinces: New Brunswick, Nova Scotia, and Prince Edward Island. The Maritimes had a population of 1,899,324 in 2021, which makes up 5.1% of Canada's population. Together with Canada's easternmost province, Newfoundland and Labrador, the Maritime provinces make up the region of Atlantic Canada.\nLocated along the Atlantic coast, various aquatic sub-basins are located in the Maritimes, such as the Gulf of Maine and Gulf of St. Lawrence. The region is located northeast of New England in the United States, south and southeast of Quebec's Gasp\u00e9 Peninsula, and southwest of the island of Newfoundland. The notion of a Maritime Union has been proposed at various times in Canada's history; the first discussions in 1864 at the Charlottetown Conference contributed to Canadian Confederation. This movement formed the larger Dominion of Canada. The Mi'kmaq, Maliseet and Passamaquoddy people are indigenous to the Maritimes, while Acadian and British settlements date to the 17th century.\nName.\nThe word maritime is an adjective that means \"of the sea\"; thus any land adjacent to the sea can be considered maritime. But the term \"Maritimes\" has historically been collectively applied to New Brunswick, Nova Scotia and Prince Edward Island, all of which border the Atlantic Ocean. In other provinces except Newfoundland and Labrador in the Atlantic Ocean and British Columbia in the Pacific Ocean, human settlement along the coast is sparse. The Hudson Bay area is northerly and has a severe climate, and the majority of the populations of Ontario, Quebec and Manitoba reside far inland.\nHistory.\nThe pre-history of the Canadian Maritimes begins after the northerly retreat of glaciers at the end of the Wisconsin glaciation over 10,000 years ago; human settlement by First Nations began in the Maritimes with Paleo-Indians during the \"Early Period\", ending around 6,000 years ago.\nThe \"Middle Period\", starting 6,000 years ago, and ending 3,000 years ago, was dominated by rising sea levels from the melting glaciers in polar regions. This is also when what is called the \"Laurentian tradition\" started among Archaic Indians, the term used for First Nations peoples of the time. Evidence of Archaic Indian burial mounds and other ceremonial sites existing in the Saint John River valley has been uncovered.\nThe \"Late Period\" extended from 3,000 years ago until first contact with European settlers. This period was dominated by the organization of First Nations peoples into the Algonquian-speaking Abenaki Nation, which occupied territory largely in present-day interior Vermont, New Hampshire, and Maine, and the Mi'kmaq Nation, which inhabited all of Nova Scotia, Prince Edward Island, eastern New Brunswick and the southern Gasp\u00e9. The primarily agrarian Maliseet Nation settled throughout the Saint John River and Allagash River valleys of present-day New Brunswick and Maine. The Passamaquoddy Nation inhabited the northwestern coastal regions of the present-day Bay of Fundy. The Mi'kmaq Nation is also believed to have crossed the present-day Cabot Strait at around this time to settle on the south coast of Newfoundland, but they were a minority compared to the Beothuk Nation.\nEuropean contact.\nAfter Newfoundland, the Maritimes were the second area in Canada to be settled by Europeans. There is evidence that Viking explorers discovered and settled in the Vinland region around 1000 AD, which is when the L'Anse aux Meadows settlement in Newfoundland and Labrador has been dated. They may have made further exploration into the present-day Maritimes and northeastern United States.\nBoth Giovanni Caboto (John Cabot) and Giovanni da Verrazzano are reported to have sailed in or near Maritime waters during their voyages of discovery for England and France, respectively. Several Portuguese explorers / cartographers have also documented various parts of the Maritimes, namely Diogo Homem. However, it was French explorer Jacques Cartier who made the first detailed reconnaissance of the region for a European power and, in so doing, claimed the region for the King of France. Cartier was followed by nobleman Pierre Dugua, Sieur de Mons, who was accompanied by explorer / cartographer Samuel de Champlain in a 1604 expedition. During this they established the second permanent European settlement in what is now the United States and Canada, following Spain's settlement at St. Augustine in present-day Florida in the American South. Champlain's settlement at Saint Croix Island, later moved to Port-Royal, survived. By contrast, the ill-fated English settlement at Roanoke Colony off the southern American coast did not. The French settlement pre-dated the more successful English settlement at Jamestown in present-day Virginia by three years. Champlain was considered the founder of New France's province of Canada, which comprises much of the present-day lower St. Lawrence River valley in the province of Quebec.\nAcadia.\nChamplain's success in the region, which came to be called \"Acadie\", led to the fertile tidal marshes surrounding the southeastern and northeastern reaches of the Bay of Fundy being populated by French immigrants who called themselves \"Acadien\". The Acadians eventually built small settlements throughout what is today mainland Nova Scotia and New Brunswick, as well as \u00cele-Saint-Jean (Prince Edward Island), \u00cele-Royale (Cape Breton Island), and other shorelines of the Gulf of St. Lawrence in present-day Newfoundland and Labrador, and Quebec. Acadian settlements had primarily agrarian economies. Early examples of Acadian fishing settlements developed in southwestern Nova Scotia and in \u00cele-Royale, as well as along the south and west coasts of Newfoundland, the Gasp\u00e9 Peninsula, and the present-day C\u00f4te-Nord region of Quebec. Most Acadian fishing activities were overshadowed by the much larger seasonal European fishing fleets that were based out of Newfoundland and took advantage of proximity to the Grand Banks.\nThe growing English colonies along the American seaboard to the south and various European wars between England and France during the 17th and 18th centuries brought Acadia to the centre of world-scale geopolitical forces. In 1613, Virginian raiders captured Port-Royal, and in 1621 France ceded Acadia to Scotland's Sir William Alexander, who renamed it as \"Nova Scotia\".\nBy 1632, Acadia was returned from Scotland to France under the \"Treaty of Saint-Germain-en-Laye.\" The Port Royale settlement was moved to the site of nearby present-day Annapolis Royal. More French immigrant settlers, primarily from the Brittany, Normandie, and Vienne regions of France, continued to populate the colony of Acadia during the latter part of the 17th and early part of the 18th centuries. Important settlements also began in the Beaubassin region of the present-day Isthmus of Chignecto, and in the Saint John River valley, as well as smaller communities on \u00cele-Saint-Jean and \u00cele-Royale.\nIn 1654, raiders from New England attacked Acadian settlements on the Annapolis Basin. Acadians lived with uncertainty throughout the English constitutional crises under Oliver Cromwell, and it was not until the Treaty of Breda in 1667 that France's claim to the region was reaffirmed. Colonial administration by France throughout the history of Acadia was of low priority. France's priorities were in settling and strengthening its claim on the larger territory of New France and the exploration and settlement of interior North America and the Mississippi River valley.\nColonial wars.\nOver 74 years (1689\u20131763) there were six colonial wars, which involved continuous warfare between New England and Acadia (see the French and Indian Wars reflecting English and French tensions in Europe, as well as Father Rale's War (Dummer's War) and Father Le Loutre's War). Throughout these wars, New England was allied with the Iroquois Confederacy based around the southern Great Lakes and west of the Hudson River. Acadian settlers were allied with the Wabanaki Confederacy. In the first war, King William's War (the North American theatre of the Nine Years' War), natives from the Maritime region participated in numerous attacks with the French on the Acadia / New England border in southern Maine (e.g., Raid on Salmon Falls). New England retaliatory raids on Acadia, such as the Raid on Chignecto, were conducted by Benjamin Church. In the second war, Queen Anne's War (the North American theatre of the War of the Spanish Succession), the British conducted the Conquest of Acadia, while the region remained primarily in control of Maliseet militia, Acadia militia and Mi'kmaw militia.\nIn 1719, to further protect strategic interests in the Gulf of St. Lawrence and St. Lawrence River, France began the 20-year construction of a large fortress at Louisbourg on \u00cele-Royale. Massachusetts was increasingly concerned over reports of the capabilities of this fortress, and of privateers staging out of its harbour to raid New England fishermen on the Grand Banks. In the fourth war, King George's War (the North American theatre of the War of the Austrian Succession), the British engaged successfully in the Siege of Louisbourg. The British returned control of \u00cele-Royale to France with the fortress virtually intact three years later under the Treaty of Aix-la-Chapelle and the French reestablished their forces there.\nIn 1749, to counter the rising threat of Louisbourg, Halifax was founded and the Royal Navy established a major naval base and citadel. The founding of Halifax sparked Father Le Loutre's War.\nDuring the sixth and final colonial war, the French and Indian War (the North American theatre of the Seven Years' War), the military conflicts in Nova Scotia continued. The British Conquest of Acadia happened in 1710. Over the next forty-five years, the Acadians refused to sign an unconditional oath of allegiance to Britain. During this time period Acadians participated in various militia operations against the British and maintained vital supply lines to the French Fortress of Louisbourg and Fort Beausejour. The British sought to neutralize any military threat Acadians posed and to interrupt the vital supply lines Acadians provided to Louisbourg by deporting Acadians from Acadia.\nThe British began the Expulsion of the Acadians with the Bay of Fundy campaign in 1775. Over the next nine years over 12,000 Acadians of 15,000 were removed from Nova Scotia.\nIn 1758, the fortress of Louisbourg was laid siege for a second time within 15 years, this time by more than 27,000 British soldiers and sailors with over 150 warships. After the French surrender, Louisbourg was thoroughly destroyed by British engineers to ensure it would never be reclaimed. With the fall of Louisbourg, French and Mi'kmaw resistance in the region crumbled. British forces seized remaining French control over Acadia in the coming months, with \u00cele-Saint-Jean falling in 1759 to British forces on their way to Quebec City for the first siege of Quebec and the ensuing Battle of the Plains of Abraham.\nThe war ended and Britain had gained control over the entire Maritime region and the Indigenous people signed the Halifax Treaties.\nAmerican Revolution.\nFollowing the Seven Years' War, empty Acadian lands were settled first by 8,000 New England Planters and then by immigrants brought from Yorkshire. \u00cele-Royale was renamed Cape Breton Island and incorporated into the Colony of Nova Scotia. Some of the Acadians who had been deported came back but went to the eastern coasts of New Brunswick.\nBoth the colonies of Nova Scotia (present-day Nova Scotia and New Brunswick) and St. John's Island (Prince Edward Island) were affected by the American Revolutionary War, largely by privateering against American shipping, but several coastal communities were also the targets of American raiders. Charlottetown, the capital of the new colony of St. John's Island, was ransacked in 1775 with the provincial secretary kidnapped and the Great Seal stolen. The largest military action in the Maritimes during the revolutionary war was the attack on Fort Cumberland (the renamed Fort Beaus\u00e9jour) in 1776 by a force of American sympathizers led by Jonathan Eddy. The fort was partially overrun after a month-long siege, but the attackers were ultimately repelled after the arrival of British reinforcements from Halifax.\nThe most significant impact from this war was the settling of large numbers of Loyalist refugees in the region (34,000 to the 17,000 settlers already there), especially in Shelburne and Parrtown (Saint John). Following the Treaty of Paris in 1783, Loyalist settlers in what would become New Brunswick persuaded British administrators to split the Colony of Nova Scotia to create the new colony of New Brunswick in 1784. At the same time, another part of the Colony of Nova Scotia, Cape Breton Island, was split off to become the Colony of Cape Breton Island. The Colony of St. John's Island was renamed to Prince Edward Island on November 29, 1798.\nThe War of 1812 had some effect on the shipping industry in the Maritime colonies of New Brunswick, Nova Scotia, Prince Edward Island, and Cape Breton Island; however, the significant Royal Navy presence in Halifax and other ports in the region prevented any serious attempts by American raiders. Maritime and American privateers targeted unprotected shipping of both the United States and Britain respectively, further reducing trade. New Brunswick's section of the Canada\u2013US border did not have any significant action during this conflict, although British forces did occupy a portion of coastal Maine at one point. The most significant incident from this war which occurred in the Maritimes was the British capture and detention of USS \"Chesapeake\", an American frigate in Halifax.\n19th century.\nIn 1820, the Colony of Cape Breton Island was merged back into the Colony of Nova Scotia for the second time by the British government.\nBritish settlement of the Maritimes, as the colonies of Nova Scotia, New Brunswick and Prince Edward Island came to be known, accelerated throughout the late 18th century and into the 19th century with significant immigration to the region as a result of Scottish migrants displaced by the Highland Clearances and Irish escaping the Great Irish Famine (1845\u20131849). As a result, significant portions of the three provinces are influenced by Celtic heritages, with Scottish Gaelic (and to a lesser degree, Irish Gaelic) having been widely spoken, particularly in Cape Breton, although it is less prevalent today.\nDuring the American Civil War, a significant number of Maritimers volunteered to fight for the armies of the Union, while a small handful joined the Confederate Army. However, the majority of the conflict's impact was felt in the shipping industry. Maritime shipping boomed during the war due to large-scale Northern imports of war supplies which were often carried by Maritime ships as Union ships were vulnerable to Confederate naval raiders. Diplomatic tensions between Britain and the Unionist North had deteriorated after some interests in Britain expressed support for the secessionist Confederate South. The Union Navy, although much smaller than the British Royal Navy and no threat to the Maritimes, did posture off Maritime coasts at times chasing Confederate naval ships which sought repairs and reprovisioning in Maritime ports, especially Halifax.\nThe immense size of the Union Army (the largest on the planet toward the end of the Civil War), however, was viewed with increasing concern by Maritimers throughout the early 1860s. Another concern was the rising threat of Fenian raids on border communities in New Brunswick by the Fenian Brotherhood seeking to end British rule in Ireland. This combination of events, coupled with an ongoing decline in British military and economic support to the region as the Home Office favoured newer colonial endeavours in Africa and elsewhere, led to a call among Maritime politicians for a conference on Maritime Union, to be held in early September 1864 in Charlottetown \u2013 chosen in part because of Prince Edward Island's reluctance to give up its jurisdictional sovereignty in favour of uniting with New Brunswick and Nova Scotia into a single colony. New Brunswick and Nova Scotia felt that if the union conference were held in Charlottetown, they might be able to convince Island politicians to support the proposal.\nThe Charlottetown Conference, as it came to be called, was also attended by a slew of visiting delegates from the neighbouring Crown colony, the Province of Canada, who had largely arrived at their own invitation with their own agenda. This agenda saw the conference dominated by discussions of creating an even larger union of the entire territory of British North America into a united colony. The Charlottetown Conference ended with an agreement to meet the following month in Quebec City, where more formal discussions ensued, culminating with meetings in London and the signing of the \"British North America Act, 1867\" (BNA Act). Of the Maritime provinces, only Nova Scotia and New Brunswick were initially party to the BNA Act: Prince Edward Island's reluctance, combined with a booming agricultural and fishing export economy having led to that colony opting not to sign on.\nMajor population centres.\nThe major communities of the region include Halifax and Cape Breton in Nova Scotia, Moncton, Saint John, and Fredericton in New Brunswick, and Charlottetown in Prince Edward Island.\nClimate.\nIn spite of its name, The Maritimes has a humid continental climate of the warm-summer subtype. Especially in coastal Nova Scotia, differences between summers and winters are narrow compared to the rest of Canada. The inland climate of New Brunswick is in stark contrast during winter, resembling more continental areas. Summers are somewhat tempered by the marine influence throughout the provinces, but due to the southerly parallels still remain similar to more continental areas further west. Yarmouth in Nova Scotia has significant marine influence to have a borderline oceanic microclimate, but winter nights are still cold even in all coastal areas. The northernmost areas of New Brunswick are only just above subarctic with very cold continental winters.\nDemographics.\nThe Maritimes were predominantly rural until recent decades, having resource-based economies of fishing, agriculture, forestry, and coal mining.\nMaritimers are predominantly of west European origin: Scottish Canadians, Irish Canadians, English Canadians, and Acadians. New Brunswick, in general, differs from the other two Maritime provinces in that it has a much higher Francophone population. There was once a significant Canadian Gaelic speaking population. Helen Creighton recorded Celtic traditions of rural Nova Scotia in the mid-1900s.\nThere are Black Canadians who are mostly descendants of Black Loyalists or black refugees from the War of 1812. This Maritime population is mainly among Black Nova Scotians.\nThere are Mi'kmaq reserves in all three provinces, and a smaller population of the Maliseet in western New Brunswick. \nEconomy.\nPresent status.\nGiven the small population of the region (compared with the Central Canadian provinces or the New England states), the regional economy is a net exporter of natural resources, manufactured goods, and services. The regional economy has long been tied to natural resources such as fishing, logging, farming, and mining activities. Significant industrialization in the second half of the 19th century brought steel to Trenton, Nova Scotia, and subsequent creation of a widespread industrial base to take advantage of the region's large underground coal deposits. After Confederation, however, this industrial base withered with technological change, and trading links to Europe and the U.S. were reduced in favour of those with Ontario and Quebec. In recent years, however, the Maritime regional economy has begun increased contributions from manufacturing again and the steady transition to a service economy.\nImportant manufacturing centres in the region include Pictou County, Truro, the Annapolis Valley and the South Shore, and the Strait of Canso area in Nova Scotia, as well as Summerside in Prince Edward Island, and the Miramichi area, the North Shore and the upper Saint John River valley of New Brunswick.\nSome predominantly coastal areas have become major tourist centres, such as parts of Prince Edward Island, Cape Breton Island, the South Shore of Nova Scotia and the Gulf of St. Lawrence and Bay of Fundy coasts of New Brunswick. Additional service-related industries in information technology, pharmaceuticals, insurance and financial sectors\u2014as well as research-related spin-offs from the region's numerous universities and colleges\u2014are significant economic contributors.\nAnother important contribution to Nova Scotia's provincial economy is through spin-offs and royalties relating to off-shore petroleum exploration and development. Mostly concentrated on the continental shelf of the province's Atlantic coast in the vicinity of Sable Island, exploration activities began in the 1960s and resulted in the first commercial production field for oil beginning in the 1980s. Natural gas was also discovered in the 1980s during exploration work, and this is being commercially recovered, beginning in the late 1990s. Initial optimism in Nova Scotia about the potential of off-shore resources appears to have diminished with the lack of new discoveries, although exploration work continues and is moving farther off-shore into waters on the continental margin.\nRegional transportation networks have also changed significantly in recent decades with port modernizations, with new freeway and ongoing arterial highway construction, the abandonment of various low-capacity railway branch lines (including the entire railway system of Prince Edward Island and southwestern Nova Scotia), and the construction of the Canso Causeway and the Confederation Bridge. There have been airport improvements at various centres providing improved connections to markets and destinations in the rest of North America and overseas.\nImprovements in infrastructure and the regional economy notwithstanding, the three provinces remain one of the poorer regions of Canada. While urban areas are growing and thriving, economic adjustments have been harsh in rural and resource-dependent communities, and emigration has been an ongoing phenomenon for some parts of the region. Another problem is seen in the lower average wages and family incomes within the region. Property values are depressed, resulting in a smaller tax base for these three provinces, particularly when compared with the national average which benefits from central and western Canadian economic growth.\nThis has been particularly problematic with the growth of the welfare state in Canada since the 1950s, resulting in the need to draw upon equalization payments to provide nationally mandated social services. Since the 1990s the region has experienced an exceptionally tumultuous period in its regional economy with the collapse of large portions of the ground fishery throughout Atlantic Canada, the closing of coal mines and a steel mill on Cape Breton Island, and the closure of military bases in all three provinces. That being said, New Brunswick has one of the largest military bases in the Commonwealth of Nations (CFB Gagetown), which plays a significant role in the cultural and economic spheres of Fredericton, the province's capital city.\nHistorical.\nGrowth.\nWhile the economic underperformance of the Maritime economy has been long lasting, it has not always been present. The mid-19th century, especially the 1850s and 1860s, has long been seen as a \"Golden Age\" in the Maritimes. Growth was strong, and the region had one of British North America's most extensive manufacturing sectors as well as a large international shipping industry. The question of why the Maritimes fell from being a centre of Canadian manufacturing to being an economic hinterland is thus a central one to the study of the region's pecuniary difficulties. The period in which the decline occurred had a great many potential culprits. In 1867 Nova Scotia and New Brunswick merged with the Canadas in Confederation, with Prince Edward Island joining them six years later in 1873. Canada was formed only a year after free trade with the United States (in the form of the Reciprocity Treaty) had ended. In the 1870s John A. Macdonald's National Policy was implemented, creating a system of protective tariffs around the new nation. Throughout the period there was also significant technological change both in the production and transportation of goods.\nReputed Golden Age.\nSeveral scholars have explored the so-called \"Golden Age\" of the Maritimes in the years just before Confederation. In Nova Scotia, the population grew steadily from 277,000 in 1851 to 388,000 in 1871, mostly from natural increase since immigration was slight. The era has been called a Golden Age, but that was a myth created in the 1930s to lure tourists to a romantic era of tall ships and antiques. Recent historians using census data have shown that is a fallacy. In 1851\u20131871 there was an overall increase in per capita wealth holding. However most of the gains went to the urban elite class, especially businessmen and financiers living in Halifax. The wealth held by the top 10% rose considerably over the two decades, but there was little improvement in the wealth levels in rural areas, which comprised the great majority of the population. Likewise Gwyn reports that gentlemen, merchants, bankers, colliery owners, shipowners, shipbuilders, and master mariners flourished. However the great majority of families were headed by farmers, fishermen, craftsmen and labourer. Most of them\u2014and many widows as well\u2014lived in poverty. Out migration became an increasingly necessary option. Thus the era was indeed a golden age but only for a small but powerful and highly visible elite.\nEconomic decline.\nThe cause of economic malaise in the Maritimes is an issue of great debate and controversy among historians, economists, and geographers. The differing opinions can approximately be divided into the \"structuralists,\" who argue that poor policy decisions are to blame, and the others, who argue that unavoidable technological and geographical factors caused the decline.\nThe exact date that the Maritimes began to fall behind the rest of Canada is difficult to determine. Historian Kris Inwood places the date very early, at least in Nova Scotia, finding clear signs that the Maritimes \"Golden Age\" of the mid-19th century was over by 1870, before Confederation or the National Policy could have had any significant impact. Richard Caves places the date closer to 1885. T.W. Acheson takes a similar view and provides considerable evidence that the early 1880s were in fact a booming period in Nova Scotia and this growth was only undermined towards the end of that decade. David Alexander argues that any earlier declines were simply part of the global Long Depression, and that the Maritimes first fell behind the rest of Canada when the great boom period of the early 20th century had little effect on the region. E.R. Forbes, however, emphasizes that the precipitous decline did not occur until after the First World War during the 1920s when new railway policies were implemented. Forbes also contends that significant Canadian defence spending during the Second World War favoured powerful political interests in Central Canada such as C. D. Howe, when major Maritime shipyards and factories, as well as Canada's largest steel mill, located in Cape Breton Island, fared poorly.\nOne of the most important changes, and one that almost certainly had an effect, was the revolution in transportation that occurred at this time. The Maritimes were connected to central Canada by the Intercolonial Railway in the 1870s, removing a longstanding barrier to trade. For the first time this placed the Maritime manufacturers in direct competition with those of Central Canada. Maritime trading patterns shifted considerably from mainly trading with New England, Britain, and the Caribbean, to being focused on commerce with the Canadian interior, enforced by the federal government's tariff policies.\nCoincident with the construction of railways in the region, the age of the wooden sailing ship began to come to an end, being replaced by larger and faster steel steamships. The Maritimes had long been a centre for shipbuilding, and this industry was hurt by the change. The larger ships were also less likely to call on the smaller population centres such as Saint John and Halifax, preferring to travel to cities like New York and Montreal. Even the Cunard Line, founded by Maritime-born Samuel Cunard, stopped making more than a single ceremonial voyage to Halifax each year.\nMore controversial than the role of technology is the argument over the role of politics in the origins of the region's decline. Confederation and the tariff and railway freight policies that followed have often been blamed for having a deleterious effect on the Maritime economies. Arguments have been made that the Maritimes' poverty was caused by control over policy by Central Canada which used the national structures for its own enrichment. This was the central view of the Maritime Rights Movement of the 1920s, which advocated greater local control over the region's finances. T.W. Acheson is one of the main proponents of this theory. He notes the growth that was occurring during the early years of the National Policy in Nova Scotia demonstrates how the effects of railway fares and the tariff structure helped undermine this growth. Capitalists from Central Canada purchased the factories and industries of the Maritimes from their bankrupt local owners and proceeded to close down many of them, consolidating the industry in Central Canada.\nThe policies in the early years of Confederation were designed by Central Canadian interests, and they reflected the needs of that region. The unified Canadian market and the introduction of railroads created a relative weakness in the Maritime economies. Central to this concept, according to Acheson, was the lack of metropolises in the Maritimes.\nMontreal and Toronto were well-suited to benefit from the development of large-scale manufacturing and extensive railway systems in Quebec and Ontario, these being the goals of the Macdonald and Laurier governments. In the Maritimes the situation was very different. Today New Brunswick has several mid-sized centres in Saint John, Moncton, and Fredericton but no significant population centre. Nova Scotia has a growing metropolitan area surrounding Halifax, but a contracting population in industrial Cape Breton County, and several smaller centres in Bridgewater, Kentville, Yarmouth, and Pictou County. Prince Edward Island's only significant population centres are in Charlottetown and Summerside. During the late 19th and early 20th centuries, just the opposite was the case with little to no population concentration in major industrial centres as the predominantly rural resource-dependent Maritime economy continued on the same path as it had since European settlement on the region's shores.\nDespite the region's absence of economic growth on the same scale as other parts of the nation, the Maritimes has changed markedly throughout the 20th century, partly as a result of global and national economic trends, and partly as a result of government intervention. Each sub-region within the Maritimes has developed over time to exploit different resources and expertise. Saint John became a centre of the timber trade and shipbuilding and is currently a centre for oil refining and some manufacturing. The northern New Brunswick communities of Edmundston, Campbellton, Dalhousie, Bathurst, and Miramichi are focused on the pulp and paper industry and some mining activity. Moncton was a centre for railways and has changed its focus to becoming a multi-modal transportation centre with associated manufacturing and retail interests. The Halifax metropolitan area has come to dominate peninsular Nova Scotia as a retail and service centre, but that province's industries were spread out from the coal and steel industries of industrial Cape Breton and Pictou counties, the mixed farming of the North Shore and Annapolis Valley, and the fishing industry was primarily focused on the South Shore and Eastern Shore. Prince Edward Island is largely dominated by farming, fishing, and tourism.\nGiven the geographic diversity of the various sub-regions within the Maritimes, policies to centralize the population and economy were not initially successful, thus Maritime factories closed while those in Ontario and Quebec prospered.\nThe traditional staples thesis, advocated by scholars such as S.A. Saunders, looks at the resource endowments of the Maritimes and argues that it was the decline of the traditional industries of shipbuilding and fishing that led to Maritime poverty, since these processes were rooted in geography, and thus all but inevitable. Kris Inwood has revived the staples approach and looks at a number of geographic weaknesses relative to Central Canada. He repeats Acheson's argument that the region lacks major urban centres, but adds that the Maritimes were also lacking the great rivers that led to the cheap and abundant hydro-electric power, key to Quebec and Ontario's urban and manufacturing development, that the extraction costs of Maritime resources were higher (particularly in the case of Cape Breton coal), and that the soils of the region were poorer and thus the agricultural sector weaker.\nThe Maritimes are the only provinces in Canada which entered Confederation in the 19th century and have kept their original colonial boundaries. All three provinces have the smallest land base in the country and have been forced to make do with resources within. By comparison, the former colony of the Province of Canada (divided into the District of Canada East, and the District of Canada West) and the western provinces were dozens of times larger and in some cases were expanded to take in territory formerly held in British Crown grants to companies such as the Hudson's Bay Company; in particular the November 19, 1869 sale of Rupert's Land to the Government of Canada under the \"Rupert's Land Act 1868\" was facilitated in part by Maritime taxpayers. The economic riches of energy and natural resources held within this larger land base were only realized by other provinces during the 20th century.\nIndustries.\nThe maritime provinces' main industry is fishing. Fishing can be found in any maritime province. This includes fishing for lobster, mackerel, tuna, salmon and many more kinds of fish. Oysters and salmonoid aquaculture is also increasingly important economically.\nNova Scotia.\nNova Scotia is very strong in agriculture, forestry and fishing.\nPrince Edward Island.\nTourism is important to the economy of PEI. \"Anne of Green Gables\" was written in PEI, and this attracts tourists to PEI. PEI is also known for its agriculture, mainly the potato, and fishing industries.\nNew Brunswick.\nAgriculture and forestry are two prominent industries found in New Brunswick. Despite having an extensive coastline, New Brunswick's industrial sector has never been entirely reliant on the success of the fisheries. Likewise, the strong shipbuilding heritage of the province directly relates to its forest resources. Because of this, New Brunswickers tend to attribute their cultural heritage less with the sea and more with their forests and rivers.\nPolitics.\nMaritime conservatism since the Second World War has been very much part of the Red Tory tradition, key influences being former Premier of Nova Scotia and federal Progressive Conservative Party leader Robert Stanfield and New Brunswick Tory strategist Dalton Camp.\nIn recent years, the social democratic New Democratic Party (NDP) has made significant inroads both federally and provincially in the region. The NDP has elected members of parliament (MPs) from New Brunswick, but most of the focus of the party at the federal and provincial levels is currently in the Halifax area of Nova Scotia. Industrial Cape Breton has historically been a region of labour activism, electing Co-operative Commonwealth Federation (and later NDP) MPs, and even produced many early members of the Communist Party of Canada in the pre-Second World War era. In the 2004 federal election, the NDP captured 28.45% of the vote in Nova Scotia, more than any other province. In the 2009 provincial election the NDP formed a majority government, the first in the region.\nIt is because of the lack of support for fiscal conservatism that federal parties such as the Canadian Alliance never had much success in the region, In the 2004 federal election, the Conservatives had one of the worst showings in the region for a right-wing party, going back to Confederation, with the exception of the 1993 election. The Conservative party improved its seat count in the 2008 and elected 13 MPs in the 2011 election. However, in the 2015 election the Liberal Party won every seat in the region, defeating all of the Conservative (and NDP) challengers.\nThe Liberal Party of Canada has done well in the Maritimes in the past because of its interventionist policies. The Acadian Peninsula region of New Brunswick tends to vote for the Liberals or NDP for social political reasons, as well as treatment of the French by various parties. In the 1997 federal election, Prime Minister Jean Chr\u00e9tien's Liberals endured a bitter defeat to the PCs and NDP in many ridings as a result of unpopular cuts to unemployment benefits for seasonal workers, as well as closures of several Canadian Forces bases, the refusal to honour a promise to rescind the Goods and Services Tax, cutbacks to provincial equalization payments, health care, post-secondary education and regional transportation infrastructure such as airports, fishing harbours, seaports, and railways . The Liberals held onto seats in Prince Edward Island and New Brunswick, while being shut out of Nova Scotia entirely, the second time in history (the only other time being the Diefenbaker sweep). In 2015 the Liberals won every seat in The Maritimes, defeating Conservative and NDP incumbents.\nThe Maritimes is currently represented in the Canadian Parliament by 25 Members of the House of Commons (Nova Scotia \u2013 11, New Brunswick \u2013 10, Prince Edward Island \u2013 4) and 24 Senators (Nova Scotia and New Brunswick \u2013 10 each, Prince Edward Island \u2013 4). This level of representation was established at the time of Confederation when the Maritimes had a much larger proportion of the national population. The comparatively large population growth of western and central Canada during the immigration boom of the 20th century has reduced the Maritimes' proportion of the national population to less than 10%, resulting in an over-representation in Parliament, with some federal ridings having fewer than 35,000 people, compared to central and western Canada where ridings typically contain 100,000\u2013120,000 people.\nThe Senate of Canada is structured along regional lines, giving an equal number of seats (24) to the Maritimes, Ontario, Quebec, and western Canada, in addition to the later entry of Newfoundland and Labrador, as well as the three territories. Enshrined in the Constitution, this model was developed to ensure that no area of the country is able to exert undue influence in the Senate. The Maritimes, with its much smaller proportion of the national population (compared to the time of Confederation) also have an over-representation in the Senate, particularly compared to the population growth of Ontario and the western provinces. This has led to calls to reform the Senate; however, such a move would entail constitutional changes.\nAnother factor related to the number of Senate seats is that a constitutional amendment in the early 20th century mandated that no province can have fewer Members of Parliament than it has senators. This court decision resulted from a complaint by the Government of Prince Edward Island after that province's number of MPs was proposed to change from 4 to 3, accounting for its declining proportion of the national population at that time. When PEI entered Confederation in 1873, it was accorded 6 MPs and 4 Senators; however this was reduced to 4 MPs by the early 20th century. Senators being appointed for life at this time, these coveted seats rarely went unfilled for a long period of time anywhere in Canada. As a result, PEI's challenge was accepted by the federal government, and its level of federal representation was secured. In the aftermath of the 1989 budget, which saw a filibuster by Liberal Senators in attempt to kill legislation creating the Goods and Services Tax, Prime Minister Brian Mulroney \"stacked\" the Senate by creating additional seats in several provinces across Canada, including New Brunswick; however, there was no attempt by these provinces to increase the number of MPs to reflect this change in Senate representation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7383", "revid": "39640083", "url": "https://en.wikipedia.org/wiki?curid=7383", "title": "Cyril of Alexandria", "text": "Head of the Coptic Church from 412 to 444\nCyril of Alexandria (; ; c.\u00a0376 \u2013 444) was the Patriarch of Alexandria from 412 to 444. He was enthroned when the city was at the height of its influence and power within the Roman Empire. Cyril wrote extensively and was a major player in the Christological controversies of the late-4th and 5th centuries. He was a central figure in the Council of Ephesus in 431, which led to the deposition of Nestorius as Patriarch of Constantinople.\nCyril is counted among the Church Fathers and also as a Doctor of the Church, and his reputation within the Christian world has resulted in his titles \"Pillar of Faith\" and \"Seal of all the Fathers\". \nThe Nestorian bishops at their synod at the Council of Ephesus declared him a heretic, labelling him as a \"monster, born and educated for the destruction of the church.\"\nCyril is well known for his dispute with Nestorius and his supporter, Patriarch John of Antioch, whom Cyril excluded from the Council of Ephesus for arriving late. He is also known for his expulsion of Novatians and Jews from Alexandria and for inflaming tensions that led to the murder of the Hellenistic philosopher Hypatia by a Christian mob. Historians disagree over the extent of his responsibility in this.\nCyril tried to oblige the pious Christian emperor Theodosius II (AD 408\u2013450) to himself by dedicating his Paschal table to him. Cyril's Paschal table was provided with a Metonic basic structure in the form of a 19-year lunar cycle adopted by him around AD 425, which was very different from the first Metonic 19-year lunar cycle invented around AD 260 by Anatolius, but exactly equal to the lunar cycle which had been introduced around AD 412 by Annianus; the Julian equivalent of this Alexandrian cycle adopted by Cyril and nowadays referred to as the 'classical (Alexandrian) 19-year lunar cycle' would emerge a century later in Rome as the basic structure of Dionysius Exiguus\u2019 Paschal table (AD 525).\nThe Catholic Church did not commemorate Saint Cyril in the Tridentine Calendar: it added his feast only in 1882, assigning to it the date of 9 February. This date is used by the Western Rite Orthodox Church. Yet the 1969 Catholic Calendar revision moved it to 27 June, considered to be the day of the saint's death, as celebrated by the Coptic Orthodox Church. The same date has been chosen for the Lutheran calendar. The Eastern Orthodox and Byzantine Catholic Churches celebrate his feast day on 9 June and also, together with Pope Athanasius I of Alexandria, on 18 January.\nCyril is remembered in the Church of England with a commemoration on 28 June.\nEarly life.\nLittle is known for certain of Cyril's early life. He was born circa 376, in the town of Didouseya, Egypt, modern-day El-Mahalla El-Kubra. A few years after his birth, his maternal uncle Theophilus rose to the powerful position of Patriarch of Alexandria. His mother remained close to her brother and under his guidance, Cyril was well educated. His writings show his knowledge of Christian writers of his day, including Eusebius, Origen, Didymus the Blind, and writers of the Church of Alexandria. He received the formal Christian education standard for his day: he studied grammar from age twelve to fourteen (390\u2013392), rhetoric and humanities from fifteen to twenty (393\u2013397) and finally theology and biblical studies (398\u2013402).\nIn 403, he accompanied his uncle to attend the \"Synod of the Oak\" in Constantinople, which deposed John Chrysostom as Archbishop of Constantinople. The prior year, Theophilus had been summoned by the emperor to Constantinople to apologize before a synod, over which Chrysostom would preside, on account of several charges which were brought against him by certain Egyptian monks. Theophilus had them persecuted as Origenists. Placing himself at the head of soldiers and armed servants, Theophilus had marched against the monks, burned their dwellings, and ill-treated those whom he captured. Theophilus arrived at Constantinople with twenty-nine of his suffragan bishops, and conferring with those opposed to the Archbishop, drafted a long list of largely unfounded accusations against Chrysostom, who refused to recognize the legality of a synod in which his open enemies were judges. Chrysostom was subsequently deposed.\nPatriarch of Alexandria.\nTheophilus died on 15 October 412, and Cyril was made Pope or Patriarch of Alexandria on 18 October 412, but only after a riot between his supporters and those of his rival Archdeacon Timotheus. According to Socrates Scholasticus, the Alexandrians were always rioting.\nThus, Cyril followed his uncle in a position that had become powerful and influential, rivalling that of the prefect in a time of turmoil and frequently violent conflict between the cosmopolitan city's pagan, Jewish, and Christian inhabitants. He began to exert his authority by causing the churches of the Novatianists to be closed and their sacred vessels to be seized.\nControversies.\nDispute with the Prefect.\nOrestes, \"Praefectus augustalis\" of the Diocese of Egypt, steadfastly resisted Cyril's ecclesiastical encroachment upon secular prerogatives.\nTension between the parties increased when in 415, Orestes published an edict that outlined new regulations regarding mime shows and dancing exhibitions in the city, which attracted large crowds and were commonly prone to civil disorder of varying degrees. Crowds gathered to read the edict shortly after it was posted in the city's theater. Cyril sent the \"grammaticus\" Hierax to discover the content of the edict. The edict angered Christians as well as Jews. At one such gathering, Hierax read the edict and applauded the new regulations, prompting a disturbance. Many people felt that Hierax was attempting to incite the crowd\u2014particularly the Jews\u2014into sedition. Orestes had Hierax tortured in public in a theatre. This order had two aims: one to quell the riot, the other to mark Orestes' authority over Cyril.\nSocrates Scholasticus recounts that upon hearing of Hierex's severe and public punishment, Cyril threatened to retaliate against the Jews of Alexandria with \"the utmost severities\" if the harassment of Christians did not cease immediately. In response to Cyril's threat, the Jews of Alexandria grew even more furious, eventually resorting to violence against the Christians. They plotted to flush the Christians out at night by running through the streets claiming that the Church of Alexander was on fire. When Christians responded to what they were led to believe was the burning down of their church, \"the Jews immediately fell upon and slew them\" by using rings to recognize one another in the dark and killing everyone else in sight. When the morning came, Cyril, along with many of his followers, took to the city's synagogues in search of the perpetrators of the massacre.\nAccording to Socrates Scholasticus, after Cyril rounded up all the Jews in Alexandria he ordered them to be stripped of all possessions, banished them from Alexandria, and allowed their goods to be pillaged by the remaining citizens of Alexandria. Scholasticus alleges that all the Jews of Alenandria were banished, while John of Niki\u00fb says it was only those involved in the ambush and massacre. Susan Wessel says that, while it is not clear whether Scholasticus was a Novationist (whose churches Cyril had closed), he was apparently sympathetic towards them, and repeatedly accuses clear Cyril of abusing his episcopal power by infringing on the rights and duties of the secular authorities. Wessel says, however, \"...Socrates probably does not provide accurate and unambiguous information about Cyril's relationship to imperial authority\".\nNonetheless, with Cyril's banishment of the Jews, however many, \"Orestes [...] was filled with great indignation at these transactions, and was excessively grieved that a city of such magnitude should have been suddenly bereft of so large a portion of its population.\" Because of this, the feud between Cyril and Orestes intensified, and both men wrote to the emperor regarding the situation. Eventually, Cyril attempted to reach out to Orestes through several peace overtures, including attempted mediation and, when that failed, showed him the Gospels, which he interpreted to indicate that the religious authority of Cyril would require Orestes' acquiescence in the bishop's policy. Nevertheless, Orestes remained unmoved by such gestures.\nThis refusal almost cost Orestes his life. Nitrian monks came from the desert and instigated a riot against Orestes among the population of Alexandria. These monks had resorted to violence 15 years before, during a controversy between Theophilus (Cyril's uncle) and the \"Tall Brothers\"; the monks assaulted Orestes and accused him of being a pagan. Orestes rejected the accusations, showing that he had been baptised by the Archbishop of Constantinople. A monk named Ammonius threw a stone hitting Orestes in the head. The prefect had Ammonius tortured to death, whereupon the Patriarch allegedly honored him as a martyr. However, at least according to Scholasticus, the Christian community displayed a general lack of enthusiasm for Ammonius's case for martyrdom. The prefect then wrote to the emperor Theodosius II, as did Cyril.\nMurder of Hypatia.\nThe Prefect Orestes enjoyed the political backing of Hypatia, an astronomer, philosopher and mathematician who had considerable moral authority in the city of Alexandria, and who had extensive influence. Indeed, many students from wealthy and influential families came to Alexandria purposely to study privately with Hypatia , and many of these later attained high posts in government and the Church. Several Christians thought that Hypatia's influence had caused Orestes to reject all conciliatory offerings by Cyril. Modern historians think that Orestes had cultivated his relationship with Hypatia to strengthen a bond with the pagan community of Alexandria, as he had done with the Jewish one, in order to better manage the tumultuous political life of the Egyptian capital. A mob, led by a lector named Peter, took Hypatia from her chariot and murdered her, hacking her body apart and burning the pieces outside the city walls.\nNeoplatonist historian Damascius (c.\u00a0458 \u2013 c.\u00a0538) was \"anxious to exploit the scandal of Hypatia's death\", and attributed responsibility for her murder to Bishop Cyril and his Christian followers. Damascius's account of the Christian murder of Hypatia is the sole historical source attributing direct responsibility to Bishop Cyril. Some modern studies, as well as the 2009 Hypatia biopic \"Agora\" represent Hypatia as falling casualty to a conflict between two Christian factions, one peaceful and moderate and led by Orestes, with the support of Hypatia, and fundamentalist faction enforced by death squads and led by Patriarch Cyril. According to lexicographer William Smith, \"She was accused of too much familiarity with Orestes, prefect of Alexandria, and the charge spread among the clergy, who took up the notion that she interrupted the friendship of Orestes with their archbishop, Cyril.\" Scholasticus, alleges that Hypatia fell \"victim to the political jealousy which at the time prevailed\" and that news of Hypatia's murder, \"brought no small disgrace\", not only to Patriarch Cyril but to the whole Christian Church in Alexandria, \"for murder and slaughter and all such things are altogether opposed to the Christian religion.\"\nAccording to Byzantinist Fr. Adrian Fortescue, however, \"In March of 415, a mob of Christians, led by the Parabolani and by a Lector named Peter, cruelly tore her to pieces on the steps of a church. Various writers have suggested more or less plainly that the Patriarch was involved in the crime. Sokrates does not say so plainly, but he implies it and adds a solemn moral reflection. As a matter of fact, not only is there no sort of evidence that he had anything to do with it, there are positive reasons for knowing that he had not. After the murder, a deputation of citizens went to Constantinople to petition the Emperor to prevent such horrors in the future and to put down the disorderly Parabolani, and the first means they urge for that purpose is that the Patriarch should stay in the city (Orestes wanted him banished). Moreover, if ever a man had bitter enemies it was Cyril. Wilful murder was considered just as unsuitable conduct for bishops in the fifth century as it is now. Why, during all the fierce conflict with the Nestorians, when they brought up every possible charge against him, did no one think of calling him Hypatia's murderer? Although to accuse our Saint of this horrid story is a gross calumny, there is no doubt that in other ways he did give annoyance to the government.\"\nConflict with Nestorius.\nAnother major conflict was between the Alexandrian and Antiochian schools of ecclesiastical reflection, piety, and discourse. This long running conflict widened with the third canon of the First Council of Constantinople which granted the see of Constantinople primacy over the older sees of Alexandria and Antioch. Thus, the struggle between the sees of Alexandria and Antioch now included Constantinople. The conflict came to a head in 428 after Nestorius, who originated in Antioch, was made Archbishop of Constantinople.\nCyril gained an opportunity to restore Alexandria's pre-eminence over both Antioch and Constantinople when an Antiochine priest who was in Constantinople at Nestorius' behest began to preach against calling Mary the \"Mother of God\" (\"Theotokos\"). As the term \"Mother of God\" had long been attached to Mary, the laity in Constantinople complained against the priest. Rather than repudiating the priest, Nestorius intervened on his behalf. Nestorius argued that Mary was neither a \"Mother of Man\" nor \"Mother of God\" as these referred to Christ's two natures; rather, Mary was the \"Mother of Christ\" (Greek: \"Christotokos\"). Christ, according to Nestorius, was the conjunction of the Godhead with his \"temple\" (which Nestorius was fond of calling his human nature). The controversy seemed to be centered on the issue of the suffering of Christ. Cyril maintained that the Son of God or the divine Word, truly suffered \"in the flesh.\" However, Nestorius claimed that the Son of God was altogether incapable of suffering, even within his union with the flesh. Eusebius of Dorylaeum went so far as to accuse Nestorius of adoptionism. By this time, news of the controversy in the capital had reached Alexandria. At Easter 429 A.D., Cyril wrote a letter to the Egyptian monks warning them of Nestorius's views. A copy of this letter reached Constantinople where Nestorius preached a sermon against it. This began a series of letters between Cyril and Nestorius which gradually became more strident in tone. Finally, Emperor Theodosius II convoked the Council of Ephesus (in 431) to solve the dispute. Cyril selected Ephesus as the venue since it supported the veneration of Mary. The council was convoked before Nestorius's supporters from Antioch and Syria had arrived and thus Nestorius refused to attend when summoned. Predictably, the Council ordered the deposition and exile of Nestorius for heresy.\nHowever, when John of Antioch and the other pro-Nestorius bishops finally reached Ephesus, they assembled their own Council, condemned Cyril for heresy, deposed him from his see, and labelled him as a \"monster, born and educated for the destruction of the church\". Theodosius, by now old enough to hold power by himself, annulled the verdict of the Council and arrested Cyril, but Cyril eventually escaped. Having fled to Egypt, Cyril bribed Theodosius's courtiers, and sent a mob led by Dalmatius, a hermit, to besiege Theodosius's palace, and shout abuse; the emperor eventually gave in, sending Nestorius into minor exile (Upper Egypt).\nCyril died about 444, but the controversies were to continue for decades, from the \"Robber Synod\" of Ephesus (449) to the Council of Chalcedon (451) and beyond.\nTheology.\nCyril regarded the embodiment of God in the person of Jesus Christ to be so mystically powerful that it spread out from the body of the God-man into the rest of the race, to reconstitute human nature into a graced and deified condition of the saints, one that promised immortality and transfiguration to believers. Nestorius, on the other hand, saw the incarnation as primarily a moral and ethical example to the faithful, to follow in the footsteps of Jesus. Cyril's constant stress was on the simple idea that it was God who walked the streets of Nazareth (hence Mary was \"Theotokos\", meaning \"God bearer\", which became in Latin \"Mater Dei or Dei Genitrix\", or Mother of God), and God who had appeared in a transfigured humanity. Nestorius spoke of the distinct \"Jesus the man\" and \"the divine Logos\" in ways that Cyril thought were too dichotomous, widening the ontological gap between man and God in a way that some of his contemporaries believed would annihilate the person of Christ.\nThe main issue that prompted this dispute between Cyril and Nestorius was the question which arose at the Council of Constantinople: What exactly was the being to which Mary gave birth? Cyril affirmed that the Holy Trinity consists of a singular divine nature, essence, and being (\"ousia\") in three distinct aspects, instantiations, or subsistencies of being (\"hypostases\"). These distinct hypostases are the Father or God in Himself, the Son or Word (\"Logos\"), and the Holy Spirit. Then, when the Son became flesh and entered the world, the pre-Incarnate divine nature and assumed human nature both remained, but became \"united\" in the person of Jesus. This resulted in the miaphysite slogan \"One Nature united out of two\" being used to encapsulate the theological position of this Alexandrian bishop. \nAccording to Cyril's theology, there were two states for the Son of God: the state that existed \"prior\" to the Son (or Word/Logos) becoming enfleshed in the person of Jesus and the state that actually became enfleshed. The Logos Incarnate suffered and died on the Cross, and therefore the Son was able to suffer without suffering. Cyril passionately argued for the continuity of a single subject, God the Word, from the pre-Incarnate state to the Incarnate state. The divine Logos was really present in the flesh and in the world\u2014not merely bestowed upon, semantically affixed to, or morally associated with the man Jesus, as the adoptionists and, he believed, Nestorius had taught.\nMariology.\nCyril of Alexandria became noted in Church history because of his spirited fight for the title \"Theotokos\" during the First Council of Ephesus (431).\nHis writings include the homily given in Ephesus and several other sermons. Some of his alleged homilies are in dispute as to his authorship. In several writings, Cyril focuses on the love of Jesus to his mother. On the Cross, he overcomes his pain and thinks of his mother. At the wedding in Cana, he bows to her wishes. Cyril created the basis for all other mariological developments through his teaching of the blessed Virgin Mary, as the \"Mother of God.\" The conflict with Nestorius was mainly over this issue, and it has often been misunderstood. \"[T]he debate was not so much about Mary as about Jesus. The question was not what honors were due to Mary, but how one was to speak of the birth of Jesus.\"\nSt. Cyril received an important recognition of his preachings by the Second Council of Constantinople (553 d.C.) which declared;\n\"St. Cyril who announced the right faith of Christians\" (Anathematism XIV, Denzinger et Schoenmetzer 437).\nWorks.\nCyril was a scholarly archbishop and a prolific writer. In the early years of his active life in the Church he wrote several exegetical documents. Among these were: \"Commentaries on the Old Testament\", \"Thesaurus\", \"Discourse Against Arians\", \"Commentary on St. John's Gospel\", and \"Dialogues on the Trinity\". In 429 as the Christological controversies increased, the output of his writings was so extensive that his opponents could not match it. His writings and his theology have remained central to the tradition of the Fathers and to all Orthodox to this day.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7385", "revid": "15135172", "url": "https://en.wikipedia.org/wiki?curid=7385", "title": "Character set", "text": ""}
{"id": "7387", "revid": "14240191", "url": "https://en.wikipedia.org/wiki?curid=7387", "title": "Cyril of Jerusalem", "text": "Christian theologian, bishop, and saint (c. 313\u00a0\u2013 386)\n \nCyril of Jerusalem (, \"K\u00fdrillos A Ierosol\u00fdmon\"; ; c.\u2009313 \u2013 AD 386) was a theologian of the early Church. About the end of AD 350 he succeeded Maximus as Bishop of Jerusalem, but was exiled on more than one occasion due to the enmity of Acacius of Caesarea, and the policies of various emperors. Cyril left important writings documenting the instruction of catechumens and the order of the Liturgy in his day.\nCyril is venerated as a saint within the Roman Catholic Church, the Eastern Orthodox Church, Oriental Orthodox Church, and the Anglican Communion. In 1883, Cyril was declared a Doctor of the Church by Pope Leo XIII. He is highly respected among Palestinian Christians.\nCyril is remembered in the Church of England with a commemoration on 18 March.\nLife and character.\nLittle is known of his life before he became a bishop; the assignment of his birth to the year 315 rests on conjecture. According to Butler, Cyril was born at or near the city of Jerusalem, and was apparently well-read in both the writings of the early Christian theologians and the Greek philosophers.\nCyril was ordained a deacon by Bishop Macarius of Jerusalem in about 335 and a priest some eight years later by Bishop Maximus. Around the end of 350 he succeeded Maximus in the See of Jerusalem, although the evidence for this relies on the \"Catecheses\" written by Cyril where he refers to himself as \"bishop\". Jerome also suggests Cyril was an Arian at this stage\nCyril is described as preacher and liturgist by the pilgrim Egeria.\nEpiscopacy.\nRelations between Metropolitan Acacius of Caesarea and Cyril became strained. Acacius is presented as a leading Arian by the orthodox historians, and his opposition to Cyril in the 350s is attributed by these writers to this. Sozomen also suggests that the tension may have been increased by Acacius's jealousy of the importance assigned to Cyril's See by the Council of Nicaea, as well as by the threat posed to Caesarea by the rising influence of the seat of Jerusalem as it developed into the prime Christian holy place and became a centre of pilgrimage.\nAcacius charged Cyril with selling church property. The city of Jerusalem had suffered drastic food shortages at which point church historians Sozomen and Theodoret report \"Cyril secretly sold sacramental ornaments of the church and a valuable holy robe, fashioned with gold thread that the emperor Constantine had once donated for the bishop to wear when he performed the rite of Baptism\", possibly to keep people from starving.\nFor two years, Cyril resisted Acacius' summons to account for his actions, but a church council held under Acacius's influence in 357 deposed Cyril in his absence, and Cyril took refuge with Silvanus, Bishop of Tarsus. The following year, 359, in an atmosphere more hostile to Acacius, the Council of Seleucia reinstated Cyril and deposed Acacius. In 360 this was reversed by Emperor Constantius again, and Cyril suffered another year's exile from Jerusalem until the Emperor Julian's accession allowed him to return in 361.\nCyril was once again banished from Jerusalem by the Arian Emperor Valens in 367, but was able to return again after Valens's death in 378, after which he remained undisturbed until his death in 386. In 380, Gregory of Nyssa came to Jerusalem on the recommendation of a council held at Antioch in the preceding year. He seemingly found the faith in good shape, but worried that the city was prey to parties and corrupt in morals. Cyril's jurisdiction over Jerusalem was expressly confirmed by the First Council of Constantinople (381), at which he was present. At that council he voted for acceptance of the term \"homoousios\" (which defined the nature between \"God the Father\", and \"God the Son\"), having been finally convinced that there was no better alternative. His story is perhaps best representative of those Eastern bishops (perhaps a majority) initially mistrustful of Nicaea, who came to accept the creed of that council, and the doctrine of the \"homoousion\".\nTheological position.\nThough his theology was at first somewhat indefinite in phraseology, he undoubtedly gave a thorough adhesion to the Nicene Orthodoxy. Even if he did avoid the debatable term \"homoousios\", he expressed its sense in many passages, which exclude equally Patripassianism, Sabellianism, and the formula \"there was a time when the Son was not\" attributed to Arius. In other points he takes the ordinary ground of the Eastern Fathers, as in the emphasis he lies on the freedom of the will, the \"autexousion\" (\u03b1\u1f50\u03c4\u03b5\u03be\u03bf\u03cd\u03c3\u03b9\u03bf\u03bd), and in his view of the nature of sin. To him sin is the consequence of freedom, not a natural condition. The body is not the cause, but the instrument of sin. The remedy for it is repentance, on which he insists. Like many of the Eastern Fathers, he focuses on high moral living as essential to true Christianity. His doctrine of the Resurrection is not quite so realistic as that of other Fathers; but his conception of the Church is decidedly empirical: the existing Church form is the true one, intended by Christ, the completion of the Church of the Old Testament. His interpretation of the Eucharist is disputed. Some argue he sometimes seems to approach the symbolic view, though he professes a strong realistic doctrine. The bread and wine are not mere elements, but the body and blood of Christ.\nCyril's writings are filled with the loving and forgiving nature of God which was somewhat uncommon during his time period. Cyril fills his writings with great lines of the healing power of forgiveness and the Holy Spirit, like \"The Spirit comes gently and makes himself known by his fragrance. He is not felt as a burden for God is light, very light. Rays of light and knowledge stream before him as the Spirit approaches. The Spirit comes with the tenderness of a true friend to save, to heal, to teach, to counsel, to strengthen and to console\". Cyril himself followed God's message of forgiveness many times throughout his life. This is most clearly seen in his two major exiles where Cyril was disgraced and forced to leave his position and his people behind. He never wrote or showed any ill will towards those who wronged him. Cyril stressed the themes of healing and regeneration in his catechesis.\nCatechetical lectures.\nCyril's famous twenty-three lectures given to catechumens in Jerusalem being prepared for, and after, baptism are best considered in two parts: the first eighteen lectures are commonly known as the \"Catechetical Lectures\", \"Catechetical Orations\" or \"Catechetical Homilies\", while the final five are often called the \"Mystagogic Catecheses\" (\u03bc\u03c5\u03c3\u03c4\u03b1\u03b3\u03c9\u03b3\u03b9\u03ba\u03b1\u03af), because they deal with the \"mysteries\" (\u03bc\u03c5\u03c3\u03c4\u03ae\u03c1\u03b9\u03b1) i.e. Sacraments of Baptism, Confirmation and the Eucharist.\nHis catechetical lectures (Greek \u039a\u03b1\u03c4\u03b7\u03c7\u03ae\u03c3\u03b5\u03b9\u03c2, \"Kat\u0113ch\u0113seis\") are generally assumed, on the basis of limited evidence, to have been delivered either in Cyril's early years as a bishop, around 350, or perhaps in 348, while Cyril was still a priest, deputising for his bishop, Maximus. The \"Catechetical Lectures\" were given in the \"Martyrion\", the basilica erected by Constantine. They contain instructions on the principal topics of Christian faith and practice, in a popular rather than scientific manner, full of a warm pastoral love and care for the catechumens to whom they were delivered. Each lecture is based upon a text of Scripture, and there is an abundance of Scriptural quotation throughout. In the \"Catechetical Lectures\", parallel with the exposition of the Creed as it was then received in the Church of Jerusalem are vigorous polemics against pagan, Jewish, and heretical errors. They are of great importance for the light which they throw upon the method of instruction usual of that age, as well as upon the liturgical practises of the period, of which they give the fullest account extant.\nIt is not only among us, who are marked with the name of Christ, that the dignity of faith is great; all the business of the world, even of those outside the Church, is accomplished by faith. By faith, marriage laws join in union persons who were strangers to one another. By faith, agriculture is sustained; for a man does not endure the toil involved unless he believes he will reap a harvest. By faith, seafaring men, entrusting themselves to a tiny wooden craft, exchange the solid element of the land for the unstable motion of the waves.\"\nIn the 13th lecture, Cyril of Jerusalem discusses the Crucifixion and burial of Jesus Christ. The main themes that Cyril focuses on in these lectures are Original sin and Jesus' sacrificing himself to save us from our sins. Also, the burial and Resurrection which occurred three days later proving the divinity of Jesus Christ and the loving nature of the Father. Cyril was very adamant about the fact that Jesus went to his death with full knowledge and willingness. Not only did he go willingly but throughout the process he maintained his faith and forgave all those who betrayed him and engaged in his execution. Cyril writes \"who did not sin, neither was deceit found in his mouth, who, when he was reviled, did not revile, when he suffered did not threaten\". This line by Cyril shows his belief in the selflessness of Jesus especially in this last final act of Love. The lecture also gives a sort of insight to what Jesus may have been feeling during the execution from the whippings and beatings, to the crown of thorns, to the nailing on the cross. Cyril intertwines the story with the messages Jesus told throughout his life before his execution relating to his final act. For example, Cyril writes \"I gave my back to those who beat me and my cheeks to blows; and my face I did not shield from the shame of spitting\". This clearly reflects the teachings of Jesus to turn the other cheek and not raising your hands against violence because violence just begets violence begets violence. The segment of the Catechesis really reflects the voice Cyril maintained in all of his writing. The writings always have the central message of the Bible; Cyril is not trying to add his own beliefs in reference to religious interpretation and remains grounded in true biblical teachings.\nDanielou sees the baptism rite as carrying eschatological overtones, in that \"to inscribe for baptism is to write one's name in the register of the elect in heaven\".\nEschatology.\nOded Irshai observed that Cyril lived in a time of intense apocalyptic expectation, when Christians were eager to find apocalyptic meaning in every historical event or natural disaster. Cyril spent a good part of his episcopacy in intermittent exile from Jerusalem. Abraham Malherbe argued that when a leader's control over a community is fragile, directing attention to the imminent arrival of the antichrist effectively diverts attention from that fragility.\nSoon after his appointment, Cyril in his \"Letter to Constantius\" of 351 recorded the appearance of a cross of light in the sky above Golgotha, witnessed by the whole population of Jerusalem. The Greek church commemorates this miracle on 7 May. Though in modern times the authenticity of the \"Letter\" has been questioned, on the grounds that the word \"homoousios\" occurs in the final blessing, many scholars believe this may be a later interpolation, and accept the letter's authenticity on the grounds of other pieces of internal evidence.\nCyril interpreted this as both a sign of support for Constantius, who was soon to face the usurper Magnentius, and as announcing the Second Coming, which was soon to take place in Jerusalem. Not surprisingly, in Cyril's eschatological analysis, Jerusalem holds a central position.\nMatthew 24:6 speaks of \"wars and reports of wars\", as a sign of the End Times, and it is within this context that Cyril read Julian's war with the Persians. Matthew 24:7 speaks of \"earthquakes from place to place\", and Jerusalem experienced an earthquake in 363 at a time when Julian was attempting to rebuild the temple in Jerusalem. Embroiled in a rivalry with Acacius of Caesarea over the relative primacy of their respective sees, Cyril saw even ecclesial discord a sign of the Lord's coming. Catechesis 15 would appear to cast Julian as the antichrist, although Irshai views this as a later interpolation.\n\"In His first coming, He endured the Cross, despising shame; in His second, He comes attended by a host of Angels, receiving glory. We rest not then upon His first advent only, but look also for His second.\" He looked forward to the Second Advent which would bring an end to the world and then the created world to be made anew. At the Second Advent he expected to rise in the resurrection if it came after his time on earth.\n\"Mystagogic Catecheses\".\nThere has been considerable controversy over the date and authorship of the \"Mystagogic Catecheses\", addressed to the newly baptized, in preparation for the reception of Holy Communion, with some scholars having attributed them to Cyril's successor as Bishop of Jerusalem, John. Many scholars would currently view the \"Mystagogic Catecheses\" as being written by Cyril, but in the 370s or 380s, rather than at the same time as the \"Catechetical Lectures\".\nAccording to the Spanish pilgrim Egeria, these \"mystagogical catecheses\" were given to the newly baptised in the Church of the \"Anastasis\" in the course of Easter Week.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7388", "revid": "6077664", "url": "https://en.wikipedia.org/wiki?curid=7388", "title": "Hanukkah", "text": "Jewish holiday\nHanukkah (; Hebrew: &lt;templatestyles src=\"Script/styles_hebrew.css\" /&gt;\u05d7\u05b2\u05e0\u05bb\u05db\u05b8\u05bc\u05d4\u200e, \u00a0', \u00a0' ) is a Jewish festival commemorating the recovery of Jerusalem and subsequent rededication of the Second Temple at the beginning of the Maccabean Revolt against the Seleucid Empire in the 2nd century BCE.\nHanukkah is observed for eight nights and days, starting on the 25th day of Kislev according to the Hebrew calendar, which may occur at any time from late November to late December in the Gregorian calendar. The festival is observed by lighting the candles of a candelabrum with nine branches, commonly called a menorah or hanukkiah. One branch is typically placed above or below the others and its candle is used to light the other eight candles. This unique candle is called the \"shammash\" (, \"attendant\"). Each night, one additional candle is lit by the \"shammash\" until all eight candles are lit together on the final night of the festival. Other Hanukkah festivities include singing Hanukkah songs, playing the game of dreidel and eating oil-based foods, such as latkes and sufganiyot, and dairy foods. Since the 1970s, the worldwide Chabad Hasidic movement has initiated public menorah lightings in open public places in many countries.\nOriginally instituted as a feast \"in the manner of Sukkot (Booths)\", it does not come with the corresponding obligations, and is therefore a relatively minor holiday in strictly religious terms. Nevertheless, Hanukkah has attained major cultural significance in North America and elsewhere, especially among secular Jews, due to often occurring around the same time as Christmas during the festive season.\nEtymology.\nThe name \"Hanukkah\" derives from the Hebrew verb \"\", meaning \"to dedicate\". On Hanukkah, the Maccabean Jews regained control of Jerusalem and rededicated the Temple.\nMany homiletical explanations have been given for the name:\nAlternative spellings.\nIn Hebrew, the word Hanukkah is written &lt;templatestyles src=\"Script/styles_hebrew.css\" /&gt;\u05d7\u05b2\u05e0\u05bb\u05db\u05b8\u05bc\u05d4\u200e or &lt;templatestyles src=\"Script/styles_hebrew.css\" /&gt;\u05d7\u05b2\u05e0\u05d5\u05bc\u05db\u05b8\u05bc\u05d4\u200e (). It is most commonly transliterated to English as \"Hanukkah\" or \"\". The spelling \"Hanukkah\", which is based on using characters of the English alphabet as symbols to re-create the word's correct spelling in Hebrew, is the most common and the preferred choice of Merriam\u2013Webster, \"Collins English Dictionary\", the \"Oxford Style Manual\", and the style guides of \"The New York Times\" and \"The Guardian\". The sound represented by \"Ch\" (, similar to the Scottish pronunciation of \"loch\") is not native to the English language, although it is native to the Welsh language. Furthermore, the letter \"\u1e25eth\" (), which is the first letter in the Hebrew spelling, is pronounced differently in modern Hebrew (voiceless uvular fricative) from in classical Hebrew (voiceless pharyngeal fricative ), and neither of those sounds is unambiguously representable in English spelling. However, its original sound is closer to the English \"H\" than to the Scottish \"Ch\", and \"Hanukkah\" more accurately represents the spelling in the Hebrew alphabet. Moreover, the 'kaf' consonant is geminate in classical (but not modern) Hebrew. Adapting the classical Hebrew pronunciation with the geminate and pharyngeal can lead to the spelling \"Hanukkah\", while adapting the modern Hebrew pronunciation with no gemination and uvular leads to the spelling .\nFestival of Lights.\nIn Modern Hebrew, Hanukkah may also be called the Festival of Lights (, ), based on a comment by Josephus in \"Antiquities of the Jews\", \u03ba\u03b1\u1f76 \u1f10\u03be \u1f10\u03ba\u03b5\u03af\u03bd\u03bf\u03c5 \u03bc\u03ad\u03c7\u03c1\u03b9 \u03c4\u03bf\u1fe6 \u03b4\u03b5\u1fe6\u03c1\u03bf \u03c4\u1f74\u03bd \u1f11\u03bf\u03c1\u03c4\u1f74\u03bd \u1f04\u03b3\u03bf\u03bc\u03b5\u03bd \u03ba\u03b1\u03bb\u03bf\u1fe6\u03bd\u03c4\u03b5\u03c2 \u03b1\u1f50\u03c4\u1f74\u03bd \u03c6\u1ff6\u03c4\u03b1 \"And from then on we celebrate this festival, and we call it Lights\". The first Hebrew translation of \"Antiquities\" (1864) used () \"Festival of Lamps\", but the translation \"Festival of Lights\" () appeared by the end of the nineteenth century.\nHistorical sources.\nBooks of Maccabees.\nThe story of Hanukkah is told in the books of the First and Second Maccabees, which describe in detail the rededication of the Temple in Jerusalem and the lighting of the menorah. These books, however, are not a part of the canonized Masoretic Text version of the Tanakh (Hebrew and Aramaic language Jewish Bible) used and accepted by normative Rabbinical Judaism and therefore modern Jews (as copied, edited and distributed by a group of Jews known as the Masoretes between the 7th and 10th centuries of the Common Era). However, the books of Maccabees were included among the deuterocanonical books added to the Septuagint, a Jewish scholarly Greek-language translation of the Hebrew Bible originally compiled in the mid-3rd century BCE. The Roman Catholic and Orthodox Churches consider the books of Maccabees as a canonical part of the Old Testament. \nThe eight-day rededication of the temple is described in 1 Maccabees, though the miracle of the oil does not appear here. A story similar in character, and older in date, is the one alluded to in 2 Maccabees according to which the relighting of the altar fire by Nehemiah was due to a miracle which occurred on the 25th of Kislev, and which appears to be given as the reason for the selection of the same date for the rededication of the altar by Judah Maccabee. The above account in 1 Maccabees, as well as 2 Maccabees portrays the feast as a delayed observation of the eight-day Feast of Booths (Sukkot); similarly 2 Maccabees explains the length of the feast as \"in the manner of the Feast of Booths\".\nEarly rabbinic sources.\nMegillat Taanit (1st century) contains a list of festive days on which fasting or eulogizing is forbidden. It specifies, \"On the 25th of [Kislev] is Hanukkah of eight days, and one is not to eulogize\" and then references the story of the rededication of the Temple.\nThe Mishna (late 2nd century) mentions Hanukkah in several places, but never describes its laws in detail and never mentions any aspect of the history behind it. To explain the Mishna's lack of a systematic discussion of Hanukkah, Rav Nissim Gaon postulated that information on the holiday was so commonplace that the Mishna felt no need to explain it. Modern scholar Reuvein Margolies suggests that as the Mishnah was redacted after the Bar Kochba revolt, its editors were reluctant to include explicit discussion of a holiday celebrating another relatively recent revolt against a foreign ruler, for fear of antagonizing the Romans.\nThe miracle of the one-day supply of oil miraculously lasting eight days is described in the Talmud, committed to writing about 600 years after the events described in the books of Maccabees. The Talmud says that after the forces of Antiochus IV had been driven from the Temple, the Maccabees discovered that almost all of the ritual olive oil had been profaned. They found only a single container that was still sealed by the High Priest, with enough oil to keep the menorah in the Temple lit for a single day. They used this, yet it burned for eight days (the time it took to have new oil pressed and made ready).\nThe Talmud presents three options:\nExcept in times of danger, the lights were to be placed outside one's door, on the opposite side of the mezuza, or in the window closest to the street. Rashi, in a note to \"Shabbat 21b,\" says their purpose is to publicize the miracle. The blessings for Hanukkah lights are discussed in tractate \"Succah,\" p.\u00a046a.\nMegillat Antiochus (probably composed in the 2nd century) concludes with the following words:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThe Al HaNissim prayer is recited on Hanukkah as an addition to the Amidah prayer, which was formalized in the late 1st century. \"Al HaNissim\" describes the history of the holiday as follows:\nIn the days of Mattiyahu ben Yohanan, high priest, the Hasmonean and his sons, when the evil Greek kingdom stood up against Your people Israel, to cause them to forget Your Torah and abandon the ways You desire \u2013 You, in Your great mercy, stood up for them in their time of trouble; You fought their fight, You judged their judgment, You took their revenge; You delivered the mighty into the hands of the weak, the many into the hands of the few, the impure into the hands of the pure, the evil into the hands of the righteous, the sinners into the hands of those who engaged in Your Torah; You made yourself a great and holy name in Your world, and for Your people Israel You made great redemption and salvation as this very day. And then Your sons came to the inner chamber of Your house, and cleared Your Temple, and purified Your sanctuary, and lit candles in Your holy courtyards, and established eight days of Hanukkah for thanksgiving and praise to Your holy name.\nNarrative of Josephus.\nThe Jewish historian Titus Flavius Josephus narrates in his book, Jewish Antiquities XII, how the victorious Judas Maccabeus ordered lavish yearly eight-day festivities after rededicating the Temple in Jerusalem that had been profaned by Antiochus IV Epiphanes. Josephus does not say the festival was called Hanukkah but rather the \"Festival of Lights\":\nNow Judas celebrated the festival of the restoration of the sacrifices of the temple for eight days, and omitted no sort of pleasures thereon; but he feasted them upon very rich and splendid sacrifices; and he honored God, and delighted them by hymns and psalms. Nay, they were so very glad at the revival of their customs, when, after a long time of intermission, they unexpectedly had regained the freedom of their worship, that they made it a law for their posterity, that they should keep a festival, on account of the restoration of their temple worship, for eight days. And from that time to this we celebrate this festival, and call it Lights. I suppose the reason was because this liberty beyond our hopes appeared to us; and that thence was the name given to that festival. Judas also rebuilt the walls round about the city, and reared towers of great height against the incursions of enemies, and set guards therein. He also fortified the city Bethsura, that it might serve as a citadel against any distresses that might come from our enemies.\nOther ancient sources.\nIn the New Testament, John 10:22\u201323 says, \"Then came the Festival of Dedication at Jerusalem. It was winter, and Jesus was in the temple courts walking in Solomon's Colonnade\" (NIV). The Greek noun used appears in the neuter plural as \"the renewals\" or \"the consecrations\" (Greek: ; ). The same root appears in 2 Esdras 6:16 in the Septuagint to refer specifically to Hanukkah. This Greek word was chosen because the Hebrew word for 'consecration' or 'dedication' is \"Hanukkah\" (). The Aramaic New Testament uses the Aramaic word (a close synonym), which literally means 'renewal' or 'to make new'.\nStory.\nBackground.\nAfter the death of Alexander the Great in 323 BCE, Judea became part of the Ptolemaic Kingdom of Egypt until 200 BCE, when King Antiochus III the Great of Syria defeated King Ptolemy V Epiphanes of Egypt at the Battle of Panium. Judea then became part of the Seleucid Empire of Syria. King Antiochus III the Great, wanting to conciliate his new Jewish subjects, guaranteed their right to \"live according to their ancestral customs\" and to continue to practice their religion in the Temple of Jerusalem. However, in 175 BCE, Antiochus IV Epiphanes, the son of Antiochus III, invaded Judea, at the request of the sons of Tobias. The Tobiads, who led the Hellenizing Jewish faction in Jerusalem, were expelled to Syria around 170 BCE when the high priest Onias and his pro-Egyptian faction wrested control from them. The exiled Tobiads lobbied Antiochus IV Epiphanes to recapture Jerusalem. As Flavius Josephus relates:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The king being thereto disposed beforehand, complied with them, and came upon the Jews with a great army, and took their city by force, and slew a great multitude of those that favored Ptolemy, and sent out his soldiers to plunder them without mercy. He also spoiled the temple, and put a stop to the constant practice of offering a daily sacrifice of expiation for three years and six months.\nTraditional view.\nWhen the Second Temple in Jerusalem was looted and services stopped, Judaism was outlawed. In 167 BCE, Antiochus ordered an altar to Zeus erected in the Temple. He banned brit milah (circumcision) and ordered pigs to be sacrificed at the altar of the temple.\nAntiochus's actions provoked a large-scale revolt. Mattathias (Mattityahu), a Jewish priest, and his five sons Jochanan, Simeon, Eleazar, Jonathan, and Judah led a rebellion against Antiochus. It started with Mattathias killing first a Jew who wanted to comply with Antiochus's order to sacrifice to Zeus, and then a Greek official who was to enforce the government's behest (1 Mac. 2, 24\u201325). Judah became known as Yehuda HaMakabi (\"Judah the Hammer\"). By 166 BCE, Mattathias had died, and Judah took his place as leader. By 164 BCE, the Jewish revolt against the Seleucid monarchy was successful. The Temple was liberated and rededicated. The festival of Hanukkah was instituted to celebrate this event. Judah ordered the Temple to be cleansed, a new altar to be built in place of the polluted one and new holy vessels to be made. According to the Talmud,\"For when the Greeks entered the Sanctuary, they defiled all the oils therein, and when the Hasmonean dynasty prevailed against and defeated them, they made search and found only one cruse of oil which lay with the seal of the kohen gadol (high priest), but which contained sufficient [oil] for one day's lighting only; yet a miracle was wrought therein, and they lit [the lamp] therewith for eight days. The following year these [days] were appointed a Festival with [the recital of] Hallel and thanksgiving.\"\n\u2014Shabbat 21b\nTertiary sources in the Jewish tradition make reference to this account.\nMaimonides (12th century) described Hanukkah as follows:\nWhen, on the twenty-fifth of Kislev, the Jews had emerged victorious over their foes and destroyed them, they re-entered the Temple where they found only one jar of pure oil, enough to be lit for only a single day; yet they used it for lighting the required set of lamps for eight days, until they managed to press olives and produce pure oil. Because of this, the sages of that generation ruled that the eight days beginning with the twenty-fifth of Kislev should be observed as days of rejoicing and praising the Lord. Lamps are lit in the evening over the doors of the homes, on each of the eight nights, so as to display the miracle. These days are called Hanukkah, when it is forbidden to lament or to fast, just as it is on the days of Purim. Lighting the lamps during the eight days of Hanukkah is a religious duty imposed by the sages.\nAcademic sources.\nSome modern scholars, following the account in 2 Maccabees, observe that the king was intervening in an internal civil war between the Maccabean Jews and the Hellenized Jews in Jerusalem. These competed violently over who would be the High Priest, with traditionalists with Hebrew/Aramaic names like Onias contesting with Hellenizing High Priests with Greek names like Jason and Menelaus. In particular, Jason's Hellenistic reforms would prove to be a decisive factor leading to eventual conflict within the ranks of Judaism. Other authors point to possible socioeconomic reasons in addition to the religious reasons behind the civil war.\nWhat began in many respects as a civil war escalated when the Hellenistic kingdom of Syria sided with the Hellenizing Jews in their conflict with the traditionalists. As the conflict escalated, Antiochus took the side of the Hellenizers by prohibiting the religious practices the traditionalists had rallied around. This may explain why the king, in a total departure from Seleucid practice in all other places and times, banned a traditional religion.\nThe miracle of the oil is widely regarded as a legend and its authenticity has been questioned since the Middle Ages. However, given the famous question Rabbi Yosef Karo posed concerning why Hanukkah is celebrated for eight days when the miracle was only for seven days (since there was enough oil for one day), it was clear that he believed it was a historical event. This belief has been adopted by most of Orthodox Judaism, in as much as Rabbi Karo's \"Shulchan Aruch\" is a main Code of Jewish Law. The menorah first began to be used as a symbol of Judaism in the Hasmonean period \u2013 appearing on coins issued by Hasmonean king Mattathias Antigonus between 40 and 37 BCE \u2013 indicating that the tradition of an oil miracle was known then.\nBattles of the Maccabean Revolt.\nSelected battles between the Maccabees and the Seleucid Syrian-Greeks:\nRituals.\nHanukkah is celebrated with a series of rituals that are performed every day throughout the eight-day holiday, some are family-based and others communal. There are special additions to the daily prayer service, and a section is added to the blessing after meals.\nHanukkah is not a \"Sabbath-like\" holiday, and there is no obligation to refrain from activities that are forbidden on the Sabbath, as specified in the \"Shulkhan Arukh\". Adherents go to work as usual but may leave early in order to be home to kindle the lights at nightfall. There is no religious reason for schools to be closed, although in Israel schools close from the second day for the whole week of Hanukkah. Many families exchange gifts each night, such as books or games, and \"Hanukkah Gelt\" is often given to children. Fried foods\u2014such as latkes (potato pancakes), jelly doughnuts (sufganiyot) and Sephardic bimuelos\u2014are eaten to commemorate the importance of oil during the celebration of Hanukkah. Some also have a custom of eating dairy products to remember Judith and how she overcame Holofernes by feeding him cheese, which made him thirsty, and giving him wine to drink. When Holofernes became very drunk, Judith cut off his head.\nKindling the Hanukkah lights.\nEach night throughout the eight-day holiday, a candle or oil-based light is lit. As a universally practiced \"beautification\" (hiddur mitzvah) of the mitzvah, the number of lights lit is increased by one each night. An extra light called a \"shammash\", meaning \"attendant\" or \"sexton,\" is also lit each night, and is given a distinct location, usually higher, lower, or to the side of the others.\nAmong Ashkenazim the tendency is for every male member of the household (and in many families, girls as well) to light a full set of lights each night, while among Sephardim the prevalent custom is to have one set of lights for the entire household.\nThe purpose of the \"shammash\" is to adhere to the prohibition, specified in the Talmud, against using the Hanukkah lights for anything other than publicizing and meditating on the Hanukkah miracle. This differs from Sabbath candles which are meant to be used for illumination and lighting. Hence, if one were to need extra illumination on Hanukkah, the \"shammash\" candle would be available, and one would avoid using the prohibited lights. Some, especially Ashkenazim, light the \"shammash\" candle first and then use it to light the others. So altogether, including the \"shammash\", two lights are lit on the first night, three on the second and so on, ending with nine on the last night, for a total of 44 (36, excluding the \"shammash\"). It is Sephardic custom not to light the shammash first and use it to light the rest. Instead, the shammash candle is the last to be lit, and a different candle or a match is used to light all the candles. Some Hasidic Jews follow this Sephardic custom as well.\nThe lights can be candles or oil lamps. Electric lights are sometimes used and are acceptable in places where open flame is not permitted, such as a hospital room, or for the very elderly and infirm; however, those who permit reciting a blessing over electric lamps only allow it if it is incandescent and battery operated (an incandescent flashlight would be acceptable for this purpose), while a blessing may not be recited over a plug-in menorah or lamp. Most Jewish homes have a special candelabrum referred to as either a \"Chanukiah\" (the modern Israeli term) or a \"menorah\" (the traditional name, simply Hebrew for 'lamp'). Many families use an oil lamp (traditionally filled with olive oil) for Hanukkah. Like the candle Chanukiah, it has eight wicks to light plus the additional \"shammash\" light.\nIn the United States, Hanukkah became a more visible festival in the public sphere from the 1970s when Rabbi Menachem M. Schneerson called for public awareness and observance of the festival and encouraged the lighting of public menorahs. Diane Ashton attributed the increased visibility and reinvention of Hanukkah by some of the American Jewish community as a way to adapt to American life, re-inventing the festival in \"the language of individualism and personal conscience derived from both Protestantism and the Enlightenment\".\nThe reason for the Hanukkah lights is not for the \"lighting of the house within\", but rather for the \"illumination of the house without,\" so that passersby should see it and be reminded of the holiday's miracle (i.e. that the sole cruse of pure oil found which held enough oil to burn for one night actually burned for eight nights). Accordingly, lamps are set up at a prominent window or near the door leading to the street. It is customary amongst some Ashkenazi Jews to have a separate menorah for each family member (customs vary), whereas most Sephardi Jews light one for the whole household. Only when there was danger of antisemitic persecution were lamps supposed to be hidden from public view, as was the case in Persia under the rule of the Zoroastrians, or in parts of Europe before and during World War II. However, most Hasidic groups light lamps near an inside doorway, not necessarily in public view. According to this tradition, the lamps are placed on the opposite side from the \"mezuzah\", so people passing through the door are surrounded by the holiness of \"mitzvot\" (the commandments).\nGenerally, women are exempt in Jewish law from time-bound positive commandments, although the Talmud requires that women engage in the mitzvah of lighting Hanukkah candles \"for they too were involved in the miracle.\"\nCandle-lighting time.\nHanukkah lights should usually burn for at least half an hour after it gets dark. Many light at sundown and those who do so should be careful to have enough oil or wax to last until half an hour after dark. Most Hasidim and many other communities light later, generally around nightfall. Many Hasidic Rebbes light much later to fulfill the obligation of publicizing the miracle by the presence of their Hasidim when they kindle the lights.\nInexpensive small wax candles sold for Hanukkah burn for approximately half an hour so should be lit no earlier than nightfall. Friday night presents a problem, however. Since candles may not be lit on Shabbat itself, the candles must be lit before sunset. However, they must remain lit through the lighting of the Shabbat candles. Therefore, the Hanukkah menorah is lit first with larger candles than usual, followed by the Shabbat candles. At the end of the Shabbat, there are those who light the Hanukkah lights before Havdalah and those who make Havdalah before the lighting Hanukkah lights.\nIf for whatever reason one didn't light at sunset or nightfall, the lights should be kindled later, as long as there are people in the streets. Later than that, the lights should still be kindled, but the blessings should be recited only if there is at least somebody else awake in the house and present at the lighting of the Hannukah lights.\nBlessings over the candles.\nTypically two blessings (\"brachot\"; singular: \"brachah\") are recited during this eight-day festival when lighting the candles. On the first night only, the shehecheyanu blessing is added, making a total of three blessings.\nThe first blessing is recited before the candles are lit, and while most recite the other blessing(s) beforehand as well, some have the custom to recite them after. On the first night of Hanukkah one light (candle or oil) is lit on the right side of the menorah, on the following night a second light is placed to the left of the first but it is lit first, and so on, proceeding from placing candles right to left but lighting them from left to right over the eight nights.\nBlessing for lighting the candles.\nhe\nTransliteration: \nTranslation: \"Blessed are You, LORD our God, King of the universe, Who has sanctified us with His commandments and commanded us to kindle the Hanukkah light[s].\"\nBlessing for the miracles of Hanukkah.\nhe\nTransliteration: \nTranslation: \"Blessed are You, LORD our God, King of the universe, Who performed miracles for our ancestors in those days at this time...\"\n\"Hanerot Halalu\".\nAfter the lights are kindled the hymn \"Hanerot Halalu\" is recited. There are several different versions; the version presented here is recited in many Ashkenazic communities:\n\"Maoz Tzur\".\nIn the Ashkenazi tradition, each night after the lighting of the candles, the hymn Ma'oz Tzur is sung. The song contains six stanzas. The first and last deal with general themes of divine salvation, and the middle four deal with events of persecution in Jewish history, praising God for survival despite these tragedies (the exodus from Egypt, the Babylonian captivity, the miracle of the holiday of Purim, the Hasmonean victory) and expressing a longing for the days when Judea will finally triumph over Rome.\nThe song was composed in the thirteenth century by a poet only known through the acrostic found in the first letters of the original five stanzas of the song: Mordechai. The familiar tune is most probably a derivation of a German Protestant church hymn or a popular folk song.\nOther customs.\nAfter lighting the candles and Ma'oz Tzur, singing other Hanukkah songs is customary in many Jewish homes. Some Hasidic and Sephardi Jews recite Psalms, such as Psalm 30, Psalm 67, and Psalm 91. In North America and in Israel it is common to exchange presents or give children presents at this time. In addition, many families encourage their children to give tzedakah (charity) in lieu of presents for themselves.\nSpecial additions to daily prayers.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"We thank You also for the miraculous deeds and for the redemption and for the mighty deeds and the saving acts wrought by You, as well as for the wars which You waged for our ancestors in ancient days at this season. In the days of the Hasmonean Mattathias, son of Johanan the high priest, and his sons, when the iniquitous Greco-Syrian kingdom rose up against Your people Israel, to make them forget Your Torah and to turn them away from the ordinances of Your will, then You in your abundant mercy rose up for them in the time of their trouble, pled their cause, executed judgment, avenged their wrong, and delivered the strong into the hands of the weak, the many into the hands of few, the impure into the hands of the pure, the wicked into the hands of the righteous, and insolent ones into the hands of those occupied with Your Torah. Both unto Yourself did you make a great and holy name in Thy world, and unto Your people did You achieve a great deliverance and redemption. Whereupon your children entered the sanctuary of Your house, cleansed Your temple, purified Your sanctuary, kindled lights in Your holy courts, and appointed these eight days of Hanukkah in order to give thanks and praises unto Your holy name.\"\nTranslation of \"Al ha-Nissim\"\nAn addition is made to the \"hoda'ah\" (thanksgiving) benediction in the Amidah (thrice-daily prayers), called \"Al HaNissim\" (\"On/about the Miracles\"). This addition refers to the victory achieved over the Syrians by the Hasmonean Mattathias and his sons.\nThe same prayer is added to the grace after meals. In addition, the \"Hallel\" (praise) Psalms are sung during each morning service and the \"Tachanun\" penitential prayers are omitted.\nThe Torah is read every day in the shacharit morning services in synagogue, on the first day beginning from Numbers 6:22 (according to some customs, Numbers 7:1), and the last day ending with Numbers 8:4. Since Hanukkah lasts eight days it includes at least one, and sometimes two, Jewish Sabbaths (Saturdays). The weekly Torah portion for the first Sabbath is almost always \"Miketz\", telling of Joseph's dream and his enslavement in Egypt. The \"Haftarah\" reading for the first Sabbath Hanukkah is Zechariah 2:14 \u2013 Zechariah 4:7. When there is a second Sabbath on Hanukkah, the \"Haftarah\" reading is from 1 Kings 7:40\u201350.\nThe Hanukkah \"menorah\" is also kindled daily in the synagogue, at night with the blessings and in the morning without the blessings.\nThe menorah is not lit during Shabbat, but rather prior to the beginning of Shabbat as described above and not at all during the day.\nDuring the Middle Ages \"Megillat Antiochus\" was read in the Italian synagogues on Hanukkah just as the Book of Esther is read on Purim. It still forms part of the liturgy of the Yemenite Jews.\n\"Zot Hanukkah\".\nThe last day of Hanukkah is known by some as \"Zot Hanukkah\" and by others as \"Chanukat HaMizbeach\", from the verse read on this day in the synagogue Numbers 7:84, \"Zot Hanukkat Hamizbe'ach\": \"This was the dedication of the altar\". According to the teachings of Kabbalah and Hasidism, this day is the final \"seal\" of the High Holiday season of Yom Kippur and is considered a time to repent out of love for God. In this spirit, many Hasidic Jews wish each other \"Gmar chatimah tovah\" (\"may you be sealed totally for good\"), a traditional greeting for the Yom Kippur season. It is taught in Hasidic and Kabbalistic literature that this day is particularly auspicious for the fulfillment of prayers.\nOther related laws and customs.\nIt is customary for women not to work for at least the first half-hour of the candles' burning, and some have the custom not to work for the entire time of burning. It is also forbidden to fast or to eulogize during Hanukkah.\nHanukkah as the end of the High Holy Days.\nSome Hasidic scholars teach that the Hanukkah is in fact the final conclusion of God's judgement extending High Holy Days of Rosh Hashana when humanity is judged and Yom Kippur when the judgment is sealed:\nHassidic masters quote from Kabbalistic sources that the God's mercy extends even further, giving the Children of Israel till the final day of Chanukah (known as \"Zot Chanukah\" based on words which appear in the Torah reading of that day), to return to Him and receive a favorable judgment. They see several hints to this in different verses. One is Isaiah 27:9: \"Through this (zot) will Jacob's sin be forgiven\" \u2013 i.e., on account of the holiness of Zot Chanukah.\nCustoms.\nMusic.\nHanukkah songs (in Hebrew except where indicated) include \"\"Ma'oz Tzur\" (Rock of Ages), \"Latke'le Latke'le\"\" (Yiddish: \"Little Latke, Little Latke\"), \"Hanukkiah Li Yesh\" (\"I Have a Hanukkah Menorah\"), \"Ocho Kandelikas\" (Judeo-Spanish: \"Eight Little Candles\"), \"Kad Katan\" (\"A Small Jug\"), \"S'vivon Sov Sov Sov\" (\"Dreidel, Spin and Spin\"), \"Haneirot Halolu\" (\"These Candles Which We Light\"), \"Mi Yimalel\" (\"Who Can Retell\") and \"Ner Li, Ner Li\" (\"I have a Candle\").\nAmong the best known songs in English-speaking countries are \"Dreidel, Dreidel, Dreidel\" and \"Oh Chanukah\".\nIn the Nadvorna Hasidic dynasty, it is customary for the rebbes to play violin after the menorah is lit.\nPenina Moise's Hannukah Hymn published in the 1842 \"Hymns Written for the Use of Hebrew Congregations\" was instrumental in the beginning of Americanization of Hanukkah.\nFoods.\nThere is a custom of eating foods fried or baked in oil (preferably olive oil) to commemorate the miracle of a small flask of oil keeping the Second Temple's Menorah alight for eight days. Traditional foods include potato pancakes, known as \"latkes\" in Yiddish, especially among Ashkenazi families. Sephardi, Polish, and Israeli families eat jam-filled doughnuts ( \"pontshkes\"), bimuelos (fritters) and sufganiyot which are deep-fried in oil. Italkim and Hungarian Jews traditionally eat cheese pancakes known as \"cassola\" or \"cheese latkes\".\nLatkes are not popular in Israel, having been largely replaced by sufganiyot due to local economic factors, convenience and the influence of trade unions. Bakeries in Israel have popularized many new types of fillings for \"sufganiyot\" besides the traditional strawberry jelly filling, including chocolate cream, vanilla cream, caramel, cappuccino and others. In recent years, downsized, \"mini\" sufganiyot containing half the calories of the regular, 400-to-600-calorie version, have become popular.\nRabbinic literature also records a tradition of eating cheese and other dairy products during Hanukkah. This custom, as mentioned above, commemorates the heroism of Judith during the Babylonian captivity of the Jews and reminds us that women also played an important role in the events of Hanukkah. The deuterocanonical book of Judith (Yehudit or Yehudis in Hebrew), which is not part of the Tanakh, records that Holofernes, an Assyrian general, had surrounded the village of Bethulia as part of his campaign to conquer Judea. After intense fighting, the water supply of the Jews was cut off and the situation became desperate. Judith, a pious widow, told the city leaders that she had a plan to save the city. Judith went to the Assyrian camps and pretended to surrender. She met Holofernes, who was smitten by her beauty. She went back to his tent with him, where she plied him with cheese and wine. When he fell into a drunken sleep, Judith beheaded him and escaped from the camp, taking the severed head with her (the beheading of Holofernes by Judith has historically been a popular theme in art). When Holofernes' soldiers found his corpse, they were overcome with fear; the Jews, on the other hand, were emboldened and launched a successful counterattack. The town was saved, and the Assyrians defeated.\nRoast goose has historically been a traditional Hanukkah food among Eastern European and American Jews, although the custom has declined in recent decades.\nIndian Jews traditionally consume gulab jamun, fried dough balls soaked in a sweet syrup, similar to teiglach or bimuelos, as part of their Hanukkah celebrations. Italian Jews eat fried chicken, cassola (a ricotta cheese latke almost similar to a cheesecake), and \"fritelle de riso par Hanukkah\" (a fried sweet rice pancake). Romanian Jews eat pasta latkes as a traditional Hanukkah dish, and Syrian Jews consume Kibbet Yatkeen, a dish made with pumpkin and bulgur wheat similar to latkes, as well as their own version of keftes de prasa spiced with allspice and cinnamon.\nDreidel.\nAfter lighting the candles, it is customary to play (or spin) the dreidel. The dreidel, or \"sevivon\" in Hebrew, is a four-sided spinning top that children play with during Hanukkah. Each side is imprinted with a Hebrew letter which is an abbreviation for the Hebrew words (, \"A great miracle happened there\"), referring to the miracle of the oil that took place in the Beit Hamikdash. The fourth side of some dreidels sold in Israel are inscribed with the letter \"(Pe)\", rendering the acronym (, \"A great miracle happened here\"), referring to the fact that the miracle occurred in the land of Israel, although this is a relatively recent innovation. Stores in Haredi neighborhoods sell the traditional \"Shin\" dreidels as well, because they understand \"there\" to refer to the Temple and not the entire Land of Israel, and because the Hasidic Masters ascribe significance to the traditional letters.\nHanukkah gelt.\nChanukkah gelt (Yiddish for \"Chanukkah money\"), known in Israel by the Hebrew translation , is often distributed to children during the festival of Hanukkah. The giving of Hanukkah gelt also adds to the holiday excitement. The amount is usually in small coins, although grandparents or relatives may give larger sums. The tradition of giving Chanukah \"gelt\" dates back to a long-standing East European custom of children presenting their teachers with a small sum of money at this time of year as a token of gratitude. One minhag favors the fifth night of Hanukkah for giving Hanukkah gelt. Unlike the other nights of Hanukkah, the fifth does not ever fall on the Shabbat, hence never conflicting with the Halachic injunction against handling money on the Shabbat.\nHanukkah in the White House.\nThe earliest Hanukkah link with the White House occurred in 1951 when Israeli Prime Minister David Ben-Gurion presented United States President Harry Truman with a Hanukkah menorah. In 1979 President Jimmy Carter took part in the first public Hanukkah candle-lighting ceremony of the National Menorah held across the White House lawn. In 1989, President George H. W. Bush displayed a menorah in the White House. In 1993, President Bill Clinton invited a group of schoolchildren to the Oval Office for a small ceremony.\nThe United States Postal Service has released several Hanukkah-themed postage stamps. In 1996, the United States Postal Service (USPS) issued a 32 cent Hanukkah stamp as a joint issue with Israel. In 2004, after eight years of reissuing the menorah design, the USPS issued a dreidel design for the Hanukkah stamp. The dreidel design was used through 2008. In 2009 a Hanukkah stamp was issued with a design featured a photograph of a menorah with nine lit candles. In 2008, President George W. Bush held an official Hanukkah reception in the White House where he linked the occasion to the 1951 gift by using that menorah for the ceremony, with a grandson of Ben-Gurion and a grandson of Truman lighting the candles.\nIn December 2014, two Hanukkah celebrations were held at the White House. The White House commissioned a menorah made by students at the Max Rayne school in Israel and invited two of its students to join U.S. President Barack Obama and First Lady Michelle Obama as they welcomed over 500 guests to the celebration. The students' school in Israel had been subjected to arson by extremists. President Obama said these \"students teach us an important lesson for this time in our history. The light of hope must outlast the fires of hate. That's what the Hanukkah story teaches us. It's what our young people can teach us \u2013 that one act of faith can make a miracle, that love is stronger than hate, that peace can triumph over conflict.\" Rabbi Angela Warnick Buchdahl, in leading prayers at the ceremony commented on the how special the scene was, asking the President if he believed America's founding fathers could possibly have pictured that a female Asian-American rabbi would one day be at the White House leading Jewish prayers in front of the African-American president.\nDates.\nThe dates of Hanukkah are determined by the Hebrew calendar. Hanukkah begins at the 25th day of Kislev and concludes on the second or third day of Tevet (Kislev can have 29 or 30 days). The Jewish day begins at sunset. Hanukkah dates for recent and upcoming:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nIn 2013, on 28 November, the American holiday of Thanksgiving fell during Hanukkah for only the third time since Thanksgiving was declared a national holiday by President Abraham Lincoln. The last time was 1899; and due to the Gregorian and Jewish calendars being slightly out of sync with each other, it will not happen again in the foreseeable future. This convergence prompted the creation of the neologism Thanksgivukkah.\nSymbolic importance.\nMajor Jewish holidays are those when all forms of work are forbidden, and that feature traditional holiday meals, kiddush, holiday candle-lighting, etc. Only biblical holidays fit these criteria, and Chanukah was instituted some two centuries after the Hebrew Bible was completed. Nevertheless, though Chanukah is of rabbinic origin, it is traditionally celebrated in a major and very public fashion. The requirement to position the menorah, or Chanukiah, at the door or window, symbolizes the desire to give the Chanukah miracle a high-profile.\nSome Jewish historians suggest a different explanation for the rabbinic reluctance to laud the militarism. First, the rabbis wrote after Hasmonean leaders had led Judea into Rome's grip and so may not have wanted to offer the family much praise. Second, they clearly wanted to promote a sense of dependence on God, urging Jews to look toward the divine for protection. They likely feared inciting Jews to another revolt that might end in disaster, as the Bar Kochba revolt did.\nWith the advent of Zionism and the state of Israel, however, these themes were reconsidered. In modern Israel, the national and military aspects of Hanukkah became, once again, more dominant.\nWhile Hanukkah is a relatively minor Jewish holiday, as indicated by the lack of religious restrictions on work other than a few minutes after lighting the candles, in North America, Hanukkah in the 21st century has taken a place equal to Passover as a symbol of Jewish identity. Both the Israeli and North American versions of Hanukkah emphasize resistance, focusing on some combination of national liberation and religious freedom as the defining meaning of the holiday.\nSome Jews in North America and Israel have taken up environmental concerns in relation to Hanukkah's \"miracle of the oil\", emphasizing reflection on energy conservation and energy independence. An example of this is the Coalition on the Environment and Jewish Life's renewable energy campaign.\nRelationship to Christmas.\nIn the Catholic Church, Christmastide has its own Octave, being eight days especially set aside to celebrate Christmas from December 25th to January 1st. This is seen as a Christian fulfillment of the original text's demand for Hanukkah to be eight days, \"And they kept the eight days with gladness, as in the feast of the tabernacles, remembering that not long afore they had held the feast of the tabernacles\" (2 Macc 10:6). Advent is considered as the season of darkness preceding the season of light, Christmas, so for this reason, Christmas can be said to be the \"New Hanukkah,\" or its fulfillment through the Nativity of Christ. This is similar to the Easter Octave being the solemn eight days of the Passover of Exodus.\nIn North America, Hanukkah became increasingly important to many Jewish individuals and families during the latter part of the 20th century, including a large number of secular Jews, who wanted to celebrate a Jewish alternative to the Christmas celebrations which frequently overlap with Hanukkah. Diane Ashton argues that Jewish immigrants to America raised the profile of Hanukkah as a kid-centered alternative to Christmas as early as the 1800s. This in parts mirrors the ascendancy of Christmas, which like Hanukkah increased in importance in the 1800s. During this time period, Jewish leaders (especially Reform) such as Max Lilienthal and Isaac Mayer Wise made an effort to rebrand Hanukkah and started creating Hanukkah celebration for kids at their synagogues, which included candy and singing songs. By the 1900s, it started to become a commercial holiday like Christmas, with Hanukkah gifts and decorations appearing in stores and Jewish Women's magazines printing articles on holiday decorations, children's celebrations, and gift giving. Ashton says that Jewish families did this in order to maintain a Jewish identity which is distinct from mainline Christian culture, on the other hand, the mirroring of Hanukkah and Christmas made Jewish families and kids feel that they were American. Though it was traditional for Ashkenazi Jews to give \"gelt\" or money to children during Hanukkah, in many families, this tradition has been supplemented with the giving of other gifts so that Jewish children can enjoy receiving gifts just like their Christmas-celebrating peers do. Children play a big role in Hanukkah, and Jewish families with children are more likely to celebrate it than childless Jewish families, and sociologists hypothesize that this is because Jewish parents do not want their kids to be alienated from their non-Jewish peers who celebrate Christmas. Recent celebrations have also seen the presence of the Hanukkah bush, which is considered a Jewish counterpart to the Christmas tree. Today, the presence of Hanukkah bushes is generally discouraged by most rabbis, but some Reform, Reconstructionist and more liberal Conservative rabbis do not object, they also do not object to the presence of Christmas trees.\nRelationship to Kwanzaa.\nIn December 2022, New York City Mayor Eric Adams, Reverends Al Sharpton and Conrad Tillard, businessman Robert F. Smith, Rabbi Shmuley Boteach, and Elisha Wiesel joined to celebrate Hanukkah and Kwanzaa together, and combat racism and antisemitism, at Carnegie Hall.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7389", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7389", "title": "Hanukkah rituals", "text": ""}
{"id": "7390", "revid": "41438237", "url": "https://en.wikipedia.org/wiki?curid=7390", "title": "Christian views on marriage", "text": "Perspective of Christianity regarding marriage\nFrom the earliest days of the Christian faith, Christians have honored \"holy matrimony\" (as Christian marriages are referred to) as a divinely blessed, lifelong, monogamous union between a man and a woman. According to the Episcopal Book of Common Prayer (1979), reflecting the traditional view, \"Christian marriage is a solemn and public covenant between a man and a woman in the presence of God,\" \"intended by God for their mutual joy; for the help and comfort given one another in prosperity and adversity; and, when it is God's will, for the procreation of children and their nurture.\" However, while many Christians might agree with the traditional definition, the terminology and theological views of marriage have varied through time in different countries, and among Christian denominations.\nMany Protestants consider marriage to be a sacred institution or \"holy ordinance\" of God. Roman Catholics and Eastern Orthodox Christians consider marriage as a holy sacrament or sacred mystery. However, there have been differing attitudes among denominations and individual Christians towards not only the concept of Christian marriage, but also concerning divorce, remarriage, gender roles, family authority (the \"headship\" of the husband), the legal status of married women, birth control, marriageable age, cousin marriage, marriage of in-laws, interfaith marriage, same-sex marriage, and polygamy, among other topics, so that in the 21st century there cannot be said to be a single, uniform, worldwide view of marriage among all who profess to be Christians.\nChristian teaching has never held that marriage is necessary for everyone; for many centuries in Western Europe, priestly or monastic celibacy was valued as highly as, if not higher than, marriage. Christians who did not marry were expected to refrain from all sexual activity, as were those who took holy orders or monastic vows.\nIn some Western countries, a separate and secular civil wedding ceremony is required for recognition by the state, while in other Western countries, couples must merely obtain a marriage license from a local government authority and can be married by Christian or other clergy if they are authorized by law to conduct weddings. In this case, the state recognizes the religious marriage as a civil marriage as well; and Christian couples married in this way have all the rights of civil marriage, including, for example, divorce, even if their church forbids divorce.\nSince the beginning of the 21st century, same-sex couples have been allowed to marry civilly in many countries, and some Christian churches in those countries allow religious marriage of same-sex couples, though others forbid it, along with all other same-sex relationships.\nBiblical foundations and history.\nChristians believe that marriage is considered in its ideal according to the purpose of God. At the heart of God's design for marriage is companionship and intimacy.\nThe biblical picture of marriage expands into something much broader, with the husband and wife relationship illustrating the relationship between Christ and the church.\nIt is also considered in its actual occurrence, sometimes involving failure. Therefore, the Bible speaks on the subject of divorce. The New Testament recognizes a place for singleness. Salvation within Christianity is not dependent on the continuation of a biological lineage.\nOld Testament.\nThe Genesis creation account tells the story of when God instituted marriage. This took place after the creation of the first woman, Eve, from Adam, the first man.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The Lord God said, \"It is not good for the man to be alone. I will make a helper suitable for him.\"\nNow the Lord God had formed out of the ground all the wild animals and all the birds in the sky. He brought them to the man to see what he would name them; and whatever the man called each living creature, that was its name. So the man gave names to all the livestock, the birds in the sky and all the wild animals.\nBut for Adam no suitable helper was found. So the Lord God caused the man to fall into a deep sleep; and while he was sleeping, he took one of the man\u2019s ribs and then closed up the place with flesh. Then the Lord God made a woman from the rib he had taken out of the man, and he brought her to the man.\nThe man said,\n\"This is now bone of my bones\n and flesh of my flesh;\nshe shall be called \u2018woman,\u2019\n for she was taken out of man.\"\nThat is why a man leaves his father and mother and is united to his wife, and they become one flesh.\nPolygyny, or men having multiple wives at once, is one of the most common marital arrangements represented in the Old Testament, yet scholars doubt that it was common among average Israelites because of the wealth needed to practice it. Both the biblical patriarchs and kings of Israel are described as engaged in polygamous relationships. Despite the various polygynous relationships in the Bible, Old Testament scholar Peter Gentry has said that it does not mean that God condones polygyny. He also made note of the various problems that polygynous relationships present with the examples of Abraham, Jacob, David, and Solomon in the Bible. Alternatively, this could be a case of graded absolutism.\nBetrothal (\"erusin\"), which is merely a binding promise to get married, is distinct from marriage itself (\"nissu'in\"), with the time between these events varying substantially. Nonetheless, when a couple is betrothed, they are held accountable to the laws against adultery, like an officially married couple. From this, it is implied that a couple is considered to be married even they've only betrothed. Since a wife was regarded as property in biblical times, the betrothal (\"erusin\") was effected simply by purchasing her from her father (or guardian) (i.e. paying the bride price to the woman and her father); the woman's consent is not explicitly required by any biblical law. Nonetheless, in one Biblical story, Rebecca was asked whether she agreed to be married before the marriage took place. Additionally, according to French anthropologist Philippe Rospab\u00e9, the payment of the bride price does not entail the purchase of a woman, as was thought in the early twentieth century. Instead, it is a purely symbolic gesture acknowledging (but never paying off) the husband's permanent debt to the wife's parents.\nLike the adjacent Arabic culture (in the pre-Islamic period), the act of marriage appears mainly to have consisted of the groom fetching the bride, although among the Israelites the procession was a festive occasion, accompanied by music, dancing, and lights. To celebrate the marriage, week-long feasts were sometimes held.\nIn Old Testament times, a wife was submissive to her husband, which may interpreted as Israelite society viewing wives as the chattel of husbands. The descriptions of the Bible suggest that she would be expected to perform tasks such as spinning, sewing, weaving, manufacture of clothing, fetching of water, baking of bread, and animal husbandry. However, wives were usually looked after with care, and bigamous men were expected to ensure that they give their first wife food, clothing, and sexual activity.\nSince a wife was regarded as property, her husband was originally free to divorce her with little restriction, at any time. A divorced couple could get back together unless the wife had married someone else after her divorce.\nJesus on marriage, divorce, and remarriage.\nThe Bible clearly addresses marriage and divorce. Those in troubled marriages are encouraged to seek counseling and restoration because, according to some advocates of traditional marriage ethics, most divorces are neither necessary nor unavoidable.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"Have you not read that at the beginning the Creator made them male and female, and said, \"For this reason a man will leave his father and mother and be united to his wife, and the two will become one flesh\"? So they \"are\" no longer two, but one. Therefore, what God has joined together, let no one separate.\"\nIn the gospels of both Matthew and Mark, Jesus appealed to God's will in creation. He builds upon the narratives in where male and female are created together and for one another. Thus Jesus takes a firm stance on the permanence of marriage in the original will of God. This corresponds closely with the position of the Pharisee school of thought led by Shammai, at the start of the first millennium, with which Jesus would have been familiar. By contrast, Rabbinic Judaism subsequently took the opposite view, espoused by Hillel, the leader of the other major Pharisee school of thought at the time; in Hillel's view, men were allowed to divorce their wives for any reason.\nSome hold that marriage vows are unbreakable, so that even in the distressing circumstances in which a couple separates, they are still married from God's point of view. This is the Roman Catholic church's position, although occasionally the church will declare a marriage to be \"null\" (in other words, it never really was a marriage). William Barclay (1907-1978) has written:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;There is no time in history when the marriage bond stood in greater peril of destruction than in the days when Christianity first came into this world. At that time the world was in danger of witnessing the almost total break-up of marriage and the collapse of the home... Theoretically no nation ever had a higher ideal of marriage than the Jews had. The voice of God had said, \"I hate divorce\"\nJesus brought together two passages from Genesis, reinforcing the basic position on marriage found in Jewish scripture. Thus, he implicitly emphasized that it is God-made (\"God has joined together\"), \"male and female,\" lifelong (\"let no one separate\"), and monogamous (\"a man...his wife\").\nJesus used the image of marriage and the family to teach the basics about the Kingdom of God. He inaugurated his ministry by blessing the wedding at Cana. In the Sermon on the Mount he set forth a new commandment concerning marriage, teaching that lustful looking constitutes adultery. He also superseded a Mosaic Law allowing divorce with his teaching that \"...anyone who divorces his wife, except for sexual immorality (Gk. \"porneia\"), causes her to become an adulteress, and anyone who marries the divorced woman commits adultery\". Similar Pauline teachings are found in Corinthians 7. The exception clause\u2014\"except for...\"\u2014uses the Greek word \"porneia\" which is variously translated \"fornication\" (KJV), \"marital unfaithfulness\" (NIV 1984), \"sexual immorality\" (NIV 2011), \"unchastity\" (RSV), \"et\u00a0al\". \"The KJV New Testament Greek Lexicon, KJV\" says \"porneia\" includes a variety of sexual \"deviations\" to include \"illicit sexual intercourse, adultery, fornication, homosexuality, lesbianism, intercourse with animals, etc., sexual intercourse with close relatives...\"\nTheologian Frank Stagg says that manuscripts disagree as to the presence in the original text of the phrase \"except for fornication\". Stagg writes: \"Divorce always represents failure...a deviation from God's will... There is grace and redemption where there is contrition and repentance... There is no clear authorization in the New Testament for remarriage after divorce.\" Stagg interprets the chief concern of Matthew 5 as being \"to condemn the criminal act of the man who divorces an innocent wife... Jesus was rebuking the husband who victimizes an innocent wife and thinks that he makes it right with her by giving her a divorce\". He points out that Jesus refused to be trapped by the Pharisees into choosing between the strict and liberal positions on divorce as held at the time in Judaism. When they asked him, \"Is it lawful for a man to divorce his wife for any cause?\" he answered by reaffirming God's will as stated in Genesis, that in marriage husband and wife are made \"one flesh\", and what God has united man must not separate.\nThere is no evidence that Jesus himself ever married, and considerable evidence that he remained single. In contrast to Judaism and many other traditions, he taught that there is a place for voluntary singleness in Christian service. He believed marriage could be a distraction from an urgent mission, that he was living in a time of crisis and urgency where the Kingdom of God would be established where there would be no marriage nor giving in marriage:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\n\"I tell you the truth,\" Jesus said to them, \"no one who has left home or wife or brothers or parents or children for the sake of the kingdom of God will fail to receive many times as much in this age and, in the age to come, eternal life.\"\nIn Matthew 22, Jesus is asked about the continuing state of marriage after death and he affirms that at the resurrection \"people neither marry nor be given in marriage; they are like the angels in heaven.\".\nNew Testament beyond the Gospels.\nThe Apostle Paul quoted passages from Genesis almost verbatim in two of his New Testament books. He used marriage not only to describe the kingdom of God, as Jesus had done, but to define also the nature of the 1st-century Christian church. His theological view was a Christian development of the Old Testament parallel between marriage and the relationship between God and Israel. He analogized the church as a bride and Christ as the bridegroom\u2500drawing parallels between Christian marriage and the relationship between Christ and the Church.\nThere is no hint in the New Testament that Jesus was ever married, and no clear evidence that Paul was ever married. However, both Jesus and Paul seem to view marriage as a legitimate calling from God for Christians. Paul elevates singleness to that of the preferable position, but does offer a caveat suggesting this is \"because of the impending crisis\"\u2014which could itself extend to present times (see also Pauline privilege). Paul's primary issue was that marriage adds concerns to one's life that detract from their ability to serve God without distraction.\nSome scholars have speculated that Paul may have been a widower since prior to his conversion to Christianity he was a Pharisee and member of the Sanhedrin, positions in which the social norm of the day required the men to be married. But it is just as likely that he never married at all.\nYet, Paul acknowledges the mutuality of marital relations, and recognizes that his own singleness is \"a particular gift from God\" that others may not necessarily have. He writes: \"Now to the unmarried and the widows I say: It is good for them to stay unmarried, as I am. But if they cannot control themselves, they should marry, for it is better to marry than to burn with passion.\"\nPaul indicates that bishops, deacons, and elders must be \"husbands of one wife\", and that women must have one husband. This is usually understood to legislate against polygamy rather than to require marriage:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Now the overseer (bishop) is to be above reproach, faithful to his wife, temperate, self-controlled, respectable, hospitable, able to teach, not given to drunkenness, not violent but gentle, not quarrelsome, not a lover of money.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;A deacon must be faithful to his wife and must manage his children and his household well.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The reason I left you in Crete was that you might put in order what was left unfinished and appoint (or ordain) elders in every town, as I directed you. An elder must be blameless, faithful to his wife, a man whose children believe and are not open to the charge of being wild and disobedient.\nIn the Roman Age, female widows who did not remarry were considered more pure than those who did. Such widows were known as \"one man woman\" (\"enos andros gune\") in the epistles of Paul. Paul writes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nPaul allowed widows to remarry. Paul says that only \"one-man women\" older than 60 years can make the list of Christian widows who did special tasks in the community, but that younger widows should remarry to hinder sin.\nMarriage and early Church Fathers.\nBuilding on what they saw the example of Jesus and Paul advocating, some early Church Fathers placed less value on the family and saw celibacy and freedom from family ties as a preferable state.\nNicene Fathers such as Augustine believed that marriage was a sacrament because it was a symbol used by Paul to express Christ's love of the Church. However, there was also an apocalyptic dimension in his teaching, and he was clear that if everybody stopped marrying and having children that would be an admirable thing; it would mean that the Kingdom of God would return all the sooner and the world would come to an end. Such a view reflects the Manichaean past of Augustine.\nWhile upholding the New Testament teaching that marriage is \"honourable in all and the bed undefiled,\" Augustine believed that \"yet, whenever it comes to the actual process of generation, the very embrace which is lawful and honourable cannot be effected without the ardour of lust...This is the carnal concupiscence, which, while it is no longer accounted sin in the regenerate, yet in no case happens to nature except from sin.\"\nBoth Tertullian and Gregory of Nyssa were church fathers who were married. They each stressed that the happiness of marriage was ultimately rooted in misery. They saw marriage as a state of bondage that could only be cured by celibacy. They wrote that at the very least, the virgin woman could expect release from the \"governance of a husband and the chains of children.\"\nTertullian argued that second marriage, having been freed from the first by death,\"will have to be termed no other than a species of fornication,\" partly based on the reasoning that this involves desiring to marry a woman out of sexual ardor, which a Christian convert is to avoid.\nAlso advocating celibacy and virginity as preferable alternatives to marriage, Jerome wrote: \"It is not disparaging wedlock to prefer virginity. No one can make a comparison between two things if one is good and the other evil.\" On First Corinthians 7:1 he reasons, \"It is good, he says, for a man not to touch a woman. If it is good not to touch a woman, it is bad to touch one: for there is no opposite to goodness but badness. But if it be bad and the evil is pardoned, the reason for the concession is to prevent worse evil.\"\nSt. John Chrysostom wrote: \"...virginity is better than marriage, however good... Celibacy is...an imitation of the angels. Therefore, virginity is as much more honorable than marriage, as the angel is higher than man. But why do I say angel? Christ, Himself, is the glory of virginity.\"\nCyprian, Bishop of Carthage, said that the first commandment given to men was to increase and multiply, but now that the earth was full there was no need to continue this process of multiplication.\nThis view of marriage was reflected in the lack of any formal liturgy formulated for marriage in the early Church. No special ceremonial was devised to celebrate Christian marriage\u2014despite the fact that the Church had produced liturgies to celebrate the Eucharist, Baptism and Confirmation. It was not important for a couple to have their nuptials blessed by a priest. People could marry by mutual agreement in the presence of witnesses.\nAt first, the old Roman pagan rite was used by Christians, although modified superficially. The first detailed account of a Christian wedding in the West dates from the 9th century. This system, known as Spousals, persisted after the Reformation.\nDenominational beliefs and practice.\nCatholicism.\nToday all Christian denominations regard marriage as a sacred institution, a covenant. Roman Catholics consider it to be a sacrament. Marriage was officially recognized as a sacrament at the 1184 Council of Verona. Before then, no specific ritual was prescribed for celebrating a marriage: \"Marriage vows did not have to be exchanged in a church, nor was a priest's presence required. A couple could exchange consent anywhere, anytime.\"\nIn the decrees on marriage of the Council of Trent (twenty-fourth session from 1563), the validity of marriage was made dependent upon the wedding taking place before a priest and two witnesses, although the lack of a requirement for parental consent ended a debate that had proceeded from the 12th century. In the case of a divorce, the right of the innocent party to marry again was denied so long as the other party was alive, even if the other party had committed adultery.\nThe Catholic Church allowed marriages to take place inside churches only starting with the 16th century, beforehand religious marriages happened on the porch of the church.\nThe Roman Catholic Church teaches that God himself is the author of the sacred institution of marriage, which is His way of showing love for those He created. Marriage is a divine institution that can never be broken, even if the husband or wife legally divorce in the civil courts; as long as they are both alive, the Church considers them bound together by God. Holy Matrimony is another name for sacramental marriage. Marriage is intended to be a faithful, exclusive, lifelong union of a man and a woman. Committing themselves completely to each other, a Catholic husband and wife strive to sanctify each other, bring children into the world, and educate them in the Catholic way of life. Man and woman, although created differently from each other, complement each other. This complementarity draws them together in a mutually loving union.\nThe valid marriage of baptized Christians is one of the seven Roman Catholic sacraments. The sacrament of marriage is the only sacrament that a priest does not administer directly; a priest, however, is the chief witness of the husband and wife's administration of the sacrament to each other at the wedding ceremony in a Catholic church.\nThe Roman Catholic Church views that Christ himself established the sacrament of marriage at the wedding feast of Cana; therefore, since it is a divine institution, neither the Church nor state can alter the basic meaning and structure of marriage. Husband and wife give themselves totally to each other in a union that lasts until death.\nPriests are instructed that marriage is part of God's natural law and to support the couple if they do choose to marry. Today it is common for Roman Catholics to enter into a \"mixed marriage\" between a Catholic and a baptized non-Catholic. Couples entering into a mixed marriage are usually allowed to marry in a Catholic church provided their decision is of their own accord and they intend to remain together for life, to be faithful to each other, and to have children which are brought up in the Catholic faith.\nIn Roman Catholic teaching, marriage has two objectives: the good of the spouses themselves, and the procreation and education of children (1983 code of canon law, c.1055; 1994 catechism, par.2363). Hence \"entering marriage with the intention of never having children is a grave wrong and more than likely grounds for an annulment.\" It is normal procedure for a priest to ask the prospective bride and groom about their plans to have children before officiating at their wedding. The Roman Catholic Church may refuse to marry anyone unwilling to have children, since procreation by \"the marriage act\" is a fundamental part of marriage. Thus usage of any form of contraception, in vitro fertilization, or birth control besides natural family planning is a grave offense against the sanctity of marriage and ultimately against God.\nProtestantism.\nPurposes.\nMost Protestant denominations hold marriage to be ordained by God for the union between a man and a woman. They see the primary purposes of this union as intimate companionship, rearing children and mutual support for both husband and wife to fulfill their life callings. Protestant Christian denominations consider marital sexual pleasure to be a gift of God, though they vary on their position on birth control, ranging from the acceptance of the use of contraception to only allowing natural family planning to teaching Quiverfull doctrine\u2014that birth control is sinful and Christians should have large families. Conservative Protestants consider marriage a solemn covenant between wife, husband and God. Most view sexual relations as appropriate only within a marriage. Protestant Churches discourage divorce though the way it is addressed varies by denomination; for example, the Reformed Church in America permits divorce and remarriage, while connexions such as the Evangelical Methodist Church Conference forbid divorce except in the case of fornication and do not allow for remarriage in any circumstance.\nMany Methodist Christians teach that marriage is \"God's gift and covenant intended to imitate God's covenant with humankind\" that \"Christians enter in their baptism.\" For example, the rite used in the Free Methodist Church proclaims that marriage is \"more than a legal contract, being a bond of union made in heaven, into which you enter discreetly and reverently.\"\nRoles and responsibilities.\nRoles and responsibilities of husband and wives now vary considerably on a continuum between the long-held male dominant/female submission view and a shift toward equality (without sameness) of the woman and the man. There is considerable debate among many Christians today\u2014not just Protestants\u2014whether equality of husband and wife or male headship is the biblically ordained view, and even if it is biblically permissible. The divergent opinions fall into two main groups: Complementarians (who call for husband-headship and wife-submission) and Christian Egalitarians (who believe in full partnership equality in which couples can discover and negotiate roles and responsibilities in marriage).\nThere is no debate that Ephesians 5 presents a historically benevolent husband-headship/wife-submission model for marriage. The questions are (a)\u00a0how these New Testament household codes are to be reconciled with the calls earlier in Chapter 5 (cf. verses 1, 18, 21) for mutual submission among all believers, and (b)\u00a0the meaning of \"head\" in v.23. It is important to note that verse 22 contains no verb in the original manuscripts, which were also not divided into verses:\nEphesians 5 (NIV)\n1 Follow God\u2019s example, therefore, as dearly loved children 2 and walk in the way of love...\n18 be filled with the Spirit...\n21 Submit to one another out of reverence for Christ.\n22 Wives, \"[submit yourselves]\" to your own husbands as you do to the Lord. 23 For the husband is the head of the wife as Christ is the head of the church, his body, of which he is the Savior. 24 Now as the church submits to Christ, so also wives should submit to their husbands in everything.\n25 Husbands, love your wives, just as Christ loved the church and gave himself up for her 26 to make her holy, cleansing her by the washing with water through the word, 27 and to present her to himself as a radiant church, without stain or wrinkle or any other blemish, but holy and blameless. 28 In this same way, husbands ought to love their wives as their own bodies. He who loves his wife loves himself. 29 After all, no one ever hated their own body, but they feed and care for their body, just as Christ does the church\u2014 30 for we are members of his body. 31 \"For this reason a man will leave his father and mother and be united to his wife, and the two will become one flesh.\" 32 This is a profound mystery\u2014but I am talking about Christ and the church. 33 However, each one of you also must love his wife as he loves himself, and the wife must respect her husband.\nEastern Orthodoxy.\nIn the Eastern Orthodox Church, marriage is treated as a Sacred Mystery (sacrament), and as an ordination. It serves to unite a woman and a man in eternal union before God. It refers to the 1st centuries of the church, where spiritual union of spouses in the first sacramental marriage was eternal. Therefore, it is considered a martyrdom as each spouse learns to die to self for the sake of the other. Like all Mysteries, Orthodox marriage is more than just a celebration of something which already exists: it is the creation of something new, the imparting to the couple of the grace which transforms them from a 'couple' into husband and wife within the Body of Christ.\nMarriage is an icon (image) of the relationship between Jesus and the Church. This is somewhat akin to the Old Testament prophets' use of marriage as an analogy to describe the relationship between God and Israel. Marriage is the simplest, most basic unity of the church: a congregation where \"two or three are gathered together in Jesus' name.\" The home is considered a consecrated space (the ritual for the Blessing of a House is based upon that of the Consecration of a Church), and the husband and wife are considered the ministers of that congregation. However, they do not \"perform\" the Sacraments in the house church; they \"live\" the Sacrament of Marriage. Because marriage is considered to be a pilgrimage wherein the couple walk side by side toward the Kingdom of Heaven, marriage to a non-Orthodox partner is discouraged, though it may be permitted.\nUnlike Western Christianity, Eastern Christians do not consider the sacramental aspect of the marriage to be conferred by the couple themselves. Rather, the marriage is conferred by the action of the Holy Spirit acting through the priest. Furthermore, no one besides a bishop or priest\u2014not even a deacon\u2014may perform the Sacred Mystery.\nThe external sign of the marriage is the placing of wedding crowns upon the heads of the couple, and their sharing in a \"Common Cup\" of wine. Once crowned, the couple walk a circle three times in a ceremonial \"dance\" in the middle of the church, while the choir intones a joyous three-part antiphonal hymn, \"Dance, Isaiah\"\nThe sharing of the Common Cup symbolizes the transformation of their union from a common marriage into a sacred union. The wedding is usually performed after the Divine Liturgy at which the couple receives Holy Communion. Traditionally, the wedding couple would wear their wedding crowns for eight days, and there is a special prayer said by the priest at the removal of the crowns.\nDivorce is discouraged. Sometimes out of \"economia\" (mercy) a marriage may be dissolved if there is no hope whatever for a marriage to fulfill even a semblance of its intended sacramental character. The standard formula for remarriage is that the Orthodox Church joyfully blesses the first marriage, merely performs the second, barely tolerates the third, and invariably forbids the fourth. \"On the basis of the ideal of the first marriage as an image of the glory of God the question is which significance such a second marriage has and whether it can be regarded as Mysterion. Even though there are opinions (particularly in the west) which deny the sacramental character to the second marriage, in the orthodox literature almost consistently either a reduced or even a full sacramentality is attributed to it. The investigation of the second marriage rite shows that both positions affirming the sacramentality to a second marriage can be justified.\"\nEarly church texts forbid marriage between an Orthodox Christian and a heretic or schismatic (which would include all non-Orthodox Christians). Traditional Orthodox Christians forbid mixed marriages with other denominations. More liberal ones perform them, provided that the couple formally commit themselves to rearing their children in the Orthodox faith.\nAll people are called to celibacy\u2014human beings are all born into virginity, and Orthodox Christians are expected by Sacred Tradition to remain in that state unless they are called into marriage and that call is sanctified. The church blesses two paths on the journey to salvation: monasticism and marriage. Mere celibacy, without the sanctification of monasticism, can fall into selfishness and tends to be regarded with disfavour by the Church.\nOrthodox priests who serve in parishes are usually married. They must marry prior to their ordination. If they marry after they are ordained they are not permitted to continue performing sacraments. If their wife dies, they are forbidden to remarry; if they do, they may no longer serve as a priest. A married man may be ordained as a priest or deacon. However, a priest or deacon is not permitted to enter into matrimony after ordination. Bishops must always be monks and are thus celibate. However, if a married priest is widowed, he may receive monastic tonsure and thus become eligible for the episcopate.\nThe Eastern Orthodox Church believes that marriage is an eternal union of spouses, but in Heaven there will not be a procreative bond of marriage.\nOriental Orthodoxy.\nThe Non-Chalcedonian Churches of Oriental Orthodoxy hold views almost identical to those of the (Chalcedonian) Eastern Orthodox Church. The Coptic Orthodox Church allows second marriages only in cases of adultery or death of spouse.\nNon-Trinitarian denominations.\nThe Church of Jesus Christ of Latter-day Saints.\nIn the teachings of the Church of Jesus Christ of Latter-day Saints (LDS Church), celestial (or eternal) marriage is a covenant between a man, a woman, and God performed by a priesthood authority in a temple of the church. Celestial marriage is intended to continue forever into the afterlife if the man and woman do not break their covenants. Thus, eternally married couples are often referred to as being \"sealed\" to each other. Sealed couples who keep their covenants are also promised to have their posterity sealed to them in the afterlife. (Thus, \"families are forever\" is a common phrase in the LDS Church.) A celestial marriage is considered a requirement for exaltation.\nIn some countries, celestial marriages can be recognized as civil marriages; in other cases, couples are civilly married outside of the temple and are later sealed in a celestial marriage. (The church will no longer perform a celestial marriage for a couple unless they are first or simultaneously legally married.) The church encourages its members to be in good standing with it so that they may marry or be sealed in the temple. A celestial marriage is not annulled by a civil divorce: a \"cancellation of a sealing\" may be granted, but only by the First Presidency, the highest authority in the church. Civil divorce and marriage outside the temple carries somewhat of a stigma in the Mormon culture; the church teaches that the \"gospel of Jesus Christ\u2014including repentance, forgiveness, integrity, and love\u2014provides the remedy for conflict in marriage.\" Regarding marriage and divorce, the church instructs its leaders: \"No priesthood officer is to counsel a person whom to marry. Nor should he counsel a person to divorce his or her spouse. Those decisions must originate and remain with the individual. When a marriage ends in divorce, or if a husband and wife separate, they should always receive counseling from Church leaders.\"\nIn church temples, members of the LDS Church perform vicarious celestial marriages for deceased couples who were legally married.\nNew Church (or Swedenborgian Church).\nThe New Church teaches that marital love (or \"conjugial love\") is \"the precious jewel of human life and the repository of the Christian religion\" because the love shared between a husband and a wife is the source of all peace and joy. Emanuel Swedenborg coined the term \"conjugial\" (rather than the more usual adjective in reference to marital union, \"conjugal\") to describe the special love experienced by married partners. When a husband and wife work together to build their marriage on earth, that marriage continues after the deaths of their bodies and they live as angels in heaven into eternity. Swedenborg claimed to have spoken with angelic couples who had been married for thousands of years. Those who never married in the natural world will, if they wish, find a spouse in heaven.\nJehovah's Witnesses.\nThe Jehovah's Witnesses view marriage to be a permanent arrangement with the only possible exception being adultery. Divorce is strongly discouraged even when adultery is committed since the wronged spouse is free to forgive the unfaithful one. There are provisions for a domestic separation in the event of \"failure to provide for one's household\" and domestic violence, or spiritual resistance on the part of a partner. Even in such situations though divorce would be considered grounds for loss of privileges in the congregation. Remarrying after death or a proper divorce is permitted. Marriage is the only situation where any type of sexual interaction is acceptable, and even then certain restrictions apply to acts such as oral and anal sex. Married persons who are known to commit such acts may in fact lose privileges in the congregation as they are supposed to be setting a good example to the congregation.\nInterdenominational marriage.\nIn Christianity, an interdenominational marriage (also known as an ecumenical marriage) is a marriage between two baptized Christians who belong to different Christian denominations, e.g. a wedding between a Lutheran Christian man and a Catholic Christian woman. Nearly all Christian denominations permit interdenominational marriages.\nIn Methodism, \u00b681 of the 2014 \"Discipline\" of the Allegheny Wesleyan Methodist Connection, states with regard to interdenominational marriages: \"We do not prohibit our people from marrying persons who are not of our connection, provided such persons have the form and are seeking the power of godliness; but we are determined to discourage their marrying persons who do not come up to this description.\"\nThe Catholic Church recognizes as sacramental, (1) the marriages between two baptized Protestants or between two baptized Orthodox Christians, as well as (2) marriages between baptized non-Catholic Christians and Catholic Christians, although in the latter case, consent from the diocesan bishop must be obtained, with this being termed \"permission to enter into a mixed marriage\". To illustrate (1), for example, \"if two Lutherans marry in the Lutheran Church in the presence of a Lutheran minister, the Catholic Church recognizes this as a valid sacrament of marriage.\" Weddings in which both parties are Catholic Christians are ordinarily held in a Catholic church, while weddings in which one party is a Catholic Christian and the other party is a non-Catholic Christian can be held in a Catholic church or a non-Catholic Christian church.\nInterreligious marriage.\nIn Christianity, an interfaith marriage is a marriage between a baptized Christian and a non-baptized person, e.g. a wedding between a Christian man and Jewish woman.\nIn the Presbyterian Church (USA), the local church congregation is tasked with supporting and including an interfaith couple with one being a baptized Presbyterian Christian and the other being a non-Christian, in the life of the Church, \"help[ing] parents make and live by commitments about the spiritual nurture of their children\", and being inclusive of the children of the interfaith couple. The pastor is to be available to help and counsel the interfaith couple in their life journey.\nAlthough the Catholic Church recognizes as natural marriages weddings between two non-Christians or those between a Catholic Christian and a non-Christian, these are not considered to be sacramental, and in the latter case, the Catholic Christian must seek permission from his/her bishop for the marriage to occur; this permission is known as \"dispensation from disparity of cult\".\nIn Methodist Christianity, the 2014 \"Discipline\" of the Allegheny Wesleyan Methodist Connection discourages interfaith marriages, stating \"Many Christians have married unconverted persons. This has produced bad effects; they have either been hindered for life, or have turned back to perdition.\" Though the United Methodist Church authorizes its clergy to preside at interfaith marriages, it notes that Corinthians 6 has been interpreted \"as at least an ideal if not an absolute ban on such [interfaith] marriages as an issue of scriptural faithfulness, if not as an issue of Christian survival.\" At the same time, for those already in an interfaith marriage (including cases in which there is a non-Christian couple and one party converts to Christianity after marriage), the Church notes that Saint Paul \"addresses persons married to unbelievers and encourages them to stay married.\"\nSame-sex marriage.\nAnglican denominations such as the Episcopal Church in United States the Anglican Church of Canada, the Anglican Church in Aotearoa, New Zealand and Polynesia, the Anglican Episcopal Church of Brazil, the Scottish Episcopal Church in Scotland and mainline Protestant denominations such as the United Church of Christ, the United Church of Canada, the Metropolitan Community Church, the Presbyterian Church (USA), the Quakers, the United Reformed Church in United Kingdom, the Church of Scotland, the Methodist Church of Great Britain, the Church of Iceland, the Church of Sweden, the Church of Denmark, the Church of Norway, the United Protestant Church in Belgium, the Protestant Church in Baden, the Evangelical Church in Berlin, Brandenburg and Silesian Upper Lusatia, the Evangelical Church of Bremen, the Evangelical Lutheran Church in Brunswick, the Evangelical Church of Hesse Electorate-Waldeck, the Evangelical Lutheran Church in Oldenburg, the Evangelical Lutheran Church of Hanover, the Church of Lippe, the Evangelical Reformed Church in Bavaria and Northwestern Germany, the Evangelical Church in the Rhineland, the Protestant Church in Hesse and Nassau, the Evangelical Lutheran Church in Northern Germany the Protestant Church of the Palatinate, the Evangelical Church of Westphalia, the Mennonite Church in the Netherlands the United Protestant Church of France, the Catholic Diocese of the Old Catholics in Germany, the Christian Catholic Church of Switzerland, some Reformed churches in Federation of Swiss Protestant Churches for example the Reformed Church of Aargau, the Protestant Church of Geneva or the Evangelical Reformed Church of the Canton of Z\u00fcrich and some non-trinitarian denominations such as the Unity Church and the Unitarians, some international evangelical denominations, such as the Association of Welcoming and Affirming Baptists and Affirming Pentecostal Church International perform weddings between same-sex couples.\nThe Evangelical Lutheran Church of America, the Evangelical Lutheran Church in Canada, some Lutheran and united churches in Evangelical Church in Germany, some Reformed churches in Federation of Swiss Protestant Churches, and the Protestant Church in the Netherlands do not administer sacramental marriage to same-sex couples, but blesses same-sex unions through the use of a specific liturgy.\nThe Roman Catholic Church, the Orthodox Christian Church, and other more conservative Protestant denominations do not perform or recognize same-sex marriage because they do not consider it as marriage at all, and considering any homosexual sexual activity to be sinful. The Global Anglican Future Conference (GAFCON) consisting of the Church of Nigeria, Anglican Church of Kenya, Anglican Church of Tanzania, Rwanda and Uganda; Anglican Church of South America, Australia, parts of England, Canada, USA and Church of India through the Jerusalem Conference clearly asserted \"the unchangeable standard of Christian marriage between one man and one woman as the proper place for sexual intimacy.\"\nLocation of the wedding.\nWith respect to religion, historic Christian belief emphasizes that Christian weddings should occur in a church as Christian marriage should begin where one also starts their faith journey (Christians receive the sacrament of baptism in church in the presence of their congregation). Catholic Christian weddings must \"take place in a church building\" as holy matrimony is a sacrament; sacraments normatively occur in the presence of Christ in the house of God, and \"members of the faith community [should be] present to witness the event and provide support and encouragement for those celebrating the sacrament.\" Bishops never grant permission \"to those requesting to be married in a garden, on the beach, or some other place outside of the church\" and a dispensation is only granted \"in extraordinary circumstances (for example, if a bride or groom is ill or disabled and unable to come to the church).\" Marriage in the church, for Christians, is seen as contributing to the fruit of the newlywed couple regularly attending church each Lord's Day and raising children in the faith.\nTheological views.\nChristians seek to uphold the seriousness of wedding vows. Yet, they respond with compassion to deep hurts by recognizing that divorce, though less than the ideal, is sometimes necessary to relieve one partner of intolerable hardship, unfaithfulness or desertion. While the voice of God had said, \"I hate divorce\", some authorities believe the divorce rate in the church is nearly comparable to that of the culture at large.\nChristians today hold three competing views as to what is the biblically ordained relationship between husband and wife. These views range from Christian egalitarianism that interprets the New Testament as teaching complete equality of authority and responsibility between the man and woman in marriage, all the way to Patriarchy that calls for a \"return to complete patriarchy\" in which relationships are based on male-dominant power and authority in marriage:\n1. Christian Egalitarians believe in an equal partnership of the wife and husband with neither being designated as the leader in the marriage or family. Instead, the wife and husband share a fully equal partnership in both their marriage and in the family. Its proponents teach \"the fundamental biblical principle of the equality of all human beings before God\".\n\"There is neither Jew nor Gentile, neither slave nor free, nor is there male and female, for you are all one in Christ Jesus.\"\nAccording to this principle, there can be no moral or theological justification for permanently granting or denying status, privilege, or prerogative solely on the basis of a person's race, class, or gender.\n2. Christian Complementarians prescribe husband-headship\u2014a male-led hierarchy. This view's core beliefs call for a husband's \"loving, humble headship\" and the wife's \"intelligent, willing submission\" to his headship. They believe women have \"different but complementary roles and responsibilities in marriage\".\n3. Biblical patriarchy, though not at all popular among mainstream Christians, prescribes a strict male-dominant hierarchy. A very strong view makes the husband the ruler over his wife and his household. Their organization's first tenet is that \"God reveals Himself as masculine, not feminine. God is the eternal Father and the eternal Son, the Holy Spirit is also addressed as He, and Jesus Christ is a male\". They consider the husband-father to be sovereign over his household\u2014the family leader, provider, and protector. They call for a wife to be obedient to her head (her husband).\nSome Christian authorities permit the practice polygamy (specifically polygyny), but this practice, besides being illegal in Western cultures, is now considered to be out of the Christian mainstream in most parts of the globe; the Lutheran World Federation hosted a regional conference in Africa, in which the acceptance of polygamists and their wives into full membership by the Lutheran Church in Liberia was defended as being permissible. While the Lutheran Church in Liberia permits men to retain their wives if they married them prior to being received into the Church, it does not permit polygamists who have become Christians to marry more wives after they have received the sacrament of Holy Baptism.\nFamily authority and responsibilities.\nMuch of the dispute hinges on how one interprets the New Testament household code \"(Haustafel)\", a term coined by Martin Luther, which has as its main focus hierarchical relationships between three pairs of social classes that were controlled by Roman law: husbands/wives, parents/children, and masters/slaves. The apostolic teachings, with variations, that constitute what has been termed the \"household code\" occurs in four epistles (letters) by the Apostle Paul and in 1\u00a0 Peter.\nIn the early Roman Republic, long before the time of Christ, the law of \"manus\" along with the concept of \"patria potestas\" (rule of the fathers), gave the husband nearly absolute autocratic power over his wife, children, and slaves, including the power of life and death. In practice, the extreme form of this right was seldom exercised, and it was eventually limited by law.\nTheologian Frank Stagg finds the basic tenets of the code in Aristotle's discussion of the household in Book\u00a01 of \"Politics\" and in Philo's \"Hypothetica 7.14\". Serious study of the New Testament Household Code \"(Haustafel)\" began with Martin Dilbelius in 1913, with a wide range of studies since then. In a T\u00fcbingen dissertation, by James E. Crouch concludes that the early Christians found in Hellenistic Judaism a code which they adapted and Christianized.\nThe Staggs believe the several occurrences of the New Testament household code in the Bible were intended to meet the needs for \"order\" within the churches and in the society of the day. They maintain that the New Testament household code is an attempt by Paul and Peter to Christianize the concept of family relationships for Roman citizens who had become followers of Christ. The Staggs write that there is some suggestion in scripture that because Paul had taught that they had newly found freedom \"in Christ\", wives, children, and slaves were taking improper advantage of the \"Haustafel\" both in the home and the church. \n\"The form of the code stressing reciprocal social duties is traced to Judaism's own Oriental background, with its strong moral/ethical demand but also with a low view of woman... At bottom is probably to be seen the perennial tension between freedom and order... What mattered to (Paul) was 'a new creation' and 'in Christ' there is 'not any Jew not Greek, not any slave nor free, not any male and female'.\nTwo of these Christianized codes are found in Ephesians 5 (which contains the phrases \"husband is the head of the wife\" and \"wives, submit to your husband\") and in Colossians 3, which instructs wives to subordinate themselves to their husbands.\nThe importance of the meaning of \"head\" as used by the Apostle Paul is pivotal in the conflict between the Complementarian position and the Egalitarian view. The word Paul used for \"head\", transliterated from Greek, is \"kephal\u0113\". Today's English word \"cephalic\" ( ) stems from the Greek \"kephal\u0113\" and means \"of or relating to the head; or located on, in, or near the head.\" A thorough concordance search by Catherine Kroeger shows that the most frequent use of \"head\" \"(kephal\u0113)\" in the New Testament is to refer to \"the anatomical head of a body\". She found that its second most frequent use in the New Testament was to convey the metaphorical sense of \"source\". Other Egalitarian authors such as Margaret Howe agree with Kroeger, writing that \"The word 'head' must be understood not as 'ruler' but as 'source'\".\nWayne Grudem criticizes commonly rendering \"kephal\u0113\" in those same passages only to mean \"source\", and argues that it denotes \"authoritative head\" in such texts as Corinthians 11. They interpret that verse to mean that God the father is the authoritative head over the Son, and in turn Jesus is the authoritative head over the church, not simply its source. By extension, they then conclude that in marriage and in the church, the man is the authoritative head over the woman.\nAnother potential way to define the word \"head\", and hence the relationship between husband and wife as found in the Bible, is through the example given in the surrounding context in which the word is found. In that context the husband and wife are compared to Christ and his church. The context seems to imply an authority structure based on a man sacrificing himself for his wife, as Christ did for the church; a love-based authority structure, where submission is not required but freely given based on the care given to the wife.\nSome biblical references on this subject are debated depending on one's school of theology. The historical grammatical method is a hermeneutic technique that strives to uncover the meaning of the text by taking into account not just the grammatical words, but also the syntactical aspects, the cultural and historical background, and the literary genre. Thus references to a patriarchal Biblical culture may or may not be relevant to other societies. What is believed to be a timeless truth to one person or denomination may be considered a cultural norm or minor opinion to another.\nEgalitarian view.\nChristian Egalitarians (from the French word \"\u00e9gal\" meaning \"equal\") believe that Christian marriage is intended to be a marriage without any hierarchy\u2014a full and equal partnership between the wife and husband. They emphasize that nowhere in the New Testament is there a requirement for a wife to \"obey\" her husband. While \"obey\" was introduced into marriage vows for much of the church during the Middle Ages, its only New Testament support is found in Peter 3, with that only being by implication from Sarah's obedience to Abraham. Scriptures such as state that in Christ, right relationships are restored and in him, \"there is neither Jew nor Greek, slave nor free, male nor female.\"\nChristian Egalitarians interpret scripture to mean that God intended spouses to practice \"mutual submission\", each in equality with the other. The phrase \"mutual submission\" comes from a verse in Ephesians 5 which precedes advice for the three domestic relationships of the day, including slavery. It reads, \"Submit to one another ('mutual submission') out of reverence for Christ\", wives to husbands, children to parents, and slaves to their master. Christian Egalitarians believe that full partnership in marriage is the most biblical view, producing the most intimate, wholesome, and reciprocally fulfilling marriages.\nThe Christian Egalitarian view of marriage asserts that gender, in and of itself, neither privileges nor curtails a believer's gifting or calling to any ministry in the church or home. It does not imply that women and men are identical or undifferentiated, but affirms that God designed men and women to complement and benefit one another. A foundational belief of Christian Egalitarians is that the husband and wife are created equally and are ordained of God to \"become one\", a biblical principle first ordained by God in Genesis 2, reaffirmed by Jesus in Matthew 19 and Mark 10, and by the Apostle Paul in Ephesians 5. Therefore, they see that \"oneness\" as pointing to gender equality in marriage. They believe the biblical model for Christian marriages is therefore for the spouses to share equal responsibility within the family\u2014not one over the other nor one under the other.\nDavid Dykes, theologian, author, and pastor of a 15,000-member Baptist church, sermonized that \"When you are in Christ, you have full equality with all other believers\". In a sermon he entitled \"The Ground Is Level at the Foot of the Cross\", he said that some theologians have called one particular Bible verse the Christian \"Magna Carta\". The Bible verse reads: \"There is neither Jew nor Gentile, neither slave nor free, nor is there male and female, for you are all one in Christ Jesus.\" Acknowledging the differences between men and women, Dykes writes that \"in Christ, these differences don't define who we are. The only category that really matters in the world is whether you are in Christ. At the cross, Jesus destroyed all the made-made barriers of hostility:\" ethnicity, social status, and gender.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThose of the egalitarian persuasion point to the biblical instruction that all Christian believers, irrespective of gender, are to submit or be subject \"to one another in the fear of God\" or \"out of reverence for Christ\". Gilbert Bilezikian writes that in the highly debated Ephesians\u00a05 passage, the verb \"to be subject\" or \"to be submitted\" appears in verse\u00a021 which he describes as serving as a \"hinge\" between two different sections. The first section consists of verses 18\u201320, verse 21 is the connection between the two, and the second section consists of verses 22\u201333. When discussion begins at verse 22 in Ephesians\u00a05, Paul appears to be reaffirming a chain of command principle within the family. However,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAdvocates of Christian egalitarianism believe that this model has firm biblical support:\nThe egalitarian paradigm leaves it up to the couple to decide who is responsible for what task or function in the home. Such decisions should be made rationally and wisely, not based on gender or tradition. Examples of a couple's decision logic might include:\nComplementarian view.\nComplementarians hold to a hierarchical structure between husband and wife. They believe men and women have different gender-specific roles that allow each to \"complement\" the other, hence the designation \"Complementarians\". The Complementarian view of marriage holds that while the husband and wife are of equal worth before God, husbands and wives are given different functions and responsibilities by God that are based on gender, and that male leadership is biblically ordained so that the husband is always the senior authority figure. They state they \"observe with deep concern\" \"accompanying distortions or neglect of the glad harmony portrayed in Scripture between the intelligent, humble leadership of redeemed husbands and the loving, willing support of that leadership by redeemed wives\". They believe \"the Bible presents a clear chain of authority\u2014above all authority and power is God; God is the head of Christ. Then in descending order, Christ is the head of man, man is the head of woman, and parents are the head of their children.\" Complementarians teach that God intended men to lead their wives as \"heads\" of the family. Wayne Grudem, in an article that interprets the \"mutual submission\" of Ephesians 5 as being hierarchical, writes that it means \"being considerate of one another, and caring for one another\u2019s needs, and being thoughtful of one another, and sacrificing for one another.\"\nScriptures such as 1 Corinthians 11:3: \"But I would have you know, that the head of every man is Christ; and the head of the woman is the man; and the head of Christ is God\", (KJV) is understood as meaning the wife is to be subject to her husband, if not unconditionally.\nAccording to Complementarian authors John Piper, Wayne Grudem, and others, historically, but to a significantly lesser extent in most of Christianity today, the predominant position in both Catholicism and conservative Protestantism places the male as the \"head\" in the home and in the church. They hold that women are commanded to be in subjection to male leadership, with a wife being obedient to her head (husband), based upon Old Testament precepts and principles. This view holds that, \"God has created men and women equal in their essential dignity and human personhood, but different and complementary in function with male headship in the home and in the Church.\"\nGrudem also acknowledges exceptions to the submission of wives to husbands where moral issues are involved. Rather than unconditional obedience, Complementarian authors such as Piper and Grudem are careful to caution that a wife's submission should never cause her to \"follow her husband into sin.\"\nRoman Catholic Church teaching on the role of women includes that of Pope Leo XIII in his 1880 encyclical \"Arcanum\" which states:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The husband is the chief of the family and the head of the wife. The woman, because she is flesh of his flesh, and bone of his bone, must be subject to her husband and obey him; not, indeed, as a servant, but as a companion, so that her obedience shall be wanting in neither honor nor dignity. Since the husband represents Christ, and since the wife represents the Church, let there always be, both in him who commands and in her who obeys, a heaven-born love guiding both in their respective duties.\" This position was affirmed in the 1930 encyclical \"Casti Connubii,\" which invokes Ephesians 5:22, \"Let women be subject to their husbands as to the Lord, because the husband is the head of the wife, and Christ is the head of the Church.\nThough each of their churches is autonomous and self-governed, the official position of the Southern Baptist Convention (the largest Protestant denomination in the United States) is:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The husband and wife are of equal worth before God, since both are created in God's image. A husband is to love his wife as Christ loved the church. He has the God-given responsibility to provide for, to protect, and to lead his family. A wife is to submit herself graciously to the servant leadership of her husband even as the church willingly submits to the headship of Christ. She, being in the image of God as is her husband and thus equal to him, has the God-given responsibility to respect her husband and to serve as his helper in managing the household and nurturing the next generation.\"\nBiblical patriarchy.\nThe patriarchal model of marriage is clearly the oldest one. It characterized the theological understanding of most Old Testament writers. It mandates the supremacy, at times the ultimate domination, of the husband-father in the family. In the first century Roman Empire, in the time of Jesus, Paul, and Peter, it was the law of the land and gave the husband absolute authority over his wife, children, and slaves\u2014even the power of life or death. It subordinates all women.\nBiblical patriarchy is similar to Complementarianism but with differences of degree and emphasis. Biblical patriarchists carry the husband-headship model considerably further and with more militancy. While Complementarians also hold to exclusively male leadership in both the home and the church, Biblical patriarchy extends that exclusion to the civic sphere as well, so that women should not be civil leaders and indeed should not have careers outside the home.\nPatriarchy is based on authoritarianism\u2014complete obedience or subjection to male authority as opposed to individual freedom. Patriarchy gives preeminence to the male in essentially all matters of religion and culture. It explicitly deprives all women of social, political, and economic rights. The marriage relationship simply reinforced this dominance of women by men, providing religious, cultural, and legal structures that clearly favor patriarchy to the exclusion of even basic human dignity for wives.\nHistorically in classical patriarchy, the wives and children were always legally dependent upon the father, as were the slaves and other servants. It was the way of life throughout most of the Old Testament, religiously, legally, and culturally. However, it was not unique to Hebrew thought. With only minor variations, it characterized virtually every pagan culture of that day\u2014including all Pre-Christian doctrine and practice.\nWhile Scripture allowed this approach in Old Testament times, nowhere does the Bible ordain it. In the Hebrew nation, patriarchy seems to have evolved as an expression of male dominance and supremacy, and of a double standard that prevailed throughout much of the Old Testament. Its contemporary advocates insist that it is the only biblically valid model for marriage today. They argue that it was established at Creation, and thus is a firm, unalterable decree of God about the relative positions of men and women.\nBiblical patriarchists see what they describe as a crisis of this era being what they term to be a systematic attack on the \"timeless truths of biblical patriarchy\". They believe such an attack includes the movement to \"subvert the biblical model of the family, and redefine the very meaning of fatherhood and motherhood, masculinity, femininity, and the parent and child relationship.\" Arguing from the biblical presentation of God revealing himself \"as masculine, not feminine\", they believe God ordained distinct gender roles for man and woman as part of the created order. They say \"Adam\u2019s headship over Eve was established at the beginning, before sin entered the world\". Their view is that the male has God-given authority and mandate to direct \"his\" household in paths of obedience to God. They refer to man's \"dominion\" beginning within the home, and a man's qualification to lead and ability to lead well in the public square is based upon his prior success in \"ruling his household\".\nThus, William Einwechter refers to the traditional Complementarian view as \"two-point Complementarianism\" (male leadership in the family and church), and regards the biblical patriarchy view as \"three-point\" or \"full\" complementarianism (male leadership in family, church \"and society\").\nThe patriarchists teach that \"the woman was created as a helper to her husband, as the bearer of children, and as a \"keeper at home\", concluding that the God-ordained and proper sphere of dominion for a wife is the household. Biblical patriarchists consider that \"faithfulness to Christ requires that (Biblical patriarchy) be believed, taught, and lived\". They claim that the \"man is...the image and glory of God in terms of authority, while the woman is the glory of man\". They teach that a wife is to be \"obedient\" to her \"head\" (husband), based upon Old Testament teachings and models.\nOther views.\nSee Christian feminism\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7392", "revid": "211905", "url": "https://en.wikipedia.org/wiki?curid=7392", "title": "Class (computer programming)", "text": "Definition in programming that specifies how an object works\nIn object-oriented programming, a class is an extensible program-code-template for creating objects, providing initial values for state (member variables) and implementations of behavior (member functions or methods). In many languages, the class name is used as the name for the class (the template itself), the name for the default constructor of the class (a subroutine that creates objects), and as the type of objects generated by instantiating the class; these distinct concepts are easily conflated. Although, to the point of conflation, one could argue that is a feature inherent in a language because of its polymorphic nature and why these languages are so powerful, dynamic and adaptable for use compared to languages without polymorphism present. Thus they can model dynamic systems (i.e. the real world, machine learning, AI) more easily.\nWhen an object is created by a constructor of the class, the resulting object is called an instance of the class, and the member variables specific to the object are called instance variables, to contrast with the class variables shared across the class.\nIn certain languages, classes are, as a matter of fact, only a compile-time feature (new classes cannot be declared at run-time), while in other languages classes are first-class citizens, and are generally themselves objects (typically of type Class or similar). In these languages, a class that creates classes within itself is called a metaclass.\nClass vs. type.\nIn its most casual usage, people often refer to the \"class\" of an object, but narrowly speaking objects have \"type\": the interface, namely the types of member variables, the signatures of member functions (methods), and properties these satisfy. At the same time, a class has an implementation (specifically the implementation of the methods), and can create objects of a given type, with a given implementation. In the terms of type theory, a class is an implementation\u200d\u2014\u200ca \"concrete\" data structure and collection of subroutines\u200d\u2014\u200cwhile a type is an interface. Different (concrete) classes can produce objects of the same (abstract) type (depending on type system); for example, the type Stack might be implemented with two classes\u00a0\u2013 SmallStack (fast for small stacks, but scales poorly) and ScalableStack (scales well but high overhead for small stacks). Similarly, a given class may have several different constructors.\nClass types generally represent nouns, such as a person, place or thing, or something nominalized, and a class represents an implementation of these. For example, a Banana type might represent the properties and functionality of bananas in general, while the ABCBanana and XYZBanana classes would represent ways of producing bananas (say, banana suppliers or data structures and functions to represent and draw bananas in a video game). The ABCBanana class could then produce particular bananas: instances of the ABCBanana class would be objects of type Banana. Often only a single implementation of a type is given, in which case the class name is often identical with the type name.\nDesign and implementation.\nClasses are composed from structural and behavioral constituents. Programming languages that include classes as a programming construct offer support, for various class-related features, and the syntax required to use these features varies greatly from one programming language to another.\nStructure.\nA class contains data field descriptions (or \"properties\", \"fields\", \"data members\", or \"attributes\"). These are usually field types and names that will be associated with state variables at program run time; these state variables either belong to the class or specific instances of the class. In most languages, the structure defined by the class determines the layout of the memory used by its instances. Other implementations are possible: for example, objects in Python use associative key-value containers.\nSome programming languages such as Eiffel support specification of invariants as part of the definition of the class, and enforce them through the type system. Encapsulation of state is necessary for being able to enforce the invariants of the class.\nBehavior.\nThe behavior of class or its instances is defined using methods. Methods are subroutines with the ability to operate on objects or classes. These operations may alter the state of an object or simply provide ways of accessing it. Many kinds of methods exist, but support for them varies across languages. Some types of methods are created and called by programmer code, while other special methods\u2014such as constructors, destructors, and conversion operators\u2014are created and called by compiler-generated code. A language may also allow the programmer to define and call these special methods.\nThe concept of class interface.\nEvery class \"implements\" (or \"realizes\") an interface by providing structure and behavior. Structure consists of data and state, and behavior consists of code that specifies how methods are implemented. There is a distinction between the definition of an interface and the implementation of that interface; however, this line is blurred in many programming languages because class declarations both define and implement an interface. Some languages, however, provide features that separate interface and implementation. For example, an abstract class can define an interface without providing implementation.\nLanguages that support class inheritance also allow classes to inherit interfaces from the classes that they are derived from.\nFor example, if \"class A\" inherits from \"class B\" and if \"class B\" implements the interface \"interface B\" then \"class A\" also inherits the functionality(constants and methods declaration) provided by \"interface B\".\nIn languages that support access specifiers, the interface of a class is considered to be the set of public members of the class, including both methods and attributes (via implicit getter and setter methods); any private members or internal data structures are not intended to be depended on by external code and thus are not part of the interface.\nObject-oriented programming methodology dictates that the operations of any interface of a class are to be independent of each other. It results in a layered design where clients of an interface use the methods declared in the interface. An interface places no requirements for clients to invoke the operations of one interface in any particular order. This approach has the benefit that client code can assume that the operations of an interface are available for use whenever the client has access to the object. \nExample.\nThe buttons on the front of your television set are the interface between you and the electrical wiring on the other side of its plastic casing. You press the \"power\" button to toggle the television on and off. In this example, your particular television is the instance, each method is represented by a button, and all the buttons together compose the interface (other television sets that are the same model as yours would have the same interface). In its most common form, an interface is a specification of a group of related methods without any associated implementation of the methods.\nA television set also has a myriad of \"attributes\", such as size and whether it supports colour, which together comprise its structure. A class represents the full description of a television, including its attributes (structure) and buttons (interface).\nGetting the total number of televisions manufactured could be a \"static method\" of the television class. This method is clearly associated with the class, yet is outside the domain of each individual instance of the class. A static method that finds a particular instance out of the set of all television objects is another example.\nMember accessibility.\nThe following is a common set of access specifiers:\nAlthough many object-oriented languages support the above access specifiers, their semantics may differ.\nObject-oriented design uses the access specifiers in conjunction with careful design of public method implementations to enforce class invariants\u2014constraints on the state of the objects. A common usage of access specifiers is to separate the internal data of a class from its interface: the internal structure is made private, while public accessor methods can be used to inspect or alter such private data.\nAccess specifiers do not necessarily control \"visibility\", in that even private members may be visible to client external code. In some languages, an inaccessible but visible member may be referred to at run-time (for example, by a pointer returned from a member function), but an attempt to use it by referring to the name of the member from client code will be prevented by the type checker.\nThe various object-oriented programming languages enforce member accessibility and visibility to various degrees, and depending on the language's type system and compilation policies, enforced at either compile-time or run-time. For example, the Java language does not allow client code that accesses the private data of a class to compile.\n In the C++ language, private methods are visible, but not accessible in the interface; however, they may be made invisible by explicitly declaring fully abstract classes that represent the interfaces of the class.\nSome languages feature other accessibility schemes:\nInter-class relationships.\nIn addition to the design of standalone classes, programming languages may support more advanced class design based upon relationships between classes. The inter-class relationship design capabilities commonly provided are \"compositional\" and \"hierarchical\".\nCompositional.\nClasses can be composed of other classes, thereby establishing a compositional relationship between the enclosing class and its embedded classes. Compositional relationship between classes is also commonly known as a \"has-a\" relationship. For example, a class \"Car\" could be composed of and contain a class \"Engine\". Therefore, a Car \"has an\" Engine. One aspect of composition is containment, which is the enclosure of component instances by the instance that has them. If an enclosing object contains component instances by value, the components and their enclosing object have a similar lifetime. If the components are contained by reference, they may not have a similar lifetime. For example, in Objective-C 2.0:\n@interface Car : NSObject\n@property NSString *name;\n@property Engine *engine\n@property NSArray *tires;\n@end\nThis Car class \"has\" an instance of NSString (a string object), Engine, and NSArray (an array object).\nHierarchical.\nClasses can be \"derived\" from one or more existing classes, thereby establishing a hierarchical relationship between the derived-from classes (\"base classes\", \"parent classes\" or \"&lt;templatestyles src=\"Template:Visible anchor/styles.css\" /&gt;superclasses\") and the derived class (\"child class\" or \"subclass\") . The relationship of the derived class to the derived-from classes is commonly known as an is-a relationship. For example, a class 'Button' could be derived from a class 'Control'. Therefore, a Button is a Control. Structural and behavioral members of the parent classes are \"inherited\" by the child class. Derived classes can define additional structural members (data fields) and behavioral members (methods) in addition to those that they \"inherit\" and are therefore \"specializations\" of their superclasses. Also, derived classes can override inherited methods if the language allows.\nNot all languages support multiple inheritance. For example, Java allows a class to implement multiple interfaces, but only inherit from one class. If multiple inheritance is allowed, the hierarchy is a directed acyclic graph (or DAG for short), otherwise it is a tree. The hierarchy has classes as nodes and inheritance relationships as links. Classes in the same level are more likely to be associated than classes in different levels. The levels of this hierarchy are called layers or levels of abstraction.\nExample (Simplified Objective-C 2.0 code, from iPhone SDK):\n@interface UIResponder : NSObject //...\n@interface UIView : UIResponder //...\n@interface UIScrollView : UIView //...\n@interface UITableView : UIScrollView //...\nIn this example, a UITableView is a UIScrollView is a UIView is a UIResponder is an NSObject.\nDefinitions of subclass.\nConceptually, a superclass is a superset of its subclasses. For example, a common class hierarchy would involve GraphicObject as a superclass of Rectangle and Ellipse, while Square would be a subclass of Rectangle. These are all subset relations in set theory as well, i.e., all squares are rectangles but not all rectangles are squares.\nA common conceptual error is to mistake a \"part of\" relation with a subclass. For example, a car and truck are both kinds of vehicles and it would be appropriate to model them as subclasses of a vehicle class. However, it would be an error to model the component parts of the car as subclass relations. For example, a car is composed of an engine and body, but it would not be appropriate to model engine or body as a subclass of car.\nIn object-oriented modeling these kinds of relations are typically modeled as object properties. In this example, the Car class would have a property called parts. parts would be typed to hold a collection of objects, such as instances of Body, Engine, Tires, etc.\nObject modeling languages such as UML include capabilities to model various aspects of \"part of\" and other kinds of relations \u2013 data such as the cardinality of the objects, constraints on input and output values, etc. This information can be utilized by developer tools to generate additional code beside the basic data definitions for the objects, such as error checking on get and set methods.\nOne important question when modeling and implementing a system of object classes is whether a class can have one or more superclasses. In the real world with actual sets it would be rare to find sets that did not intersect with more than one other set. However, while some systems such as Flavors and CLOS provide a capability for more than one parent to do so at run time introduces complexity that many in the object-oriented community consider antithetical to the goals of using object classes in the first place. Understanding which class will be responsible for handling a message can get complex when dealing with more than one superclass. If used carelessly this feature can introduce some of the same system complexity and ambiguity classes were designed to avoid.\nMost modern object-oriented languages such as Smalltalk and Java require single inheritance at run time. For these languages, multiple inheritance may be useful for modeling but not for an implementation.\nHowever, semantic web application objects do have multiple superclasses. The volatility of the Internet requires this level of flexibility and the technology standards such as the Web Ontology Language (OWL) are designed to support it.\nA similar issue is whether or not the class hierarchy can be modified at run time. Languages such as Flavors, CLOS, and Smalltalk all support this feature as part of their meta-object protocols. Since classes are themselves first-class objects, it is possible to have them dynamically alter their structure by sending them the appropriate messages. Other languages that focus more on strong typing such as Java and C++ do not allow the class hierarchy to be modified at run time. Semantic web objects have the capability for run time changes to classes. The rational is similar to the justification for allowing multiple superclasses, that the Internet is so dynamic and flexible that dynamic changes to the hierarchy are required to manage this volatility.\nOrthogonality of the class concept and inheritance.\nAlthough class-based languages are commonly assumed to support inheritance, inheritance is not an intrinsic aspect of the concept of classes. Some languages, often referred to as \"object-based languages\", support classes yet do not support inheritance. Examples of object-based languages include earlier versions of Visual Basic.\nWithin object-oriented analysis.\nIn object-oriented analysis and in UML, an association between two classes represents a collaboration between the classes or their corresponding instances. Associations have direction; for example, a bi-directional association between two classes indicates that both of the classes are aware of their relationship. Associations may be labeled according to their name or purpose.\nAn association role is given end of an association and describes the role of the corresponding class. For example, a \"subscriber\" role describes the way instances of the class \"Person\" participate in a \"subscribes-to\" association with the class \"Magazine\". Also, a \"Magazine\" has the \"subscribed magazine\" role in the same association. Association role multiplicity describes how many instances correspond to each instance of the other class of the association. Common multiplicities are \"0..1\", \"1..1\", \"1..*\" and \"0..*\", where the \"*\" specifies any number of instances.\nTaxonomy of classes.\nThere are many categories of classes, some of which overlap.\nAbstract and concrete.\nIn a language that supports inheritance, an abstract class, or abstract base class (ABC), is a class that cannot be instantiated because it is either labeled as abstract or it simply specifies abstract methods (or \"virtual methods\"). An abstract class may provide implementations of some methods, and may also specify virtual methods via signatures that are to be implemented by direct or indirect descendants of the abstract class. Before a class derived from an abstract class can be instantiated, all abstract methods of its parent classes must be implemented by some class in the derivation chain.\nMost object-oriented programming languages allow the programmer to specify which classes are considered abstract and will not allow these to be instantiated. For example, in Java, C# and PHP, the keyword \"abstract\" is used. In C++, an abstract class is a class having at least one abstract method given by the appropriate syntax in that language (a pure virtual function in C++ parlance).\nA class consisting of only virtual methods is called a Pure Abstract Base Class (or \"Pure ABC\") in C++ and is also known as an \"interface\" by users of the language. Other languages, notably Java and C#, support a variant of abstract classes called an interface via a keyword in the language. In these languages, multiple inheritance is not allowed, but a class can implement multiple interfaces. Such a class can only contain abstract publicly accessible methods.\nA concrete class is a class that can be instantiated, as opposed to abstract classes, which cannot. \nLocal and inner.\nIn some languages, classes can be declared in scopes other than the global scope. There are various types of such classes.\nAn inner class is a class defined within another class. The relationship between an inner class and its containing class can also be treated as another type of class association. An inner class is typically neither associated with instances of the enclosing class nor instantiated along with its enclosing class. Depending on language, it may or may not be possible to refer to the class from outside the enclosing class. A related concept is \"inner types\", also known as \"inner data type\" or \"nested type\", which is a generalization of the concept of inner classes. C++ is an example of a language that supports both inner classes and inner types (via \"typedef\" declarations).\nAnother type is a local class, which is a class defined within a procedure or function. This limits references to the class name to within the scope where the class is declared. Depending on the semantic rules of the language, there may be additional restrictions on local classes compared to non-local ones. One common restriction is to disallow local class methods to access local variables of the enclosing function. For example, in C++, a local class may refer to static variables declared within its enclosing function, but may not access the function's automatic variables.\nMetaclasses.\nMetaclasses are classes whose instances are classes. A metaclass describes a common structure of a collection of classes and can implement a design pattern or describe particular kinds of classes. Metaclasses are often used to describe frameworks.\nIn some languages, such as Python, Ruby or Smalltalk, a class is also an object; thus each class is an instance of a unique metaclass that is built into the language.\nThe Common Lisp Object System (CLOS) provides metaobject protocols (MOPs) to implement those classes and metaclasses.\nNon-subclassable.\nNon-subclassable classes allow programmers to design classes and hierarchies of classes where at some level in the hierarchy, further derivation is prohibited (a stand-alone class may be also designated as non-subclassable, preventing the formation of any hierarchy). Contrast this to \"abstract\" classes, which imply, encourage, and require derivation in order to be used at all. A non-subclassable class is implicitly \"concrete\".\nA non-subclassable class is created by declaring the class as in C# or as in Java or PHP. For example, Java's \n class is designated as \"final\".\nNon-subclassable classes may allow a compiler (in compiled languages) to perform optimizations that are not available for subclassable classes. \nOpen class.\nAn open class is one that can be changed. Typically, an executable program cannot be changed by customers. Developers can often change some classes, but typically cannot change standard or built-in ones. In Ruby, all classes are open. In Python, classes can be created at runtime, and all can be modified afterwards. Objective-C categories permit the programmer to add methods to an existing class without the need to recompile that class or even have access to its source code.\nMixins.\nSome languages have special support for mixins, though in any language with multiple inheritance a mixin is simply a class that does not represent an is-a-type-of relationship. Mixins are typically used to add the same methods to multiple classes; for example, a class UnicodeConversionMixin might provide a method called unicode_to_ascii when included in classes FileReader and WebPageScraper that do not share a common parent.\nPartial.\nIn languages supporting the feature, a partial class is a class whose definition may be split into multiple pieces, within a single source-code file or across multiple files. The pieces are merged at compile-time, making compiler output the same as for a non-partial class.\nThe primary motivation for introduction of partial classes is to facilitate the implementation of code generators, such as visual designers. It is otherwise a challenge or compromise to develop code generators that can manage the generated code when it is interleaved within developer-written code. Using partial classes, a code generator can process a separate file or coarse-grained partial class within a file, and is thus alleviated from intricately interjecting generated code via extensive parsing, increasing compiler efficiency and eliminating the potential risk of corrupting developer code. In a simple implementation of partial classes, the compiler can perform a phase of precompilation where it \"unifies\" all the parts of a partial class. Then, compilation can proceed as usual.\nOther benefits and effects of the partial class feature include:\nPartial classes have existed in Smalltalk under the name of \"Class Extensions\" for considerable time. With the arrival of the .NET framework 2, Microsoft introduced partial classes, supported in both C# 2.0 and Visual Basic 2005. WinRT also supports partial classes.\nExample in VB.NET.\nThis simple example, written in Visual Basic .NET, shows how parts of the same class are defined in two different files.\nPartial Class MyClass\n Private _name As String\nEnd Class\nPartial Class MyClass\n Public Readonly Property Name() As String\n Get\n Return _name\n End Get\n End Property\nEnd Class\nWhen compiled, the result is the same as if the two files were written as one, like this:\nClass MyClass\n Private _name As String\n Public Readonly Property Name() As String\n Get\n Return _name\n End Get\n End Property\nEnd Class\nExample in Objective-C.\nIn Objective-C, partial classes, also known as categories, may even spread over multiple libraries and executables, like the following example. But a key difference is that Objective-C's categories can overwrite definitions in another interface declaration, and that categories are not equal to original class definition (the first requires the last). Instead, .NET partial class can not have conflicting definitions, and all partial definitions are equal to the others.\nIn Foundation, header file NSData.h:\n@interface NSData : NSObject\n- (id)initWithContentsOfURL:(NSURL *)URL;\n@end\nIn user-supplied library, a separate binary from Foundation framework, header file NSData+base64.h:\n@interface NSData (base64)\n- (NSString *)base64String;\n- (id)initWithBase64String:(NSString *)base64String;\n@end\nAnd in an app, yet another separate binary file, source code file main.m:\nint main(int argc, char *argv[])\n if (argc &lt; 2)\n return EXIT_FAILURE;\n NSString *sourceURLString = [NSString stringWithCString:argv[1]];\n NSData *data = ;\n NSLog(@\"%@\", [data base64String]);\n return EXIT_SUCCESS;\nThe dispatcher will find both methods called over the NSData instance and invoke both of them correctly.\nUninstantiable.\nUninstantiable classes allow programmers to group together per-class fields and methods that are accessible at runtime without an instance of the class. Indeed, instantiation is prohibited for this kind of class.\nFor example, in C#, a class marked \"static\" can not be instantiated, can only have static members (fields, methods, other), may not have \"instance constructors\", and is \"sealed\".\nUnnamed.\nAn unnamed class or anonymous class is a class that is not bound to a name or identifier upon definition. This is analogous to named versus unnamed functions.\nBenefits.\nThe benefits of organizing software into object classes fall into three categories:\nObject classes facilitate rapid development because they lessen the semantic gap between the code and the users. System analysts can talk to both developers and users using essentially the same vocabulary, talking about accounts, customers, bills, etc. Object classes often facilitate rapid development because most object-oriented environments come with powerful debugging and testing tools. Instances of classes can be inspected at run time to verify that the system is performing as expected. Also, rather than get dumps of core memory, most object-oriented environments have interpreted debugging capabilities so that the developer can analyze exactly where in the program the error occurred and can see which methods were called to which arguments and with what arguments.\nObject classes facilitate ease of maintenance via encapsulation. When developers need to change the behavior of an object they can localize the change to just that object and its component parts. This reduces the potential for unwanted side effects from maintenance enhancements.\nSoftware re-use is also a major benefit of using Object classes. Classes facilitate re-use via inheritance and interfaces. When a new behavior is required it can often be achieved by creating a new class and having that class inherit the default behaviors and data of its superclass and then tailor some aspect of the behavior or data accordingly. Re-use via interfaces (also known as methods) occurs when another object wants to invoke (rather than create a new kind of) some object class. This method for re-use removes many of the common errors that can make their way into software when one program re-uses code from another.\nRun-time representation.\nAs a data type, a class is usually considered as a compile-time construct. A language or library may also support prototype or factory metaobjects that represent run-time information about classes, or even represent metadata that provides access to reflection facilities and ability to manipulate data structure formats at run-time. Many languages distinguish this kind of run-time type information about classes from a class on the basis that the information is not needed at run-time. Some dynamic languages do not make strict distinctions between run-time and compile-time constructs, and therefore may not distinguish between metaobjects and classes.\nFor example, if Human is a metaobject representing the class Person, then instances of class Person can be created by using the facilities of the Human metaobject.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7394", "revid": "949717", "url": "https://en.wikipedia.org/wiki?curid=7394", "title": "Canterbury (disambiguation)", "text": "Canterbury is a city located in the county of Kent in southeast England. It may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "7395", "revid": "194203", "url": "https://en.wikipedia.org/wiki?curid=7395", "title": "Cryptographer", "text": ""}
{"id": "7397", "revid": "11445145", "url": "https://en.wikipedia.org/wiki?curid=7397", "title": "Color blindness", "text": "Decreased ability to see color or color differences\nMedical condition\nColor blindness or color vision deficiency (CVD) is the decreased ability to see color or differences in color. It can impair tasks such as selecting ripe fruit, choosing clothing, and reading traffic lights. Color blindness may make some academic activities more difficult. However, issues are generally minor, and the colorblind automatically develop adaptations and coping mechanisms. People with total color blindness (achromatopsia) may also be uncomfortable in bright environments and have decreased visual acuity.\nThe most common cause of color blindness is an inherited problem or variation in the functionality of one or more of the three classes of cone cells in the retina, which mediate color vision. The most common form is caused by a genetic disorder called congenital red\u2013green color blindness. Males are more likely to be color blind than females, because the genes responsible for the most common forms of color blindness are on the X chromosome. Non-color-blind females can carry genes for color blindness and pass them on to their children. Color blindness can also result from physical or chemical damage to the eye, the optic nerve, or parts of the brain. Screening for color blindness is typically done with the Ishihara color test.\nThere is no cure for color blindness. Diagnosis may allow an individual, or their parents/teachers to actively accommodate the condition. Special lenses such as \"EnChroma\" glasses or \"X-chrom\" contact lenses may help people with red\u2013green color blindness at some color tasks, but they do not grant the wearer \"normal color vision\". Mobile apps can help people identify colors.\nRed\u2013green color blindness is the most common form, followed by blue\u2013yellow color blindness and total color blindness. Red\u2013green color blindness affects \"up to\" 1 in 12 males (8%) and 1 in 200 females (0.5%). The ability to see color also decreases in old age. In certain countries, color blindness may make people ineligible for certain jobs, such as those of aircraft pilots, train drivers, crane operators, and people in the armed forces. The effect of color blindness on artistic ability is controversial, but a number of famous artists are believed to have been color blind.\n&lt;templatestyles src=\"Template:TOC limit/styles.css\" /&gt;\nThis article is about color blindness in humans, but other organisms also have color blindness. Many species have color vision which is different from human vision, with either a limited or extended range of visible colors as compared with humans.\nEffects.\nA colorblind subject will have decreased (or no) color discrimination along the red\u2013green axis, blue\u2013yellow axis, or both, though the vast majority of the colorblind are only affected on their red\u2013green axis.\nThe first indication of colorblindness generally consists of a person using the wrong color for an object, such as when painting, or calling a color by the wrong name. The colors that are confused are very consistent among people with the same type of color blindness.\nConfusion colors.\nConfusion colors are pairs or groups of colors that will often be mistaken by the colorblind. Confusion colors for red\u2013green color blindness include:\nConfusion colors for blue\u2013yellow color blindness include:\nThese colors of confusion are defined quantitatively by straight confusion lines plotted in CIEXYZ, usually plotted on the corresponding chromaticity diagram. The lines all intersect at a \"copunctal point\", which varies with the type of color blindness. Chromaticities along a confusion line will appear metameric to dichromats of that type. Anomalous trichromats of that type will see the Chromaticities as metameric if they are close enough, depending on the strength of their CVD. For two colors on a confusion line to be metameric, the Chromaticities first have to be made \"isoluminant\", i.e. to have the same Lightness. Note also that colors that may be isoluminant to the standard observer (typical trichromat) may not be isoluminant to a dichromat.\nColor tasks.\nCole describes four color tasks, all of which are impeded to some degree by color blindness:\nThe following sections describe specific color tasks with which the colorblind typically have difficulty.\nFood.\nColorblindness causes difficulty with the \"connotative\" color tasks associated with selecting or preparing food, for example:\nSkin color.\nChanges in skin color due to bruising, sunburn, rashes or even blushing are easily missed by those with red\u2013green colorblindness. These discolorations are often linked to the blood oxygen saturation, which affects skin reflectance.\nTraffic lights.\nThe colors of traffic lights can be difficult for the red\u2013green colorblind. This includes distinguishing:\nThe main coping mechanism to overcome these challenges is to memorize the position of lights. The order of the common triplet traffic light is standardized as red\u2013amber\u2013green from top to bottom or left to right. Cases that deviate from this standard are rare. One such case is a traffic light in Tipperary Hill in Syracuse, New York, which is upside-down (green\u2013amber\u2013red top to bottom) due to the sentiments of its Irish American community. However, it has been criticized due to the potential hazard it poses for color-blind drivers.\nThere are several features of traffic lights available that help accommodate the colorblind:\nSignal lights.\nNavigation lights in marine and aviation settings employ red and green lights to signal the relative position of other ships or aircraft. Railway signal lights also rely heavily on red\u2013green\u2013yellow colors. In both cases, these color combinations can be difficult for the red\u2013green colorblind. Lantern Tests are a common means of simulating these light sources to determine not necessarily whether someone is colorblind, but whether they can functionally distinguish these specific signal colors. Those who cannot pass this test are generally completely restricted from working on aircraft, ships or rail.\nFashion.\nColor analysis is the analysis of color in its use in fashion, to determine personal color combinations that are most aesthetically pleasing. Colors to combine can include clothing, accessories, makeup, hair color, skin color, eye color, etc. Color analysis involves many aesthetic and comparative color task that can be difficult for the color blind.\nArt.\nInability to distinguish color does not necessarily preclude the ability to become a celebrated artist. The 20th century expressionist painter Clifton Pugh, three-time winner of Australia's Archibald Prize, on biographical, gene inheritance and other grounds has been identified as a protanope. 19th century French artist Charles M\u00e9ryon became successful by concentrating on etching rather than painting after he was diagnosed as having a red\u2013green deficiency. Jin Kim's red\u2013green color blindness did not stop him from becoming first an animator and later a character designer with Walt Disney Animation Studios.\nAdvantages.\nPeople with deuteranomaly are better at distinguishing shades of khaki, which may be advantageous when looking for predators, food, or camouflaged objects hidden among foliage. Dichromats tend to learn to use texture and shape clues and so may be able to penetrate camouflage that has been designed to deceive individuals with normal color vision.\nSome tentative evidence finds that color blind people are better at penetrating certain color camouflages. Such findings may give an evolutionary reason for the high rate of red\u2013green color blindness. There is also a study suggesting that people with some types of color blindness can distinguish colors that people with normal color vision are not able to distinguish. In World War II, color blind observers were used to penetrate camouflage.\nIn the presence of chromatic noise, the colorblind are more capable of seeing a luminous signal, as long as the chromatic noise appears metameric to them. This is the effect behind most \"reverse\" Pseudoisochromatic plates (e.g. \"hidden digit\" Ishihara plates) that are discernible to the colorblind, but unreadable to color normals.\nDigital design.\nColor codes are useful tools for designers to convey information. The interpretation of this information requires users to perform a variety of Color Tasks, usually comparative but also sometimes connotative or denotative. However, these tasks are often problematic for the colorblind when design of the color code has not followed best practices for accessibility. For example, one of the most ubiquitous connotative color codes is the \"red means bad and green means good\" or similar systems, based on the classic signal light colors. However, this color coding will almost always be undifferentiable to either Deutans or Protans and therefore should be avoided or supplemented with a parallel connotative system (symbols, smileys, etc.).\nGood practices to ensure design is accessible to the colorblind, include:\nUnordered Information.\nA common task for designers is to select a subset of colors (\"qualitative\" colormap) that are as mutually differentiable as possible (salient). For example, player pieces in a board game should be as different as possible.\nClassic advice suggests using Brewer palettes, but several of these are \"not\" actually colorblind-accessible. A recent, free, powerful tool that checks color contrast of a group of colors is Adobe's Color Blind Safe Tool.\nUnfortunately, the colors with the greatest contrast to the red\u2013green colorblind tend to be colors of confusion to the blue\u2013yellow colorblind, and vice versa. However, since red\u2013green is much more prevalent than blue\u2013yellow CVD, design should generally prioritize those users (Deutans, then Protans).\nOrdered Information.\nA common task for data visualization is to represent a color scale, or \"sequential\" colormap, often in the form of a heat map or choropleth. Several scales are designed with special consideration for the colorblind and are widespread in academia, including Cividis, Viridis and Parula (Matlab). These comprise a light-to-dark scale superimposed on a yellow-to-blue scale, making them monotonic and perceptually uniform to all forms of color vision.\nClassification.\nMuch terminology has existed and does exist for the classification of color blindness, but the typical classification for color blindness follows the von Kries classifications, which uses severity and affected cone for naming.\nBased on severity.\nBased on clinical appearance, color blindness may be described as total or partial. Total color blindness (monochromacy) is much less common than partial color blindness. Partial colorblindness includes dichromacy and anomalous trichromacy, but is often clinically defined as mild, moderate or strong.\nMonochromacy.\nMonochromacy is often called \"total color blindness\" since there is no ability to see color. Although the term may refer to acquired disorders such as cerebral achromatopsia, it typically refers to congenital color vision disorders, namely rod monochromacy and blue cone monochromacy).\nIn cerebral achromatopsia, a person cannot perceive colors even though the eyes are capable of distinguishing them. Some sources do not consider these to be true color blindness, because the failure is of perception, not of vision. They are forms of visual agnosia.\nMonochromacy is the condition of possessing only a single channel for conveying information about color. Monochromats are unable to distinguish any colors and perceive only variations in brightness. Congenital monochromacy occurs in two primary forms:\nDichromacy.\nDichromats can match any color they see with some mixture of just two primary colors (in contrast to those with normal sight (trichromats) who can distinguish three primary colors). Dichromats usually know they have a color vision problem, and it can affect their daily lives. Dichromacy in humans includes protanopia, deuteranopia, and tritanopia. Out of the male population, 2% have severe difficulties distinguishing between red, orange, yellow, and green. (Orange and yellow are different combinations of red and green light.) Colors in this range, which appear very different to a normal viewer, appear to a dichromat to be the same or a similar color. The terms protanopia, deuteranopia, and tritanopia come from Greek, and respectively mean \"inability to see (\"anopia\") with the first (\"prot-\"), second (\"deuter-\"), or third (\"trit-\") [cone]\".\nAnomalous trichromacy.\nAnomalous trichromacy is the mildest type of color deficiency, but the severity ranges from almost dichromacy (strong) to almost normal trichromacy (mild). In fact, many mild anomalous trichromats have very little difficulty carrying out tasks that require normal color vision and some may not even be aware that they have a color vision deficiency. The types of anomalous trichromacy include protanomaly, deuteranomaly and tritanomaly. It is approximately three times more common than dichromacy. Anomalous trichromats exhibit trichromacy, but the color matches they make differ from normal trichromats. In order to match a given spectral yellow light, protanomalous observers need more red light in a red/green mixture than a normal observer, and deuteranomalous observers need more green. This difference can be measured by an instrument called an Anomaloscope, where red and green lights are mixed by a subject to match a yellow light.\nBased on affected cone.\nThere are two major types of color blindness: difficulty distinguishing between red and green, and difficulty distinguishing between blue and yellow. These definitions are based on the phenotype of the partial colorblindness. Clinically, it is more common to use a genotypical definition, which describes which cone/opsin is affected.\nRed\u2013green color blindness.\nRed\u2013green color blindness includes protan and deutan CVD. Protan CVD is related to the L-cone and includes protanomaly (anomalous trichromacy) and protanopia (dichromacy). Deutan CVD is related to the M-cone and includes deuteranomaly (anomalous trichromacy) and deuteranopia (dichromacy). The phenotype (visual experience) of deutans and protans is quite similar. Common colors of confusion include red/brown/green/yellow as well as blue/purple. Both forms are almost always symptomatic of congenital red\u2013green color blindness, so affects males disproportionately more than females. This form of colorblindness is sometimes referred to as \"daltonism\" after John Dalton, who had red\u2013green dichromacy. In some languages, \"daltonism\" is still used to describe red\u2013green color blindness.\nBlue\u2013yellow color blindness.\nBlue\u2013yellow color blindness includes tritan CVD. Tritan CVD is related to the S-cone and includes tritanomaly (anomalous trichromacy) and tritanopia (dichromacy). Blue\u2013yellow color blindness is much less common than red\u2013green color blindness, and more often has acquired causes than genetic. Tritans have difficulty discerning between bluish and greenish hues. Tritans have a neutral point at 571\u00a0nm (yellowish).\nSummary of cone complements.\nThe below table shows the cone complements for different types of human color vision, including those considered color blindness, normal color vision and 'superior' color vision. The cone complement contains the types of cones (or their opsins) expressed by an individual.\nCauses.\nColor blindness is any deviation of color vision from normal trichromatic color vision (often as defined by the standard observer) that produces a reduced gamut. Mechanisms for color blindness are related to the functionality of cone cells, and often to the expression of photopsins, the photopigments that 'catch' photons and thereby convert light into chemical signals.\nColor vision deficiencies can be classified as inherited or acquired.\nGenetics.\nColor blindness is typically an inherited genetic disorder. The most common forms of colorblindness are associated with the Photopsin genes, but the mapping of the human genome has shown there are many causative mutations that don't directly affect the opsins. Mutations capable of causing color blindness originate from at least 19\u00a0different chromosomes and 56\u00a0different genes (as shown online at the Online Mendelian Inheritance in Man [OMIM]).\nGenetics of red\u2013green color blindness.\nBy far the most common form of colorblindness is congenital red\u2013green color blindness (Daltonism), which includes protanopia/protanomaly and deuteranopia/deuteranomaly. These conditions are mediated by the OPN1LW and OPN1MW genes, respectively, both on the X chromosome. An 'affected' gene is either missing (as in Protanopia and Deuteranopia - Dichromacy) or is a chimeric gene (as in Protanomaly and Deuteranomaly).\nSince the OPN1LW and OPN1MW genes are on the X\u00a0chromosome, they are sex-linked, and therefore affect males and females disproportionately. Because the colorblind 'affected' alleles are recessive, color blindness specifically follows X-linked recessive inheritance. Males have only one X\u00a0chromosome (XY), and females have two (XX); Because the male only has one of each gene, if it is affected, the male will be colorblind. Because a female has two alleles of each gene (one on each chromosome), if only one gene is affected, the dominant normal alleles will \"override\" the affected, recessive allele and the female will have normal color vision. However, if the female has two mutated alleles, she will still be colorblind. This is why there is a disproportionate prevalence of colorblindness, with ~8% of males exhibiting colorblindness and ~0.5% of females.\nGenetics of blue\u2013yellow color blindness.\nBlue\u2013yellow color blindness is a rarer form of colorblindness including tritanopia/tritanomaly. These conditions are mediated by the OPN1SW gene on Chromosome 7.\nOther genetic causes.\nSeveral inherited diseases are known to cause color blindness:\nThey can be congenital (from birth) or can commence in childhood or adulthood. They can be stationary, that is, remain the same throughout a person's lifetime, or progressive. As progressive phenotypes involve deterioration of the retina and other parts of the eye, many of the above forms of color blindness can progress to legal blindness, i.e. an acuity of 6/60 (20/200) or worse, and often leave a person with complete blindness.\nNon-genetic causes.\nPhysical trauma can cause color blindness, either neurologically \u2013 brain trauma which produces swelling of the brain in the occipital lobe \u2013 or retinally, either acute (e.g. from laser exposure) or chronic (e.g. from ultraviolet light exposure).\nColor blindness may also present itself as a symptom of degenerative diseases of the eye, such as cataract and age-related macular degeneration, and as part of the retinal damage caused by diabetes. Vitamin A deficiency may also cause color blindness.\nColor blindness may be a side effect of prescription drug use. For example, red\u2013green color blindness can be caused by ethambutol, a drug used in the treatment of tuberculosis. Blue\u2013yellow color blindness can be caused by sildenafil, an active component of Viagra. Hydroxychloroquine can also lead to hydroxychloroquine retinopathy, which includes various color defects. Exposure to chemicals such as styrene or organic solvents can also lead to color vision defects.\nSimple colored filters can also create mild color vision deficiencies. John Dalton's original hypothesis for his deuteranopia was actually that the vitreous humor of his eye was discolored:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;I was led to conjecture that one of the humours of my eye must be a transparent, but coloured, medium, so constituted as to absorb red and green rays principally... I suppose it must be the vitreous humor.\nAn autopsy of his eye after his death in 1844 showed this to be definitively untrue, though other filters are possible. Actual physiological examples usually affect the blue\u2013yellow opponent channel and are named Cyanopsia and Xanthopsia, and are most typically an effect of yellowing or removal of the lens.\nThe opponent channels can also be affected by the prevalence of certain cones in the retinal mosaic. The cones are not equally prevalent and not evenly distributed in the retina. When the number of one of these cone types is significantly reduced, this can also lead to or contribute to a color vision deficiency. This is one of the causes of tritanomaly.\nDiagnosis.\nColor vision test.\nThe main method for diagnosing a color vision deficiency is in testing the color vision directly. The Ishihara color test is the test most often used to detect red\u2013green deficiencies and most often recognized by the public. Some tests are clinical in nature, designed to be fast, simple, and effective at identifying broad categories of color blindness. Others focus on precision and are generally available only in academic settings.\nGenetic testing.\nWhile genetic testing cannot directly evaluate a subject's color vision (phenotype), most congenital color vision deficiencies are well-correlated with genotype. Therefore, the genotype can be directly evaluated and used to predict the phenotype. This is especially useful for progressive forms that do not have a strongly color deficient phenotype at a young age. However, it can also be used to sequence the L- and M-Opsins on the X-Chromosome, since the most common alleles of these two genes are known and have even been related to exact spectral sensitivities and peak wavelengths. A subject's color vision can therefore be classified through genetic testing, but this is just a prediction of the phenotype, since color vision can be affected by countless non-genetic factors such as your cone mosaic.\nManagement.\nDespite much recent improvement in gene therapy for color blindness, there is currently no FDA approved treatment for any form of CVD, and otherwise no cure for CVD currently exists. Management of the condition by using lenses to alleviate symptoms or smartphone apps to aid with daily tasks is possible.\nLenses.\nThere are three kinds of lenses that an individual can wear that can increase their accuracy in some color related tasks (although none of these will \"fix\" color blindness or grant the wearer normal color vision):\nAids.\nMany mobile and computer applications have been developed to aid color blind individuals in completing color tasks:\nIn 2003, a cybernetic device called eyeborg was developed to allow the wearer to hear sounds representing different colors. Achromatopsic artist Neil Harbisson was the first to use such a device in early 2004; the eyeborg allowed him to start painting in color by memorizing the sound corresponding to each color. In 2012, at a TED Conference, Harbisson explained how he could now perceive colors outside the ability of human vision.\nEpidemiology.\nColor blindness affects a large number of individuals, with protans and deutans being the most common types. In individuals with Northern European ancestry, as many as 8 percent of men and 0.4 percent of women experience congenital color deficiency. Interestingly, even Dalton's very first paper already arrived upon this 8% number:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...it is remarkable that, out of 25 pupils I once had, to whom I explained this subject, 2 were found to agree with me...\nHistory.\nDuring the 17th and 18th century, several philosophers hypothesized that not all individuals perceived colors in the same way:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...there is no reason to suppose a perfect resemblance in the disposition of the Optic Nerve in all Men, since there is an infinite variety in every thing in Nature, and chiefly in those that are Material, 'tis therefore very probable that all Men see not the same Colours in the same Objects.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the power of conceiving \"colors\", too, there are striking differences among individuals: and, indeed, I am inclined to suspect, that, in the greater number of instances, the supposed defects of sight in this respect ought to be ascribed rather to a defect in the power of conception.\nThe phenomenon only came to be scientifically studied in 1794, when English chemist John Dalton gave the first account of colour blindness in a paper to the Manchester Literary and Philosophical Society, which was published in 1798 as \"Extraordinary Facts relating to the Vision of Colours: With Observations\". Genetic analysis of Dalton's preserved eyeball confirmed him as having deuteranopia in 1995, some 150 years after his death.\nInfluenced by Dalton, German writer J. W. von Goethe studied color vision abnormalities in 1798 by asking two young subjects to match pairs of colors.\nIn 1875, the Lagerlunda train crash in Sweden brought color blindness to the forefront. Following the crash, Professor Alarik Frithiof Holmgren, a physiologist, investigated and concluded that the color blindness of the engineer (who had died) had caused the crash. Professor Holmgren then created the first test for color vision using multicolored skeins of wool to detect color blindness and thereby exclude the colorblind from jobs in the transportation industry requiring color vision to interpret safety signals. However, there is a claim that there is no firm evidence that color deficiency did cause the collision, or that it might have not been the sole cause.\nIn 1920, Frederick William Edridge-Green devised an alternative theory of color vision and color blindness based on Newton's classification of 7 fundamental colors (ROYGBIV). Edridge-Green classified color vision based on how many distinct colors a subject could see in the spectrum. Normal subjects were termed \"hexachromic\" as they could not discern Indigo. Subjects with superior color vision, who could discern indigo, where \"heptachromic\". The colorblind were therefore \"dichromic\" (equivalent to dichromacy) or \"tri-\", \"tetra-\" or \"pentachromic\" (anomalous trichromacy).\nRights.\nIn the United States, under federal anti-discrimination laws such as the Americans with Disabilities Act, color vision deficiencies have not been found to constitute a disability that triggers protection from workplace discrimination.\nA Brazilian court ruled that people with color blindness are protected by the Inter-American Convention on the Elimination of All Forms of Discrimination against Person with Disabilities. At trial, it was decided that the carriers of color blindness have a right of access to wider knowledge, or the full enjoyment of their human condition.\nOccupations.\nColor blindness may make it difficult or impossible for a person to engage in certain activities. Persons with color blindness may be legally or practically barred from occupations in which color perception is an essential part of the job (\"e.g.,\" mixing paint colors), or in which color perception is important for safety (\"e.g.,\" operating vehicles in response to color-coded signals). This occupational safety principle originates from the aftermath of the 1875 Lagerlunda train crash, which Alarik Frithiof Holmgren blamed on the color blindness of the engineer and created the first occupational screening test (Holmgren's wool test) against the colorblind.\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...I consider that to [Holmgren] above all others do we owe the present and future control of color-blindness on land and sea, by which life and property are safer, and the risks of travelling less.\nColor vision is important for occupations using telephone or computer networking cabling, as the individual wires inside the cables are color-coded using green, orange, brown, blue and white colors. Electronic wiring, transformers, resistors, and capacitors are color-coded as well, using black, brown, red, orange, yellow, green, blue, violet, gray, white, silver, gold.\nParticipation, officiating and viewing sporting events can be impacted by color blindness. Professional football players Thomas Delaney and Fabio Carvalho have discussed the difficulties when colour clashes occur, and research undertaken by FIFA has shown that enjoyment and player progression can be hampered by issues distinguishing the difference between the pitch and training objects or field markings. \nDriving.\nRed\u2013green colorblindness can make it difficult to drive, primarily due to the inability to differentiate red\u2013amber\u2013green traffic lights. Protans are further disadvantaged due to the darkened perception of reds, which can make it more difficult to quickly recognize brake lights. In response, some countries have refused to grant driver's licenses to individuals with color blindness:\nPiloting aircraft.\nAlthough many aspects of aviation depend on color coding, only a few of them are critical enough to be interfered with by some milder types of color blindness. Some examples include color-gun signaling of aircraft that have lost radio communication, color-coded glide-path indications on runways, and the like. Some jurisdictions restrict the issuance of pilot credentials to persons with color blindness for this reason. Restrictions may be partial, allowing color-blind persons to obtain certification but with restrictions, or total, in which case color-blind persons are not permitted to obtain piloting credentials at all.\nIn the United States, the Federal Aviation Administration requires that pilots be tested for normal color vision as part of their medical clearance in order to obtain the required medical certificate, a prerequisite to obtaining a pilot's certification. If testing reveals color blindness, the applicant may be issued a license with restrictions, such as no night flying and no flying by color signals\u2014such a restriction effectively prevents a pilot from holding certain flying occupations, such as that of an airline pilot, although commercial pilot certification is still possible, and there are a few flying occupations that do not require night flight and thus are still available to those with restrictions due to color blindness (e.g., agricultural aviation). The government allows several types of tests, including medical standard tests (\"e.g.,\" the Ishihara, Dvorine, and others) and specialized tests oriented specifically to the needs of aviation. If an applicant fails the standard tests, they will receive a restriction on their medical certificate that states: \"Not valid for night flying or by color signal control\". They may apply to the FAA to take a specialized test, administered by the FAA. Typically, this test is the \"color vision light gun test\". For this test an FAA inspector will meet the pilot at an airport with an operating control tower. The color signal light gun will be shone at the pilot from the tower, and they must identify the color. If they pass they may be issued a waiver, which states that the color vision test is no longer required during medical examinations. They will then receive a new medical certificate with the restriction removed. This was once a Statement of Demonstrated Ability (SODA), but the SODA was dropped, and converted to a simple waiver (letter) early in the 2000s.\nResearch published in 2009 carried out by the City University of London's Applied Vision Research Centre, sponsored by the UK's Civil Aviation Authority and the U.S. Federal Aviation Administration, has established a more accurate assessment of color deficiencies in pilot applicants' red/green and yellow\u2013blue color range which could lead to a 35% reduction in the number of prospective pilots who fail to meet the minimum medical threshold.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7398", "revid": "10612139", "url": "https://en.wikipedia.org/wiki?curid=7398", "title": "Computer security", "text": "Protection of computer systems from information disclosure, theft or damage\nComputer security, cyber security , digital security or information technology security (IT security) is the protection of computer systems and networks from attack by malicious actors that may result in unauthorized information disclosure, theft of, or damage to hardware, software, or data, as well as from the disruption or misdirection of the services they provide.\nThe field is significant due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi. Also, due to the growth of smart devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is one of the most significant challenges of the contemporary world, due to both the complexity of information systems and the societies they support. Security is of especially high importance for systems that govern large-scale systems with far-reaching physical effects, such as power distribution, elections, and finance.\nHistory.\nSince the Internet's arrival and with the digital transformation initiated in recent years, the notion of cybersecurity has become a familiar subject in both our professional and personal lives. Cybersecurity and cyber threats have been consistently present for the last 60 years of technological change. In the 1970s and 1980s, computer security was mainly limited to academia until the conception of the Internet, where, with increased connectivity, computer viruses and network intrusions began to take off. After the spread of viruses in the 1990s, the 2000s marked the institutionalization of cyber threats and cybersecurity.\nThe April 1967 session organized by Willis Ware at the Spring Joint Computer Conference, and the later publication of the Ware Report, were foundational moments in the history of the field of computer security. Ware's work straddled the intersection of material, cultural, political, and social concerns.\nA 1977 NIST publication introduced the \"CIA triad\" of confidentiality, integrity, and availability as a clear and simple way to describe key security goals. While still relevant, many more elaborate frameworks have since been proposed.\nHowever, in the 1970s and 1980s, there were no grave computer threats because computers and the internet were still developing, and security threats were easily identifiable. More often, threats came from malicious insiders who gained unauthorized access to sensitive documents and files. Although malware and network breaches existed during the early years, they did not use them for financial gain. By the second half of the 1970s, established computer firms like IBM started offering commercial access control systems and computer security software products.\nOne of the earliest examples of an attack on a computer network was the computer worm Creeper written by Bob Thomas at BBN, which propagated through the ARPANET in 1971. The program was purely experimental in nature and carried no malicious payload. A later program, Reaper, was created by Ray Tomlinson in 1972 and used to destroy Creeper.\nBetween September 1986 and June 1987, a group of German hackers performed the first documented case of cyber espionage. The group hacked into American defense contractors, universities, and military base networks and sold gathered information to the Soviet KGB. The group was led by Markus Hess, who was arrested on 29 June, 1987. He was convicted of espionage (along with two co-conspirators) on 15 Feb 1990.\nIn 1988, one of the first computer worms, called the Morris worm, was distributed via the Internet. It gained significant mainstream media attention.\nIn 1993, Netscape started developing the protocol SSL, shortly after the National Center for Supercomputing Applications (NCSA) launched Mosaic 1.0, the first web browser, in 1993. Netscape had SSL version 1.0 ready in 1994, but it was never released to the public due to many serious security vulnerabilities. These weaknesses included replay attacks and a vulnerability that allowed hackers to alter unencrypted communications sent by users. However, in February 1995, Netscape launched Version 2.0.\nThe National Security Agency (NSA) is responsible for the protection of U.S. information systems and also for collecting foreign intelligence.\nThe agency analyzes commonly used software in order to find security flaws, which it reserves for offensive purposes against competitors of the United States. The agency seldom takes defensive action by reporting the flaws to software producers so that they can eliminate them.\nNSA contractors created and sold \"click-and-shoot\" attack tools to US agencies and close allies, but eventually, the tools made their way to foreign adversaries. In 2016, NSAs own hacking tools were hacked, and they have been used by Russia and North Korea. NSA's employees and contractors have been recruited at high salaries by adversaries, anxious to compete in cyberwarfare. In 2007, the United States and Israel began exploiting security flaws in the Microsoft Windows operating system to attack and damage equipment used in Iran to refine nuclear materials. Iran responded by heavily investing in their own cyberwarfare capability, which it began using against the United States.\nVulnerabilities and attacks.\nA vulnerability is a weakness in design, implementation, operation, or internal control. Most of the vulnerabilities that have been discovered are documented in the Common Vulnerabilities and Exposures (CVE) database. An \"exploitable\" vulnerability is one for which at least one working attack or \"exploit\" exists. Vulnerabilities can be researched, reverse-engineered, hunted, or exploited using automated tools or customized scripts. To secure a computer system, it is important to understand the attacks that can be made against it, and these threats can typically be classified into one of these categories below:\nBackdoor.\nA backdoor in a computer system, a cryptosystem, or an algorithm, is any secret method of bypassing normal authentication or security controls. They may exist for many reasons, including original design or poor configuration. They may have been added by an authorized party to allow some legitimate access, or by an attacker for malicious reasons; but regardless of the motives for their existence, they create a vulnerability. Backdoors can be very hard to detect, and backdoors are usually discovered by someone who has access to application source code or intimate knowledge of the operating system of the computer.\nDenial-of-service attack.\nDenial of service attacks (DoS) are designed to make a machine or network resource unavailable to its intended users. Attackers can deny service to individual victims, such as by deliberately entering a wrong password enough consecutive times to cause the victim's account to be locked, or they may overload the capabilities of a machine or network and block all users at once. While a network attack from a single IP address can be blocked by adding a new firewall rule, many forms of Distributed denial of service (DDoS) attacks are possible, where the attack comes from a large number of points \u2013 and defending is much more difficult. Such attacks can originate from the zombie computers of a botnet or from a range of other possible techniques, including distributed reflective denial of service (DRDoS), where innocent systems are fooled into sending traffic to the victim. With such attacks, the amplification factor makes the attack easier for the attacker because they have to use little bandwidth themselves.\nDirect-access attacks.\nAn unauthorized user gaining physical access to a computer is most likely able to directly copy data from it. They may also compromise security by making operating system modifications, installing software worms, keyloggers, covert listening devices or using wireless microphones. Even when the system is protected by standard security measures, these may be bypassed by booting another operating system or tool from a CD-ROM or other bootable media. Disk encryption and Trusted Platform Module are designed to prevent these attacks.\nEavesdropping.\nEavesdropping is the act of surreptitiously listening to a private computer conversation (communication), typically between hosts on a network. For instance, programs such as Carnivore and NarusInSight have been used by the Federal Bureau of Investigation (FBI) and NSA to eavesdrop on the systems of internet service providers. Even machines that operate as a closed system (i.e., with no contact with the outside world) can be eavesdropped upon by monitoring the faint electromagnetic transmissions generated by the hardware. TEMPEST is a specification by the NSA referring to these attacks.\nMulti-vector, polymorphic attacks.\nSurfacing in 2017, a new class of multi-vector, polymorphic cyber threats combined several types of attacks and changed form to avoid cybersecurity controls as they spread.\nPhishing.\nPhishing is the attempt of acquiring sensitive information such as usernames, passwords, and credit card details directly from users by deceiving the users. Phishing is typically carried out by email spoofing or instant messaging, and it often directs users to enter details at a fake website whose look and feel are almost identical to the legitimate one. The fake website often asks for personal information, such as login details and passwords. This information can then be used to gain access to the individual's real account on the real website. Preying on a victim's trust, phishing can be classified as a form of social engineering. Attackers are using creative ways to gain access to real accounts. A common scam is for attackers to send fake electronic invoices to individuals showing that they recently purchased music, apps, or others, and instructing them to click on a link if the purchases were not authorized. A more strategic type of phishing is spear-phishing which leverages personal or organization-specific details to make the attacker appear like a trusted source. Spear-phishing attacks target specific individuals, rather than the broad net cast by phishing attempts.\nPrivilege escalation.\nPrivilege escalation describes a situation where an attacker with some level of restricted access is able to, without authorization, elevate their privileges or access level. For example, a standard computer user may be able to exploit a vulnerability in the system to gain access to restricted data; or even become \"root\" and have full unrestricted access to a system.\nReverse engineering.\nReverse engineering is the process by which a man-made object is deconstructed to reveal its designs, code, and architecture, or to extract knowledge from the object; similar to scientific research, the only difference being that scientific research is about a natural phenomenon.\nSide-channel attack.\nAny computational system affects its environment in some form. This effect it has on its environment includes a wide range of criteria, which can range from electromagnetic radiation to residual effect on RAM cells which as a consequence make a Cold boot attack possible, to hardware implementation faults that allow for access and or guessing of other values that normally should be inaccessible. In Side-channel attack scenarios, the attacker would gather such information about a system or network to guess its internal state and as a result access the information which is assumed by the victim to be secure.\nSocial engineering.\nSocial engineering, in the context of computer security, aims to convince a user to disclose secrets such as passwords, card numbers, etc. or grant physical access by, for example, impersonating a senior executive, bank, a contractor, or a customer. This generally involves exploiting peoples trust, and relying on their cognitive biases. A common scam involves emails sent to accounting and finance department personnel, impersonating their CEO and urgently requesting some action. In early 2016, the FBI reported that such business email compromise (BEC) scams had cost US businesses more than $2 billion in about two years.\nIn May 2016, the Milwaukee Bucks NBA team was the victim of this type of cyber scam with a perpetrator impersonating the team's president Peter Feigin, resulting in the handover of all the team's employees' 2015 W-2 tax forms.\nSpoofing.\nSpoofing is an act of masquerading as a valid entity through the falsification of data (such as an IP address or username), in order to gain access to information or resources that one is otherwise unauthorized to obtain. There are several types of spoofing, including:\nIn 2018, the cybersecurity firm Trellix published research on the life-threatening risk of spoofing in the healthcare industry.\nTampering.\nTampering describes a malicious modification or alteration of data. An intentional but unauthorized act resulting in the modification of a system, components of systems, its intended behavior, or data. So-called Evil Maid attacks and security services planting of surveillance capability into routers are examples.\nMalware.\nMalicious software (malware) installed on a computer can leak any information, such as personal information, business information and passwords, can give control of the system to the attacker, and can corrupt or delete data permanently.\nHTML smuggling.\nHTML files can carry payloads concealed as benign, inert data in order to defeat content filters. These payloads can be reconstructed on the other side of the filter.\nInformation security culture.\nEmployee behavior can have a big impact on information security in organizations. Cultural concepts can help different segments of the organization work effectively or work against effectiveness toward information security within an organization. Information security culture is the \"...totality of patterns of behavior in an organization that contributes to the protection of information of all kinds.\"\nAndersson and Reimers (2014) found that employees often do not see themselves as part of their organization's information security effort and often take actions that impede organizational changes. Indeed, the Verizon Data Breach Investigations Report 2020, which examined 3,950 security breaches, discovered 30% of cybersecurity incidents involved internal actors within a company. Research shows information security culture needs to be improved continuously. In \"Information Security Culture from Analysis to Change\", authors commented, \"It's a never-ending process, a cycle of evaluation and change or maintenance.\" To manage the information security culture, five steps should be taken: pre-evaluation, strategic planning, operative planning, implementation, and post-evaluation.\n# Commitment of the management\n# Communication with organizational members\n# Courses for all organizational members\n# Commitment of the employees\nSystems at risk.\nThe growth in the number of computer systems and the increasing reliance upon them by individuals, businesses, industries, and governments means that there are an increasing number of systems at risk.\nFinancial systems.\nThe computer systems of financial regulators and financial institutions like the U.S. Securities and Exchange Commission, SWIFT, investment banks, and commercial banks are prominent hacking targets for cybercriminals interested in manipulating markets and making illicit gains. Websites and apps that accept or store credit card numbers, brokerage accounts, and bank account information are also prominent hacking targets, because of the potential for immediate financial gain from transferring money, making purchases, or selling the information on the black market. In-store payment systems and ATMs have also been tampered with in order to gather customer account data and PINs.\nThe UCLA Internet Report: Surveying the Digital Future (2000) found that the privacy of personal data created barriers to online sales and that more than nine out of 10 internet users were somewhat or very concerned about credit card security.\nThe most common web technologies for improving security between browsers and websites are named SSL (Secure Sockets Layer), and its successor TLS (Transport Layer Security), identity management and authentication services, and domain name services allow companies and consumers to engage in secure communications and commerce. Several versions of SSL and TLS are commonly used today in applications such as web browsing, e-mail, internet faxing, instant messaging, and VoIP (voice-over-IP). There are various interoperable implementations of these technologies, including at least one implementation that is open source. Open source allows anyone to view the application's source code, and look for and report vulnerabilities.\nThe credit card companies Visa and MasterCard cooperated to develop the secure EMV chip which is embedded in credit cards. Further developments include the Chip Authentication Program where banks give customers hand-held card readers to perform online secure transactions. Other developments in this arena include the development of technology such as Instant Issuance which has enabled shopping mall kiosks acting on behalf of banks to issue on-the-spot credit cards to interested customers.\nUtilities and industrial equipment.\nComputers control functions at many utilities, including coordination of telecommunications, the power grid, nuclear power plants, and valve opening and closing in water and gas networks. The Internet is a potential attack vector for such machines if connected, but the Stuxnet worm demonstrated that even equipment controlled by computers not connected to the Internet can be vulnerable. In 2014, the Computer Emergency Readiness Team, a division of the Department of Homeland Security, investigated 79 hacking incidents at energy companies.\nAviation.\nThe aviation industry is very reliant on a series of complex systems which could be attacked. A simple power outage at one airport can cause repercussions worldwide, much of the system relies on radio transmissions which could be disrupted, and controlling aircraft over oceans is especially dangerous because radar surveillance only extends 175 to 225 miles offshore. There is also potential for attack from within an aircraft.\nIn Europe, with the (Pan-European Network Service) and NewPENS, and in the US with the NextGen program, air navigation service providers are moving to create their own dedicated networks.\nMany modern passports are now biometric passports, containing an embedded microchip that stores a digitized photograph and personal information such as name, gender, and date of birth. In addition, more countries are introducing facial recognition technology to reduce identity-related fraud. The introduction of the ePassport has assisted border officials in verifying the identity of the passport holder, thus allowing for quick passenger processing. Plans are under way in the US, the UK, and Australia to introduce SmartGate kiosks with both retina and fingerprint recognition technology. The airline industry is moving from the use of traditional paper tickets towards the use of electronic tickets (e-tickets). These have been made possible by advances in online credit card transactions in partnership with the airlines. Long-distance bus companies are also switching over to e-ticketing transactions today.\nThe consequences of a successful attack range from loss of confidentiality to loss of system integrity, air traffic control outages, loss of aircraft, and even loss of life.\nConsumer devices.\nDesktop computers and laptops are commonly targeted to gather passwords or financial account information or to construct a botnet to attack another target. Smartphones, tablet computers, smart watches, and other mobile devices such as quantified self devices like activity trackers have sensors such as cameras, microphones, GPS receivers, compasses, and accelerometers which could be exploited, and may collect personal information, including sensitive health information. WiFi, Bluetooth, and cell phone networks on any of these devices could be used as attack vectors, and sensors might be remotely activated after a successful breach.\nThe increasing number of home automation devices such as the Nest thermostat are also potential targets.\nHealthcare.\nToday many health-care providers and health insurance companies use the internet to provide enhanced products and services, for example through use of tele-health to potentially offer better quality and access to healthcare, or fitness trackers to lower insurance premiums.\nThe health care company Humana partners with WebMD, Oracle Corporation, EDS and Microsoft to enable its members to access their health care records, as well as to provide an overview of health care plans. Patient records are increasingly being placed on secure in-house networks, alleviating the need for extra storage space.\nLarge corporations.\nLarge corporations are common targets. In many cases attacks are aimed at financial gain through identity theft and involve data breaches. Examples include the loss of millions of clients' credit card and financial details by Home Depot, Staples, Target Corporation, and Equifax.\nMedical records have been targeted in general identify theft, health insurance fraud, and impersonating patients to obtain prescription drugs for recreational purposes or resale. Although cyber threats continue to increase, 62% of all organizations did not increase security training for their business in 2015.\nNot all attacks are financially motivated, however: security firm HBGary Federal had a serious series of attacks in 2011 from hacktivist group Anonymous in retaliation for the firm's CEO claiming to have infiltrated their group, and Sony Pictures was hacked in 2014 with the apparent dual motive of embarrassing the company through data leaks and crippling the company by wiping workstations and servers.\nAutomobiles.\nVehicles are increasingly computerized, with engine timing, cruise control, anti-lock brakes, seat belt tensioners, door locks, airbags and advanced driver-assistance systems on many models. Additionally, connected cars may use WiFi and Bluetooth to communicate with onboard consumer devices and the cell phone network. Self-driving cars are expected to be even more complex. All of these systems carry some security risk, and such issues have gained wide attention.\nSimple examples of risk include a malicious compact disc being used as an attack vector, and the car's onboard microphones being used for eavesdropping. However, if access is gained to a car's internal controller area network, the danger is much greater \u2013 and in a widely publicized 2015 test, hackers remotely carjacked a vehicle from 10 miles away and drove it into a ditch.\nManufacturers are reacting in numerous ways, with Tesla in 2016 pushing out some security fixes \"over the air\" into its cars' computer systems. In the area of autonomous vehicles, in September 2016 the United States Department of Transportation announced some initial safety standards, and called for states to come up with uniform policies.\nAdditionally, e-Drivers\u2019 licenses are being developed using the same technology. For example, Mexico\u2019s licensing authority (ICV) has used a smart card platform to issue the first e-Drivers\u2019 licenses to the city of Monterrey, in the state of Nuevo Le\u00f3n.\nShipping.\nShipping companies have adopted RFID (Radio Frequency Identification) technology as an efficient, digitally secure, tracking device. Unlike a barcode, RFID can be read up to 20 feet away. RFID is used by FedEx and UPS.\nGovernment.\nGovernment and military computer systems are commonly attacked by activists and foreign powers. Local and regional government infrastructure such as traffic light controls, police and intelligence agency communications, personnel records, as well as student records.\nThe FBI, CIA, and Pentagon, all utilize secure controlled access technology for any of their buildings. However, the use of this form of technology is spreading into the entrepreneurial world. More and more companies are taking advantage of the development of digitally secure controlled access technology. GE's ACUVision, for example, offers a single panel platform for access control, alarm monitoring and digital recording.\nInternet of things and physical vulnerabilities.\nThe Internet of things (IoT) is the network of physical objects such as devices, vehicles, and buildings that are embedded with electronics, software, sensors, and network connectivity that enables them to collect and exchange data. Concerns have been raised that this is being developed without appropriate consideration of the security challenges involved.\nWhile the IoT creates opportunities for more direct integration of the physical world into computer-based systems,\nit also provides opportunities for misuse. In particular, as the Internet of Things spreads widely, cyberattacks are likely to become an increasingly physical (rather than simply virtual) threat. If a front door's lock is connected to the Internet, and can be locked/unlocked from a phone, then a criminal could enter the home at the press of a button from a stolen or hacked phone. People could stand to lose much more than their credit card numbers in a world controlled by IoT-enabled devices. Thieves have also used electronic means to circumvent non-Internet-connected hotel door locks.\nAn attack that targets physical infrastructure and/or human lives is sometimes referred to as a cyber-kinetic attack. As IoT devices and appliances gain currency, cyber-kinetic attacks can become pervasive and significantly damaging.\nMedical systems.\nMedical devices have either been successfully attacked or had potentially deadly vulnerabilities demonstrated, including both in-hospital diagnostic equipment and implanted devices including pacemakers and insulin pumps. There are many reports of hospitals and hospital organizations getting hacked, including ransomware attacks, Windows XP exploits, viruses, and data breaches of sensitive data stored on hospital servers. On 28 December 2016 the US Food and Drug Administration released its recommendations for how medical device manufacturers should maintain the security of Internet-connected devices \u2013 but no structure for enforcement.\nEnergy sector.\nIn distributed generation systems, the risk of a cyber attack is real, according to \"Daily Energy Insider\". An attack could cause a loss of power in a large area for a long period of time, and such an attack could have just as severe consequences as a natural disaster. The District of Columbia is considering creating a Distributed Energy Resources (DER) Authority within the city, with the goal being for customers to have more insight into their own energy use and giving the local electric utility, Pepco, the chance to better estimate energy demand. The D.C. proposal, however, would \"allow third-party vendors to create numerous points of energy distribution, which could potentially create more opportunities for cyber attackers to threaten the electric grid.\"\nTelecommunications.\nPerhaps the most widely known digitally secure telecommunication device is the SIM (Subscriber Identity Module) card, a device that is embedded in most of the world\u2019s cellular devices before any service can be obtained. The SIM card is just the beginning of this digitally secure environment.\nThe Smart Card Web Servers draft standard (SCWS) defines the interfaces to an HTTP server in a smart card. Tests are being conducted to secure OTA (\"over-the-air\") payment and credit card information from and to a mobile phone. \nCombination SIM/DVD devices are being developed through Smart Video Card technology which embeds a DVD-compliant optical disc into the card body of a regular SIM card.\nOther telecommunication developments involving digital security include mobile signatures, which use the embedded SIM card to generate a legally binding electronic signature.\nImpact of security breaches.\nSerious financial damage has been caused by security breaches, but because there is no standard model for estimating the cost of an incident, the only data available is that which is made public by the organizations involved. \"Several computer security consulting firms produce estimates of total worldwide losses attributable to virus and worm attacks and to hostile digital acts in general. The 2003 loss estimates by these firms range from $13 billion (worms and viruses only) to $226 billion (for all forms of covert attacks). The reliability of these estimates is often challenged; the underlying methodology is basically anecdotal.\"\nHowever, reasonable estimates of the financial cost of security breaches can actually help organizations make rational investment decisions. According to the classic Gordon-Loeb Model analyzing the optimal investment level in information security, one can conclude that the amount a firm spends to protect information should generally be only a small fraction of the expected loss (i.e., the expected value of the loss resulting from a cyber/information security breach).\nAttacker motivation.\nAs with physical security, the motivations for breaches of computer security vary between attackers. Some are thrill-seekers or vandals, some are activists, others are criminals looking for financial gain. State-sponsored attackers are now common and well resourced but started with amateurs such as Markus Hess who hacked for the KGB, as recounted by Clifford Stoll in \"The Cuckoo's Egg\".\nAdditionally, recent attacker motivations can be traced back to extremist organizations seeking to gain political advantage or disrupt social agendas. The growth of the internet, mobile technologies, and inexpensive computing devices have led to a rise in capabilities but also to the risk to environments that are deemed as vital to operations. All critical targeted environments are susceptible to compromise and this has led to a series of proactive studies on how to migrate the risk by taking into consideration motivations by these types of actors. Several stark differences exist between the hacker motivation and that of nation state actors seeking to attack based on an ideological preference.\nA standard part of threat modeling for any particular system is to identify what might motivate an attack on that system, and who might be motivated to breach it. The level and detail of precautions will vary depending on the system to be secured. A home personal computer, bank, and classified military network face very different threats, even when the underlying technologies in use are similar.\nComputer protection (countermeasures).\nIn computer security, a countermeasure is an action, device, procedure or technique that reduces a threat, a vulnerability, or an attack by eliminating or preventing it, by minimizing the harm it can cause, or by discovering and reporting it so that corrective action can be taken.\nSome common countermeasures are listed in the following sections:\nSecurity by design.\nSecurity by design, or alternately secure by design, means that the software has been designed from the ground up to be secure. In this case, security is considered as a main feature.\nSome of the techniques in this approach include:\nSecurity architecture.\nThe Open Security Architecture organization defines IT security architecture as \"the design artifacts that describe how the security controls (security countermeasures) are positioned, and how they relate to the overall information technology architecture. These controls serve the purpose to maintain the system's quality attributes: confidentiality, integrity, availability, accountability and assurance services\".\nTechopedia defines security architecture as \"a unified security design that addresses the necessities and potential risks involved in a certain scenario or environment. It also specifies when and where to apply security controls. The design process is generally reproducible.\" The key attributes of security architecture are:\nPracticing security architecture provides the right foundation to systematically address business, IT and security concerns in an organization.\nSecurity measures.\nA state of computer security is the conceptual ideal, attained by the use of the three processes: threat prevention, detection, and response. These processes are based on various policies and system components, which include the following:\nToday, computer security consists mainly of preventive measures, like firewalls or an exit procedure. A firewall can be defined as a way of filtering network data between a host or a network and another network, such as the Internet, and can be implemented as software running on the machine, hooking into the network stack (or, in the case of most UNIX-based operating systems such as Linux, built into the operating system kernel) to provide real-time filtering and blocking. Another implementation is a so-called \"physical firewall\", which consists of a separate machine filtering network traffic. Firewalls are common amongst machines that are permanently connected to the Internet.\nSome organizations are turning to big data platforms, such as Apache Hadoop, to extend data accessibility and machine learning to detect advanced persistent threats.\nHowever, relatively few organizations maintain computer systems with effective detection systems, and fewer still have organized response mechanisms in place. As a result, as Reuters pointed out in 2010: \"Companies for the first time report they are losing more through electronic theft of data than physical stealing of assets\". The primary obstacle to effective eradication of cybercrime could be traced to excessive reliance on firewalls and other automated detection systems. Yet it is basic evidence gathering by using packet capture appliances that puts criminals behind bars.\nIn order to ensure adequate security, the confidentiality, integrity and availability of a network, better known as the CIA triad, must be protected and is considered the foundation to information security. To achieve those objectives, administrative, physical and technical security measures should be employed. The amount of security afforded to an asset can only be determined when its value is known.\nVulnerability management.\nVulnerability management is the cycle of identifying, remediating or mitigating vulnerabilities, especially in software and firmware. Vulnerability management is integral to computer security and network security.\nVulnerabilities can be discovered with a vulnerability scanner, which analyzes a computer system in search of known vulnerabilities, such as open ports, insecure software configuration, and susceptibility to malware. In order for these tools to be effective, they must be kept up to date with every new update the vendor release. Typically, these updates will scan for the new vulnerabilities that were introduced recently.\nBeyond vulnerability scanning, many organizations contract outside security auditors to run regular penetration tests against their systems to identify vulnerabilities. In some sectors, this is a contractual requirement.\nReducing vulnerabilities.\nWhile formal verification of the correctness of computer systems is possible, it is not yet common. Operating systems formally verified include seL4, and SYSGO's PikeOS \u2013 but these make up a very small percentage of the market.\nTwo factor authentication is a method for mitigating unauthorized access to a system or sensitive information. It requires \"something you know\"; a password or PIN, and \"something you have\"; a card, dongle, cellphone, or another piece of hardware. This increases security as an unauthorized person needs both of these to gain access.\nSocial engineering and direct computer access (physical) attacks can only be prevented by non-computer means, which can be difficult to enforce, relative to the sensitivity of the information. Training is often involved to help mitigate this risk, but even in highly disciplined environments (e.g. military organizations), social engineering attacks can still be difficult to foresee and prevent.\nInoculation, derived from inoculation theory, seeks to prevent social engineering and other fraudulent tricks or traps by instilling a resistance to persuasion attempts through exposure to similar or related attempts.\nIt is possible to reduce an attacker's chances by keeping systems up to date with security patches and updates, using a security scanner and/or hiring people with expertise in security, though none of these guarantee the prevention of an attack. The effects of data loss/damage can be reduced by careful backing up and insurance.\nHardware protection mechanisms.\nWhile hardware may be a source of insecurity, such as with microchip vulnerabilities maliciously introduced during the manufacturing process, hardware-based or assisted computer security also offers an alternative to software-only computer security. Using devices and methods such as dongles, trusted platform modules, intrusion-aware cases, drive locks, disabling USB ports, and mobile-enabled access may be considered more secure due to the physical access (or sophisticated backdoor access) required in order to be compromised. Each of these is covered in more detail below.\nSecure operating systems.\nOne use of the term \"computer security\" refers to technology that is used to implement secure operating systems. In the 1980s, the United States Department of Defense (DoD) used the \"Orange Book\" standards, but the current international standard ISO/IEC 15408, Common Criteria defines a number of progressively more stringent Evaluation Assurance Levels. Many common operating systems meet the EAL4 standard of being \"Methodically Designed, Tested and Reviewed\", but the formal verification required for the highest levels means that they are uncommon. An example of an EAL6 (\"Semiformally Verified Design and Tested\") system is INTEGRITY-178B, which is used in the Airbus A380\nand several military jets.\nSecure coding.\nIn software engineering, secure coding aims to guard against the accidental introduction of security vulnerabilities. It is also possible to create software designed from the ground up to be secure. Such systems are \"secure by design\". Beyond this, formal verification aims to prove the correctness of the algorithms underlying a system;\nimportant for cryptographic protocols for example.\nCapabilities and access control lists.\nWithin computer systems, two of the main security models capable of enforcing privilege separation are access control lists (ACLs) and role-based access control (RBAC).\nAn access-control list (ACL), with respect to a computer file system, is a list of permissions associated with an object. An ACL specifies which users or system processes are granted access to objects, as well as what operations are allowed on given objects.\nRole-based access control is an approach to restricting system access to authorized users, used by the majority of enterprises with more than 500 employees, and can implement mandatory access control (MAC) or discretionary access control (DAC).\nA further approach, capability-based security has been mostly restricted to research operating systems. Capabilities can, however, also be implemented at the language level, leading to a style of programming that is essentially a refinement of standard object-oriented design. An open-source project in the area is the E language.\nEnd user security training.\nThe end-user is widely recognized as the weakest link in the security chain and it is estimated that more than 90% of security incidents and breaches involve some kind of human error. Among the most commonly recorded forms of errors and misjudgment are poor password management, sending emails containing sensitive data and attachments to the wrong recipient, the inability to recognize misleading URLs and to identify fake websites and dangerous email attachments. A common mistake that users make is saving their user id/password in their browsers to make it easier to log in to banking sites. This is a gift to attackers who have obtained access to a machine by some means. The risk may be mitigated by the use of two-factor authentication.\nAs the human component of cyber risk is particularly relevant in determining the global cyber risk an organization is facing, security awareness training, at all levels, not only provides formal compliance with regulatory and industry mandates but is considered essential in reducing cyber risk and protecting individuals and companies from the great majority of cyber threats.\nThe focus on the end-user represents a profound cultural change for many security practitioners, who have traditionally approached cybersecurity exclusively from a technical perspective, and moves along the lines suggested by major security centers to develop a culture of cyber awareness within the organization, recognizing that a security-aware user provides an important line of defense against cyber attacks.\nDigital hygiene.\nRelated to end-user training, digital hygiene or cyber hygiene is a fundamental principle relating to information security and, as the analogy with personal hygiene shows, is the equivalent of establishing simple routine measures to minimize the risks from cyber threats. The assumption is that good cyber hygiene practices can give networked users another layer of protection, reducing the risk that one vulnerable node will be used to either mount attacks or compromise another node or network, especially from common cyberattacks. Cyber hygiene should also not be mistaken for proactive cyber defence, a military term.\nAs opposed to a purely technology-based defense against threats, cyber hygiene mostly regards routine measures that are technically simple to implement and mostly dependent on discipline or education. It can be thought of as an abstract list of tips or measures that have been demonstrated as having a positive effect on personal and/or collective digital security. As such, these measures can be performed by laypeople, not just security experts.\nCyber hygiene relates to personal hygiene as computer viruses relate to biological viruses (or pathogens). However, while the term \"computer virus\" was coined almost simultaneously with the creation of the first working computer viruses, the term \"cyber hygiene\" is a much later invention, perhaps as late as 2000 by Internet pioneer Vint Cerf. It has since been adopted by the Congress and Senate of the United States, the FBI, EU institutions and heads of state.\nResponse to breaches.\nResponding to attempted security breaches is often very difficult for a variety of reasons, including:\nWhere an attack succeeds and a breach occurs, many jurisdictions now have in place mandatory security breach notification laws.\nTypes of security and privacy.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nComputer security incident management.\nComputer security incident management is an organized approach to addressing and managing the aftermath of a computer security incident or compromise with the goal of preventing a breach or thwarting a cyberattack. An incident that is not identified and managed at the time of intrusion typically escalates to a more damaging event such as a data breach or system failure. The intended outcome of a computer security incident response plan is to contain the incident, limit damage and assist recovery to business as usual. Responding to compromises quickly can mitigate exploited vulnerabilities, restore services and processes and minimize losses.\nIncident response planning allows an organization to establish a series of best practices to stop an intrusion before it causes damage. Typical incident response plans contain a set of written instructions that outline the organization's response to a cyberattack. Without a documented plan in place, an organization may not successfully detect an intrusion or compromise and stakeholders may not understand their roles, processes and procedures during an escalation, slowing the organization's response and resolution.\nThere are four key components of a computer security incident response plan:\nNotable attacks and breaches.\nSome illustrative examples of different types of computer security breaches are given below.\nRobert Morris and the first computer worm.\nIn 1988, 60,000 computers were connected to the Internet, and most were mainframes, minicomputers and professional workstations. On 2 November 1988, many started to slow down, because they were running a malicious code that demanded processor time and that spread itself to other computers \u2013 the first internet computer worm. The software was traced back to 23-year-old Cornell University graduate student Robert Tappan Morris who said \"he wanted to count how many machines were connected to the Internet\".\nRome Laboratory.\nIn 1994, over a hundred intrusions were made by unidentified crackers into the Rome Laboratory, the US Air Force's main command and research facility. Using trojan horses, hackers were able to obtain unrestricted access to Rome's networking systems and remove traces of their activities. The intruders were able to obtain classified files, such as air tasking order systems data and furthermore able to penetrate connected networks of National Aeronautics and Space Administration's Goddard Space Flight Center, Wright-Patterson Air Force Base, some Defense contractors, and other private sector organizations, by posing as a trusted Rome center user.\nTJX customer credit card details.\nIn early 2007, American apparel and home goods company TJX announced that it was the victim of an unauthorized computer systems intrusion and that the hackers had accessed a system that stored data on credit card, debit card, check, and merchandise return transactions.\nStuxnet attack.\nIn 2010, the computer worm known as Stuxnet reportedly ruined almost one-fifth of Iran's nuclear centrifuges. It did so by disrupting industrial programmable logic controllers (PLCs) in a targeted attack. This is generally believed to have been launched by Israel and the United States to disrupt Iran's nuclear program \u2013 although neither has publicly admitted this.\nGlobal surveillance disclosures.\nIn early 2013, documents provided by Edward Snowden were published by \"The Washington Post\" and \"The Guardian\" exposing the massive scale of NSA global surveillance. There were also indications that the NSA may have inserted a backdoor in a NIST standard for encryption. This standard was later withdrawn due to widespread criticism. The NSA additionally were revealed to have tapped the links between Google's data centers.\nTarget and Home Depot breaches.\nA Ukrainian hacker known as Rescator broke into Target Corporation computers in 2013, stealing roughly 40 million credit cards, and then Home Depot computers in 2014, stealing between 53 and 56 million credit card numbers. Warnings were delivered at both corporations, but ignored; physical security breaches using self checkout machines are believed to have played a large role. \"The malware utilized is absolutely unsophisticated and uninteresting,\" says Jim Walter, director of threat intelligence operations at security technology company McAfee \u2013 meaning that the heists could have easily been stopped by existing antivirus software had administrators responded to the warnings. The size of the thefts has resulted in major attention from state and Federal United States authorities and the investigation is ongoing.\nOffice of Personnel Management data breach.\nIn April 2015, the Office of Personnel Management discovered it had been hacked more than a year earlier in a data breach, resulting in the theft of approximately 21.5\u00a0million personnel records handled by the office. The Office of Personnel Management hack has been described by federal officials as among the largest breaches of government data in the history of the United States. Data targeted in the breach included personally identifiable information such as Social Security numbers, names, dates and places of birth, addresses, and fingerprints of current and former government employees as well as anyone who had undergone a government background check. It is believed the hack was perpetrated by Chinese hackers.\nAshley Madison breach.\nIn July 2015, a hacker group is known as The Impact Team successfully breached the extramarital relationship website Ashley Madison, created by Avid Life Media. The group claimed that they had taken not only company data but user data as well. After the breach, The Impact Team dumped emails from the company's CEO, to prove their point, and threatened to dump customer data unless the website was taken down permanently. When Avid Life Media did not take the site offline the group released two more compressed files, one 9.7GB and the second 20GB. After the second data dump, Avid Life Media CEO Noel Biderman resigned; but the website remained to function.\nColonial Pipeline ransomware attack.\nIn June 2021, the cyber attack took down the largest fuel pipeline in the U.S. and led to shortages across the East Coast.\nLegal issues and global regulation.\nInternational legal issues of cyber attacks are complicated in nature. There is no global base of common rules to judge, and eventually punish, cybercrimes and cybercriminals - and where security firms or agencies do locate the cybercriminal behind the creation of a particular piece of malware or form of cyber attack, often the local authorities cannot take action due to lack of laws under which to prosecute. Proving attribution for cybercrimes and cyberattacks is also a major problem for all law enforcement agencies. \"Computer viruses switch from one country to another, from one jurisdiction to another \u2013 moving around the world, using the fact that we don't have the capability to globally police operations like this. So the Internet is as if someone [had] given free plane tickets to all the online criminals of the world.\" The use of techniques such as dynamic DNS, fast flux and bullet proof servers add to the difficulty of investigation and enforcement.\nRole of government.\nThe role of the government is to make regulations to force companies and organizations to protect their systems, infrastructure and information from any cyberattacks, but also to protect its own national infrastructure such as the national power-grid.\nThe government's regulatory role in cyberspace is complicated. For some, cyberspace was seen as a virtual space that was to remain free of government intervention, as can be seen in many of today's libertarian blockchain and bitcoin discussions.\nMany government officials and experts think that the government should do more and that there is a crucial need for improved regulation, mainly due to the failure of the private sector to solve efficiently the cybersecurity problem. R. Clarke said during a panel discussion at the RSA Security Conference in San Francisco, he believes that the \"industry only responds when you threaten regulation. If the industry doesn't respond (to the threat), you have to follow through.\" On the other hand, executives from the private sector agree that improvements are necessary, but think that government intervention would affect their ability to innovate efficiently. Daniel R. McCarthy analyzed this public-private partnership in cybersecurity and reflected on the role of cybersecurity in the broader constitution of political order.\nOn 22 May 2020, the UN Security Council held its second ever informal meeting on cybersecurity to focus on cyber challenges to international peace. According to UN Secretary-General Ant\u00f3nio Guterres, new technologies are too often used to violate rights.\nInternational actions.\nMany different teams and organizations exist, including:\nEurope.\nOn 14 April 2016, the European Parliament and Council of the European Union adopted The General Data Protection Regulation (GDPR) (EU) 2016/679. GDPR, which became enforceable beginning 25 May 2018, provides for data protection and privacy for all individuals within the European Union (EU) and the European Economic Area (EEA). GDPR requires that business processes that handle personal data be built with data protection by design and by default. GDPR also requires that certain organizations appoint a Data Protection Officer (DPO).\nNational actions.\nComputer emergency response teams.\nMost countries have their own computer emergency response team to protect network security.\nCanada.\nSince 2010, Canada has had a cybersecurity strategy. This functions as a counterpart document to the National Strategy and Action Plan for Critical Infrastructure. The strategy has three main pillars: securing government systems, securing vital private cyber systems, and helping Canadians to be secure online. There is also a Cyber Incident Management Framework to provide a coordinated response in the event of a cyber incident.\nThe Canadian Cyber Incident Response Centre (CCIRC) is responsible for mitigating and responding to threats to Canada's critical infrastructure and cyber systems. It provides support to mitigate cyber threats, technical support to respond &amp; recover from targeted cyber attacks, and provides online tools for members of Canada's critical infrastructure sectors. It posts regular cybersecurity bulletins &amp; operates an online reporting tool where individuals and organizations can report a cyber incident.\nTo inform the general public on how to protect themselves online, Public Safety Canada has partnered with STOP.THINK.CONNECT, a coalition of non-profit, private sector, and government organizations, and launched the Cyber Security Cooperation Program. They also run the GetCyberSafe portal for Canadian citizens, and Cyber Security Awareness Month during October.\nPublic Safety Canada aims to begin an evaluation of Canada's cybersecurity strategy in early 2015.\nChina.\nChina's Central Leading Group for Internet Security and Informatization () was established on 27 February 2014. This Leading Small Group (LSG) of the Chinese Communist Party is headed by General Secretary Xi Jinping himself and is staffed with relevant Party and state decision-makers. The LSG was created to overcome the incoherent policies and overlapping responsibilities that characterized China's former cyberspace decision-making mechanisms. The LSG oversees policy-making in the economic, political, cultural, social and military fields as they relate to network security and IT strategy. This LSG also coordinates major policy initiatives in the international arena that promote norms and standards favored by the Chinese government and that emphasizes the principle of national sovereignty in cyberspace.\nGermany.\nBerlin starts National Cyber Defense Initiative: On 16 June 2011, the German Minister for Home Affairs, officially opened the new German NCAZ (National Center for Cyber Defense) Nationales Cyber-Abwehrzentrum located in Bonn. The NCAZ closely cooperates with BSI (Federal Office for Information Security) Bundesamt f\u00fcr Sicherheit in der Informationstechnik, BKA (Federal Police Organisation) Bundeskriminalamt (Deutschland), BND (Federal Intelligence Service) Bundesnachrichtendienst, MAD (Military Intelligence Service) Amt f\u00fcr den Milit\u00e4rischen Abschirmdienst and other national organizations in Germany taking care of national security aspects. According to the Minister, the primary task of the new organization founded on 23 February 2011, is to detect and prevent attacks against the national infrastructure and mentioned incidents like Stuxnet. Germany has also established the largest research institution for IT security in Europe, the Center for Research in Security and Privacy (CRISP) in Darmstadt.\nIndia.\nSome provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000.\nThe National Cyber Security Policy 2013 is a policy framework by the Ministry of Electronics and Information Technology (MeitY) which aims to protect the public and private infrastructure from cyberattacks, and safeguard \"information, such as personal information (of web users), financial and banking information and sovereign data\". CERT- In is the nodal agency which monitors the cyber threats in the country. The post of National Cyber Security Coordinator has also been created in the Prime Minister's Office (PMO).\nThe Indian Companies Act 2013 has also introduced cyber law and cybersecurity obligations on the part of Indian directors. Some provisions for cybersecurity have been incorporated into rules framed under the Information Technology Act 2000 Update in 2013.\nSouth Korea.\nFollowing cyberattacks in the first half of 2013, when the government, news media, television stations, and bank websites were compromised, the national government committed to the training of 5,000 new cybersecurity experts by 2017. The South Korean government blamed its northern counterpart for these attacks, as well as incidents that occurred in 2009, 2011, and 2012, but Pyongyang denies the accusations.\nUnited States.\nLegislation.\nThe 1986 \u00a0\u00a7\u00a01030, the Computer Fraud and Abuse Act is the key legislation. It prohibits unauthorized access or damage of \"protected computers\" as defined in \u00a0\u00a7\u00a01030(e)(2). Although various other measures have been proposed \u2013 none has succeeded.\nIn 2013, executive order \"Improving Critical Infrastructure Cybersecurity\" was signed, which prompted the creation of the NIST Cybersecurity Framework.\nIn response to the Colonial Pipeline ransomware attack President Joe Biden signed Executive Order 14028 on May 12, 2021, to increase software security standards for sales to the government, tighten detection and security on existing systems, improve information sharing and training, establish a Cyber Safety Review Board, and improve incident response.\nStandardized government testing services.\nThe General Services Administration (GSA) has standardized the \"penetration test\" service as a pre-vetted support service, to rapidly address potential vulnerabilities, and stop adversaries before they impact US federal, state and local governments. These services are commonly referred to as Highly Adaptive Cybersecurity Services (HACS).\nAgencies.\nThe Department of Homeland Security has a dedicated division responsible for the response system, risk management program and requirements for cybersecurity in the United States called the National Cyber Security Division. The division is home to US-CERT operations and the National Cyber Alert System. The National Cybersecurity and Communications Integration Center brings together government organizations responsible for protecting computer networks and networked infrastructure.\nThe third priority of the FBI is to: \"Protect the United States against cyber-based attacks and high-technology crimes\", and they, along with the National White Collar Crime Center (NW3C), and the Bureau of Justice Assistance (BJA) are part of the multi-agency task force, The Internet Crime Complaint Center, also known as IC3.\nIn addition to its own specific duties, the FBI participates alongside non-profit organizations such as InfraGard.\nThe Computer Crime and Intellectual Property Section (CCIPS) operates in the United States Department of Justice Criminal Division. The CCIPS is in charge of investigating computer crime and intellectual property crime and is specialized in the search and seizure of digital evidence in computers and networks. In 2017, CCIPS published A Framework for a Vulnerability Disclosure Program for Online Systems to help organizations \"clearly describe authorized vulnerability disclosure and discovery conduct, thereby substantially reducing the likelihood that such described activities will result in a civil or criminal violation of law under the Computer Fraud and Abuse Act (18 U.S.C. \u00a7 1030).\"\nThe United States Cyber Command, also known as USCYBERCOM, \"has the mission to direct, synchronize, and coordinate cyberspace planning and operations to defend and advance national interests in collaboration with domestic and international partners.\" It has no role in the protection of civilian networks.\nThe U.S. Federal Communications Commission's role in cybersecurity is to strengthen the protection of critical communications infrastructure, to assist in maintaining the reliability of networks during disasters, to aid in swift recovery after, and to ensure that first responders have access to effective communications services.\nThe Food and Drug Administration has issued guidance for medical devices, and the National Highway Traffic Safety Administration is concerned with automotive cybersecurity. After being criticized by the Government Accountability Office, and following successful attacks on airports and claimed attacks on airplanes, the Federal Aviation Administration has devoted funding to securing systems on board the planes of private manufacturers, and the Aircraft Communications Addressing and Reporting System. Concerns have also been raised about the future Next Generation Air Transportation System.\nThe US Department of Defense (DoD) issued DoD Directive 8570 in 2004, supplemented by DoD Directive 8140, requiring all DoD employees and all DoD contract personnel involved in information assurance roles and activities to earn and maintain various industry Information Technology (IT) certifications in an effort to ensure that all DoD personnel involved in network infrastructure defense have minimum levels of IT industry recognized knowledge, skills and abilities (KSA). Andersson and Reimers (2019) report these certifications range from CompTIA's A+ and Security+ through the ICS2.org's CISSP, etc.. \nComputer emergency readiness team.\n\"Computer emergency response team\" is a name given to expert groups that handle computer security incidents. In the US, two distinct organizations exist, although they do work closely together.\nModern warfare.\nThere is growing concern that cyberspace will become the next theater of warfare. As Mark Clayton from \"The Christian Science Monitor\" wrote in a 2015 article titled \"The New Cyber Arms Race\":\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nThis has led to new terms such as \"cyberwarfare\" and \"cyberterrorism\". The United States Cyber Command was created in 2009 and many other countries have similar forces.\nThere are a few critical voices that question whether cybersecurity is as significant a threat as it is made out to be.\nImportance of Cyber Security.\n\u2022 Protection of Sensitive Information\nBoth network security and cybersecurity are important for protecting sensitive information such as personal data, financial information, and trade secrets. A breach of this information can result in significant financial and reputational losses for businesses and individuals alike.\n\u2022 Protection of Devices and Systems\nNetwork security and cybersecurity are also important for protecting devices and systems from unauthorized access and malicious attacks. This is particularly important for businesses that rely on digital technologies to store and transmit sensitive data.\n\u2022 Compliance with Regulations\nMany industries are subject to regulatory compliance requirements, such as HIPAA, PCI DSS, and GDPR, which mandate the protection of sensitive information. Compliance with these regulations requires implementing robust network security and cybersecurity measures.\nCareers.\nCybersecurity is a fast-growing field of IT concerned with reducing organizations' risk of hack or data breaches. According to research from the Enterprise Strategy Group, 46% of organizations say that they have a \"problematic shortage\" of cybersecurity skills in 2016, up from 28% in 2015. Commercial, government and non-governmental organizations all employ cybersecurity professionals. The fastest increases in demand for cybersecurity workers are in industries managing increasing volumes of consumer data such as finance, health care, and retail. However, the use of the term \"cybersecurity\" is more prevalent in government job descriptions.\nTypical cybersecurity job titles and descriptions include:\n Analyzes and assesses vulnerabilities in the infrastructure (software, hardware, networks), investigates using available tools and countermeasures to remedy the detected vulnerabilities and recommends solutions and best practices. Analyzes and assesses damage to the data/infrastructure as a result of security incidents, examines available recovery tools and processes, and recommends solutions. Tests for compliance with security policies and procedures. May assist in the creation, implementation, or management of security solutions.\n Performs security monitoring, security and data/logs analysis, and forensic analysis, to detect security incidents, and mount the incident response. Investigates and utilizes new technologies and processes to enhance security capabilities and implement improvements. May also review code or perform other security engineering methodologies.\n Designs a security system or major components of a security system, and may head a security design team building a new security system.\n Installs and manages organization-wide security systems. This position may also include taking on some of the tasks of a security analyst in smaller organizations.\n A high-level management position responsible for the entire information security division/staff. The position may include hands-on technical work.\n A high-level management position responsible for the entire security division/staff. A newer position is now deemed needed as security risks grow.\n A DPO is tasked with monitoring compliance with the UK GDPR and other data protection laws, our data protection policies, awareness-raising, training, and audits.\n Broad titles that encompass any one or all of the other roles or titles tasked with protecting computers, networks, software, data or information systems against viruses, worms, spyware, malware, intrusion detection, unauthorized access, denial-of-service attacks, and an ever-increasing list of attacks by hackers acting as individuals or as part of organized crime or foreign governments.\nSecurity Consultant/Specialist/Intelligence.\nStudent programs are also available for people interested in beginning a career in cybersecurity. Meanwhile, a flexible and effective option for information security professionals of all experience levels to keep studying is online security training, including webcasts. A wide range of certified courses are also available.\nIn the United Kingdom, a nationwide set of cybersecurity forums, known as the U.K Cyber Security Forum, were established supported by the Government's cybersecurity strategy in order to encourage start-ups and innovation and to address the skills gap identified by the U.K Government.\nIn Singapore, the Cyber Security Agency has issued a Singapore Operational Technology (OT) Cybersecurity Competency Framework (OTCCF). The framework defines emerging cybersecurity roles in Operational Technology. The OTCCF was endorsed by the Infocomm Media Development Authority (IMDA). It outlines the different OT cybersecurity job positions as well as the technical skills and core competencies necessary. It also depicts the many career paths available, including vertical and lateral advancement opportunities.\nTerminology.\nThe following terms used with regards to computer security are explained below:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7400", "revid": "4796325", "url": "https://en.wikipedia.org/wiki?curid=7400", "title": "Chris Cunningham", "text": "British video director\nChris Cunningham (born 15 October 1970) is a British video artist and music video director, best known for his music videos for electronic musicians such as Autechre, Squarepusher, and most notably Aphex Twin on videos for \"Windowlicker\" and \"Come to Daddy\", and Bj\u00f6rk's \"All is Full of Love\". All were used in Chris' chapter in Director's Label. He has also created art installations and directed short movies. He was approached to direct a movie version of William Gibson's cyberpunk novel \"Neuromancer\"; the project has been in development hell for more than two decades. In the 2000s, Cunningham began doing music production work, and has also designed album artwork for a variety of musicians.\nEarly work.\nAfter seeing Cunningham's work on the 1995 film version of \"Judge Dredd\", Stanley Kubrick head-hunted Cunningham to design and supervise animatronic tests of the central robot child character in his version of the film \"A.I. Artificial Intelligence\". Cunningham worked for over a year on the film before leaving to pursue a career as a director.\nEarlier work in film included model-making, prosthetic make-up and concept illustrations for \"Hardware\" and \"Dust Devil\" for director Richard Stanley, work on \"Nightbreed\" for Clive Barker, and on \"Alien3\" for David Fincher. Between 1990 and 1992, he contributed the occasional cover painting and strip to \"Judge Dredd Megazine\", working under the pseudonym \"Chris Halls\"; Halls is his stepfather's surname.\nMusic videos.\nCunningham has had close ties to Warp Records since his first video for Autechre, \"Second Bad Vilbel\", which received airplay on MTV's \"Amp\". Videos for Aphex Twin's \"Come to Daddy\" and \"Windowlicker\" are perhaps his best known. His video for Bj\u00f6rk's \"All Is Full of Love\" won multiple awards, including an MTV music video award for Breakthrough Video and was nominated for a Grammy for Best Short Form Music Video. It was also the first ever music video to win a Gold Pencil at the D&amp;AD Awards. It can still be seen at the Museum of Modern Art in New York. His video for Aphex Twin's \"Windowlicker\" was nominated for the \"Best Video\" award at the Brit Awards 2000. He also directed Madonna's \"Frozen\" video which became an international hit and won the award for Best Special Effects at the 1998 MTV Music Video Awards. Cunningham also came out of a seven-year hiatus from making music videos to direct the video for \"Sheena Is a Parasite\" by the Horrors.\nVideo art.\nHis video installation \"Flex\" was first shown in 2000 at the Royal Academy of Arts, and subsequently at the Anthony d'Offay Gallery and other art galleries. \"Flex\" was commissioned by the Anthony d'Offay Gallery for the exhibition curated by Norman Rosenthal and Max Wigram at the Royal Academy of Arts in 2000.\nThe Anthony d'Offay Gallery also commissioned \"Monkey Drummer\", a 2\u00bd minute piece intended for exhibition as a companion to \"Flex\" at the 2000 \"Apocalypse\" exhibition at the Royal Academy of Arts: however, the piece was not finished in time. In it an automaton with nine appendages and the head of a monkey plays the drums to \"Mt Saint Michel + Saint Michaels Mount\", the 10th track on Aphex Twin's 2001 album \"drukqs\". \"Monkey Drummer\" debuted as part of Cunningham's installation at the 49th International Exhibition of Art at the 2001 Venice Biennale, which consisted of a loop of \"Monkey Drummer\", \"Flex\", and his video for Bj\u00f6rk's \"All Is Full of Love\". In 2002 both \"Flex\" and \"Monkey Drummer\" were exhibited by 5th Gallery in Dublin, Ireland, in an exhibition curated by Artist/Curator Paul Murnaghan,\nIn 2007, an excerpt from \"Flex\" was shown in the Barbican's exhibition Seduced: Art and Sex from Antiquity to Now curated by Martin Kemp, Marina Wallace and Joanne Bernstein. alongside other pieces by Bacon, Klimt, Rembrandt, Rodin and Picasso.\nShort films.\nIn 2005, Cunningham released the short film \"Rubber Johnny\" as a DVD accompanied by a book of photographs and drawings. \"Rubber Johnny\", a six-minute experimental short film cut to a soundtrack by Aphex Twin remixed by Cunningham, was shot between 2001 and 2004. Shot on DV night-vision, it was made in Cunningham's own time as a home movie of sorts, and took three and half years of weekends to complete. \"The Telegraph\" called it \"like a Looney Tunes short for a generation raised on video nasties and rave music\".\nDuring this period Cunningham also made another short film for Warp Films, \"Spectral Musicians\", which remains unreleased. The short film was set to Squarepusher's \"My Fucking Sound\" from his album \"Go Plastic\"; and to a piece called \"Mutilation Colony\" which was written especially for the short, and was released on the studio album \"Do You Know Squarepusher\".\nCommercials.\nCunningham has directed a handful of commercials for companies and brands, including Gucci, Sony (PlayStation), Levi's, Telecom Italia, Nissan, and Orange.\nMusic production.\nIn 2004/2005, Cunningham took a sabbatical from filmmaking to learn about music production and recording and to develop his own music projects. In December 2007 Cunningham produced two tracks, \"Three Decades\" and \"Primary Colours\", for \"Primary Colours\", the second album by the Horrors. In the summer of 2008, due to scheduling conflicts with his feature film script writing he could not work on the rest of the album which was subsequently recorded by Geoff Barrow from Portishead.\nIn 2008, he produced and arranged a new version of 'I Feel Love' for the Gucci commercial that he also directed. He travelled to Nashville to work with Donna Summer to record a brand new vocal for it.\n\"Chris Cunningham Live\".\nIn 2005, Cunningham played a 45-minute audio visual piece performed live in Tokyo and Osaka in front of 30,000+ fans over the two nights at the Japanese electronic music festival Electraglide. These performances evolved into \"Chris Cunningham Live\", a 55-minute long performance piece combining original and remixed music and film. It features remixed, unreleased and brand new videos and music dynamically edited together into a new live piece spread over three screens. The sound accompanying these images includes Cunningham's first publicly performed compositions interspersed with his remixes of other artist's work. \"Chris Cunningham Live\" debuted as one of the headline attractions at Warp 20 in Paris on 8 May 2009 with other performances scheduled at festivals in UK, and a number of European cities later in the year. \"Chris Cunningham Live\" continued in June 2011, with performances in London, Barcelona, and Sydney, Australia.\nPhotography.\nCunningham has created photography and cover artwork for various people including Bj\u00f6rk's \"All Is Full of Love\", Aphex Twin's \"Windowlicker\" and \"Come to Daddy\".\nIn 2008, Cunningham produced a fashion shoot for \"Dazed &amp; Confused\" using Grace Jones as a model to create \"Nubian versions\" of Rubber Johnny. In an interview for BBC's \"The Culture Show\", it was suggested that the collaboration may expand into a video project. In regards to the collaboration, Cunningham stated \"For me, Grace has the strongest iconography of any artist in music. She\u2019s definitely the most inspiring person I\u2019ve worked with so far\".\nIn November 2008, Cunningham followed on with another photoshoot for \"Vice Magazine\".\n\"Neuromancer\".\nIn an August 1999 \"Spike Magazine\" interview, cyberpunk author William Gibson stated \"He (Chris) was brought to my attention by someone else. We were told, third-hand, that he was extremely wary of the Hollywood process, and wouldn't return calls. But someone else told us that \"Neuromancer\" had been his \"The Wind in the Willows\", that he'd read it when he was a kid. I went to London and we met.\" Gibson is also quoted in the article as saying \"Chris is my own 100 percent personal choice...My only choice. The only person I've met who I thought might have a hope in hell of doing it right. I went back to see him in London just after he'd finished the Bjork video, and I sat on a couch beside this dead sex little Bjork robot, except it was wearing Aphex Twin's head. We talked.\"\nIn 2000, Cunningham and William Gibson began work on the script for Gibson's 1984 novel \"Neuromancer\". However, because \"Neuromancer\" was due to be a big budget studio film, it is rumoured that Cunningham pulled out due to being a first time director without final cut approval. He also felt that too much of the original book's ideas had been cannibalised by other recent films.\nOn 18 November 2004, in the FAQ on the William Gibson Board, Gibson was asked:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Q: Is it true there's a movie of \"Neuromancer\" in the works?\nA: Perpetually, it seems, and going on a quarter of a century now. The most recently rumoured version, to have been directed by Chris Cunningham, is now definitely not happening.\nPersonal life.\nCunningham was married to Warpaint's bassist Jenny Lee Lindberg. They are currently no longer together.\nVideography.\nThe video collection \"The Work of Director Chris Cunningham\" was released in November 2004 as part of the Directors Label set. This DVD includes selected highlights from 1995 to 2000.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7401", "revid": "15885572", "url": "https://en.wikipedia.org/wiki?curid=7401", "title": "Centaur", "text": "Greek mythological creature\nA centaur ( ; ; ), or occasionally hippocentaur, is a creature from Greek mythology with the upper body of a human and the lower body and legs of a horse.\nCentaurs are thought of in many Greek myths as being as wild as untamed horses, and were said to have inhabited the region of Magnesia and Mount Pelion in Thessaly, the Foloi oak forest in Elis, and the Malean peninsula in southern Laconia. Centaurs are subsequently featured in Roman mythology, and were familiar figures in the medieval bestiary. They remain a staple of modern fantastic literature.\nEtymology.\nThe Greek word \"kentauros\" is generally regarded as being of obscure origin. The etymology from \"ken\" + \"tauros\", 'piercing bull', was a euhemerist suggestion in Palaephatus' rationalizing text on Greek mythology, \"On Incredible Tales\" (\u03a0\u03b5\u03c1\u1f76 \u1f00\u03c0\u03af\u03c3\u03c4\u03c9\u03bd), which included mounted archers from a village called \"Nephele\" eliminating a herd of bulls that were the scourge of Ixion's kingdom. Another possible related etymology can be \"bull-slayer\".\nMythology.\nCreation of centaurs.\nThe centaurs were usually said to have been born of Ixion and Nephele. As the story goes, Nephele was a cloud made into the likeness of Hera in a plot to trick Ixion into revealing his lust for Hera to Zeus. Ixion seduced Nephele and from that relationship centaurs were created. Another version, however, makes them children of Centaurus, a man who mated with the Magnesian mares. Centaurus was either himself the son of Ixion and Nephele (inserting an additional generation) or of Apollo and the nymph Stilbe. In the latter version of the story, Centaurus's twin brother was Lapithes, ancestor of the Lapiths.\nAnother tribe of centaurs was said to have lived on Cyprus. According to Nonnus, they were fathered by Zeus, who, in frustration after Aphrodite had eluded him, spilled his seed on the ground of that land. Unlike those of mainland Greece, the Cyprian centaurs were horned.\nThere were also the Lamian Pheres, twelve rustic daimones (spirits) of the Lamos river. They were set by Zeus to guard the infant Dionysos, protecting him from the machinations of Hera, but the enraged goddess transformed them into ox-horned Centaurs. The Lamian Pheres later accompanied Dionysos in his campaign against the Indians.\nThe centaur's half-human, half-horse composition has led many writers to treat them as liminal beings, caught between the two natures they embody in contrasting myths; they are both the embodiment of untamed nature, as in their battle with the Lapiths (their kin), and conversely, teachers like Chiron.\nCentauromachy.\nThe Centaurs are best known for their fight with the Lapiths who, according to one origin myth, would have been cousins to the centaurs. The battle, called the Centauromachy, was caused by the centaurs' attempt to carry off Hippodamia and the rest of the Lapith women on the day of Hippodamia's marriage to Pirithous, who was the king of the Lapithae and a son of Ixion. Theseus, a hero and founder of cities, who happened to be present, threw the balance in favour of the Lapiths by assisting Pirithous in the battle. The Centaurs were driven off or destroyed. Another Lapith hero, Caeneus, who was invulnerable to weapons, was beaten into the earth by Centaurs wielding rocks and the branches of trees. In her article \"The Centaur: Its History and Meaning in Human Culture,\" Elizabeth Lawrence claims that the contests between the centaurs and the Lapiths typify the struggle between civilization and barbarism.\nThe Centauromachy is most famously portrayed in the Parthenon metopes by Phidias and in a Renaissance-era sculpture by Michelangelo.\nOrigin of the myth.\nThe most common theory holds that the idea of centaurs came from the first reaction of a non-riding culture, as in the Minoan Aegean world, to nomads who were mounted on horses. The theory suggests that such riders would appear as half-man, half-animal. Bernal D\u00edaz del Castillo reported that the Aztecs also had this misapprehension about Spanish cavalrymen. The Lapith tribe of Thessaly, who were the kinsmen of the Centaurs in myth, were described as the inventors of horse-riding by Greek writers. The Thessalian tribes also claimed their horse breeds were descended from the centaurs.\nRobert Graves (relying on the work of Georges Dum\u00e9zil, who argued for tracing the centaurs back to the Indian Gandharva), speculated that the centaurs were a dimly remembered, pre-Hellenic fraternal earth cult who had the horse as a totem. A similar theory was incorporated into Mary Renault's \"The Bull from the Sea.\"\nVariations.\nFemale centaurs.\nThough female centaurs, called centaurides or centauresses, are not mentioned in early Greek literature and art, they do appear occasionally in later antiquity. A Macedonian mosaic of the 4th century BC is one of the earliest examples of the centauress in art. Ovid also mentions a centauress named Hylonome who committed suicide when her husband Cyllarus was killed in the war with the Lapiths.\nIndia.\nThe Kalibangan cylinder seal, dated to be around 2600-1900 BC, found at the site of Indus-Valley civilization shows a battle between men in the presence of centaur-like creatures. Other sources claim the creatures represented are actually half human and half tigers, later evolving into the Hindu Goddess of War. These seals are also evidence of Indus-Mesopotamia relations in the 3rd millennium BC.\nIn a popular legend associated with Pazhaya Sreekanteswaram Temple in Thiruvananthapuram, the curse of a saintly Brahmin transformed a handsome Yadava prince into a creature having a horse's body and the prince's head, arms, and torso in place of the head and neck of the horse.\nKinnaras, another half-man, half-horse mythical creature from Indian mythology, appeared in various ancient texts, arts, and sculptures from all around India. It is shown as a horse with the torso of a man where the horse's head would be, and is similar to a Greek centaur.\nRussia.\nA centaur-like half-human, half-equine creature called \"Polkan\" appeared in Russian folk art and lubok prints of the 17th\u201319th centuries. Polkan is originally based on \"Pulicane\", a half-dog from Andrea da Barberino's poem \"I Reali di Francia\", which was once popular in the Slavonic world in prosaic translations.\nArtistic representations.\nClassical art.\nThe extensive Mycenaean pottery found at Ugarit included two fragmentary Mycenaean terracotta figures which have been tentatively identified as centaurs. This finding suggests a Bronze Age origin for these creatures of myth. A painted terracotta centaur was found in the \"Hero's tomb\" at Lefkandi, and by the Geometric period, centaurs figure among the first representational figures painted on Greek pottery. An often-published Geometric period bronze of a warrior face-to-face with a centaur is at the Metropolitan Museum of Art.\nIn Greek art of the Archaic period, centaurs are depicted in three different forms. Some centaurs are depicted with a human torso attached to the body of a horse at the withers, where the horse's neck would be; this form, designated \"Class A\" by Professor Paul Baur, later became standard. \"Class B\" centaurs are depicted with a human body and legs joined at the waist to the hindquarters of a horse; in some cases centaurs of both Class A and Class B appear together. A third type, designated \"Class C\", depicts centaurs with human forelegs terminating in hooves. Baur describes this as an apparent development of Aeolic art, which never became particularly widespread. At a later period, paintings on some \"amphorae\" depict winged centaurs.\nCentaurs were also frequently depicted in Roman art. One example is the pair of centaurs drawing the chariot of Constantine the Great and his family in the Great Cameo of Constantine (\"circa\" AD 314\u201316), which embodies wholly pagan imagery, and contrasts sharply with the popular image of Constantine as the patron of early Christianity.\nMedieval art.\nCentaurs preserved a Dionysian connection in the 12th-century Romanesque carved capitals of Mozac Abbey in the Auvergne. Other similar capitals depict harvesters, boys riding goats (a further Dionysiac theme), and griffins guarding the chalice that held the wine. Centaurs are also shown on a number of Pictish carved stones from north-east Scotland erected in the 8th\u20139th centuries AD (e.g., at Meigle, Perthshire). Though outside the limits of the Roman Empire, these depictions appear to be derived from Classical prototypes.\nModern art.\nThe John C. Hodges library at The University of Tennessee hosts a permanent exhibit of a \"Centaur from Volos\" in its library. The exhibit, made by sculptor Bill Willers by combining a study human skeleton with the skeleton of a Shetland pony, is entitled \"Do you believe in Centaurs?\". According to the exhibitors, it was meant to mislead students in order to make them more critically aware.\nIn heraldry.\nCentaurs are common in European heraldry, although more frequent in continental than in British arms. A centaur holding a bow is referred to as a sagittarius.\nLiterature.\nClassical literature.\nJerome's version of the \"Life\" of St Anthony the Great, written by Athanasius of Alexandria about the hermit monk of Egypt, was widely disseminated in the Middle Ages; it relates Anthony's encounter with a centaur who challenged the saint, but was forced to admit that the old gods had been overthrown. The episode was often depicted in \"The Meeting of St Anthony Abbot and St Paul the Hermit\" by the painter Stefano di Giovanni, who was known as \"Sassetta\". Of the two episodic depictions of the hermit Anthony's travel to greet the hermit Paul, one is his encounter with the demonic figure of a centaur along the pathway in a wood.\nLucretius, in his first-century BC philosophical poem \"On the Nature of Things,\" denied the existence of centaurs based on their differing rate of growth. He states that at the age of three years, horses are in the prime of their life while humans at the same age are still little more than babies, making hybrid animals impossible.\nMedieval literature.\nCentaurs are among the creatures which 14th-century Italian poet Dante placed as guardians in his \"Inferno\". In Canto XII, Dante and his guide Virgil meet a band led by Chiron and Pholus, guarding the bank of Phlegethon in the seventh circle of Hell, a river of boiling blood in which the violent against their neighbours are immersed, shooting arrows into any who move to a shallower spot than their allotted station. The two poets are treated with courtesy, and Nessus guides them to a ford. In Canto XXIV, in the eighth circle, in Bolgia 7, a ditch where thieves are confined, they meet but do not converse with Cacus (who is a giant in the ancient sources), wreathed in serpents and with a fire-breathing dragon on his shoulders, arriving to punish a sinner who has just cursed God. In his \"Purgatorio\", an unseen spirit on the sixth terrace cites the centaurs (\"the drunken double-breasted ones who fought Theseus\") as examples of the sin of gluttony.\nModern day literature.\nC.S. Lewis' \"The Chronicles of Narnia\" series depicts centaurs as the wisest and noblest of creatures. Narnian Centaurs are gifted at stargazing, prophecy, healing, and warfare; a fierce and valiant race always faithful to the High King Aslan the Lion.\nIn J.K. Rowling's \"Harry Potter\" series, centaurs live in the Forbidden Forest close to Hogwarts, preferring to avoid contact with humans. They live in societies called herds and are skilled at archery, healing, and astrology, but like in the original myths, they are known to have some wild and barbarous tendencies.\nWith the exception of Chiron, the centaurs in Rick Riordan's \"Percy Jackson &amp; the Olympians\" are seen as wild party-goers who use a lot of American slang. Chiron retains his mythological role as a trainer of heroes and is skilled in archery. In Riordan's subsequent series, \"Heroes of Olympus\", another group of centaurs are depicted with more animalistic features (such as horns) and appear as villains, serving the Gigantes.\nPhilip Jose Farmer's \"World of Tiers\" series (1965) includes centaurs, called Half-Horses or Hoi Kentauroi. His creations address several of the metabolic problems of such creatures\u2014how could the human mouth and nose intake sufficient air to sustain both itself and the horse body and, similarly, how could the human ingest sufficient food to sustain both parts.\nBrandon Mull's \"Fablehaven\" series features centaurs that live in an area called Grunhold. The centaurs are portrayed as a proud, elitist group of beings that consider themselves superior to all other creatures. The fourth book also has a variation on the species called an Alcetaur, which is part man, part moose.\nThe myth of the centaur appears in John Updike's novel \"The Centaur\". The author depicts a rural Pennsylvanian town as seen through the optics of the myth of the centaur. An unknown and marginalized local school teacher, just like the mythological Chiron did for Prometheus, gave up his life for the future of his son who had chosen to be an independent artist in New York.\nSee also.\nOther hybrid creatures appear in Greek mythology, always with some liminal connection that links Hellenic culture with archaic or non-Hellenic cultures:\nAlso,\nAdditionally, \"Bucentaur\", the name of several historically important Venetian vessels, was linked to a posited ox-centaur or \"\u03b2\u03bf\u03c5\u03ba\u03ad\u03bd\u03c4\u03b1\u03c5\u03c1\u03bf\u03c2\" \"(boukentauros)\" by fanciful and likely spurious folk-etymology.\nFootnotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "7403", "revid": "3236687", "url": "https://en.wikipedia.org/wiki?curid=7403", "title": "Chemotaxis", "text": "Movement of an organism or entity in response to a chemical stimulus\nChemotaxis (from \"chemo-\" + \"taxis\") is the movement of an organism or entity in response to a chemical stimulus. Somatic cells, bacteria, and other single-cell or multicellular organisms direct their movements according to certain chemicals in their environment. This is important for bacteria to find food (e.g., glucose) by swimming toward the highest concentration of food molecules, or to flee from poisons (e.g., phenol). In multicellular organisms, chemotaxis is critical to early development (e.g., movement of sperm towards the egg during fertilization) and development (e.g., migration of neurons or lymphocytes) as well as in normal function and health (e.g., migration of leukocytes during injury or infection). In addition, it has been recognized that mechanisms that allow chemotaxis in animals can be subverted during cancer metastasis. The aberrant chemotaxis of leukocytes and lymphocytes also contribute to inflammatory diseases such as atherosclerosis, asthma, and arthritis. Sub-cellular components, such as the polarity patch generated by mating yeast, may also display chemotactic behavior.\n\"Positive\" chemotaxis occurs if the movement is toward a higher concentration of the chemical in question; \"negative\" chemotaxis if the movement is in the opposite direction. Chemically prompted kinesis (randomly directed or nondirectional) can be called chemokinesis.\nHistory of chemotaxis research.\nAlthough migration of cells was detected from the early days of the development of microscopy by Leeuwenhoek, a Caltech lecture regarding chemotaxis propounds that 'erudite description of chemotaxis was only first made by T. W. Engelmann (1881) and W. F. Pfeffer (1884) in bacteria, and H. S. Jennings (1906) in ciliates'. The Nobel Prize laureate I. Metchnikoff also contributed to the study of the field during 1882 to 1886, with investigations of the process as an initial step of phagocytosis. The significance of chemotaxis in biology and clinical pathology was widely accepted in the 1930s, and the most fundamental definitions underlying the phenomenon were drafted by this time. The most important aspects in quality control of chemotaxis assays were described by H. Harris in the 1950s. In the 1960s and 1970s, the revolution of modern cell biology and biochemistry provided a series of novel techniques that became available to investigate the migratory responder cells and subcellular fractions responsible for chemotactic activity. The availability of this technology led to the discovery of C5a, a major chemotactic factor involved in acute inflammation. The pioneering works of J. Adler modernized Pfeffer's capillary assay and represented a significant turning point in understanding the whole process of intracellular signal transduction of bacteria.\nBacterial chemotaxis\u2014general characteristics.\nSome bacteria, such as \"E. coli\", have several flagella per cell (4\u201310 typically). These can rotate in two ways:\nThe directions of rotation are given for an observer outside the cell looking down the flagella toward the cell.\nBehavior.\nThe overall movement of a bacterium is the result of alternating tumble and swim phases, called run-and-tumble motion. As a result, the trajectory of a bacterium swimming in a uniform environment will form a random walk with relatively straight swims interrupted by random tumbles that reorient the bacterium. Bacteria such as \"E. coli\" are unable to choose the direction in which they swim, and are unable to swim in a straight line for more than a few seconds due to rotational diffusion; in other words, bacteria \"forget\" the direction in which they are going. By repeatedly evaluating their course, and adjusting if they are moving in the wrong direction, bacteria can direct their random walk motion toward favorable locations.\nIn the presence of a chemical gradient bacteria will chemotax, or direct their overall motion based on the gradient. If the bacterium senses that it is moving in the correct direction (toward attractant/away from repellent), it will keep swimming in a straight line for a longer time before tumbling; however, if it is moving in the wrong direction, it will tumble sooner. Bacteria like \"E. coli\" use temporal sensing to decide whether their situation is improving or not, and in this way, find the location with the highest concentration of attractant, detecting even small differences in concentration.\nThis biased random walk is a result of simply choosing between two methods of random movement; namely tumbling and straight swimming. The helical nature of the individual flagellar filament is critical for this movement to occur. The protein structure that makes up the flagellar filament, flagellin, is conserved among all flagellated bacteria. Vertebrates seem to have taken advantage of this fact by possessing an immune receptor (TLR5) designed to recognize this conserved protein. \nAs in many instances in biology, there are bacteria that do not follow this rule. Many bacteria, such as \"Vibrio\", are monoflagellated and have a single flagellum at one pole of the cell. Their method of chemotaxis is different. Others possess a single flagellum that is kept inside the cell wall. These bacteria move by spinning the whole cell, which is shaped like a corkscrew.\nSignal transduction.\nChemical gradients are sensed through multiple transmembrane receptors, called methyl-accepting chemotaxis proteins (MCPs), which vary in the molecules that they detect. Thousands of MCP receptors are known to be encoded across the bacterial kingdom. These receptors may bind attractants or repellents directly or indirectly through interaction with proteins of periplasmatic space. The signals from these receptors are transmitted across the plasma membrane into the cytosol, where \"Che proteins\" are activated. The Che proteins alter the tumbling frequency, and alter the receptors.\nFlagellum regulation.\nThe proteins CheW and CheA bind to the receptor. The absence of receptor activation results in autophosphorylation in the histidine kinase, CheA, at a single highly conserved histidine residue. CheA, in turn, transfers phosphoryl groups to conserved aspartate residues in the response regulators CheB and CheY; CheA is a histidine kinase and it does not actively transfer the phosphoryl group, rather, the response regulator CheB takes the phosphoryl group from CheA. This mechanism of signal transduction is called a two-component system, and it is a common form of signal transduction in bacteria. CheY induces tumbling by interacting with the flagellar switch protein FliM, inducing a change from counter-clockwise to clockwise rotation of the flagellum. Change in the rotation state of a single flagellum can disrupt the entire flagella bundle and cause a tumble.\nReceptor regulation.\nCheB, when activated by CheA, acts as a methylesterase, removing methyl groups from glutamate residues on the cytosolic side of the receptor; it works antagonistically with CheR, a methyltransferase, which adds methyl residues to the same glutamate residues. If the level of an attractant remains high, the level of phosphorylation of CheA (and, therefore, CheY and CheB) will remain low, the cell will swim smoothly, and the level of methylation of the MCPs will increase (because CheB-P is not present to demethylate). The MCPs no longer respond to the attractant when they are fully methylated; therefore, even though the level of attractant might remain high, the level of CheA-P (and CheB-P) increases and the cell begins to tumble. The MCPs can be demethylated by CheB-P, and, when this happens, the receptors can once again respond to attractants. The situation is the opposite with regard to repellents: fully methylated MCPs respond best to repellents, while least-methylated MCPs respond worst to repellents. This regulation allows the bacterium to 'remember' chemical concentrations from the recent past, a few seconds, and compare them to those it is currently experiencing, thus 'know' whether it is traveling up or down a gradient.\n that bacteria have to chemical gradients, other mechanisms are involved in increasing the absolute value of the sensitivity on a given background. Well-established examples are the ultra-sensitive response of the motor to the CheY-P signal, and the clustering of chemoreceptors.\nChemoattractants and chemorepellents.\nChemoattractants and chemorepellents are inorganic or organic substances possessing chemotaxis-inducer effect in motile cells. These chemotactic ligands create chemical concentration gradients that organisms, prokaryotic and eukaryotic, move toward or away from, respectively.\nEffects of chemoattractants are elicited via chemoreceptors such as methyl-accepting chemotaxis proteins (MCP). MCPs in E.coli include Tar, Tsr, Trg and Tap. Chemoattracttants to Trg include ribose and galactose with phenol as a chemorepellent. Tap and Tsr recognize dipeptides and serine as chemoattractants, respectively.\nChemoattractants or chemorepellents bind MCPs at its extracellular domain; an intracellular signaling domain relays the changes in concentration of these chemotactic ligands to downstream proteins like that of CheA which then relays this signal to flagellar motors via phosphorylated CheY (CheY-P). CheY-P can then control flagellar rotation influencing the direction of cell motility.\nFor \"E.coli\", \"S. meliloti\", and \"R. spheroids,\" the binding of chemoattractants to MCPs inhibit CheA and therefore CheY-P activity, resulting in smooth runs, but for \"B. substilis\", CheA activity increases. Methylation events in \"E.coli\" cause MCPs to have lower affinity to chemoattractants which causes increased activity of CheA and CheY-P resulting in tumbles. In this way cells are able to adapt to the immediate chemoattractant concentration and detect further changes to modulate cell motility.\nChemoattractants in eukaryotes are well characterized for immune cells. Formyl peptides, such as fMLF, attract leukocytes such as neutrophils and macrophages, causing movement toward infection sites. Non-acylated methioninyl peptides do not act as chemoattractants to neutrophils and macrophages. Leukocytes also move toward chemoattractants C5a, a complement component, and pathogen-specific ligands on bacteria.\nMechanisms concerning chemorepellents are less known than chemoattractants. Although chemorepellents work to confer an avoidance response in organisms, \"Tetrahymena thermophila\" adapt to a chemorepellent, Netrin-1 peptide, within 10 minutes of exposure; however, exposure to chemorepellents such as GTP, PACAP-38, and nociceptin show no such adaptations. GTP and ATP are chemorepellents in micro-molar concentrations to both \"Tetrahymena\" and \"Paramecium\". These organisms avoid these molecules by producing avoiding reactions to re-orient themselves away from the gradient.\nEukaryotic chemotaxis.\nThe mechanism of chemotaxis that eukaryotic cells employ is quite different from that in the bacteria \"E. coli\"; however, sensing of chemical gradients is still a crucial step in the process. Due to their small size and other biophysical constraints, \"E. coli\" cannot directly detect a concentration gradient. Instead, they employ temporal gradient sensing, where they move over larger distances several times their own width and measure the rate at which perceived chemical concentration changes.\nEukaryotic cells are much larger than prokaryotes and have receptors embedded uniformly throughout the cell membrane. Eukaryotic chemotaxis involves detecting a concentration gradient spatially by comparing the asymmetric activation of these receptors at the different ends of the cell. Activation of these receptors results in migration towards chemoattractants, or away from chemorepellants. In mating yeast, which are non-motile, patches of polarity proteins on the cell cortex can relocate in a chemotactic fashion up pheromone gradients.\nIt has also been shown that both prokaryotic and eukaryotic cells are capable of chemotactic memory. In prokaryotes, this mechanism involves the methylation of receptors called methyl-accepting chemotaxis proteins (MCPs). This results in their desensitization and allows prokaryotes to \"remember\" and adapt to a chemical gradient. In contrast, chemotactic memory in eukaryotes can be explained by the Local Excitation Global Inhibition (LEGI) model. LEGI involves the balance between a fast excitation and delayed inhibition which controls downstream signaling such as Ras activation and PIP3 production.\nLevels of receptors, intracellular signalling pathways and the effector mechanisms all represent diverse, eukaryotic-type components. In eukaryotic unicellular cells, amoeboid movement and cilium or the eukaryotic flagellum are the main effectors (e.g., Amoeba or Tetrahymena). Some eukaryotic cells of higher vertebrate origin, such as immune cells also move to where they need to be. Besides immune competent cells (granulocyte, monocyte, lymphocyte) a large group of cells\u2014considered previously to be fixed into tissues\u2014are also motile in special physiological (e.g., mast cell, fibroblast, endothelial cells) or pathological conditions (e.g., metastases). Chemotaxis has high significance in the early phases of embryogenesis as development of germ layers is guided by gradients of signal molecules.\nMotility.\nUnlike motility in bacterial chemotaxis, the mechanism by which eukaryotic cells physically move is unclear. There appear to be mechanisms by which an external chemotactic gradient is sensed and turned into an intracellular PIP3 gradient, which results in a gradient and the activation of a signaling pathway, culminating in the polymerisation of actin filaments. The growing distal end of actin filaments develops connections with the internal surface of the plasma membrane via different sets of peptides and results in the formation of anterior pseudopods and posterior uropods.\nCilia of eukaryotic cells can also produce chemotaxis; in this case, it is mainly a Ca2+-dependent induction of the microtubular system of the basal body and the beat of the 9\u00a0+\u00a02 microtubules within cilia. The orchestrated beating of hundreds of cilia is synchronized by a submembranous system built between basal bodies.\nThe details of the signaling pathways are still not totally clear.\nChemotaxis-related migratory responses.\nChemotaxis refers to the directional migration of cells in response to chemical gradients; several variations of chemical-induced migration exist as listed below. \nReceptors.\nIn general, eukaryotic cells sense the presence of chemotactic stimuli through the use of 7-transmembrane (or serpentine) heterotrimeric G-protein-coupled receptors, a class representing a significant portion of the genome. Some members of this gene superfamily are used in eyesight (rhodopsins) as well as in olfaction (smelling). The main classes of chemotaxis receptors are triggered by:\nHowever, induction of a wide set of membrane receptors (e.g., cyclic nucleotides, amino acids, insulin, vasoactive peptides) also elicit migration of the cell.\nChemotactic selection.\nWhile some chemotaxis receptors are expressed in the surface membrane with long-term characteristics, as they are determined genetically, others have short-term dynamics, as they are assembled \"ad hoc\" in the presence of the ligand. The diverse features of the chemotaxis receptors and ligands allows for the possibility of selecting chemotactic responder cells with a simple chemotaxis assay By chemotactic selection, we can determine whether a still-uncharacterized molecule acts via the long- or the short-term receptor pathway. The term \"chemotactic selection\" is also used to designate a technique that separates eukaryotic or prokaryotic cells according to their chemotactic responsiveness to selector ligands.\nChemotactic ligands.\nThe number of molecules capable of eliciting chemotactic responses is relatively high, and we can distinguish primary and secondary chemotactic molecules. The main groups of the primary ligands are as follows:\nChemotactic range fitting.\nChemotactic responses elicited by ligand-receptor interactions vary with the concentration of the ligand. Investigations of ligand families (e.g. amino acids or oligopeptides) demonstrates that chemoattractant activity occurs over a wide range, while chemorepellent activities have narrow ranges.\nClinical significance.\nA changed migratory potential of cells has relatively high importance in the development of several clinical symptoms and syndromes.\nAltered chemotactic activity of extracellular (e.g., Escherichia coli) or intracellular (e.g., Listeria monocytogenes) pathogens itself represents a significant clinical target. Modification of endogenous chemotactic ability of these microorganisms by pharmaceutical agents can decrease or inhibit the ratio of infections or spreading of infectious diseases.\nApart from infections, there are some other diseases wherein impaired chemotaxis is the primary etiological factor, as in Ch\u00e9diak\u2013Higashi syndrome, where giant intracellular vesicles inhibit normal migration of cells.\nMathematical models.\nSeveral mathematical models of chemotaxis were developed depending on the type of\nAlthough interactions of the factors listed above make the behavior of the solutions of mathematical models of chemotaxis rather complex, it is possible to describe the basic phenomenon of chemotaxis-driven motion in a straightforward way. Indeed, let us denote with formula_1 the spatially non-uniform concentration of the chemo-attractant and formula_2 as its gradient. Then the chemotactic cellular flow (also called current) formula_3 that is generated by the chemotaxis is linked to the above gradient by the law:\nformula_4\nwhere formula_5 is the spatial density of the cells and formula_6 is the so-called \u2019Chemotactic coefficient\u2019 - formula_7 is often not constant, but a decreasing function of the chemo-attractant. For some quantity formula_8 that is subject to total flux formula_9 and generation/destruction term formula_10, it is possible to formulate a continuity equation:\nformula_11\nwhere formula_12 is the divergence. This general equation applies to both the cell density and the chemo-attractant. Therefore, incorporating a diffusion flux into the total flux term, the interactions between these quantities are governed by a set of coupled reaction-diffusion partial differential equations describing the change in formula_13 and formula_14:\nformula_15|border colour=#0073CF|background colour=#F5FFFA}}where formula_16 describes the growth in cell density, formula_17 is the kinetics/source term for the chemo-attractant, and the diffusion coefficients for cell density and the chemo-attractant are respectively formula_18 and formula_19.\nSpatial ecology of soil microorganisms is a function of their chemotactic sensitivities towards substrate and fellow organisms. The chemotactic behavior of the bacteria was proven to lead to non-trivial population patterns even in the absence of environmental heterogeneities. The presence of structural pore scale heterogeneities has an extra impact on the emerging bacterial patterns.\nMeasurement of chemotaxis.\nA wide range of techniques is available to evaluate chemotactic activity of cells or the chemoattractant and chemorepellent character of ligands.\nThe basic requirements of the measurement are as follows:\nDespite the fact that an ideal chemotaxis assay is still not available, there are several protocols and pieces of equipment that offer good correspondence with the conditions described above. The most commonly used are summarised in the table below:\nArtificial chemotactic systems.\n\"Chemical robots\" that use artificial chemotaxis to navigate autonomously have been designed. Applications include targeted delivery of drugs in the body. More recently, enzyme molecules have also shown positive chemotactic behavior in the gradient of their substrates. The thermodynamically-favorable binding of enzymes to their specific substrates is recognized as the origin of enzymatic chemotaxis. Additionally, enzymes in cascades have also shown substrate-driven chemotactic aggregation.\nApart from active enzymes, non-reacting molecules also show chemotactic behavior. This has been demonstrated by using dye molecules that move directionally in gradients of polymer solution through favorable hydrophobic interactions.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "7405", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=7405", "title": "Crimean war", "text": ""}
