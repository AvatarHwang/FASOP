{"id": "5658", "revid": "45627014", "url": "https://en.wikipedia.org/wiki?curid=5658", "title": "Human cannibalism", "text": "Practice of humans eating other humans\nHuman cannibalism is the act or practice of humans eating the flesh or internal organs of other human beings. A person who practices cannibalism is called a cannibal. The meaning of \"cannibalism\" has been extended into zoology to describe an individual of a species consuming all or part of another individual of the same species as food, including sexual cannibalism.\nNeanderthals are believed to have practiced cannibalism, and Neanderthals may have been eaten by anatomically modern humans. Cannibalism was also practiced in ancient Egypt, Roman Egypt and during famines in Egypt such as the great famine of 1199\u20131202. The Island Carib people of the Lesser Antilles, from whom the word \"cannibalism\" is derived, acquired a long-standing reputation as cannibals after their legends were recorded in the 17th century. Some controversy exists over the accuracy of these legends and the prevalence of actual cannibalism in the culture.\nCannibalism has been well documented in much of the world, including Fiji, the Amazon Basin, the Congo, and the M\u0101ori people of New Zealand. Cannibalism was also practiced in New Guinea and in parts of the Solomon Islands, and human flesh was sold at markets in some parts of Melanesia. Fiji was once known as the \"Cannibal Isles\".\nCannibalism has recently been both practiced and fiercely condemned in several wars, especially in Liberia and the Democratic Republic of the Congo. It was still practiced in Papua New Guinea as of 2012, for cultural reasons and in ritual as well as in war in various Melanesian tribes. Cannibalism has been said to test the bounds of cultural relativism because it challenges anthropologists \"to define what is or is not beyond the pale of acceptable human behavior\". Some scholars argue that no firm evidence exists that cannibalism has ever been a socially acceptable practice anywhere in the world, at any time in history, although this has been consistently debated against.\nA form of cannibalism popular in early modern Europe was the consumption of body parts or blood for medical purposes. This practice was at its height during the 17th century, although as late as the second half of the 19th century some peasants attending an execution are recorded to have \"rushed forward and scraped the ground with their hands that they might collect some of the bloody earth, which they subsequently crammed in their mouth, in hope that they might thus get rid of their disease.\"\nCannibalism has occasionally been practiced as a last resort by people suffering from famine, even in modern times. Famous examples include the ill-fated Donner Party (1846\u20131847) and, more recently, the crash of Uruguayan Air Force Flight 571 (1972), after which some survivors ate the bodies of the dead. Additionally, there are cases of people engaging in cannibalism for sexual pleasure, such as Jeffrey Dahmer, Issei Sagawa, and Albert Fish. There is resistance to formally labeling cannibalism a mental disorder.\nEtymology.\nThe word \"cannibalism\" is derived from \"Can\u00edbales\", the Spanish name for the Caribs, a West Indies tribe that may have practiced cannibalism, from Spanish \"canibal\" or \"caribal\", \"a savage\". The term \"anthropophagy\", meaning \"eating humans\", is also used for human cannibalism.\nReasons.\nConsumption of a person from within the same community is called endocannibalism; ritual cannibalism of the recently deceased can be part of the grieving process or be seen as a way of guiding the souls of the dead into the bodies of living descendants. Exocannibalism is the consumption of a person from outside the community, usually as a celebration of victory against a rival tribe. Both types of cannibalism can also be fueled by the belief that eating a person's flesh or internal organs will endow the cannibal with some of the characteristics of the deceased.\nIn \"Guns, Germs and Steel\", Jared Diamond suggests \"Protein starvation is probably also the ultimate reason why cannibalism was widespread in traditional New Guinea highland societies\".\nIn most parts of the world, cannibalism is not a societal norm, but is sometimes resorted to in situations of extreme necessity. The survivors of the shipwrecks of the \"Essex\" and \"M\u00e9duse\" in the 19th century are said to have engaged in cannibalism, as did the members of Franklin's lost expedition and the Donner Party. Such cases generally involve necro-cannibalism (eating the corpse of someone who is already dead) as opposed to homicidal cannibalism (killing someone for food). In English law, the latter is always considered a crime, even in the most trying circumstances. The case of \"R v Dudley and Stephens\", in which two men were found guilty of murder for killing and eating a cabin boy while adrift at sea in a lifeboat, set the precedent that necessity is no defence to a charge of murder.\nIn pre-modern medicine, the explanation given by the now-discredited theory of humorism for cannibalism was that it came about within a black acrimonious humor, which, being lodged in the linings of the ventricle, produced the voracity for human flesh.\nMedical aspects.\nA well-known case of mortuary cannibalism is that of the Fore tribe in New Guinea, which resulted in the spread of the prion disease kuru. Although the Fore's mortuary cannibalism was well-documented, the practice had ceased before the cause of the disease was recognized. However, some scholars argue that although post-mortem dismemberment was the practice during funeral rites, cannibalism was not. Marvin Harris theorizes that it happened during a famine period coincident with the arrival of Europeans and was rationalized as a religious rite.\nIn 2003, a publication in \"Science\" received a large amount of press attention when it suggested that early humans may have practiced extensive cannibalism. According to this research, genetic markers commonly found in modern humans worldwide suggest that today many people carry a gene that evolved as protection against the brain diseases that can be spread by consuming human brain tissue. A 2006 reanalysis of the data questioned this hypothesis, because it claimed to have found a data collection bias, which led to an erroneous conclusion. This claimed bias came from incidents of cannibalism used in the analysis not being due to local cultures, but having been carried out by explorers, stranded seafarers or escaped convicts. The original authors published a subsequent paper in 2008 defending their conclusions.\nMyths, legends and folklore.\nCannibalism features in the folklore and legends of many cultures and is most often attributed to evil characters or as extreme retribution for some wrongdoing. Examples include the witch in \"Hansel and Gretel\", Lamia of Greek mythology and the witch Baba Yaga of Slavic folklore.\nA number of stories in Greek mythology involve cannibalism, in particular the eating of close family members, e.g., the stories of Thyestes, Tereus and especially Cronus, who became Saturn in the Roman pantheon. The story of Tantalus is another example, though here a family member is prepared for consumption by others.\nThe wendigo is a creature appearing in the legends of the Algonquian people. It is thought of variously as a malevolent cannibalistic spirit that could possess humans or a monster that humans could physically transform into. Those who indulged in cannibalism were at particular risk, and the legend appears to have reinforced this practice as taboo. The Zuni people tell the story of the \u00c1tahsaia \u2013 a giant who cannibalizes his fellow demons and seeks out human flesh.\nThe wechuge is a demonic cannibalistic creature that seeks out human flesh appearing in the mythology of the Athabaskan people. It is said to be half monster and half human-like; however, it has many shapes and forms.\nSkepticism.\nWilliam Arens, author of \"The Man-Eating Myth: Anthropology and Anthropophagy\", questions the credibility of reports of cannibalism and argues that the description by one group of people of another people as cannibals is a consistent and demonstrable ideological and rhetorical device to establish perceived cultural superiority. Arens bases his thesis on a detailed analysis of various \"classic\" cases of cannibalism reported by explorers, missionaries, and anthropologists. He claims that all of them were steeped in racism, unsubstantiated, or based on second-hand or hearsay evidence. Though widely discussed, Arens's book generally failed to convince the academic community. Claude L\u00e9vi-Strauss observes that, in spite of his \"brilliant but superficial book ... [n]o serious ethnologist disputes the reality of cannibalism\". Shirley Lindenbaum notes that, while after \"Arens['s] ... provocative suggestion ... many anthropologists ... reevaluated their data\", the outcome was an improved and \"more nuanced\" understanding of where, why and under which circumstances cannibalism took place rather than a confirmation of his claims: \"Anthropologists working in the Americas, Africa, and Melanesia now acknowledge that institutionalized cannibalism occurred in some places at some times. Archaeologists and evolutionary biologists are taking cannibalism seriously.\"\nLindenbaum and others point out that Arens displays a \"strong ethnocentrism\". His refusal to admit that institutionalized cannibalism ever existed seems to be motivated by the implied idea \"that cannibalism is the worst thing of all\" \u2013 worse than any other behavior people engaged in, and therefore uniquely suited to vilify others. Kajsa Ekholm Friedman calls this \"a remarkable opinion in a culture [the European/American one] that has been capable of the most extreme cruelty and destructive behavior, both at home and in other parts of the world.\"\nShe observes that, contrary to European values and expectations, \"[i]n many parts of the Congo region there was no negative evaluation of cannibalism. On the contrary, people expressed their strong appreciation of this very special meat and could not understand the hysterical reactions from the white man's side.\" And why indeed, she goes on to ask, should they have had the same negative reactions to cannibalism as Arens and his contemporaries? Implicitly he assumes that everybody throughout human history must have shared the strong taboo placed by his own culture on cannibalism, but he never attempts to explain why this should be so, and \"neither logic nor historical evidence justifies\" this viewpoint, as Christian Siefkes commented.\nAccusations of cannibalism could be used to characterize indigenous peoples as \"uncivilized\", \"primitive\", or even \"inhuman.\" While this means that the reliability of reports of cannibal practices must be carefully evaluated especially if their wording suggests such a context, many actual accounts do not fit this pattern. The earliest firsthand account of cannibal customs in the Caribbean comes from Diego \u00c1lvarez Chanca, who accompanied Christopher Columbus on his second voyage. His description of the customs of the Caribs of Guadeloupe includes their cannibalism (men killed or captured in war were eaten, while captured boys were \"castrated [and used as] servants until they gr[e]w up, when they [were] slaughtered\" for consumption), but he nevertheless notes \"that these people are more civilized than the other islanders\" (who did not practice cannibalism). Nor was he an exception. Among the earliest reports of cannibalism in the Caribbean and the Americas, there are some (like those of Amerigo Vespucci) that seem to mostly consist of hearsay and \"gross exaggerations\", but others (by Chanca, Columbus himself, and other early travelers) show \"genuine interest and respect for the natives\" and include \"numerous cases of sincere praise\".\nReports of cannibalism from other continents follow similar patterns. Condescending remarks can be found, but many Europeans who described cannibal customs in Central Africa wrote about those who practiced them in quite positive terms, calling them \"splendid\" and \"the finest people\" and not rarely, like Chanca, actually considering them as \"far in advance of\" and \"intellectually and morally superior\" to the non-cannibals around them. Writing from Melanesia, the missionary George Brown explicitly rejects the European prejudice of picturing cannibals as \"particularly ferocious and repulsive\", noting instead that many cannibals he met were \"no more ferocious than\" others and \"indeed ... very nice people\".\nReports or assertions of cannibal practices could nevertheless be used to promote the use of military force as a means of \"civilizing\" and \"pacifying\" the \"savages\". During the Spanish conquest of the Aztec Empire and its earlier conquests in the Caribbean there were widespread reports of cannibalism, and cannibals became exempted from Queen Isabella's prohibition on enslaving the indigenous. Another example of the sensationalism of cannibalism and its connection to imperialism occurred during Japan's 1874 expedition to Taiwan. As Robert Eskildsen describes, Japan's popular media \"exaggerated the aborigines' violent nature\", in some cases by wrongly accusing them of cannibalism.\n\"This Horrid Practice: The Myth and Reality of Traditional Maori Cannibalism\" (2008) by New Zealand historian Paul Moon received a hostile reception by some M\u0101ori, who felt the book tarnished their whole people. However, the factual accuracy of the book was not seriously disputed and even critics such as Margaret Mutu grant that cannibalism was \"definitely\" practiced and that it was \"part of our [M\u0101ori] culture.\"\nHistory.\nAmong modern humans, cannibalism has been practiced by various groups. It was practiced by humans in Prehistoric Europe, Mesoamerica, South America, among Iroquoian peoples in North America, Maori in New Zealand, the Solomon Islands, parts of West Africa and Central Africa, some of the islands of Polynesia, New Guinea, Sumatra, and Fiji. Evidence of cannibalism has been found in ruins associated with the Ancestral Puebloans of the Southwestern United States as well (at Cowboy Wash in Colorado).\nPrehistory.\nThere is evidence, both archaeological and genetic, that cannibalism has been practiced for hundreds of thousands of years by early \"Homo sapiens\" and archaic hominins. Human bones that have been \"de-fleshed\" by other humans go back 600,000 years. The oldest \"Homo sapiens\" bones (from Ethiopia) show signs of this as well. Some anthropologists, such as Tim D. White, suggest that cannibalism was common in human societies prior to the beginning of the Upper Paleolithic period. This theory is based on the large amount of \"butchered human\" bones found in Neanderthal and other Lower/Middle Paleolithic sites.\nIt seems likely that not all instances of prehistoric cannibalism were due to the same reason, just as cannibalistic acts known from the historical record have been motivated by a variety of reasons. One suggested reason for cannibalism in the Lower and Middle Paleolithic have been food shortages. It has been also suggested that removing dead bodies through ritual (funerary) cannibalism was a means of predator control, aiming to eliminate predators' and scavengers' access to hominid (and early human) bodies. Jim Corbett proposed that after major epidemics, when human corpses are easily accessible to predators, there are more cases of man-eating leopards, so removing dead bodies through ritual cannibalism (before the cultural traditions of burying and burning bodies appeared in human history) might have had practical reasons for hominids and early humans to control predation.\nThe oldest archaeological evidence of hominid cannibalism comes from the Gran Dolina cave in northern Spain. The remains of several individuals who died about 800,000 years ago and may have belongs to the \"Homo antecessor\" species show unmistakable signs of having been butchered and consumed in the same way as animals whose bones were also found at the site. They belong to at least eleven individuals, all of which were young (ranging from infancy to late teenhood). A study of this case considers it an instance of \"nutritional\" cannibalism, where individuals belonging to hostile or unrelated groups were hunted, killed, and eaten much like animals. Based on the placement and processing of human and animal remains, the authors conclude that cannibalism was likely a \"repetitive behavior over time as part of a culinary tradition\", not caused by starvation or other exceptional circumstances. They suggest that young individuals (more than half of which were children under ten) were targeted because they \"posed a lower risk for hunters\" and because this was an effective means for limiting the growth of competing groups.\nSeveral sites in Croatia, France, and Spain yield evidence that the Neanderthals sometimes practiced cannibalism, though the interpretation of some of the finds remains controversial.\nNeanderthals could also fall victim to cannibalism by anatomically modern humans. Evidence found in southwestern France indicates that the latter butchered and ate a Neanderthal child about 30,000 years ago; it is unknown whether the child was killed by them or died of other reasons. The find has been considered as strengthening the conjecture that modern humans might have hunted Neanderthals and in this way contributed to their extinction.\nIn Gough's Cave, England, remains of human bones and skulls, around 14,700 years old, suggest that cannibalism took place amongst the people living in or visiting the cave, and that they may have used human skulls as drinking vessels.\nThe archaeological site of Herxheim in southwestern Germany was a ritual center and a mass grave formed by people of the Linear Pottery culture in Neolithic Europe. It contained the scattered remains of more than 1000 individuals from different, in some cases faraway regions, who died around 5000 BCE. Whether they were war captives or human sacrifices is unclear, but the evidence indicates that their corpses were spit-roasted whole and then consumed.\nAt Fontbr\u00e9goua Cave in southeastern France, the remains of six people who lived about 7,000 years ago were found (two children, one adolescent, and three adults), in addition to animal bones. The patterns of cut marks indicate that both humans and animals were skinned and processed in similar ways. Since the human victims were all processed at the same time, the main excavator, Paola Villa, suspects that they all belonged to the same family or extended family and were killed and butchered together, probably during some kind of violent conflict. Others have argued that the traces were caused by defleshing rituals preceding a secondary burial, but the fact that both humans and wild and domestic animals were processed in the same way makes this unlikely; moreover, Villa argues that the observed traces better fit a typical butchering process than a secondary burial.\nResearchers have also found physical evidence of cannibalism from more recent times, including from Prehistoric Britain. In 2001, archaeologists at the University of Bristol found evidence of cannibalism practiced around 2000 years ago in Gloucestershire, South West England. This is in agreement with Ancient Roman reports that the Celts in Britain practiced human sacrifice, killing and eating captured enemies as well as convicted criminals.\nEarly history.\nCannibalism is mentioned many times in early history and literature. The oldest written reference may be from the tomb of the ancient Egyptian king Unas (24th century BCE). It contained a hymn in praise of the king portraying him as a cannibal who eats both \"men\" and \"gods\", thus indicating an attitude towards cannibalism quite different from the modern one.\nHerodotus claimed in his \"Histories\" (5th century BCE) that after eleven days' voyage up the Borysthenes (Dnieper River) one reached a desolated land that extended for a long way, followed by a country of man-eaters (other than the Scythians), and beyond it by another desolated and uninhabited area.\nThe Stoic philosopher Chrysippus approved of eating one's dead relatives in a funerary ritual, noting that such rituals were common among many peoples.\nCassius Dio recorded cannibalism practiced by the \"bucoli\", Egyptian tribes led by Isidorus against Rome. They sacrificed and consumed two Roman officers in a ritualistic fashion, swearing an oath over their entrails.\nAccording to Appian, during the Roman siege of Numantia in the 2nd century BCE, the population of Numantia (in today's Spain) was reduced to cannibalism and suicide. Cannibalism was also reported by Josephus during the siege of Jerusalem in 70 CE.\nJerome, in his letter \"Against Jovinianus\" (written 393 CE), discusses how people come to their present condition as a result of their heritage, and lists several examples of peoples and their customs. In the list, he mentions that he has heard that the Attacotti (in Britain) eat human flesh and that the Massagetae and \"Derbices\" (two Central Asian peoples) kill and eat old people, considering this a more desirable fate than dying of old age and illness.\nMiddle Ages.\nDuring the Chinese Tang dynasty, cannibalism was supposedly resorted to by rebel forces early in the period (who were said to raid neighboring areas for victims to eat), as well as both soldiers and civilians besieged during the rebellion of An Lushan. Eating an enemy's heart and liver was also claimed to be a feature of both official punishments and private vengeance. References to cannibalizing the enemy have also been seen in poetry written in the Song dynasty (for example, in \"Man Jiang Hong\"), although the cannibalizing is perhaps poetic symbolism, expressing hatred towards the enemy.\nReports of cannibalism were recorded during the First Crusade, as Crusaders were alleged to have fed on the bodies of their dead opponents following the siege of Ma'arra. Amin Maalouf also alleges further cannibalism incidents on the march to Jerusalem, and to the efforts made to delete mention of these from Western history. Even though this account does not appear in any contemporary Muslim chronicle. The famine and cannibalism are recognised as described by Fulcher of Chartres, but the torture and the killing of Muslim captives for cannibalism by Radulph of Caen are very unlikely since no Arab or Muslim records of the events exist. Had they occurred, they would have probably been recorded. That has been noted by BBC Timewatch series, the episode \"The Crusades: A Timewatch Guide\", which included experts Thomas Asbridge and Muslim Arabic historian Fozia Bora, who state that Radulph of Caen's description does not appear in any contemporary Muslim chronicle. During Europe's Great Famine of 1315\u201317, there were many reports of cannibalism among the starving populations. In North Africa, as in Europe, there are references to cannibalism as a last resort in times of famine.\nWhen the Moroccan explorer Ibn Battuta visited the Mali Empire in the 1350s, he was surprised to see sultan Sulayman give \"a slave girl as part of his reception-gift\" to a group of warriors from a cannibal region who had come to visit his court. \"They slaughtered her and ate her and smeared their faces and hands with her blood and came in gratitude to the sultan.\" He was told that the sultan did so every time he received the cannibal guests. Though a Muslim like Ibn Battuta himself, he apparently considered catering to his visitors' preferences more important than whatever reservations he may have had about the practice. Other Muslim authors writing around that time also report that cannibalism was practiced in some West Africa regions and that slave girls were sometimes slaughtered for food, since \"[t]heir flesh is the best thing we have to eat.\"\nFor a brief time in Europe, an unusual form of cannibalism occurred when thousands of Egyptian mummies preserved in bitumen were ground up and sold as medicine. The practice developed into a wide-scale business which flourished until the late 16th century. This \"fad\" ended because the mummies were revealed actually to be recently killed slaves. Two centuries ago, mummies were still believed to have medicinal properties against bleeding, and were sold as pharmaceuticals in powdered form (see human mummy confection and mummia).\nCharges of cannibalism were levied against the Qizilbash of the Safavid Ismail.\nThere is universal agreement that some Mesoamerican people practiced human sacrifice, but there is a lack of scholarly consensus as to whether cannibalism in pre-Columbian America was widespread. At one extreme, anthropologist Marvin Harris, author of \"Cannibals and Kings\", has suggested that the flesh of the victims was a part of an aristocratic diet as a reward, since the Aztec diet was lacking in proteins. While most historians of the pre-Columbian era believe that there was ritual cannibalism related to human sacrifices, they do not support Harris's thesis that human flesh was ever a significant portion of the Aztec diet. Others have hypothesized that cannibalism was part of a blood revenge in war.\nEarly modern and colonial era.\nThe Americas.\nEuropean explorers and colonizers brought home many stories of cannibalism practiced by the native peoples they encountered. In Spain's overseas expansion to the New World, the practice of cannibalism was reported by Christopher Columbus in the Caribbean islands, and the Caribs were greatly feared because of their supposed practice of it. Queen Isabel of Castile had forbidden the Spaniards to enslave the indigenous, unless they were \"guilty\" of cannibalism. The accusation of cannibalism became a pretext for attacks on indigenous groups and justification for the Spanish conquest. In Yucat\u00e1n, shipwrecked Spaniard Jer\u00f3nimo de Aguilar, who later became a translator for Hern\u00e1n Cort\u00e9s, reported to have witnessed fellow Spaniards sacrificed and eaten, but escaped from captivity where he was being fattened for sacrifice himself. In the Florentine Codex (1576) compiled by Franciscan Bernardino de Sahag\u00fan from information provided by indigenous eyewitnesses has questionable evidence of Mexica (Aztec) cannibalism. Franciscan friar Diego de Landa reported on Yucat\u00e1n instances.In early Brazil, there is reportage of cannibalism among the Tupinamba. It is recorded about the natives of the captaincy of Sergipe in Brazil: \"They eat human flesh when they can get it, and if a woman miscarries devour the abortive immediately. If she goes her time out, she herself cuts the navel-string with a shell, which she boils along with the secondine [i.e. placenta], and eats them both.\" (see human placentophagy).\nThe 1913 \"Handbook of Indians of Canada\" (reprinting 1907 material from the Bureau of American Ethnology), claims that North American natives practicing cannibalism included \"...\u00a0the Montagnais, and some of the tribes of Maine; the Algonkin, Armouchiquois, Iroquois, and Micmac; farther west the Assiniboine, Cree, Foxes, Chippewa, Miami, Ottawa, Kickapoo, Illinois, Sioux, and Winnebago; in the south the people who built the mounds in Florida, and the Tonkawa, Attacapa, Karankawa, Caddo, and Comanche; in the northwest and west, portions of the continent, the Thlingchadinneh and other Athapascan tribes, the Tlingit, Heiltsuk, Kwakiutl, Tsimshian, Nootka, Siksika, some of the Californian tribes, and the Ute. There is also a tradition of the practice among the Hopi, and mentions of the custom among other tribes of New Mexico and Arizona. The Mohawk, and the Attacapa, Tonkawa, and other Texas tribes were known to their neighbours as 'man-eaters.'\" The forms of cannibalism described included both resorting to human flesh during famines and ritual cannibalism, the latter usually consisting of eating a small portion of an enemy warrior. From another source, according to Hans Egede, when the Inuit killed a woman accused of witchcraft, they ate a portion of her heart.\nAs with most lurid tales of native cannibalism, these stories are treated with a great deal of scrutiny, as accusations of cannibalism were often used as justifications for the subjugation or destruction of \"savages\". The historian Patrick Brantlinger suggests that Indigenous peoples that were colonized were being dehumanized as part of the justification for the atrocities.\nPolynesia and Melanesia.\nThe very first encounter between Europeans and M\u0101ori may have involved cannibalism of a Dutch sailor. In June 1772, the French explorer Marion du Fresne and 26 members of his crew were killed and eaten in the Bay of Islands. In an 1809 incident known as the Boyd massacre, about 66 passengers and crew of the \"Boyd\" were killed and eaten by M\u0101ori on the Whangaroa peninsula, Northland. Cannibalism was already a regular practice in M\u0101ori wars. In another instance, on July 11, 1821, warriors from the Ngapuhi tribe killed 2,000 enemies and remained on the battlefield \"eating the vanquished until they were driven off by the smell of decaying bodies\". M\u0101ori warriors fighting the New Zealand government in Titokowaru's War in New Zealand's North Island in 1868\u201369 revived ancient rites of cannibalism as part of the radical Hauhau movement of the Pai Marire religion.\nIn parts of Melanesia, cannibalism was still practiced in the early 20th century, for a variety of reasons \u2013 including retaliation, to insult an enemy people, or to absorb the dead person's qualities. One tribal chief, Ratu Udre Udre in Rakiraki, Fiji, is said to have consumed 872 people and to have made a pile of stones to record his achievement. Fiji was nicknamed the \"Cannibal Isles\" by European sailors, who avoided disembarking there. The dense population of the Marquesas Islands, in what is now French Polynesia, was concentrated in narrow valleys, and consisted of warring tribes, who sometimes practiced cannibalism on their enemies. Human flesh was called \"long pig\". W. D. Rubinstein wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\nAmong settlers, sailors, and explorers.\nThis period of time was also rife with instances of explorers and seafarers resorting to cannibalism for survival. There is archeological and written evidence for English settlers' cannibalism in 1609 in the Jamestown Colony under famine conditions, during a period which became known as Starving Time.\nSailors shipwrecked or lost at sea repeatedly resorted to cannibalism to face off starvation. The survivors of the sinking of the French ship \"M\u00e9duse\" in 1816 resorted to cannibalism after four days adrift on a raft. Their plight was made famous by Th\u00e9odore G\u00e9ricault's painting \"Raft of the Medusa\". After a whale sank the \"Essex\" of Nantucket on 20 November 1820 the survivors, in three small boats, resorted, by common consent, to cannibalism in order for some to survive. This event became an important source of inspiration for Herman Melville's \"Moby-Dick\".\nThe case of \"R v Dudley and Stephens\" (1884) is an English criminal case which dealt with four crew members of an English yacht, the \"Mignonette\", who were cast away in a storm some from the Cape of Good Hope. After several days, one of the crew, a seventeen-year-old cabin boy, fell unconscious due to a combination of the famine and drinking seawater. The others (one possibly objecting) decided to kill him and eat him. They were picked up four days later. Two of the three survivors were found guilty of murder. A significant outcome of this case was that necessity in English criminal law was determined to be no defence against a charge of murder. This was a break with the traditional understanding among sailors, which had been that selecting a victim for killing and consumption was acceptable in a starvation situation as long as lots were drawn so that all faced an equal risk of being killed.\nOn land, travelers through sparsely inhabited regions and explorers of unknown areas sometimes ate human flesh after running out of other provisions. In a famous example from the 1840s, the members of Donner Party found themselves stranded by snow in the Donner Pass, a high mountain pass in California, without adequate supplies during the Mexican\u2013American War, leading to several instances of cannibalism, including the murder of two young Native American men for food. Sir John Franklin's lost polar expedition, which took place at approximately the same time, is another example of cannibalism out of desperation.\nIn frontier situations where there was no strong authority, some individuals got used to killing and eating others even in situations where other food would have been available. One notorious case was the mountain man Boone Helm, who become known as \"The Kentucky Cannibal\" for eating several of his fellow travelers, from 1850 until his eventual hanging in 1864.\nCentral and West Africa.\nRoger Casement, writing to a consular colleague in Lisbon on August 3, 1903, from Lake Mantumba in the Congo Free State, said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;\"The people round here are all cannibals. You never saw such a weird looking lot in your life. There are also dwarfs (called Batwas) in the forest who are even worse cannibals than the taller human environment. They eat man flesh raw! It's a fact.\" Casement then added how assailants would \"bring down a dwarf on the way home, for the marital cooking pot\u00a0... The Dwarfs, as I say, dispense with cooking pots and eat and drink their human prey fresh cut on the battlefield while the blood is still warm and running. These are not fairy tales, my dear Cowper, but actual gruesome reality in the heart of this poor, benighted savage land.\"\nDuring the 1892\u20131894 war between the Congo Free State and the Swahili\u2013Arab city-states of Nyangwe and Kasongo in Eastern Congo, there were reports of widespread cannibalization of the bodies of defeated Arab combatants by the Batetela allies of Belgian commander Francis Dhanis. The Batetela, \"like most of their neighbors were inveterate cannibals.\" According to Dhanis's medical officer, Captain Hinde, their town of Ngandu had \"at least 2,000 polished human skulls\" as a \"solid white pavement in front\" of its gates, with human skulls crowning every post of the stockade.\nIn April 1892, 10,000 of the Batetela, under the command of Gongo Lutete, joined forces with Dhanis in a campaign against the Swahili\u2013Arab leaders Sefu and Mohara. After one early skirmish in the campaign, Hinde \"noticed that the bodies of both the killed and wounded had vanished.\" When fighting broke out again, Hinde saw his Batetela allies drop human arms, legs and heads on the road. One young Belgian officer wrote home: \"Happily Gongo's men ate them up [in a few hours]. It's horrible but exceedingly useful and hygienic\u00a0... I should have been horrified at the idea in Europe! But it seems quite natural to me here. Don't show this letter to anyone indiscreet.\" After the massacre at Nyangwe, Lutete \"hid himself in his quarters, appalled by the sight of thousands of men smoking human hands and human chops on their camp fires, enough to feed his army for many days.\"\nIn West Africa, the Leopard Society was a cannibalistic secret society that existed until the mid-1900s. Centered in Sierra Leone, Liberia and Ivory Coast, the \"Leopard men\" would dress in leopard skins, and waylay travelers with sharp claw-like weapons in the form of leopards' claws and teeth. The victims' flesh would be cut from their bodies and distributed to members of the society.\nEarly 20th century to present.\nAfter World War I, cannibalism continued to occur as a ritual practice and in times of drought or famine. Occasional cannibal acts committed by individual criminals are documented as well throughout the 20th and 21st centuries.\nWorld War II.\nMany instances of cannibalism by necessity were recorded during World War II. For example, during the 872-day siege of Leningrad, reports of cannibalism began to appear in the winter of 1941\u20131942, after all birds, rats, and pets were eaten by survivors. Leningrad police even formed a special division to combat cannibalism.\nSome 2.8 million Soviet POWs died in Nazi custody in less than eight months during 1941\u201342. According to the USHMM, by the winter of 1941, \"starvation and disease resulted in mass death of unimaginable proportions\". This deliberate starvation led to many incidents of cannibalism.\nFollowing the Soviet victory at Stalingrad it was found that some German soldiers in the besieged city, cut off from supplies, resorted to cannibalism. Later, following the German surrender in January 1943, roughly 100,000 German soldiers were taken prisoner of war (POW). Almost all of them were sent to POW camps in Siberia or Central Asia where, due to being chronically underfed by their Soviet captors, many resorted to cannibalism. Fewer than 5,000 of the prisoners taken at Stalingrad survived captivity.\nCannibalism took place in the concentration and death camps in the Independent State of Croatia (NDH), a Nazi German puppet state which was governed by the fascist Ustasha organization, who committed the Genocide of Serbs and the Holocaust in NDH. Some survivors testified that some of the Ustashas drank the blood from the slashed throats of the victims.\nThe Australian War Crimes Section of the Tokyo tribunal, led by prosecutor William Webb (the future Judge-in-Chief), collected numerous written reports and testimonies that documented Japanese soldiers' acts of cannibalism among their own troops, on enemy dead, as well as on Allied prisoners of war in many parts of the Greater East Asia Co-Prosperity Sphere. In September 1942, Japanese daily rations on New Guinea consisted of 800\u00a0grams of rice and tinned meat. However, by December, this had fallen to 50\u00a0grams. According to historian Yuki Tanaka, \"cannibalism was often a systematic activity conducted by whole squads and under the command of officers\".\nIn some cases, flesh was cut from living people. A prisoner of war from the British Indian Army, Lance Naik Hatam Ali, testified that in New Guinea: \"the Japanese started selecting prisoners and every day one prisoner was taken out and killed and eaten by the soldiers. I personally saw this happen and about 100 prisoners were eaten at this place by the Japanese. The remainder of us were taken to another spot away where 10 prisoners died of sickness. At this place, the Japanese again started selecting prisoners to eat. Those selected were taken to a hut where their flesh was cut from their bodies while they were alive and they were thrown into a ditch where they later died.\"\nAnother well-documented case occurred in Chichi-jima in February 1945, when Japanese soldiers killed and consumed five American airmen. This case was investigated in 1947 in a war crimes trial, and of 30 Japanese soldiers prosecuted, five (Maj. Matoba, Gen. Tachibana, Adm. Mori, Capt. Yoshii, and Dr. Teraki) were found guilty and hanged. In his book \"\", James Bradley details several instances of cannibalism of World War II Allied prisoners by their Japanese captors. The author claims that this included not only ritual cannibalization of the livers of freshly killed prisoners, but also the cannibalization-for-sustenance of living prisoners over the course of several days, amputating limbs only as needed to keep the meat fresh.\nThere are more than 100 documented cases in Australia's government archives of Japanese soldiers practicing cannibalism on enemy soldiers and civilians in New Guinea during the war. For instance, from an archived case, an Australian lieutenant describes how he discovered a scene with cannibalized bodies, including one \"consisting only of a head which had been scalped and a spinal column\" and that \"[i]n all cases, the condition of the remains were such that there can be no doubt that the bodies had been dismembered and portions of the flesh cooked\". In another archived case, a Pakistani corporal (who was captured in Singapore and transported to New Guinea by the Japanese) testified that Japanese soldiers cannibalized a prisoner (some were still alive) per day for about 100 days. There was also an archived memo, in which a Japanese general stated that eating anyone except enemy soldiers was punishable by death. Toshiyuki Tanaka, a Japanese scholar in Australia, mentions that it was done \"to consolidate the group feeling of the troops\" rather than due to food shortage in many of the cases. Tanaka also states that the Japanese committed the cannibalism under supervision of their senior officers and to serve as a power projection tool.\nJemadar Abdul Latif (VCO of the 4/9 Jat Regiment of the British Indian Army and POW rescued by the Australians at Sepik Bay in 1945) stated that the Japanese soldiers ate both Indian POWs and local New Guinean people. At the camp for Indian POWs in Wewak, where many died and 19 POWs were eaten, the Japanese doctor and lieutenant Tumisa would send an Indian out of the camp after which a Japanese party would kill and eat flesh from the body as well as cut off and cook certain body parts (liver, buttock muscles, thighs, legs, and arms), according to Captain R. U. Pirzai in a \"The Courier-Mail\" report of 25 August 1945.\nSouth America.\nWhen Uruguayan Air Force Flight 571 crashed on a glacier in the Andes on October 13, 1972, many survivors resorted to eating the deceased during their 72 days in the mountains. The experiences and memories of the survivors became the source of several books and films. In an account of the accident and aftermath, survivor Roberto Canessa described the decision to eat the pilots and their dead friends and family members:\nNorth America.\nIn 1991, Jeffrey Dahmer of Milwaukee, Wisconsin, was arrested after one of his intended victims managed to escape. Found in Dahmer's apartment were two human hearts, an entire torso, a bag full of human organs from his victims, and a portion of arm muscle. He stated that he planned to consume all of the body parts over the next few weeks.\nWest Africa.\nIn the 1980s, M\u00e9decins Sans Fronti\u00e8res, the international medical charity, supplied photographic and other documentary evidence of ritualized cannibal feasts among the participants in Liberia's internecine strife preceding the First Liberian Civil War to representatives of Amnesty International. Amnesty International declined to publicize this material; the Secretary-General of the organization, Pierre Sane, said at the time in an internal communication that \"what they do with the bodies after human rights violations are committed is not part of our mandate or concern\". The existence of cannibalism on a wide scale in Liberia was subsequently verified.\nA few years later, reported of cannibal acts committed during the Second Liberian Civil War and Sierra Leone Civil War emerged.\nCentral Africa.\nReports from the Belgian Congo indicate that cannibalism was still widely practiced in some regions in the 1920s. Hermann Norden, an American who visited the Kasai region in 1923, found that \"cannibalism was commonplace\". People were afraid of walking outside of populated places because there was a risk of being attacked, killed, and eaten. Norden talked with a Belgian who \"admitted that it was quite likely he had occasionally been served human flesh without knowing what he was eating\" \u2013 it was simply a dish that appeared on the tables from time.\nOther travelers heard persistent rumors that there was still a certain underground trade in slaves, some of whom (adults and children alike) were regularly killed and then \"cut up and cooked as ordinary meat\", around both the Kasai and the Ubangi River. The colonial state seems to have done little to discourage or punish such acts. There are also reports that human flesh was sometimes sold at markets in both Kinshasa and Brazzaville, \"right in the middle of European life.\"\nNorden observed that cannibalism was so common that people talked about it quite \"casual[ly]\": \"No stress was put upon it, nor horror shown. This person had died of fever; that one had been eaten. It was all a matter of the way one's luck held.\"\nEven after World War II, human flesh still occasionally appeared on the tables. In 1950, a Belgian administrator ate a \"remarkably delicious\" dish, learning after he had finished \"that the meat came from a young girl.\" A few years later, a Danish traveler was served a piece of the \"soft and tender\" flesh of a butchered woman.\nDuring the Congo Crisis, which followed the country's independence in 1960, body parts of killed enemies were eaten and the flesh of war victims was sometimes sold for consumption. In Luluabourg (today Kananga), an American journalist saw a truck smeared with blood. A police commissioner investigating the scene told her that \"[s]ixteen women and children\" had been lured in a nearby village to enter the truck, kidnapped, and \"butchered ... for meat.\" She also talked with a Presbyterian missionary, who excused this act as due to \"protein need... The bodies of their enemies are the only source of protein available.\"\nIn conflict situations, cannibalism persisted into the 21st century. During the first decade of the new century, cannibal acts have been reported from the Second Congo War and the Ituri conflict in the northeast of the Democratic Republic of the Congo. According to UN investigators, fighters belonging to several factions \"grilled\" human bodies \"on a barbecue\"; young girls were boiled \"alive in ... big pots filled with boiling water and oil\" or \"cut into small pieces ... and then eaten.\"\nA UN human rights expert reported in July 2007 that sexual atrocities committed by rebel groups as well as by armed forces and national police against Congolese women go \"far beyond rape\" and include sexual slavery, forced incest, and cannibalism. In the Ituri region, much of the violence, which included \"widespread cannibalism\", was consciously directed against pygmies, who were believed to be relatively helpless and even considered subhuman by some other Congolese.\nUN investigators also collected eyewitness accounts of cannibalism during a violent conflict that shook the Kasai region in 2016/2017. Various parts of killed enemies and beheaded captives were cooked and eaten, including their heads, thighs, and penises.\nCannibalism has also been reported from the Central African Republic, north of the Congo Basin. Jean-B\u00e9del Bokassa ruled the country from 1966 to 1979 as dictator and finally as self-declared emperor. Tenacious rumors that he liked to dine on the flesh of opponents and political prisoners were substantiated by several testimonies during his eventual trial in 1986/1987. Bokassa's successor David Dacko stated that he had seen photographs of butchered bodies hanging in the cold-storage rooms of Bokassa's palace immediately after taking power in 1979. These or similar photos, said to show a walk-in freezer containing the bodies of schoolchildren arrested in April 1979 during protests and beat to death in the 1979 Ngaragba Prison massacre, were also published in Paris Match magazine. During the trial, Bokassa's former chef testified that he had repeatedly cooked human flesh from the palace's freezers for his boss's table. While Bokassa was found guilty of murder in at least twenty cases, the charge of cannibalism was nevertheless not taken into account for the final verdict, since the consumption of human remains is considered a misdemeanor under CAR law and all previously committed misdemeanors had been forgiven by a general amnesty declared in 1981.\nFurther acts of cannibalism were reported to have targeted the Muslim minority during the Central African Republic Civil War which started in 2012.\nEast Africa.\nIn the 1970s the Ugandan dictator Idi Amin was reputed to practice cannibalism. More recently, the Lord's Resistance Army has been accused of routinely engaging in ritual or magical cannibalism. It is also reported by some that witch doctors in the country sometimes use the body parts of children in their medicine.\nDuring the South Sudanese Civil War, cannibalism and forced cannibalism have been reported from South Sudan.\nCentral and Western Europe.\nBefore 1931, \"The New York Times\" reporter William Seabrook, apparently disappointed that he had been unable to taste human flesh in West Africa, obtained from a hospital intern at the Sorbonne a chunk of this meat from the body of a healthy man killed in an accident, then cooked and ate it. He reported,\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It was like good, fully developed veal, not young, but not yet beef. It was very definitely like that, and it was not like any other meat I had ever tasted. It was so nearly like good, fully developed veal that I think no person with a palate of ordinary, normal sensitiveness could distinguish it from veal. It was mild, good meat with no other sharply defined or highly characteristic taste such as for instance, goat, high game, and pork have. The steak was slightly tougher than prime veal, a little stringy, but not too tough or stringy to be agreeably edible. The roast, from which I cut and ate a central slice, was tender, and in color, texture, smell as well as taste, strengthened my certainty that of all the meats we habitually know, veal is the one meat to which this meat is accurately comparable.\nKarl Denke, possible Carl Gro\u00dfmann and Fritz Haarmann, as well as Joachim Kroll were German murderers and cannibals active between the early 20th century and the 1970s. Armin Meiwes is a former computer repair technician who achieved international notoriety for killing and eating a voluntary victim in 2001, whom he had found via the Internet. After Meiwes and the victim jointly attempted to eat the victim's severed penis, Meiwes killed his victim and proceeded to eat a large amount of his flesh. He was arrested in December 2002. In January 2004, Meiwes was convicted of manslaughter and sentenced to eight years and six months in prison. Despite the victim's undisputed consent, the prosecutors successfully appealed this decision, and in a retrial that ended in May 2006, Meiwes was convicted of murder and sentenced to life imprisonment.\nOn July 23, 1988, Rick Gibson ate the flesh of another person in public. Because England does not have a specific law against cannibalism, he legally ate a canap\u00e9 of donated human tonsils in Walthamstow High Street, London. A year later, on April 15, 1989, he publicly ate a slice of human testicle in Lewisham High Street, London. When he tried to eat another slice of human testicle at the Pitt International Galleries in Vancouver on July 14, 1989, the Vancouver police confiscated the testicle hors d'\u0153uvre. However, the charge of publicly exhibiting a disgusting object was dropped, and he finally ate the piece of human testicle on the steps of the Vancouver court house on September 22, 1989.\nIn 2008, a British model called Anthony Morley was imprisoned for the killing, dismemberment and partial cannibalisation of his lover, magazine executive Damian Oldfield. In 1996, Morley was a contestant on the television programme \"God's Gift\"; one of the audience members of that edition was Damian Oldfield. Oldfield was a contestant of another edition of the show in October 1996. On 2 May 2008, it was announced that Morley had been arrested for the murder of Oldfield, who worked for the gay lifestyle magazine \"Bent\". After inviting Oldfield into his Leeds flat, police believed that Morley killed him, removed a section of his leg and began cooking it, before he stumbled into a nearby kebab house around 2:30 in the morning, drenched in blood and asking that someone call the police. He was found guilty on 17 October 2008 and sentenced to life imprisonment for the crime.\nEastern Europe and the Soviet Union.\nIn his book, \"The Gulag Archipelago\", Soviet writer Aleksandr Solzhenitsyn described cases of cannibalism in 20th-century Soviet Union. Of the famine in Povolzhie (1921\u20131922) he wrote: \"That horrible famine was up to cannibalism, up to consuming children by their own parents\u00a0\u2013 the famine, which Russia had never known even in the Time of Troubles [in 1601\u20131603]\".\nThe historian Orlando Figes observes that \"thousands of cases\" of cannibalism were reported, while the number of cases that were never reported was doubtless even higher. In Pugachyov, \"it was dangerous for children to go out after dark since there were known to be bands of cannibals and traders who killed them to eat or sell their tender flesh.\" An inhabitant of a nearby village stated: \"There are several cafeterias in the village \u2013 and all of them serve up young children.\" This was no exception \u2013 Figes estimates \"that a considerable proportion of the meat in Soviet factories in the Volga area\u00a0... was human flesh.\" Various gangs specialized in \"capturing children, murdering them and selling the human flesh as horse meat or beef\", with the buyers happy to have found a source of meat in a situation of extreme shortage and often willing not to \"ask too many questions\".\nCannibalism was also widespread during the Holodomor, a man-made famine in Soviet Ukraine between 1932 and 1933.\nSurvival was a moral as well as a physical struggle. A woman doctor wrote to a friend in June 1933 that she had not yet become a cannibal, but was \"not sure that I shall not be one by the time my letter reaches you\". The good people died first. Those who refused to steal or to prostitute themselves died. Those who gave food to others died. Those who refused to eat corpses died. Those who refused to kill their fellow man died.\u00a0... At least 2,505 people were sentenced for cannibalism in the years 1932 and 1933 in Ukraine, though the actual number of cases was certainly much higher.\nMost cases of cannibalism were \"necrophagy, the consumption of corpses of people who had died of starvation\". But the murder of children for food was common as well. Many survivors told of neighbors who had killed and eaten their own children. One woman, asked why she had done this, \"answered that her children would not survive anyway, but this way she would\". She was arrested by the police. The police also documented cases of children being kidnapped, killed, and eaten, and \"stories of children being hunted down as food\" circulated in many areas. A man who lived through the famine in his youth later remembered that \"the availability of human flesh at market[s] was an open and acknowledged secret. People were glad\" if they could buy it since \"[t]here was no other means to survive.\"\nIn March 1933 the secret police in Kyiv province collected \"ten or more reports of cannibalism every day\" but concluded that \"in reality there are many more such incidents\", most of which went unreported. Those found guilty of cannibalism were often \"imprisoned, executed, or lynched\". But while the authorities were well informed about the extent of cannibalism, they also tried to suppress this information from becoming widely known, the chief of the secret police warning \"that written notes on the subject do not circulate among the officials where they might cause rumours\".\nThe Holodomor was part of the Soviet famine of 1930\u20131933, which devastated also other parts of the Soviet Union in the early 1930s. Multiple cases of cannibalism were also reported from Kazakhstan.\nA few years later, starving people again resorted to cannibalism during the siege of Leningrad (1941\u20131944). About this time, Solzhenitsyn writes: \"Those who consumed human flesh, or dealt with the human liver trading from dissecting rooms\u00a0... were accounted as the political criminals\".\nOf the building of Northern Railway Labor Camp (\"Sevzheldorlag\") Solzhenitsyn reports, \"An ordinary hard working political prisoner almost could not survive at that penal camp. In the camp Sevzheldorlag (chief: colonel Klyuchkin) in 1946\u201347 there were many cases of cannibalism: they cut human bodies, cooked and ate.\"\nThe Soviet journalist Yevgenia Ginzburg was a long-term political prisoner who spent time in the Soviet prisons, Gulag camps and settlements from 1938 to 1955. She described in her memoir, \"Harsh Route\" (or \"Steep Route\"), of a case which she was directly involved in during the late 1940s, after she had been moved to the prisoners' hospital.\nThe chief warder shows me the black smoked pot, filled with some food: \"I need your medical expertise regarding this meat.\" I look into the pot, and hardly hold vomiting. The fibres of that meat are very small, and don't resemble me anything I have seen before. The skin on some pieces bristles with black hair\u00a0... A former smith from Poltava, Kulesh worked together with Centurashvili. At this time, Centurashvili was only one month away from being discharged from the camp\u00a0... And suddenly he surprisingly disappeared\u00a0... The wardens searched for two more days, and then assumed that it was an escape case, though they wondered why, since his imprisonment period was almost over\u00a0... The crime was there. Approaching the fireplace, Kulesh killed Centurashvili with an axe, burned his clothes, then dismembered him and hid the pieces in snow, in different places, putting specific marks on each burial place.\u00a0... Just yesterday, one body part was found under two crossed logs.\nIndia.\nThe Aghori are Indian ascetics who believe that eating human flesh confers spiritual and physical benefits, such as prevention of aging. They claim to only eat those who have voluntarily granted their body to the sect upon their death, but an Indian TV crew witnessed one Aghori feasting on a corpse discovered floating in the Ganges and a member of the Dom caste reports that Aghori often take bodies from cremation \"ghats\" (or funeral pyres).\nChina.\nCannibalism is documented to have occurred in rural China during the severe famine that resulted from the Great Leap Forward (1958\u20131962).\nDuring Mao Zedong's Cultural Revolution (1966\u20131976), local governments' documents revealed hundreds of incidents of cannibalism for ideological reasons, including large-scale cannibalism during the Guangxi Massacre. Cannibal acts occurred at public events organised by local Communist Party officials, with people taking part in them in order to prove their revolutionary passion. The writer Zheng Yi documented many of these incidents, especially those in Guangxi, in his 1993 book, \"Scarlet Memorial\".\nPills made of human flesh were said to be used by some Tibetan Buddhists, motivated by a belief that mystical powers were bestowed upon those who consumed Brahmin flesh.\nSoutheast Asia.\nIn Joshua Oppenheimer's film \"The Look of Silence\", several of the anti-Communist militias active in the Indonesian mass killings of 1965\u201366 claim that drinking blood from their victims prevented them from going mad.\nEast Asia.\nReports of widespread cannibalism began to emerge from North Korea during the famine of the 1990s and subsequent ongoing starvation. Kim Jong-il was reported to have ordered a crackdown on cannibalism in 1996, but Chinese travelers reported in 1998 that cannibalism had occurred. Three people in North Korea were reported to have been executed for selling or eating human flesh in 2006. Further reports of cannibalism emerged in early 2013, including reports of a man executed for killing his two children for food.\nThere are competing claims about how widespread cannibalism was in North Korea. While refugees reported that it was widespread, Barbara Demick wrote in her book, \"\" (2010), that it did not seem to be.\nMelanesia.\nThe Korowai tribe of south-eastern Papua could be one of the last surviving tribes in the world engaging in cannibalism. A local cannibal cult killed and ate victims as late as 2012.\nAs in some other Papuan societies, the Urapmin people engaged in cannibalism in war. Notably, the Urapmin also had a system of food taboos wherein dogs could not be eaten and they had to be kept from breathing on food, unlike humans who could be eaten and with whom food could be shared.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "5659", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=5659", "title": "Chemical element", "text": "Species of atoms with a specific number of protons\nA chemical element is a chemical substance that cannot be broken down into other substances. The basic particle that constitutes a chemical element is the atom, and each chemical element is distinguished by the number of protons in the nuclei of its atoms, known as its atomic number. For example, oxygen has an atomic number of 8, meaning that each oxygen atom has 8 protons in its nucleus. This is in contrast to chemical compounds and mixtures, which contain atoms with more than one atomic number.\nAlmost all of the baryonic matter of the universe is composed of chemical elements (among rare exceptions are neutron stars). When different elements undergo chemical reactions, atoms are rearranged into new compounds held together by chemical bonds. Only a minority of elements, such as silver and gold, are found uncombined as relatively pure native element minerals. Nearly all other naturally occurring elements occur in the Earth as compounds or mixtures. Air is primarily a mixture of the elements nitrogen, oxygen, and argon, though it does contain compounds including carbon dioxide and water.\nThe history of the discovery and use of the elements began with primitive human societies that discovered native minerals like carbon, sulfur, copper and gold (though the concept of a chemical element was not yet understood). Attempts to classify materials such as these resulted in the concepts of classical elements, alchemy, and various similar theories throughout human history. Much of the modern understanding of elements developed from the work of Dmitri Mendeleev, a Russian chemist who published the first recognizable periodic table in 1869. This table organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The periodic table summarizes various properties of the elements, allowing chemists to derive relationships between them and to make predictions about compounds and potential new ones.\nBy November 2016, the International Union of Pure and Applied Chemistry had recognized a total of 118 elements. The first 94 occur naturally on Earth, and the remaining 24 are synthetic elements produced in nuclear reactions. Save for unstable radioactive elements (radionuclides) which decay quickly, nearly all of the elements are available industrially in varying amounts. The discovery and synthesis of further new elements is an ongoing area of scientific study.\nDescription.\nThe lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.\nOf the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy metals before the formation of our Solar System. At over 1.9\u00d71019 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element, and is almost always considered on par with the 80 stable elements. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they are not found in nature and must be synthesized.\nThere are now 118 known elements. In this context, \"known\" means observed well enough, even from just a few decay products, to have been differentiated from other elements. Most recently, the synthesis of element 118 (since named oganesson) was reported in October 2006, and the synthesis of element 117 (tennessine) was reported in April 2010. Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the Solar System, or as naturally occurring fission or transmutation products of uranium and thorium.\nThe remaining 24 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally occurring rare elements.\nList of the elements are available by name, atomic number, density, melting point, boiling point and by symbol, as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).\nAtomic number.\nThe atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.\nThe number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.\nThe symbol for atomic number is \"Z\".\nIsotopes.\nIsotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having \"different\" numbers of neutrons. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to 12C, 13C, and 14C. Carbon in everyday life and in chemistry is a mixture of 12C (about 98.9%), 13C (about 1.1%) and about 1 atom per trillion of 14C.\nMost (66 of 94) naturally occurring elements have more than one stable isotope. Except for the isotopes of hydrogen (which differ greatly from each other in relative mass\u2014enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.\nAll of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed \"stable\" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed \"only\" of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.\nOf the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).\nIsotopic mass and atomic mass.\nThe mass number of an element, \"A\", is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g. 238U). The mass number is always a whole number and has units of \"nucleons\". For example, magnesium-24 (24 is the mass number) is an atom with 24 nucleons (12 protons and 12 neutrons).\nWhereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number giving the mass of a particular isotope (or \"nuclide\") of the element, expressed in atomic mass units (symbol: u). In general, the mass number of a given nuclide differs in value slightly from its atomic mass, since the mass of each proton and neutron is not exactly 1\u00a0u; since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number; and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969\u00a0u and that of chlorine-37 is 36.966\u00a0u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is 12C, which by definition has a mass of exactly 12 because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.\nThe standard atomic weight (commonly called \"atomic weight\") of an element is the \"average\" of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit. This number may be a fraction that is \"not\" close to a whole number. For example, the relative atomic mass of chlorine is 35.453\u00a0u, which differs greatly from a whole number as it is an average of about 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect, as significant amounts of more than one isotope are naturally present in a sample of that element.\nChemically pure and isotopically pure.\nChemists and nuclear scientists have different definitions of a \"pure element\". In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.\nFor example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% 63Cu and 31% 65Cu, with different numbers of neutrons. However, a pure gold ingot would be both chemically and isotopically pure, since ordinary gold consists only of one isotope, 197Au.\nAllotropes.\nAtoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple chemical structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.\nThe reference state of an element is defined by convention, usually as the thermodynamically most stable allotrope and physical state at a pressure of 1 bar and a given temperature (typically at 298.15K). However, for phosphorus, the reference state is white phosphorus even though it is not the most stable allotrope. In thermochemistry, an element is defined to have an enthalpy of formation of zero in its reference state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.\nProperties.\nSeveral kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.\nGeneral properties.\nSeveral terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the \"metalloids\"), having intermediate properties and often behaving as semiconductors.\nA more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms \"metal\" and \"nonmetal\" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals, metalloids, reactive nonmetals, and noble gases. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the reactive nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.\nStates of matter.\nAnother commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4\u00a0\u00b0C (83.2\u00a0\u00b0F) and 29.8\u00a0\u00b0C (85.6\u00a0\u00b0F), respectively.\nMelting and boiling points.\nMelting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.\nDensities.\nThe density at selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm3). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.\nWhen an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8\u20132.1, 2.267, and 3.515\u00a0g/cm3, respectively.\nCrystal structures.\nThe elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.\nOccurrence and origin on Earth.\nChemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of human-made nuclear reactions.\nOf the 94 naturally occurring elements, 83 are considered primordial and either stable or weakly radioactive. The remaining 11 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. Of these 11 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium and uranium. The remaining 6 transient elements (technetium, promethium, astatine, francium, neptunium, and plutonium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.\nNo radioactive decay has been observed for elements with atomic numbers 1 through 82, except 43 (technetium) and 61 (promethium). Observationally stable isotopes of some elements (such as tungsten and lead), however, are predicted to be slightly radioactive with very long half-lives: for example, the half-lives predicted for the observationally stable lead isotopes range from 1035 to 10189 years. Elements with atomic numbers 43, 61, and 83 through 94 are unstable enough that their radioactive decay can readily be detected. Three of these elements, bismuth (element 83), thorium (element 90), and uranium (element 92) have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of the Solar System. For example, at over 1.9\u00d71019 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest 24 elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and cannot be produced as daughters of longer-lived elements, and thus are not known to occur in nature at all.\nPeriodic table.\nPrimordial\u2003 From decay\u2003 Synthetic\u2003Border shows natural occurrence of the element \nStandard atomic weight \"A\"r, std(E)&lt;templatestyles src=\"Plainlist/styles.css\"/&gt; \nThe properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The current standard table contains 118 confirmed elements as of 2021.\nAlthough earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.\nUse of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.\nNomenclature and symbols.\nThe various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.\nAtomic numbers.\nThe known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as \"through\", \"beyond\", or \"from ... through\", as in \"through iron\", \"beyond uranium\", or \"from lanthanum through lutetium\". The terms \"light\" and \"heavy\" are sometimes also used informally to indicate relative atomic numbers (not densities), as in \"lighter than carbon\" or \"heavier than lead\", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.\nElement names.\nThe naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use \"Wasserstoff\" (water substance) for \"hydrogen\", \"Sauerstoff\" (acid substance) for \"oxygen\" and \"Stickstoff\" (smothering substance) for \"nitrogen\", while English and some romance languages use \"sodium\" for \"natrium\" and \"potassium\" for \"kalium\", and the French, Italians, Greeks, Portuguese and Poles prefer \"azote/azot/azoto\" (from roots meaning \"no life\") for \"nitrogen\".\nFor purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting \"gold\" rather than \"aurum\" as the name for the 79th element (Au). IUPAC prefers the British spellings \"aluminium\" and \"caesium\" over the U.S. spellings \"aluminum\" and \"cesium\", and the U.S. \"sulfur\" over the British \"sulphur\". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.\nAccording to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, \"e.g.,\" carbon-12 or uranium-235. Chemical element \"symbols\" (such as Cf for californium and Es for einsteinium), are always capitalized (see below).\nIn the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy).\nPrecursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, \"lutetium\" was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it \"cassiopeium\". Similarly, the British discoverer of \"niobium\" originally named it \"columbium,\" in reference to the New World. It was used extensively as such by American publications before the international standardization (in 1950).\nChemical symbols.\nSpecific chemical elements.\nBefore chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.\nThe current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations\u2014though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.\nThe first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from cuprum, Fe comes from ferrum, Ag from argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin \"natrium\". The same applies to \"Fe\" (ferrum) for iron, \"Hg\" (hydrargyrum) for mercury, \"Sn\" (stannum) for tin, \"Au\" (aurum) for gold, \"Ag\" (argentum) for silver, \"Pb\" (plumbum) for lead, \"Cu\" (cuprum) for copper, and \"Sb\" (stibium) for antimony. \"W\" (wolfram) for tungsten ultimately derives from German, \"K\" (kalium) for potassium ultimately from Arabic.\nChemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used \"J\" (for the alternate name Jod) for iodine, but now use \"I\" and \"Iod\".\nThe first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium and einsteinium are Cf and Es.\nGeneral chemical symbols.\nThere are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an \"X\" indicates a variable group (usually a halogen) in a class of compounds, while \"R\" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter \"Q\" is reserved for \"heat\" in a chemical reaction. \"Y\" is also often used as a general chemical symbol, although it is also the symbol of yttrium. \"Z\" is also frequently used as a general variable group. \"E\" is used in organic chemistry to denote an electron-withdrawing group or an electrophile; similarly \"Nu\" denotes a nucleophile. \"L\" is used to represent a general ligand in inorganic and organometallic chemistry. \"M\" is also often used in place of a general metal.\nAt least two additional, two-letter generic chemical symbols are also in informal usage, \"Ln\" for any lanthanide element and \"An\" for any actinide element. \"Rg\" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol \"Rg\" has now been assigned to the element roentgenium.\nIsotope symbols.\nIsotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example 12C and 235U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.\nAs a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for 1H (protium), D for 2H (deuterium), and T for 3H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written D2O instead of 2H2O.\nOrigin of the elements.\nOnly about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even less well understood dark energy).\nThe 94 naturally occurring chemical elements were produced by at least four classes of astrophysical process. Most of the hydrogen, helium and a very small quantity of lithium were produced in the first few minutes of the Big Bang. This Big Bang nucleosynthesis happened only once; the other processes are ongoing. Nuclear fusion inside stars produces elements through stellar nucleosynthesis, including all elements from carbon to iron in atomic number. Elements higher in atomic number than iron, including heavy elements like uranium and plutonium, are produced by various forms of explosive nucleosynthesis in supernovae and neutron star mergers. The light elements lithium, beryllium and boron are produced mostly through cosmic ray spallation (fragmentation induced by cosmic rays) of carbon, nitrogen, and oxygen.\nDuring the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, 1H) and helium-4 (4He), as well as a smaller amount of deuterium (2H) and very minuscule amounts (on the order of 10\u221210) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. No elements heavier than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% 1H, 25% 4He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.\nOn Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of nuclear transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (14C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (40Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (40K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by nuclear fission of the nuclei of various heavy elements or in other rare nuclear processes.\nIn addition to the 94 naturally occurring elements, several artificial elements have been produced by human nuclear physics technology. As of 2021[ [update]], these experiments have produced all elements up to atomic number 118.\nAbundance.\nThe following graph (note log scale) shows the abundance of elements in our Solar System. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.\nThe abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.\nThe abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar System (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminium at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminium (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.\nThe composition of the human body, by contrast, more closely follows the composition of seawater\u2014save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.\nHistory.\nEvolving definitions.\nThe concept of an \"element\" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.\nClassical definitions.\nAncient philosophy posited a set of classical elements to explain observed patterns in nature. These \"elements\" originally referred to \"earth\", \"water\", \"air\" and \"fire\" rather than the chemical elements of modern science.\nThe term 'elements' (\"stoicheia\") was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).\nAristotle, c. 350 BCE, also used the term \"stoicheia\" and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Element \u2013 one of those bodies into which other bodies can decompose, and that itself is not capable of being divided into other.\nChemical definitions.\nIn 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 \"Elements of Chemistry\", which contained thirty-three elements, including light and caloric. By 1818, J\u00f6ns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.\nFrom Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.\nAtomic definitions.\nThe 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10\u221214 seconds it takes the nucleus to form an electronic cloud.\nBy 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner.\nDiscovery and recognition of various elements.\nTen materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.\nMost of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:\nElements isolated or produced since 1900 include:\nRecently discovered elements.\nThe first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999, claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of January 2016, all 118 elements have been confirmed by IUPAC as being discovered. The discovery of element 112 was acknowledged in 2009, and the name \"copernicium\" and the atomic symbol \"Cn\" were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, oganesson, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Tennessine, element 117 was the latest element claimed to be discovered, in 2009. On 28 November 2016, scientists at the IUPAC officially recognized the names for four of the newest chemical elements, with atomic numbers 113, 115, 117, and 118.\nList of the 118 known chemical elements.\nThe following sortable table shows the 118 known chemical elements.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5661", "revid": "44120587", "url": "https://en.wikipedia.org/wiki?curid=5661", "title": "Centime", "text": "Centime (from ) is French for \"cent\", and is used in English as the name of the fraction currency in several Francophone countries (including Switzerland, Algeria, Belgium, Morocco and France).\nIn France, the usage of \"centime\" goes back to the introduction of the decimal monetary system under Napoleon. This system aimed at replacing non-decimal fractions of older coins. A five-centime coin was known as a \"sou\", i.e. a solidus or shilling.\nIn Francophone Canada &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044100 of a Canadian dollar is officially known as a \"cent\" (pronounced /s\u025bnt/) in both English and French. However, in practice, the form of \"cenne\" (pronounced /s\u025bn/) has completely replaced the official \"cent\". Spoken and written use of the official form \"cent\" in Francophone Canada is exceptionally uncommon.\nIn the Canadian French vernacular \"sou\", \"sou noir\" ( means \"black\" in French), \"cenne\", and \"cenne noire\" are all widely known, used, and accepted monikers when referring to either &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044100 of a Canadian dollar or the 1\u00a2 coin (colloquially known as a \"penny\" in North American English).\nSubdivision of euro: cent or centime?\nIn the European community, \"cent\" is the official name for one hundredth of a euro. However, in French-speaking countries, the word \"centime \"is the preferred term. The Superior Council of the French language of Belgium recommended in 2001 the use of \"centime\", since \"cent\" is also the French word for \"hundred\". An analogous decision was published in the \"Journal officiel\" in France (2 December 1997).\nIn Morocco, dirhams are divided into 100 \"centime\"s and one may find prices in the country quoted in \"centime\"s rather than in dirhams. Sometimes \"centime\"s are known as francs or, in former Spanish areas, pesetas.\nUsage.\nA centime is one-hundredth of the following basic monetary units:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5662", "revid": "1159057856", "url": "https://en.wikipedia.org/wiki?curid=5662", "title": "Calendar year", "text": "Period of time from a calendar's New Year's Day to the day before the next New Year's Day\nGenerally speaking, a calendar year begins on the New Year's Day of the given calendar system and ends on the day before the following New Year's Day, and thus consists of a whole number of days. A year can also be measured by starting on any other named day of the calendar, and ending on the day before this named day in the following year. This may be termed a \"year's time\", but not a \"calendar year\". To reconcile the calendar year with the astronomical cycle (which has a fractional number of days) certain years contain extra days (\"leap days\" or \"intercalary days\"). The Gregorian year, which is in use in most of the world, begins on January 1 and ends on December 31. It has a length of 365 days in an ordinary year, with 8760 hours, 525,600 minutes, or 31,536,000 seconds; but 366 days in a leap year, with 8784 hours, 527,040 minutes, or 31,622,400 seconds. With 97 leap years every 400 years, the year has an average length of 365.2425 days. Other formula-based calendars can have lengths which are further out of step with the solar cycle: for example, the Julian calendar has an average length of 365.25 days, and the Hebrew calendar has an average length of 365.2468 days. The Islamic calendar is a lunar calendar consisting of 12 months in a year of 354 or 355 days. The astronomer's mean tropical year, which is averaged over equinoxes and solstices, is currently 365.24219 days, slightly shorter than the average length of the year in most calendars.\nQuarters.\nThe calendar year can be divided into four quarters, often abbreviated as Q1, Q2, Q3, and Q4. In the Gregorian calendar:\nWhile in the Chinese calendar, the quarters are traditionally associated with the 4 seasons of the year:\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5663", "revid": "1159325012", "url": "https://en.wikipedia.org/wiki?curid=5663", "title": "CFA franc", "text": "Two common currencies of 14 African countries\nThe CFA franc (, ], Franc of the Financial Community of Africa, originally Franc of the French Colonies in Africa, or colloquially ; abbreviation: F.CFA) is the name of two currencies, the West African CFA franc, used in eight West African countries, and the Central African CFA franc, used in six Central African countries. Although separate, the two CFA franc currencies have always been at parity and are effectively interchangeable. The ISO currency codes are XAF for the Central African CFA franc and XOF for the West African CFA franc. On 22 December 2019, it was announced that the West African currency would be reformed and replaced by an independent currency to be called Eco.\nBoth CFA francs have a fixed exchange rate to the euro: \u20ac1 = F.CFA\u00a0655.957 exactly.\nUsage.\nCFA francs are used in fourteen countries: twelve nations formerly ruled by France in West and Central Africa (excluding Guinea and Mauritania, which withdrew), plus Guinea-Bissau (a former Portuguese colony), and Equatorial Guinea (a former Spanish colony). These fourteen countries have a combined population of 193.1\u00a0million people (as of 2021), and a combined GDP of US$283.0\u00a0billion (as of 2021).\nEvaluation.\nThe currency has been criticized for making economic planning for the developing countries of French West Africa all but impossible since the CFA's value is pegged to the euro (whose monetary policy is set by the European Central Bank). Others disagree and argue that the CFA \"helps stabilize the national currencies of Franc Zone member-countries and greatly facilitates the flow of exports and imports between France and the member-countries\". The European Union's own assessment of the CFA's link to the euro, carried out in 2008, noted that \"benefits from economic integration within each of the two monetary unions of the CFA franc zone, and even more so between them, remained remarkably low\" but that \"the peg to the French franc and, since 1999, to the euro as exchange rate anchor is usually found to have had favourable effects in the region in terms of macroeconomic stability\".\nName.\nBetween 1945 and 1958, CFA stood for (\"French colonies of Africa\"); then for (\"French Community of Africa\") between 1958 (establishment of the French Fifth Republic) and the independence of these African countries at the beginning of the 1960s. Since independence, CFA is taken to mean (African Financial Community), but in actual use, the term can have two meanings (see Institutions below).\nHistory.\nCreation.\nThe CFA franc was created on 26 December 1945, along with the CFP franc. The reason for their creation was the weakness of the French franc immediately after World War II. When France ratified the Bretton Woods Agreement in December 1945, the French franc was devalued in order to set a fixed exchange rate with the US dollar. New currencies were created in the French colonies to spare them the strong devaluation, thereby making it easier for them to import goods from France (and simultaneously making it harder for them to export goods to France). French officials presented the decision as an act of generosity. Ren\u00e9 Pleven, the French Minister of Finance, was quoted as saying:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In a show of her generosity and selflessness, metropolitan France, wishing not to impose on her far-away daughters the consequences of her own poverty, is setting different exchange rates for their currency.\nExchange rate.\nThe CFA franc was created with a fixed exchange rate versus the French franc. This exchange rate was changed only twice, in 1948 and in 1994 (besides nominal adaptation to the new French franc in 1960 and the Euro in 1999).\nExchange rate:\nThe 1960 and 1999 events merely reflect changes of currency in use in France: the actual relative value of the CFA franc versus the French franc/euro only changed in 1948 and 1994.\nChanges in countries using the franc.\nOver time, the number of countries and territories using the CFA franc has changed as some countries began introducing their own separate currencies. A couple of nations in West Africa have also chosen to adopt the CFA franc since its introduction, despite the fact that they had never been French colonies.\nEuropean Monetary Union.\nIn 1998, in anticipation of Economic and Monetary Union of the European Union, the Council of the European Union addressed the monetary agreements France had with the CFA Zone and Comoros and ruled that:\nCriticism and replacement in West Africa.\nCritics point out that the currency is controlled by the French treasury, and in turn African countries channel more money to France than they receive in aid and have no sovereignty over their monetary policies. In January 2019, Italian ministers accused France of impoverishing Africa through the CFA franc. However, criticism of the CFA franc, coming from various African organizations, continued. On 21 December 2019, President Alassane Ouattara of the Ivory Coast and President Emmanuel Macron of France announced an initiative to replace the West African CFA Franc with the Eco. Subsequently, a reform of the West African CFA franc was initiated. In May 2020, the French National Assembly agreed to end the French engagement in the West African CFA franc. The countries using the currency will no longer have to deposit half of their foreign exchange reserves with the French Treasury. The West African CFA franc is expected to be renamed to the \"Eco\" in the near future.\nThe broader Economic Community of West African States (ECOWAS), of which the members of UEMOA are also members, plans to introduce its own common currency for its member states by 2027, for which they have also formally adopted the name of eco.\nPublic debate for the end of the currency in Central Africa CEMAC.\nOn April 25, 2023, the ministerial meeting of the Economic and Monetary Community of Central Africa (Cemac) and France is held. In particular, the subject of the CFA franc was discussed. On the French side, the guarantee provided to the CFA franc, and the assurance of its convertibility, is perceived as a vector of economic stability for the region. France remains \u201copen\u201d and \u201cavailable\u201d to move forward on a reform of monetary cooperation in Central Africa, such as it has been able to take place in West Africa. France says it is ready to receive CEMAC's proposals..\nInstitutions.\nThere are two different currencies called the CFA franc: the West African CFA franc (ISO 4217 currency code XOF), and the Central Africa CFA franc (ISO 4217 currency code XAF). They are distinguished in French by the meaning of the abbreviation CFA. These two CFA francs have the same exchange rate with the euro (1 euro = 655.957 XOF = 655.957 XAF), and they are both guaranteed by the French treasury (), but the West African CFA franc is not legal tender in Central African countries, and the Central Africa CFA franc is not legal tender in West African countries.\nWest African.\nThe West African CFA franc (XOF) is known in French as the , where CFA stands for ('Financial Community of Africa') or (\"African Financial Community\"). It is issued by the BCEAO (, i.e., \"Central Bank of the West African States\"), located in Dakar, Senegal, for the eight countries of the UEMOA (, i.e., \"West African Economic and Monetary Union\"):\nThese eight countries have a combined population of 134.7\u00a0million people (as of 2021), and a combined GDP of US$179.7\u00a0billion (as of 2021).\nCentral African.\nThe Central Africa CFA franc (XAF) is known in French as the , where CFA stands for (\"Financial Cooperation in Central Africa\"). It is issued by the BEAC (, i.e., \"Bank of the Central African States\"), located in Yaound\u00e9, Cameroon, for the six countries of the CEMAC (, i.e., \"Economic and Monetary Community of Central Africa\"):\nThese six countries have a combined population of 58.4\u00a0million people (as of 2021), and a combined GDP of US$103.3\u00a0billion (as of 2021).\nIn 1975, Central African CFA banknotes were issued with an obverse unique to each participating country, and common reverse, in a fashion similar to euro coins.\nEquatorial Guinea, the only former Spanish colony in the zone, adopted the CFA in 1984.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5664", "revid": "31384714", "url": "https://en.wikipedia.org/wiki?curid=5664", "title": "Consciousness", "text": "Awareness\u00a0of internal and external existence\nConsciousness, at its simplest, is awareness of internal and external existence. However, its nature has led to millennia of analyses, explanations and debates by philosophers, theologians, linguists, and scientists. Opinions differ about what exactly needs to be studied or even considered consciousness. In some explanations, it is synonymous with the mind, and at other times, an aspect of mind. In the past, it was one's \"inner life\", the world of introspection, of private thought, imagination and volition. Today, it often includes any kind of cognition, experience, feeling or perception. It may be awareness, awareness of awareness, or self-awareness either continuously changing or not. The disparate range of research, notions and speculations raises a curiosity about whether the right questions are being asked.\nExamples of the range of descriptions, definitions or explanations are: simple wakefulness, one's sense of selfhood or soul explored by \"looking within\"; being a metaphorical \"stream\" of contents, or being a mental state, mental event or mental process of the brain.\nEtymology.\nIn the late 20th century, philosophers like Hamlyn, Rorty, and Wilkes have disagreed with Kahn, Hardie and Modrak as to whether Aristotle even had a concept of consciousness. Aristotle does not use any single word or terminology to name the phenomenon; it is used only much later, especially by John Locke. Caston contends that for Aristotle, perceptual awareness was somewhat the same as what modern philosophers call consciousness.\nThe origin of the modern concept of consciousness is often attributed to Locke's \"Essay Concerning Human Understanding\", published in 1690. Locke defined consciousness as \"the perception of what passes in a man's own mind\". His essay influenced the 18th-century view of consciousness, and his definition appeared in Samuel Johnson's celebrated \"Dictionary\" (1755).\n\"Consciousness\" (French: \"conscience\") is also defined in the 1753 volume of Diderot and d'Alembert's Encyclop\u00e9die, as \"the opinion or internal feeling that we ourselves have from what we do\".\nThe earliest English language uses of \"conscious\" and \"consciousness\" date back, however, to the 1500s. The English word \"conscious\" originally derived from the Latin \"conscius\" (\"con-\" \"together\" and \"scio\" \"to know\"), but the Latin word did not have the same meaning as the English word\u2014it meant \"knowing with\", in other words, \"having joint or common knowledge with another\". There were, however, many occurrences in Latin writings of the phrase \"conscius sibi\", which translates literally as \"knowing with oneself\", or in other words \"sharing knowledge with oneself about something\". This phrase had the figurative meaning of \"knowing that one knows\", as the modern English word \"conscious\" does. In its earliest uses in the 1500s, the English word \"conscious\" retained the meaning of the Latin \"conscius\". For example, Thomas Hobbes in \"Leviathan\" wrote: \"Where two, or more men, know of one and the same fact, they are said to be Conscious of it one to another.\" The Latin phrase \"conscius sibi\", whose meaning was more closely related to the current concept of consciousness, was rendered in English as \"conscious to oneself\" or \"conscious unto oneself\". For example, Archbishop Ussher wrote in 1613 of \"being so conscious unto myself of my great weakness\". Locke's definition from 1690 illustrates that a gradual shift in meaning had taken place.\nA related word was ', which primarily means moral conscience. In the literal sense, \"conscientia\" means knowledge-with, that is, shared knowledge. The word first appears in Latin juridical texts by writers such as Cicero. Here, \"conscientia\" is the knowledge that a witness has of the deed of someone else. Ren\u00e9 Descartes (1596\u20131650) is generally taken to be the first philosopher to use \"conscientia\" in a way that does not fit this traditional meaning. Descartes used \"conscientia\" the way modern speakers would use \"conscience\". In \"Search after Truth\" (', Amsterdam 1701) he says \"conscience or internal testimony\" (\"conscienti\u00e2, vel interno testimonio\").\nThe problem of definition.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nAbout forty meanings attributed to the term \"consciousness\" can be identified and categorized based on \"functions\" and \"experiences\". The prospects for reaching any single, agreed-upon, theory-independent definition of consciousness appear remote.\nThe dictionary definitions of the word \"consciousness\" extend through several centuries and reflect a range of seemingly related meanings, with some differences that have been controversial, such as the distinction between 'inward awareness' and 'perception' of the physical world, or the distinction between 'conscious' and 'unconscious', or the notion of a \"mental entity\" or \"mental activity\" that is not physical.\nThe common usage definitions of \"consciousness\" in \"Webster's Third New International Dictionary\" (1966 edition, Volume 1, page 482) are as follows: \nThe Cambridge Dictionary defines consciousness as \"\"the state of understanding and realizing something.\"\nThe Oxford Living Dictionary defines consciousness as \"The state of being aware of and responsive to one's surroundings.\", \"A person's awareness or perception of something.\" and \"The fact of awareness by the mind of itself and the world.\"\"\nPhilosophers have attempted to clarify technical distinctions by using a jargon of their own. The \"Routledge Encyclopedia of Philosophy\" in 1998 defines consciousness as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Consciousness\u2014Philosophers have used the term 'consciousness' for four main topics: knowledge in general, intentionality, introspection (and the knowledge it specifically generates) and phenomenal experience... Something within one's mind is 'introspectively conscious' just in case one introspects it (or is poised to do so). Introspection is often thought to deliver one's primary knowledge of one's mental life. An experience or other mental entity is 'phenomenally conscious' just in case there is 'something it is like' for one to have it. The clearest examples are: perceptual experience, such as tastings and seeings; bodily-sensational experiences, such as those of pains, tickles and itches; imaginative experiences, such as those of one's own actions or perceptions; and streams of thought, as in the experience of thinking 'in words' or 'in images'. Introspection and phenomenality seem independent, or dissociable, although this is controversial.\nMany philosophers and scientists have been unhappy about the difficulty of producing a definition that does not involve circularity or fuzziness. In The \"Macmillan Dictionary of Psychology\" (1989 edition), Stuart Sutherland expressed a skeptical attitude more than a definition:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Consciousness\u2014The having of perceptions, thoughts, and feelings; awareness. The term is impossible to define except in terms that are unintelligible without a grasp of what consciousness means. Many fall into the trap of equating consciousness with self-consciousness\u2014to be conscious it is only necessary to be aware of the external world. Consciousness is a fascinating but elusive phenomenon: it is impossible to specify what it is, what it does, or why it has evolved. Nothing worth reading has been written on it.\nA partisan definition such as Sutherland's can hugely affect researchers' assumptions and the direction of their work:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;If awareness of the environment . . . is the criterion of consciousness, then even the protozoans are conscious. If awareness of awareness is required, then it is doubtful whether the great apes and human infants are conscious.\nMany philosophers have argued that consciousness is a unitary concept that is understood by the majority of people despite the difficulty philosophers have had defining it. Others, though, have argued that the level of disagreement about the meaning of the word indicates that it either means different things to different people (for instance, the objective versus subjective aspects of consciousness), that it encompasses a variety of distinct meanings with no simple element in common, or that we should eliminate this concept from our understanding of the mind, a position known as consciousness semanticism.\nInter-disciplinary perspectives.\nWestern philosophers since the time of Descartes and Locke have struggled to comprehend the nature of consciousness and how it fits into a larger picture of the world. These questions remain central to both continental and analytic philosophy, in phenomenology and the philosophy of mind, respectively.\nConsciousness has also become a significant topic of interdisciplinary research in cognitive science, involving fields such as psychology, linguistics, anthropology, neuropsychology and neuroscience. The primary focus is on understanding what it means biologically and psychologically for information to be present in consciousness\u2014that is, on determining the neural and psychological correlates of consciousness.\nIn medicine, consciousness is assessed by observing a patient's arousal and responsiveness, and can be seen as a continuum of states ranging from full alertness and comprehension, through disorientation, delirium, loss of meaningful communication, and finally loss of movement in response to painful stimuli. Issues of practical concern include how the presence of consciousness can be assessed in severely ill, comatose, or anesthetized people, and how to treat conditions in which consciousness is impaired or disrupted. The degree of consciousness is measured by standardized behavior observation scales such as the Glasgow Coma Scale.\nPhilosophy of mind.\nMost writers on the philosophy of consciousness have been concerned with defending a particular point of view, and have organized their material accordingly. For surveys, the most common approach is to follow a historical path by associating stances with the philosophers who are most strongly associated with them, for example, Descartes, Locke, Kant, etc. An alternative is to organize philosophical stances according to basic issues.\nCoherence of the concept.\nPhilosophers differ from non-philosophers in their intuitions about what consciousness is. While most people have a strong intuition for the existence of what they refer to as consciousness, skeptics argue that this intuition is false, either because the concept of consciousness is intrinsically incoherent, or because our intuitions about it are based in illusions. Gilbert Ryle, for example, argued that traditional understanding of consciousness depends on a Cartesian dualist outlook that improperly distinguishes between mind and body, or between mind and world. He proposed that we speak not of minds, bodies, and the world, but of individuals, or persons, acting in the world. Thus, by speaking of \"consciousness\" we end up misleading ourselves by thinking that there is any sort of thing as consciousness separated from behavioral and linguistic understandings.\nTypes.\nNed Block argued that discussions on consciousness often failed to properly distinguish \"phenomenal\" (P-consciousness) from \"access\" (A-consciousness), though these terms had been used before Block. P-consciousness, according to Block, is raw experience: it is moving, colored forms, sounds, sensations, emotions and feelings with our bodies and responses at the center. These experiences, considered independently of any impact on behavior, are called qualia. A-consciousness, on the other hand, is the phenomenon whereby information in our minds is accessible for verbal report, reasoning, and the control of behavior. So, when we perceive, information about what we perceive is access conscious; when we introspect, information about our thoughts is access conscious; when we remember, information about the past is access conscious, and so on. Although some philosophers, such as Daniel Dennett, have disputed the validity of this distinction, others have broadly accepted it. David Chalmers has argued that A-consciousness can in principle be understood in mechanistic terms, but that understanding P-consciousness is much more challenging: he calls this the hard problem of consciousness.\nSome philosophers believe that Block's two types of consciousness are not the end of the story. William Lycan, for example, argued in his book \"Consciousness and Experience\" that at least eight clearly distinct types of consciousness can be identified (organism consciousness; control consciousness; consciousness \"of\"; state/event consciousness; reportability; introspective consciousness; subjective consciousness; self-consciousness)\u2014and that even this list omits several more obscure forms.\nThere is also debate over whether or not A-consciousness and P-consciousness always coexist or if they can exist separately. Although P-consciousness without A-consciousness is more widely accepted, there have been some hypothetical examples of A without P. Block, for instance, suggests the case of a \"zombie\" that is computationally identical to a person but without any subjectivity. However, he remains somewhat skeptical concluding \"I don't know whether there are any actual cases of A-consciousness without P-consciousness, but I hope I have illustrated their conceptual possibility.\"\nDistinguishing consciousness from its contents.\nSam Harris observes: \"At the level of your experience, you are not a body of cells, organelles, and atoms; you are consciousness and its ever-changing contents\". Seen in this way, consciousness is a subjectively experienced, ever-present field in which things (the contents of consciousness) come and go.\nChristopher Tricker argues that this field of consciousness is symbolised by the mythical bird that opens the Daoist classic the \"Zhuangzi.\" This bird\u2019s name is Of a Flock (\"peng\" \u9d6c), yet its back is countless thousands of miles across and its wings are like clouds arcing across the heavens. \"Like Of a Flock, whose wings arc across the heavens, the wings of your consciousness span to the horizon. At the same time, the wings of every other being\u2019s consciousness span to the horizon. You are of a flock, one bird among kin.\"\nMind\u2013body problem.\nMental processes (such as consciousness) and physical processes (such as brain events) seem to be correlated, however the specific nature of the connection is unknown.\nThe first influential philosopher to discuss this question specifically was Descartes, and the answer he gave is known as Cartesian dualism. Descartes proposed that consciousness resides within an immaterial domain he called \"res cogitans\" (the realm of thought), in contrast to the domain of material things, which he called \"res extensa\" (the realm of extension). He suggested that the interaction between these two domains occurs inside the brain, perhaps in a small midline structure called the pineal gland.\nAlthough it is widely accepted that Descartes explained the problem cogently, few later philosophers have been happy with his solution, and his ideas about the pineal gland have especially been ridiculed. However, no alternative solution has gained general acceptance. Proposed solutions can be divided broadly into two categories: dualist solutions that maintain Descartes's rigid distinction between the realm of consciousness and the realm of matter but give different answers for how the two realms relate to each other; and monist solutions that maintain that there is really only one realm of being, of which consciousness and matter are both aspects. Each of these categories itself contains numerous variants. The two main types of dualism are substance dualism (which holds that the mind is formed of a distinct type of substance not governed by the laws of physics) and property dualism (which holds that the laws of physics are universally valid but cannot be used to explain the mind). The three main types of monism are physicalism (which holds that the mind consists of matter organized in a particular way), idealism (which holds that only thought or experience truly exists, and matter is merely an illusion), and neutral monism (which holds that both mind and matter are aspects of a distinct essence that is itself identical to neither of them). There are also, however, a large number of idiosyncratic theories that cannot cleanly be assigned to any of these schools of thought.\nSince the dawn of Newtonian science with its vision of simple mechanical principles governing the entire universe, some philosophers have been tempted by the idea that consciousness could be explained in purely physical terms. The first influential writer to propose such an idea explicitly was Julien Offray de La Mettrie, in his book \"Man a Machine\" (\"L'homme machine\"). His arguments, however, were very abstract. The most influential modern physical theories of consciousness are based on psychology and neuroscience. Theories proposed by neuroscientists such as Gerald Edelman and Antonio Damasio, and by philosophers such as Daniel Dennett, seek to explain consciousness in terms of neural events occurring within the brain. Many other neuroscientists, such as Christof Koch, have explored the neural basis of consciousness without attempting to frame all-encompassing global theories. At the same time, computer scientists working in the field of artificial intelligence have pursued the goal of creating digital computer programs that can simulate or embody consciousness.\nA few theoretical physicists have argued that classical physics is intrinsically incapable of explaining the holistic aspects of consciousness, but that quantum theory may provide the missing ingredients. Several theorists have therefore proposed quantum mind (QM) theories of consciousness. Notable theories falling into this category include the holonomic brain theory of Karl Pribram and David Bohm, and the Orch-OR theory formulated by Stuart Hameroff and Roger Penrose. Some of these QM theories offer descriptions of phenomenal consciousness, as well as QM interpretations of access consciousness. None of the quantum mechanical theories have been confirmed by experiment. Recent publications by G. Guerreshi, J. Cia, S. Popescu, and H. Briegel could falsify proposals such as those of Hameroff, which rely on quantum entanglement in protein. At the present time many scientists and philosophers consider the arguments for an important role of quantum phenomena to be unconvincing.\nApart from the general question of the \"hard problem\" of consciousness (which is, roughly speaking, the question of how mental experience can arise from a physical basis), a more specialized question is how to square the subjective notion that we are in control of our decisions (at least in some small measure) with the customary view of causality that subsequent events are caused by prior events. The topic of free will is the philosophical and scientific examination of this conundrum.\nProblem of other minds.\nMany philosophers consider experience to be the essence of consciousness, and believe that experience can only fully be known from the inside, subjectively. But if consciousness is subjective and not visible from the outside, why do the vast majority of people believe that other people are conscious, but rocks and trees are not? This is called the problem of other minds. It is particularly acute for people who believe in the possibility of philosophical zombies, that is, people who think it is possible in principle to have an entity that is physically indistinguishable from a human being and behaves like a human being in every way but nevertheless lacks consciousness. Related issues have also been studied extensively by Greg Littmann of the University of Illinois, and by Colin Allen (a professor at the University of Pittsburgh) regarding the literature and research studying artificial intelligence in androids.\nThe most commonly given answer is that we attribute consciousness to other people because we see that they resemble us in appearance and behavior; we reason that if they look like us and act like us, they must be like us in other ways, including having experiences of the sort that we do. There are, however, a variety of problems with that explanation. For one thing, it seems to violate the principle of parsimony, by postulating an invisible entity that is not necessary to explain what we observe. Some philosophers, such as Daniel Dennett in a research paper titled \"The Unimagined Preposterousness of Zombies\", argue that people who give this explanation do not really understand what they are saying. More broadly, philosophers who do not accept the possibility of zombies generally believe that consciousness is reflected in behavior (including verbal behavior), and that we attribute consciousness on the basis of behavior. A more straightforward way of saying this is that we attribute experiences to people because of what they can \"do\", including the fact that they can tell us about their experiences.\nScientific study.\nFor many decades, consciousness as a research topic was avoided by the majority of mainstream scientists, because of a general feeling that a phenomenon defined in subjective terms could not properly be studied using objective experimental methods. In 1975 George Mandler published an influential psychological study which distinguished between slow, serial, and limited conscious processes and fast, parallel and extensive unconscious ones. The Science and Religion Forum 1984 annual conference, \"'From Artificial Intelligence to Human Consciousness\"' identified the nature of consciousness as a matter for investigation; Donald Michie was a keynote speaker. Starting in the 1980s, an expanding community of neuroscientists and psychologists have associated themselves with a field called \"Consciousness Studies\", giving rise to a stream of experimental work published in books, journals such as \"Consciousness and Cognition\", \"Frontiers in Consciousness Research\", \"Psyche\", and the \"Journal of Consciousness Studies\", along with regular conferences organized by groups such as the Association for the Scientific Study of Consciousness and the Society for Consciousness Studies.\nModern medical and psychological investigations into consciousness are based on psychological experiments (including, for example, the investigation of priming effects using subliminal stimuli), and on case studies of alterations in consciousness produced by trauma, illness, or drugs. Broadly viewed, scientific approaches are based on two core concepts. The first identifies the content of consciousness with the experiences that are reported by human subjects; the second makes use of the concept of consciousness that has been developed by neurologists and other medical professionals who deal with patients whose behavior is impaired. In either case, the ultimate goals are to develop techniques for assessing consciousness objectively in humans as well as other animals, and to understand the neural and psychological mechanisms that underlie it.\nMeasurement.\nExperimental research on consciousness presents special difficulties, due to the lack of a universally accepted operational definition. In the majority of experiments that are specifically about consciousness, the subjects are human, and the criterion used is verbal report: in other words, subjects are asked to describe their experiences, and their descriptions are treated as observations of the contents of consciousness. For example, subjects who stare continuously at a Necker cube usually report that they experience it \"flipping\" between two 3D configurations, even though the stimulus itself remains the same. The objective is to understand the relationship between the conscious awareness of stimuli (as indicated by verbal report) and the effects the stimuli have on brain activity and behavior. In several paradigms, such as the technique of response priming, the behavior of subjects is clearly influenced by stimuli for which they report no awareness, and suitable experimental manipulations can lead to increasing priming effects despite decreasing prime identification (double dissociation).\nVerbal report is widely considered to be the most reliable indicator of consciousness, but it raises a number of issues. For one thing, if verbal reports are treated as observations, akin to observations in other branches of science, then the possibility arises that they may contain errors\u2014but it is difficult to make sense of the idea that subjects could be wrong about their own experiences, and even more difficult to see how such an error could be detected. Daniel Dennett has argued for an approach he calls heterophenomenology, which means treating verbal reports as stories that may or may not be true, but his ideas about how to do this have not been widely adopted. Another issue with verbal report as a criterion is that it restricts the field of study to humans who have language: this approach cannot be used to study consciousness in other species, pre-linguistic children, or people with types of brain damage that impair language. As a third issue, philosophers who dispute the validity of the Turing test may feel that it is possible, at least in principle, for verbal report to be dissociated from consciousness entirely: a philosophical zombie may give detailed verbal reports of awareness in the absence of any genuine awareness.\nAlthough verbal report is in practice the \"gold standard\" for ascribing consciousness, it is not the only possible criterion. In medicine, consciousness is assessed as a combination of verbal behavior, arousal, brain activity and purposeful movement. The last three of these can be used as indicators of consciousness when verbal behavior is absent. The scientific literature regarding the neural bases of arousal and purposeful movement is very extensive. Their reliability as indicators of consciousness is disputed, however, due to numerous studies showing that alert human subjects can be induced to behave purposefully in a variety of ways in spite of reporting a complete lack of awareness. Studies of the neuroscience of free will have also shown that the experiences that people report when they behave purposefully sometimes do not correspond to their actual behaviors or to the patterns of electrical activity recorded from their brains.\nAnother approach applies specifically to the study of self-awareness, that is, the ability to distinguish oneself from others. In the 1970s Gordon Gallup developed an operational test for self-awareness, known as the mirror test. The test examines whether animals are able to differentiate between seeing themselves in a mirror versus seeing other animals. The classic example involves placing a spot of coloring on the skin or fur near the individual's forehead and seeing if they attempt to remove it or at least touch the spot, thus indicating that they recognize that the individual they are seeing in the mirror is themselves. Humans (older than 18 months) and other great apes, bottlenose dolphins, orcas, pigeons, European magpies and elephants have all been observed to pass this test.\nNeural correlates.\nA major part of the scientific literature on consciousness consists of studies that examine the relationship between the experiences reported by subjects and the activity that simultaneously takes place in their brains\u2014that is, studies of the neural correlates of consciousness. The hope is to find that activity in a particular part of the brain, or a particular pattern of global brain activity, which will be strongly predictive of conscious awareness. Several brain imaging techniques, such as EEG and fMRI, have been used for physical measures of brain activity in these studies.\nAnother idea that has drawn attention for several decades is that consciousness is associated with high-frequency (gamma band) oscillations in brain activity. This idea arose from proposals in the 1980s, by Christof von der Malsburg and Wolf Singer, that gamma oscillations could solve the so-called binding problem, by linking information represented in different parts of the brain into a unified experience. Rodolfo Llin\u00e1s, for example, proposed that consciousness results from recurrent thalamo-cortical resonance where the specific thalamocortical systems (content) and the non-specific (centromedial thalamus) thalamocortical systems (context) interact in the gamma band frequency via synchronous oscillations.\nA number of studies have shown that activity in primary sensory areas of the brain is not sufficient to produce consciousness: it is possible for subjects to report a lack of awareness even when areas such as the primary visual cortex (V1) show clear electrical responses to a stimulus. Higher brain areas are seen as more promising, especially the prefrontal cortex, which is involved in a range of higher cognitive functions collectively known as executive functions. There is substantial evidence that a \"top-down\" flow of neural activity (i.e., activity propagating from the frontal cortex to sensory areas) is more predictive of conscious awareness than a \"bottom-up\" flow of activity. The prefrontal cortex is not the only candidate area, however: studies by Nikos Logothetis and his colleagues have shown, for example, that visually responsive neurons in parts of the temporal lobe reflect the visual perception in the situation when conflicting visual images are presented to different eyes (i.e., bistable percepts during binocular rivalry). Furthermore, top-down feedback from higher to lower visual brain areas may be weaker or absent in the peripheral visual field, as suggested by some experimental data and theoretical arguments; nevertheless humans can perceive visual inputs in the peripheral visual field arising from bottom-up V1 neural activities. Meanwhile, bottom-up V1 activities for the central visual fields can be vetoed, and thus made invisible to perception, by the top-down feedback, when these bottom-up signals are inconsistent with brain's internal model of the visual world.\nModulation of neural responses may correlate with phenomenal experiences. In contrast to the raw electrical responses that do not correlate with consciousness, the modulation of these responses by other stimuli correlates surprisingly well with an important aspect of consciousness: namely with the phenomenal experience of stimulus intensity (brightness, contrast). In the research group of Danko Nikoli\u0107 it has been shown that some of the changes in the subjectively perceived brightness correlated with the modulation of firing rates while others correlated with the modulation of neural synchrony. An fMRI investigation suggested that these findings were strictly limited to the primary visual areas. This indicates that, in the primary visual areas, changes in firing rates and synchrony can be considered as neural correlates of qualia\u2014at least for some type of qualia.\nIn 2013, the perturbational complexity index (PCI) was proposed, a measure of the algorithmic complexity of the electrophysiological response of the cortex to transcranial magnetic stimulation. This measure was shown to be higher in individuals that are awake, in REM sleep or in a locked-in state than in those who are in deep sleep or in a vegetative state, making it potentially useful as a quantitative assessment of consciousness states.\nAssuming that not only humans but even some non-mammalian species are conscious, a number of evolutionary approaches to the problem of neural correlates of consciousness open up. For example, assuming that birds are conscious\u2014a common assumption among neuroscientists and ethologists due to the extensive cognitive repertoire of birds\u2014there are comparative neuroanatomical ways to validate some of the principal, currently competing, mammalian consciousness\u2013brain theories. The rationale for such a comparative study is that the avian brain deviates structurally from the mammalian brain. So how similar are they? What homologues can be identified? The general conclusion from the study by Butler, et al., is that some of the major theories for the mammalian brain also appear to be valid for the avian brain. The structures assumed to be critical for consciousness in mammalian brains have homologous counterparts in avian brains. Thus the main portions of the theories of Crick and Koch, Edelman and Tononi, and Cotterill seem to be compatible with the assumption that birds are conscious. Edelman also differentiates between what he calls primary consciousness (which is a trait shared by humans and non-human animals) and higher-order consciousness as it appears in humans alone along with human language capacity. Certain aspects of the three theories, however, seem less easy to apply to the hypothesis of avian consciousness. For instance, the suggestion by Crick and Koch that layer 5 neurons of the mammalian brain have a special role, seems difficult to apply to the avian brain, since the avian homologues have a different morphology. Likewise, the theory of Eccles seems incompatible, since a structural homologue/analogue to the dendron has not been found in avian brains. The assumption of an avian consciousness also brings the reptilian brain into focus. The reason is the structural continuity between avian and reptilian brains, meaning that the phylogenetic origin of consciousness may be earlier than suggested by many leading neuroscientists.\nJoaquin Fuster of UCLA has advocated the position of the importance of the prefrontal cortex in humans, along with the areas of Wernicke and Broca, as being of particular importance to the development of human language capacities neuro-anatomically necessary for the emergence of higher-order consciousness in humans.\nA study in 2016 looked at lesions in specific areas of the brainstem that were associated with coma and vegetative states. A small region of the rostral dorsolateral pontine tegmentum in the brainstem was suggested to drive consciousness through functional connectivity with two cortical regions, the left ventral anterior insular cortex, and the pregenual anterior cingulate cortex. These three regions may work together as a triad to maintain consciousness.\nModels.\nA wide range of empirical theories of consciousness have been proposed. Adrian Doerig and colleagues list 13 notable theories, while Anil Seth and Tim Bayne list 22 notable theories.\nGlobal workspace theory (GWT) is a cognitive architecture and theory of consciousness proposed by the cognitive psychologist Bernard Baars in 1988. Baars explains the theory with the metaphor of a theater, with conscious processes represented by an illuminated stage. This theater integrates inputs from a variety of unconscious and otherwise autonomous networks in the brain and then broadcasts them to unconscious networks (represented in the metaphor by a broad, unlit \"audience\"). The theory has since been expanded upon by other scientists including cognitive neuroscientist Stanislas Dehaene and Lionel Naccache.\nIntegrated information theory (IIT) postulates that consciousness resides in the information being processed and arises once the information reaches a certain level of complexity. Proponents of this model suggest that it may provide a physical grounding for consciousness in neurons, as they provide the mechanism by which information is integrated.\nOrchestrated objective reduction (Orch OR) postulates that consciousness originates at the quantum level inside neurons. The mechanism is held to be a quantum process called objective reduction that is orchestrated by cellular structures called microtubules. However the details of the mechanism would go beyond current quantum theory.\nIn 2011, Graziano and Kastner proposed the \"attention schema\" theory of awareness. In that theory, specific cortical areas, notably in the superior temporal sulcus and the temporo-parietal junction, are used to build the construct of awareness and attribute it to other people. The same cortical machinery is also used to attribute awareness to oneself. Damage to these cortical regions can lead to deficits in consciousness such as hemispatial neglect. In the attention schema theory, the value of explaining the feature of awareness and attributing it to a person is to gain a useful predictive model of that person's attentional processing. Attention is a style of information processing in which a brain focuses its resources on a limited set of interrelated signals. Awareness, in this theory, is a useful, simplified schema that represents attentional states. To be aware of X is explained by constructing a model of one's attentional focus on X.\nThe entropic brain is a theory of conscious states informed by neuroimaging research with psychedelic drugs. The theory suggests that the brain in primary states such as rapid eye movement (REM) sleep, early psychosis and under the influence of psychedelic drugs, is in a disordered state; normal waking consciousness constrains some of this freedom and makes possible metacognitive functions such as internal self-administered reality testing and self-awareness. Criticism has included questioning whether the theory has been adequately tested.\nIn 2017, work by David Rudrauf and colleagues, including Karl Friston, applied the active inference paradigm to consciousness, a model of how sensory data is integrated with priors in a process of projective transformation. The authors argue that, while their model identifies a key relationship between computation and phenomenology, it does not completely solve the hard problem of consciousness or completely close the explanatory gap.\nBiological function and evolution.\nOpinions are divided as to where in biological evolution consciousness emerged and about whether or not consciousness has any survival value. Some argue that consciousness is a byproduct of evolution. It has been argued that consciousness emerged (i) exclusively with the first humans, (ii) exclusively with the first mammals, (iii) independently in mammals and birds, or (iv) with the first reptiles. Other authors date the origins of consciousness to the first animals with nervous systems or early vertebrates in the Cambrian over 500 million years ago. Donald Griffin suggests in his book \"Animal Minds\" a gradual evolution of consciousness. Each of these scenarios raises the question of the possible survival value of consciousness.\nThomas Henry Huxley defends in an essay titled \"On the Hypothesis that Animals are Automata, and its History\" an epiphenomenalist theory of consciousness according to which consciousness is a causally inert effect of neural activity\u2014\"as the steam-whistle which accompanies the work of a locomotive engine is without influence upon its machinery\". To this William James objects in his essay \"Are We Automata?\" by stating an evolutionary argument for mind-brain interaction implying that if the preservation and development of consciousness in the biological evolution is a result of natural selection, it is plausible that consciousness has not only been influenced by neural processes, but has had a survival value itself; and it could only have had this if it had been efficacious. Karl Popper develops a similar evolutionary argument in the book \"The Self and Its Brain\".\nRegarding the primary function of conscious processing, a recurring idea in recent theories is that phenomenal states somehow integrate neural activities and information-processing that would otherwise be independent. This has been called the \"integration consensus\". Another example has been proposed by Gerald Edelman called dynamic core hypothesis which puts emphasis on reentrant connections that reciprocally link areas of the brain in a massively parallel manner. Edelman also stresses the importance of the evolutionary emergence of higher-order consciousness in humans from the historically older trait of primary consciousness which humans share with non-human animals (see \"Neural correlates\" section above). These theories of integrative function present solutions to two classic problems associated with consciousness: differentiation and unity. They show how our conscious experience can discriminate between a virtually unlimited number of different possible scenes and details (differentiation) because it integrates those details from our sensory systems, while the integrative nature of consciousness in this view easily explains how our experience can seem unified as one whole despite all of these individual parts. However, it remains unspecified which kinds of information are integrated in a conscious manner and which kinds can be integrated without consciousness. Nor is it explained what specific causal role conscious integration plays, nor why the same functionality cannot be achieved without consciousness. Obviously not all kinds of information are capable of being disseminated consciously (e.g., neural activity related to vegetative functions, reflexes, unconscious motor programs, low-level perceptual analyses, etc.) and many kinds of information can be disseminated and combined with other kinds without consciousness, as in intersensory interactions such as the ventriloquism effect. Hence it remains unclear why any of it is conscious. For a review of the differences between conscious and unconscious integrations, see the article of Ezequiel Morsella.\nAs noted earlier, even among writers who consider consciousness to be well-defined, there is widespread dispute about which animals other than humans can be said to possess it. Edelman has described this distinction as that of humans possessing higher-order consciousness while sharing the trait of primary consciousness with non-human animals (see previous paragraph). Thus, any examination of the evolution of consciousness is faced with great difficulties. Nevertheless, some writers have argued that consciousness can be viewed from the standpoint of evolutionary biology as an adaptation in the sense of a trait that increases fitness. In his article \"Evolution of consciousness\", John Eccles argued that special anatomical and physical properties of the mammalian cerebral cortex gave rise to consciousness (\"[a] psychon ... linked to [a] dendron through quantum physics\"). Bernard Baars proposed that once in place, this \"recursive\" circuitry may have provided a basis for the subsequent development of many of the functions that consciousness facilitates in higher organisms. Peter Carruthers has put forth one such potential adaptive advantage gained by conscious creatures by suggesting that consciousness allows an individual to make distinctions between appearance and reality. This ability would enable a creature to recognize the likelihood that their perceptions are deceiving them (e.g. that water in the distance may be a mirage) and behave accordingly, and it could also facilitate the manipulation of others by recognizing how things appear to them for both cooperative and devious ends.\nOther philosophers, however, have suggested that consciousness would not be necessary for any functional advantage in evolutionary processes. No one has given a causal explanation, they argue, of why it would not be possible for a functionally equivalent non-conscious organism (i.e., a philosophical zombie) to achieve the very same survival advantages as a conscious organism. If evolutionary processes are blind to the difference between function \"F\" being performed by conscious organism \"O\" and non-conscious organism \"O*\", it is unclear what adaptive advantage consciousness could provide. As a result, an exaptive explanation of consciousness has gained favor with some theorists that posit consciousness did not evolve as an adaptation but was an exaptation arising as a consequence of other developments such as increases in brain size or cortical rearrangement. Consciousness in this sense has been compared to the blind spot in the retina where it is not an adaption of the retina, but instead just a by-product of the way the retinal axons were wired. Several scholars including Pinker, Chomsky, Edelman, and Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness (see \"Neural correlates\" section above).\nAltered states.\nThere are some brain states in which consciousness seems to be absent, including dreamless sleep or coma. There are also a variety of circumstances that can change the relationship between the mind and the world in less drastic ways, producing what are known as altered states of consciousness. Some altered states occur naturally; others can be produced by drugs or brain damage. Altered states can be accompanied by changes in thinking, disturbances in the sense of time, feelings of loss of control, changes in emotional expression, alternations in body image and changes in meaning or significance.\nThe two most widely accepted altered states are sleep and dreaming. Although dream sleep and non-dream sleep appear very similar to an outside observer, each is associated with a distinct pattern of brain activity, metabolic activity, and eye movement; each is also associated with a distinct pattern of experience and cognition. During ordinary non-dream sleep, people who are awakened report only vague and sketchy thoughts, and their experiences do not cohere into a continuous narrative. During dream sleep, in contrast, people who are awakened report rich and detailed experiences in which events form a continuous progression, which may however be interrupted by bizarre or fantastic intrusions. Thought processes during the dream state frequently show a high level of irrationality. Both dream and non-dream states are associated with severe disruption of memory: it usually disappears in seconds during the non-dream state, and in minutes after awakening from a dream unless actively refreshed.\nResearch conducted on the effects of partial epileptic seizures on consciousness found that patients who have partial epileptic seizures experience altered states of consciousness. In partial epileptic seizures, consciousness is impaired or lost while some aspects of consciousness, often automated behaviors, remain intact. Studies found that when measuring the qualitative features during partial epileptic seizures, patients exhibited an increase in arousal and became absorbed in the experience of the seizure, followed by difficulty in focusing and shifting attention.\nA variety of psychoactive drugs, including alcohol, have notable effects on consciousness. These range from a simple dulling of awareness produced by sedatives, to increases in the intensity of sensory qualities produced by stimulants, cannabis, empathogens\u2013entactogens such as MDMA (\"Ecstasy\"), or most notably by the class of drugs known as psychedelics. LSD, mescaline, psilocybin, dimethyltryptamine, and others in this group can produce major distortions of perception, including hallucinations; some users even describe their drug-induced experiences as mystical or spiritual in quality. The brain mechanisms underlying these effects are not as well understood as those induced by use of alcohol, but there is substantial evidence that alterations in the brain system that uses the chemical neurotransmitter serotonin play an essential role.\nThere has been some research into physiological changes in yogis and people who practise various techniques of meditation. Some research with brain waves during meditation has reported differences between those corresponding to ordinary relaxation and those corresponding to meditation. It has been disputed, however, whether there is enough evidence to count these as physiologically distinct states of consciousness.\nThe most extensive study of the characteristics of altered states of consciousness was made by psychologist Charles Tart in the 1960s and 1970s. Tart analyzed a state of consciousness as made up of a number of component processes, including exteroception (sensing the external world); interoception (sensing the body); input-processing (seeing meaning); emotions; memory; time sense; sense of identity; evaluation and cognitive processing; motor output; and interaction with the environment. Each of these, in his view, could be altered in multiple ways by drugs or other manipulations. The components that Tart identified have not, however, been validated by empirical studies. Research in this area has not yet reached firm conclusions, but a recent questionnaire-based study identified eleven significant factors contributing to drug-induced states of consciousness: experience of unity; spiritual experience; blissful state; insightfulness; disembodiment; impaired control and cognition; anxiety; complex imagery; elementary imagery; audio-visual synesthesia; and changed meaning of percepts.\nMedical aspects.\nThe medical approach to consciousness is scientifically oriented. It derives from a need to treat people whose brain function has been impaired as a result of disease, brain damage, toxins, or drugs. In medicine, conceptual distinctions are considered useful to the degree that they can help to guide treatments. The medical approach focuses mostly on the amount of consciousness a person has: in medicine, consciousness is assessed as a \"level\" ranging from coma and brain death at the low end, to full alertness and purposeful responsiveness at the high end.\nConsciousness is of concern to patients and physicians, especially neurologists and anesthesiologists. Patients may have disorders of consciousness or may need to be anesthetized for a surgical procedure. Physicians may perform consciousness-related interventions such as instructing the patient to sleep, administering general anesthesia, or inducing medical coma. Also, bioethicists may be concerned with the ethical implications of consciousness in medical cases of patients such as the Karen Ann Quinlan case, while neuroscientists may study patients with impaired consciousness in hopes of gaining information about how the brain works.\nAssessment.\nIn medicine, consciousness is examined using a set of procedures known as neuropsychological assessment. There are two commonly used methods for assessing the level of consciousness of a patient: a simple procedure that requires minimal training, and a more complex procedure that requires substantial expertise. The simple procedure begins by asking whether the patient is able to move and react to physical stimuli. If so, the next question is whether the patient can respond in a meaningful way to questions and commands. If so, the patient is asked for name, current location, and current day and time. A patient who can answer all of these questions is said to be \"alert and oriented times four\" (sometimes denoted \"A&amp;Ox4\" on a medical chart), and is usually considered fully conscious.\nThe more complex procedure is known as a neurological examination, and is usually carried out by a neurologist in a hospital setting. A formal neurological examination runs through a precisely delineated series of tests, beginning with tests for basic sensorimotor reflexes, and culminating with tests for sophisticated use of language. The outcome may be summarized using the Glasgow Coma Scale, which yields a number in the range 3\u201315, with a score of 3 to 8 indicating coma, and 15 indicating full consciousness. The Glasgow Coma Scale has three subscales, measuring the best motor response (ranging from \"no motor response\" to \"obeys commands\"), the best eye response (ranging from \"no eye opening\" to \"eyes opening spontaneously\") and the best verbal response (ranging from \"no verbal response\" to \"fully oriented\"). There is also a simpler pediatric version of the scale, for children too young to be able to use language.\nIn 2013, an experimental procedure was developed to measure degrees of consciousness, the procedure involving stimulating the brain with a magnetic pulse, measuring resulting waves of electrical activity, and developing a consciousness score based on the complexity of the brain activity.\nDisorders.\nMedical conditions that inhibit consciousness are considered disorders of consciousness. This category generally includes minimally conscious state and persistent vegetative state, but sometimes also includes the less severe locked-in syndrome and more severe chronic coma. Differential diagnosis of these disorders is an active area of biomedical research. Finally, brain death results in possible irreversible disruption of consciousness. While other conditions may cause a moderate deterioration (e.g., dementia and delirium) or transient interruption (e.g., grand mal and petit mal seizures) of consciousness, they are not included in this category.\nMedical experts increasingly view anosognosia as a disorder of consciousness. \"Anosognosia\" a Greek-derived term meaning \"unawareness of disease\". This is a condition in which patients are disabled in some way, most commonly as a result of a stroke, but either misunderstand the nature of the problem or deny that there is anything wrong with them. The most frequently occurring form is seen in people who have experienced a stroke damaging the parietal lobe in the right hemisphere of the brain, giving rise to a syndrome known as hemispatial neglect, characterized by an inability to direct action or attention toward objects located to the left with respect to their bodies. Patients with hemispatial neglect are often paralyzed on the left side of the body, but sometimes deny being unable to move. When questioned about the obvious problem, the patient may avoid giving a direct answer, or may give an explanation that doesn't make sense. Patients with hemispatial neglect may also fail to recognize paralyzed parts of their bodies: one frequently mentioned case is of a man who repeatedly tried to throw his own paralyzed right leg out of the bed he was lying in, and when asked what he was doing, complained that somebody had put a dead leg into the bed with him. An even more striking type of anosognosia is Anton\u2013Babinski syndrome, a rarely occurring condition in which patients become blind but claim to be able to see normally, and persist in this claim in spite of all evidence to the contrary.\nOutside human adults.\nIn children.\nOf the eight types of consciousness in the Lycan classification, some are detectable in utero and others develop years after birth. Psychologist and educator William Foulkes studied children's dreams and concluded that prior to the shift in cognitive maturation that humans experience during ages five to seven, children lack the Lockean consciousness that Lycan had labeled \"introspective consciousness\" and that Foulkes labels \"self-reflection.\" In a 2020 paper, Katherine Nelson and Robyn Fivush use \"autobiographical consciousness\" to label essentially the same faculty, and agree with Foulkes on the timing of this faculty's acquisition. Nelson and Fivush contend that \"language is the tool by which humans create a new, uniquely human form of consciousness, namely, autobiographical consciousness.\" Julian Jaynes had staked out these positions decades earlier. Citing the developmental steps that lead the infant to autobiographical consciousness, Nelson and Fivush point to the acquisition of \"theory of mind,\" calling theory of mind \"necessary for autobiographical consciousness\" and defining it as \"understanding differences between one's own mind and others' minds in terms of beliefs, desires, emotions and thoughts.\" They write, \"The hallmark of theory of mind, the understanding of false belief, occurs ... at five to six years of age.\"\nIn animals.\nThe topic of animal consciousness is beset by a number of difficulties. It poses the problem of other minds in an especially severe form, because non-human animals, lacking the ability to express human language, cannot tell humans about their experiences. Also, it is difficult to reason objectively about the question, because a denial that an animal is conscious is often taken to imply that it does not feel, its life has no value, and that harming it is not morally wrong. Descartes, for example, has sometimes been blamed for mistreatment of animals due to the fact that he believed only humans have a non-physical mind. Most people have a strong intuition that some animals, such as cats and dogs, are conscious, while others, such as insects, are not; but the sources of this intuition are not obvious, and are often based on personal interactions with pets and other animals they have observed.\nPhilosophers who consider subjective experience the essence of consciousness also generally believe, as a correlate, that the existence and nature of animal consciousness can never rigorously be known. Thomas Nagel spelled out this point of view in an influential essay titled \"What Is it Like to Be a Bat?\". He said that an organism is conscious \"if and only if there is something that it is like to be that organism\u2014something it is like \"for\" the organism\"; and he argued that no matter how much we know about an animal's brain and behavior, we can never really put ourselves into the mind of the animal and experience its world in the way it does itself. Other thinkers, such as Douglas Hofstadter, dismiss this argument as incoherent. Several psychologists and ethologists have argued for the existence of animal consciousness by describing a range of behaviors that appear to show animals holding beliefs about things they cannot directly perceive\u2014Donald Griffin's 2001 book \"Animal Minds\" reviews a substantial portion of the evidence.\nOn July 7, 2012, eminent scientists from different branches of neuroscience gathered at the University of Cambridge to celebrate the Francis Crick Memorial Conference, which deals with consciousness in humans and pre-linguistic consciousness in nonhuman animals. After the conference, they signed in the presence of Stephen Hawking, the 'Cambridge Declaration on Consciousness', which summarizes the most important findings of the survey:\n\"We decided to reach a consensus and make a statement directed to the public that is not scientific. It's obvious to everyone in this room that animals have consciousness, but it is not obvious to the rest of the world. It is not obvious to the rest of the Western world or the Far East. It is not obvious to the society.\"\n\"Convergent evidence indicates that non-human animals ..., including all mammals and birds, and other creatures, ... have the necessary neural substrates of consciousness and the capacity to exhibit intentional behaviors.\"\nIn artificial intelligence.\nThe idea of an artifact made conscious is an ancient theme of mythology, appearing for example in the Greek myth of Pygmalion, who carved a statue that was magically brought to life, and in medieval Jewish stories of the Golem, a magically animated homunculus built of clay. However, the possibility of actually constructing a conscious machine was probably first discussed by Ada Lovelace, in a set of notes written in 1842 about the Analytical Engine invented by Charles Babbage, a precursor (never built) to modern electronic computers. Lovelace was essentially dismissive of the idea that a machine such as the Analytical Engine could think in a humanlike way. She wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It is desirable to guard against the possibility of exaggerated ideas that might arise as to the powers of the Analytical Engine.\u00a0... The Analytical Engine has no pretensions whatever to \"originate\" anything. It can do whatever we \"know how to order it\" to perform. It can \"follow\" analysis; but it has no power of \"anticipating\" any analytical relations or truths. Its province is to assist us in making \"available\" what we are already acquainted with.\nOne of the most influential contributions to this question was an essay written in 1950 by pioneering computer scientist Alan Turing, titled \"Computing Machinery and Intelligence\". Turing disavowed any interest in terminology, saying that even \"Can machines think?\" is too loaded with spurious connotations to be meaningful; but he proposed to replace all such questions with a specific operational test, which has become known as the Turing test. To pass the test, a computer must be able to imitate a human well enough to fool interrogators. In his essay Turing discussed a variety of possible objections, and presented a counterargument to each of them. The Turing test is commonly cited in discussions of artificial intelligence as a proposed criterion for machine consciousness; it has provoked a great deal of philosophical debate. For example, Daniel Dennett and Douglas Hofstadter argue that anything capable of passing the Turing test is necessarily conscious, while David Chalmers argues that a philosophical zombie could pass the test, yet fail to be conscious. A third group of scholars have argued that with technological growth once machines begin to display any substantial signs of human-like behavior then the dichotomy (of human consciousness compared to human-like consciousness) becomes pass\u00e9 and issues of machine autonomy begin to prevail even as observed in its nascent form within contemporary industry and technology. J\u00fcrgen Schmidhuber argues that consciousness is the result of compression. As an agent sees representation of itself recurring in the environment, the compression of this representation can be called consciousness.\nIn a lively exchange over what has come to be referred to as \"the Chinese room argument\", John Searle sought to refute the claim of proponents of what he calls \"strong artificial intelligence (AI)\" that a computer program can be conscious, though he does agree with advocates of \"weak AI\" that computer programs can be formatted to \"simulate\" conscious states. His own view is that consciousness has subjective, first-person causal powers by being essentially intentional due to the way human brains function biologically; conscious persons can perform computations, but consciousness is not inherently computational the way computer programs are. To make a Turing machine that speaks Chinese, Searle imagines a room with one monolingual English speaker (Searle himself, in fact), a book that designates a combination of Chinese symbols to be output paired with Chinese symbol input, and boxes filled with Chinese symbols. In this case, the English speaker is acting as a computer and the rulebook as a program. Searle argues that with such a machine, he would be able to process the inputs to outputs perfectly without having any understanding of Chinese, nor having any idea what the questions and answers could possibly mean. If the experiment were done in English, since Searle knows English, he would be able to take questions and give answers without any algorithms for English questions, and he would be effectively aware of what was being said and the purposes it might serve. Searle would pass the Turing test of answering the questions in both languages, but he is only conscious of what he is doing when he speaks English. Another way of putting the argument is to say that computer programs can pass the Turing test for processing the syntax of a language, but that the syntax cannot lead to semantic meaning in the way strong AI advocates hoped.\nIn the literature concerning artificial intelligence, Searle's essay has been second only to Turing's in the volume of debate it has generated. Searle himself was vague about what extra ingredients it would take to make a machine conscious: all he proposed was that what was needed was \"causal powers\" of the sort that the brain has and that computers lack. But other thinkers sympathetic to his basic argument have suggested that the necessary (though perhaps still not sufficient) extra conditions may include the ability to pass not just the verbal version of the Turing test, but the robotic version, which requires grounding the robot's words in the robot's sensorimotor capacity to categorize and interact with the things in the world that its words are about, Turing-indistinguishably from a real person. Turing-scale robotics is an empirical branch of research on embodied cognition and situated cognition.\nIn 2014, Victor Argonov has suggested a non-Turing test for machine consciousness based on machine's ability to produce philosophical judgments. He argues that a deterministic machine must be regarded as conscious if it is able to produce judgments on all problematic properties of consciousness (such as qualia or binding) having no innate (preloaded) philosophical knowledge on these issues, no philosophical discussions while learning, and no informational models of other creatures in its memory (such models may implicitly or explicitly contain knowledge about these creatures' consciousness). However, this test can be used only to detect, but not refute the existence of consciousness. A positive result proves that machine is conscious but a negative result proves nothing. For example, absence of philosophical judgments may be caused by lack of the machine's intellect, not by absence of consciousness.\nStream of consciousness.\nWilliam James is usually credited with popularizing the idea that human consciousness flows like a stream, in his \"Principles of Psychology\" of 1890.\nAccording to James, the \"stream of thought\" is governed by five characteristics: \nA similar concept appears in Buddhist philosophy, expressed by the Sanskrit term \"Citta-sa\u1e43t\u0101na\", which is usually translated as mindstream or \"mental continuum\". Buddhist teachings describe that consciousness manifests moment to moment as sense impressions and mental phenomena that are continuously changing. The teachings list six triggers that can result in the generation of different mental events. These triggers are input from the five senses (seeing, hearing, smelling, tasting or touch sensations), or a thought (relating to the past, present or the future) that happen to arise in the mind. The mental events generated as a result of these triggers are: feelings, perceptions and intentions/behaviour. The moment-by-moment manifestation of the mind-stream is said to happen in every person all the time. It even happens in a scientist who analyses various phenomena in the world, or analyses the material body including the organ brain. The manifestation of the mindstream is also described as being influenced by physical laws, biological laws, psychological laws, volitional laws, and universal laws. The purpose of the Buddhist practice of mindfulness is to understand the inherent nature of the consciousness and its characteristics.\nNarrative form.\nIn the West, the primary impact of the idea has been on literature rather than science: \"stream of consciousness as a narrative mode\" means writing in a way that attempts to portray the moment-to-moment thoughts and experiences of a character. This technique perhaps had its beginnings in the monologues of Shakespeare's plays and reached its fullest development in the novels of James Joyce and Virginia Woolf, although it has also been used by many other noted writers.\nHere, for example, is a passage from Joyce's \"Ulysses\" about the thoughts of Molly Bloom:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Yes because he never did a thing like that before as ask to get his breakfast in bed with a couple of eggs since the City Arms hotel when he used to be pretending to be laid up with a sick voice doing his highness to make himself interesting for that old faggot Mrs Riordan that he thought he had a great leg of and she never left us a farthing all for masses for herself and her soul greatest miser ever was actually afraid to lay out 4d for her methylated spirit telling me all her ailments she had too much old chat in her about politics and earthquakes and the end of the world let us have a bit of fun first God help the world if all the women were her sort down on bathingsuits and lownecks of course nobody wanted her to wear them I suppose she was pious because no man would look at her twice I hope Ill never be like her a wonder she didnt want us to cover our faces but she was a welleducated woman certainly and her gabby talk about Mr Riordan here and Mr Riordan there I suppose he was glad to get shut of her.\nSpiritual approaches.\nTo most philosophers, the word \"consciousness\" connotes the relationship between the mind and the world. To writers on spiritual or religious topics, it frequently connotes the relationship between the mind and God, or the relationship between the mind and deeper truths that are thought to be more fundamental than the physical world. The mystical psychiatrist Richard Maurice Bucke, author of the 1901 book \"Cosmic Consciousness: A Study in the Evolution of the Human Mind\",\ndistinguished between three types of consciousness: 'Simple Consciousness', awareness of the body, possessed by many animals; 'Self Consciousness', awareness of being aware, possessed only by humans; and 'Cosmic Consciousness', awareness of the life and order of the universe, possessed only by humans who are enlightened. Many more examples could be given, such as the various levels of spiritual consciousness presented by Prem Saran Satsangi and Stuart Hameroff.\nAnother thorough account of the spiritual approach is Ken Wilber's 1977 book \"The Spectrum of Consciousness\", a comparison of western and eastern ways of thinking about the mind. Wilber described consciousness as a spectrum with ordinary awareness at one end, and more profound types of awareness at higher levels.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;"}
{"id": "5665", "revid": "35498457", "url": "https://en.wikipedia.org/wiki?curid=5665", "title": "Currency", "text": "Generally accepted medium of exchange for goods or services\nA currency is a standardization of money in any form, in use or circulation as a medium of exchange, for example banknotes and coins.\nA more general definition is that a currency is a \"system of money\" in common use within a specific environment over time, especially for people in a nation state. Under this definition, the British Pound Sterling (\u00a3), euros (\u20ac), Japanese yen (\u00a5), and U.S. dollars (US$) are examples of (government-issued) fiat currencies. Currencies may act as stores of value and be traded between nations in foreign exchange markets, which determine the relative values of the different currencies. Currencies in this sense are either chosen by users or decreed by governments, and each type has limited boundaries of acceptance; i.e., legal tender laws may require a particular unit of account for payments to government agencies.\nOther definitions of the term \"currency\" appear in the respective synonymous articles: banknote, coin, and money. This article uses the definition which focuses on the currency systems of countries.\nOne can classify currencies into three monetary systems: fiat money, commodity money, and representative money, depending on what guarantees a currency's value (the economy at large vs. the government's physical metal reserves). Some currencies function as legal tender in certain jurisdictions, or for specific purposes, such as payment to a government (taxes), or government agencies (fees, fines). Others simply get traded for their economic value.\nDigital currency has arisen with the popularity of computers and the Internet. Whether government-backed digital notes and coins (such as the digital renminbi in China, for example) will be successfully developed and utilized remains dubious. Decentralized digital currencies, such as cryptocurrencies, are different because they are not issued by a government monetary authority; specifically, bitcoin, the first cryptocurrency and leader in terms of market capitalization, has a fixed supply and is therefore ostensibly deflationary. Many warnings issued by various countries note the opportunities that cryptocurrencies create for illegal activities such as scams, ransomware, money laundering and terrorism.\nIn 2014, the United States IRS issued a statement explaining that virtual currency is treated as property for Federal income-tax purposes, and it provide examples of how long-standing tax principles applicable to transactions involving property apply to virtual currency. \nHistory.\nEarly currency.\nOriginally, currency was a form of receipt, representing grain stored in temple granaries in Sumer in ancient Mesopotamia and in Ancient Egypt.\nIn this first stage of currency, metals were used as symbols to represent value stored in the form of commodities. This formed the basis of trade in the Fertile Crescent for over 1500 years. However, the collapse of the Near Eastern trading system pointed to a flaw: in an era where there was no place that was safe to store value, the value of a circulating medium could only be as sound as the forces that defended that store. A trade could only reach as far as the credibility of that military. By the late Bronze Age, however, a series of treaties had established safe passage for merchants around the Eastern Mediterranean, spreading from Minoan Crete and Mycenae in the northwest to Elam and Bahrain in the southeast. It is not known what was used as a currency for these exchanges, but it is thought that oxhide-shaped ingots of copper, produced in Cyprus, may have functioned as a currency.\nIt is thought that the increase in piracy and raiding associated with the Bronze Age collapse, possibly produced by the Peoples of the Sea, brought the trading system of oxhide ingots to an end. It was only the recovery of Phoenician trade in the 10th and 9th centuries BC that led to a return to prosperity, and the appearance of real coinage, possibly first in Anatolia with Croesus of Lydia and subsequently with the Greeks and Persians. In Africa, many forms of value store have been used, including beads, ingots, ivory, various forms of weapons, livestock, the manilla currency, and ochre and other earth oxides. The manilla rings of West Africa were one of the currencies used from the 15th century onwards to sell slaves. African currency is still notable for its variety, and in many places, various forms of barter still apply.\nCoinage.\nThe prevalence of metal coins possibly led to the metal itself being the store of value: first copper, then both silver and gold, and at one point also bronze. Today other non-precious metals are used for coins. Metals were mined, weighed, and stamped into coins. This was to assure the individual accepting the coin that he was getting a certain known weight of precious metal. Coins could be counterfeited, but the existence of standard coins also created a new unit of account, which helped lead to banking. Archimedes' principle provided the next link: coins could now be easily tested for their fine weight of the metal, and thus the value of a coin could be determined, even if it had been shaved, debased or otherwise tampered with (see Numismatics).\nMost major economies using coinage had several tiers of coins of different values, made of copper, silver, and gold. Gold coins were the most valuable and were used for large purchases, payment of the military, and backing of state activities. Units of account were often defined as the value of a particular type of gold coin. Silver coins were used for midsized transactions, and sometimes also defined a unit of account, while coins of copper or silver, or some mixture of them (see debasement), might be used for everyday transactions. This system had been used in ancient India since the time of the Mahajanapadas. The exact ratios between the values of the three metals varied greatly between different eras and places; for example, the opening of silver mines in the Harz mountains of central Europe made silver relatively less valuable, as did the flood of New World silver after the Spanish conquests. However, the rarity of gold consistently made it more valuable than silver, and likewise silver was consistently worth more than copper.\nPaper money.\nIn premodern China, the need for lending and for a medium of exchange that was less physically cumbersome than large numbers of copper coins led to the introduction of paper money, i.e. banknotes. Their introduction was a gradual process that lasted from the late Tang dynasty (618\u2013907) into the Song dynasty (960\u20131279). It began as a means for merchants to exchange heavy coinage for receipts of deposit issued as promissory notes by wholesalers' shops. These notes were valid for temporary use in a small regional territory. In the 10th century, the Song dynasty government began to circulate these notes amongst the traders in its monopolized salt industry. The Song government granted several shops the right to issue banknotes, and in the early 12th century the government finally took over these shops to produce state-issued currency. Yet the banknotes issued were still only locally and temporarily valid: it was not until the mid 13th century that a standard and uniform government issue of paper money became an acceptable nationwide currency. The already widespread methods of woodblock printing and then Bi Sheng's movable type printing by the 11th century were the impetus for the mass production of paper money in premodern China.\nAt around the same time in the medieval Islamic world, a vigorous monetary economy was created during the 7th\u201312th centuries on the basis of the expanding levels of circulation of a stable high-value currency (the dinar). Innovations introduced by Muslim economists, traders and merchants include the earliest uses of credit, cheques, promissory notes, savings accounts, transaction accounts, loaning, trusts, exchange rates, the transfer of credit and debt, and banking institutions for loans and deposits.\nIn Europe, paper currency was first introduced on a regular basis in Sweden in 1661 (although Washington Irving records an earlier emergency use of it, by the Spanish in a siege during the Conquest of Granada). As Sweden was rich in copper, many copper coins were in circulation, but its relatively low value necessitated extraordinarily big coins, often weighing several kilograms.\nThe advantages of paper currency were numerous: it reduced the need to transport gold and silver, which was risky; it facilitated loans of gold or silver at interest, since the underlying specie (money in the form of gold or silver coins rather than notes) never left the possession of the lender until someone else redeemed the note; and it allowed a division of currency into credit- and specie-backed forms. It enabled the sale of investment in joint-stock companies and the redemption of those shares in a paper.\nBut there were also disadvantages. First, since a note has no intrinsic value, there was nothing to stop issuing authorities from printing more notes than they had specie to back them with. Second, because this increased the money supply, it increased inflationary pressures, a fact observed by David Hume in the 18th century. Thus paper money would often lead to an inflationary bubble, which could collapse if people began demanding hard money, causing the demand for paper notes to fall to zero. The printing of paper money was also associated with wars, and financing of wars, and therefore regarded as part of maintaining a standing army. For these reasons, paper currency was held in suspicion and hostility in Europe and America. It was also addictive since the speculative profits of trade and capital creation were quite large. Major nations established mints to print money and mint coins, and branches of their treasury to collect taxes and hold gold and silver stock.\nAt that time, both silver and gold were considered a legal tender and accepted by governments for taxes. However, the instability in the exchange rate between the two grew over the course of the 19th century, with the increases both in the supply of these metals, particularly silver, and in trade. The parallel use of both metals is called bimetallism, and the attempt to create a bimetallic standard where both gold and silver backed currency remained in circulation occupied the efforts of inflationists. Governments at this point could use currency as an instrument of policy, printing paper currency such as the United States greenback, to pay for military expenditures. They could also set the terms at which they would redeem notes for specie, by limiting the amount of purchase, or the minimum amount that could be redeemed.\nBy 1900, most of the industrializing nations were on some form of gold standard, with paper notes and silver coins constituting the circulating medium. Private banks and governments across the world followed Gresham's law: keeping the gold and silver they received but paying out in notes. This did not happen all around the world at the same time, but occurred sporadically, generally in times of war or financial crisis, beginning in the early 20th century and continuing across the world until the late 20th century, when the regime of floating fiat currencies came into force. One of the last countries to break away from the gold standard was the United States in 1971, an action which was known as the Nixon shock. No country has an enforceable gold standard or silver standard currency system.\nBanknote era.\nA banknote or a bill is a type of currency and it is commonly used as legal tender in many jurisdictions. Together with coins, banknotes make up the cash form of a currency. Banknotes were initially mostly paper, but Australia's Commonwealth Scientific and Industrial Research Organisation developed a polymer currency in the 1980s; it went into circulation on the nation's bicentenary in 1988. Polymer banknotes had already been introduced in the Isle of Man in 1983. As of 2016, polymer currency is used in over 20 countries (over 40 if counting commemorative issues), and dramatically increases the life span of banknotes and reduces counterfeiting.\nModern currencies.\nThe currency used is based on the concept of lex monetae; that a sovereign state decides which currency it shall use. The International Organization for Standardization has introduced a system of three-letter codes (ISO 4217) to denote currency (as opposed to simple names or currency signs), in order to remove the confusion arising because there are dozens of currencies called the dollar and several called the franc. Even the \"pound\" is used in nearly a dozen different countries; most of these are tied to the pound sterling, while the remainder has varying values. In general, the three-letter code uses the ISO 3166-1 country code for the first two letters and the first letter of the name of the currency (D for dollar, for example) as the third letter. United States currency, for instance, is globally referred to as USD. Currencies such as the pound sterling have different codes, as the first two letters denote not the exact country name but an alternative name also used to describe the country. The pound's code is GBP where \"GB\" denotes Great Britain instead of the United Kingdom.\nThe former currencies include the marks that were in circulation in Germany and Finland.\nThe International Monetary Fund uses a different system when referring to national currencies.\nAlternative currencies.\nDistinct from centrally controlled government-issued currencies, private decentralized trust-reduced networks support alternative currencies such as bitcoin and Ethereum's ether, which are classified as cryptocurrency since the transfer of value is assured through cryptographic signatures validated by all users. There are also branded currencies, for example 'obligation' based stores of value, such as quasi-regulated BarterCard, Loyalty Points (Credit Cards, Airlines) or Game-Credits (MMO games) that are based on reputation of commercial products, or highly regulated 'asset-backed' 'alternative currencies' such as mobile-money schemes like MPESA (called E-Money Issuance).\nThe currency may be Internet-based and digital, for instance, bitcoin is not tied to any specific country, or the IMF's SDR that is based on a basket of currencies (and assets held).\nPossession and sale of alternative forms of currencies is often outlawed by governments in order to preserve the legitimacy of the constitutional currency for the benefit of all citizens. For example, Article I, section 8, clause 5 of the United States Constitution delegates to Congress the power to coin money and to regulate the value thereof. This power was delegated to Congress in order to establish and preserve a uniform standard of value and to insure a singular monetary system for all purchases and debts in the United States, public and private. Along with the power to coin money, the United States Congress has the concurrent power to restrain the circulation of money which is not issued under its own authority in order to protect and preserve the constitutional currency. It is a violation of federal law for individuals, or organizations to create private coin or currency systems to compete with the official coinage and currency of the United States.\nControl and production.\nIn most cases, a central bank has the exclusive power to issue all forms of currency, including coins and banknotes (fiat money), and to restrain the circulation alternative currencies for its own area of circulation (a country or group of countries); it regulates the production of currency by banks (credit) through monetary policy.\nAn exchange rate is a price at which two currencies can be exchanged against each other. This is used for trade between the two currency zones. Exchange rates can be classified as either floating or fixed. In the former, day-to-day movements in exchange rates are determined by the market; in the latter, governments intervene in the market to buy or sell their currency to balance supply and demand at a static exchange rate.\nIn cases where a country has control of its own currency, that control is exercised either by a central bank or by a Ministry of Finance. The institution that has control of monetary policy is referred to as the monetary authority. Monetary authorities have varying degrees of autonomy from the governments that create them. A monetary authority is created and supported by its sponsoring government, so independence can be reduced by the legislative or executive authority that creates it.\nSeveral countries can use the same name for their own separate currencies (for example, a \"dollar\" in Australia, Canada, and the United States). By contrast, several countries can also use the same currency (for example, the euro or the CFA franc), or one country can declare the currency of another country to be legal tender. For example, Panama and El Salvador have declared US currency to be legal tender, and from 1791 to 1857, Spanish dollars were legal tender in the United States. At various times countries have either re-stamped foreign coins or used currency boards, issuing one note of currency for each note of a foreign government held, as Ecuador currently does.\nEach currency typically has a main currency unit (the dollar, for example, or the euro) and a fractional unit, often defined as &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u2044100 of the main unit: 100 cents\u00a0= 1\u00a0dollar, 100 centimes\u00a0= 1\u00a0franc, 100 pence\u00a0= 1\u00a0pound, although units of &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u204410 or &lt;templatestyles src=\"Fraction/styles.css\" /&gt;1\u20441000 occasionally also occur. Some currencies do not have any smaller units at all, such as the Icelandic kr\u00f3na and the Japanese yen.\nMauritania and Madagascar are the only remaining countries that have theoretical fractional units not based on the decimal system; instead, the Mauritanian ouguiya is in theory divided into 5 khoums, while the Malagasy ariary is theoretically divided into 5 iraimbilanja. In these countries, words like \"dollar\" or \"pound\" \"were simply names for given weights of gold\". Due to inflation khoums and iraimbilanja have in practice fallen into disuse. (See non-decimal currencies for other historic currencies with non-decimal divisions.)\nCurrency convertibility.\nSubject to variation around the world, local currency can be converted to another currency or vice versa with or without central bank/government intervention. Such conversions take place in the foreign exchange market. Based on the above restrictions or free and readily conversion features, currencies are classified as:\nAccording to the three aspects of trade in goods and services, capital flows and national policies, the supply-demand relationship of different currencies determines the exchange ratio between currencies.\nTrade in goods and services\nThrough cost transfer, goods and services circulating in the country (such as hotels, tourism, catering, advertising, household services) will indirectly affect the trade cost of goods and services and the price of export trade. Therefore, services and goods involved in international trade are not the only reason affecting the exchange rate. The large number of international tourists and overseas students has resulted in the flow of services and goods at home and abroad. It also represents that the competitiveness of global goods and services directly affects the change of international exchange rates.\nCapital flows\nNational currencies will be traded on international markets for investment purposes. Investment opportunities in each country attract other countries into investment programs, so that these foreign currencies become the reserves of the central banks of each country. The exchange rate mechanism, in which currencies are quoted continuously between countries, is based on foreign exchange markets in which currencies are invested by individuals and traded or speculated by central banks and investment institutions. In addition, changes in interest rates, capital market fluctuations and changes in investment opportunities will affect the global capital inflows and outflows of countries around the world, and exchange rates will fluctuate accordingly.\nNational policies\nThe country's foreign trade, monetary and fiscal policies affect the exchange rate fluctuations. Foreign trade includes policies such as tariffs and import standards for commodity exports. The impact of monetary policy on the total amount and yield of money directly determines the changes in the international exchange rate. Fiscal policies, such as transfer payments, taxation ratios, and other factors, dominate the profitability of capital and economic development, and the ratio of national debt issuance to deficit determines the repayment capacity and credit rating of the country. Such policies determine the mechanism of linking domestic and foreign currencies and therefore have a significant impact on the generation of exchange rates.\nCurrency convertibility is closely linked to economic development and finance. There are strict conditions for countries to achieve currency convertibility, which is a good way for countries to improve their economies. The currencies of some countries or regions in the world are freely convertible, such as the US dollar, Australian dollar and Japanese yen. The requirements for currency convertibility can be roughly divided into four parts:\nWith a freely convertible currency, domestic firms will have to compete fiercely with their foreign counterparts. The development of competition among them will affect the implementation effect of currency convertibility. In addition, microeconomics is a prerequisite for macroeconomic conditions.\nSince currency convertibility is the cross-border flow of goods and capital, it will have an impact on the macro economy. This requires that the national economy be in a normal and orderly state, that is, there is no serious inflation and economic overheating. In addition, the government should use macro policies to make mature adjustments to deal with the impact of currency exchange on the economy.\nThe maintainability of international balance of payments is the main performance of reasonable economic structure. Currency convertibility not only causes difficulties in the sustainability of international balance of payments but also affects the government's direct control over international economic transactions. To eliminate the foreign exchange shortage, the government needs adequate international reserves.\nThe level of exchange rate is an important factor in maintaining exchange rate stability, both before and after currency convertibility. The exchange rate of freely convertible currency is too high or too low, which can easily trigger speculation and undermine the stability of macroeconomic and financial markets. Therefore, to maintain the level of exchange rate, a proper exchange rate regime is crucial.\nLocal currency.\nIn economics, a local currency is a currency not backed by a national government and intended to trade only in a small area. Advocates such as Jane Jacobs argue that this enables an economically depressed region to pull itself up, by giving the people living there a medium of exchange that they can use to exchange services and locally produced goods (in a broader sense, this is the original purpose of all money). Opponents of this concept argue that local currency creates a barrier that can interfere with economies of scale and comparative advantage and that in some cases they can serve as a means of tax evasion.\nLocal currencies can also come into being when there is economic turmoil involving the national currency. An example of this is the Argentinian economic crisis of 2002 in which IOUs issued by local governments quickly took on some of the characteristics of local currencies.\nOne of the best examples of a local currency is the original LETS currency, founded on Vancouver Island in the early 1980s. In 1982, the Canadian Central Bank's lending rates ran up to 14% which drove chartered bank lending rates as high as 19%. The resulting currency and credit scarcity left island residents with few options other than to create a local currency.\nList of major world payment currencies.\nThe following table are estimates of the 20 most frequently used currencies in world payments in April 2023 by SWIFT.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5666", "revid": "10951369", "url": "https://en.wikipedia.org/wiki?curid=5666", "title": "Central bank", "text": "Government body that manages currency and monetary policy\nA central bank, reserve bank, or monetary authority is an institution that manages the currency and monetary policy of a country or monetary union,\nand oversees their commercial banking system. In contrast to a commercial bank, a central bank possesses a monopoly on increasing the monetary base. Most central banks also have supervisory and regulatory powers to ensure the stability of member institutions, to prevent bank runs, and to discourage reckless or fraudulent behavior by member banks.\nCentral banks in most developed nations are institutionally independent from political interference. Still, limited control by the executive and legislative bodies exists.\nIssues like central bank independence, central bank policies and rhetoric in central bank governors discourse or the premises of macroeconomic policies (monetary and fiscal policy) of the state are a focus of contention and criticism by some policymakers, researchers and specialized business, economics and finance media.\nActivities of central banks.\nFunctions of a central bank usually include:\nMonetary policy.\nCentral banks implement a country's chosen monetary policy.\nCurrency issuance.\nAt the most basic level, monetary policy involves establishing what form of currency the country may have, whether a fiat currency, gold-backed currency (disallowed for countries in the International Monetary Fund), currency board or a currency union. When a country has its own national currency, this involves the issue of some form of standardized currency, which is essentially a form of promissory note: \"money\" under certain circumstances. Historically, this was often a promise to exchange the money for precious metals in some fixed amount. Now, when many currencies are fiat money, the \"promise to pay\" consists of the promise to accept that currency to pay for taxes.\nA central bank may use another country's currency either directly in a currency union, or indirectly on a currency board. In the latter case, exemplified by the Bulgarian National Bank, Hong Kong and Latvia (until 2014), the local currency is backed at a fixed rate by the central bank's holdings of a foreign currency.\nSimilar to commercial banks, central banks hold assets (government bonds, foreign exchange, gold, and other financial assets) and incur liabilities (currency outstanding). Central banks create money by issuing banknotes and loaning them to the government in exchange for interest-bearing assets such as government bonds. When central banks decide to increase the money supply by an amount which is greater than the amount their national governments decide to borrow, the central banks may purchase private bonds or assets denominated in foreign currencies.\nThe European Central Bank remits its interest income to the central banks of the member countries of the European Union. The US Federal Reserve remits most of its profits to the U.S. Treasury. This income, derived from the power to issue currency, is referred to as seigniorage, and usually belongs to the national government. The state-sanctioned power to create currency is called the Right of Issuance. Throughout history, there have been disagreements over this power, since whoever controls the creation of currency controls the seigniorage income.\nThe expression \"monetary policy\" may also refer more narrowly to the interest-rate targets and other active measures undertaken by the monetary authority.\nGoals of central banks.\nPrice stability.\nThe primary role of central banks is usually to maintain price stability, as defined as a specific level of inflation. Inflation is defined either as the devaluation of a currency or equivalently the rise of prices relative to a currency. Most central banks currently have an inflation target close to 2%.\nSince inflation lowers real wages, Keynesians view inflation as the solution to involuntary unemployment. However, \"unanticipated\" inflation leads to lender losses as the real interest rate will be lower than expected. Thus, Keynesian monetary policy aims for a steady rate of inflation. A publication from the Austrian School, \"The Case Against the Fed\", argues that the efforts of the central banks to control inflation have been counterproductive.\nCentral banks as monetary authorities in representative states are intertwined through globalized financial markets. As a regulator of one of the most widespread currencies in the global economy, Federal Reserve (FED) plays a huge role in the international monetary market. Being the main supplier and rate adjusted for USD, FED implements a certain set of requirements to regulate inflation and unemployment in the US, willingly or unwillingly influencing the actions of Central Bank of Armenia (CBA). Armenia is a small country with a relatively weak economy and bears the consequences of FED policies the most.\nHigh employment.\nFrictional unemployment is the time period between jobs when a worker is searching for, or transitioning from one job to another. Unemployment beyond frictional unemployment is classified as unintended unemployment.\nFor example, structural unemployment is a form of unemployment resulting from a mismatch between demand in the labour market and the skills and locations of the workers seeking employment. Macroeconomic policy generally aims to reduce unintended unemployment.\nKeynes labeled any jobs that would be created by a rise in wage-goods (i.e., a decrease in real-wages) as involuntary unemployment:\nMen are involuntarily unemployed if, in the event of a small rise in the price of wage-goods relatively to the money-wage, both the aggregate supply of labour willing to work for the current money-wage and the aggregate demand for it at that wage would be greater than the existing volume of employment.\u2014 John Maynard Keynes, \"The General Theory of Employment, Interest and Money\" p1\nEconomic growth.\nEconomic growth can be enhanced by investment in capital, such as more or better machinery. A low interest rate implies that firms can borrow money to invest in their capital stock and pay less interest for it. Lowering the interest is therefore considered to encourage economic growth and is often used to alleviate times of low economic growth. On the other hand, raising the interest rate is often used in times of high economic growth as a contra-cyclical device to keep the economy from overheating and avoid market bubbles.\nFurther goals of monetary policy are stability of interest rates, of the financial market, and of the foreign exchange market.\nGoals frequently cannot be separated from each other and often conflict. Costs must therefore be carefully weighed before policy implementation.\nClimate change.\nIn the aftermath of the Paris agreement on climate change, a debate is now underway on whether central banks should also pursue environmental goals as part of their activities. In 2017, eight central banks formed the Network for Greening the Financial System (NGFS) to evaluate the way in which central banks can use their regulatory and monetary policy tools to support climate change mitigation. Today more than 70 central banks are part of the NGFS.\nIn January 2020, the European Central Bank has announced it will consider climate considerations when reviewing its monetary policy framework.\nProponents of \"green monetary policy\" are proposing that central banks include climate-related criteria in their collateral eligibility frameworks, when conducting asset purchases and also in their refinancing operations. But critics such as Jens Weidmann are arguing it is not central banks' role to conduct climate policy. China is among the most advanced central banks when it comes to green monetary policy. It has given green bonds preferential status to lower their yield and uses window policy to direct green lending.\nMonetary policy instruments.\nThe primary tools available to central banks are open market operations (including repurchase agreements), reserve requirements, interest rate policy (through control of the discount rate), and control of the money supply.\nA central bank affects the monetary base through open market operations, if its country has a well developed market for its government bonds. This entails managing the quantity of money in circulation through the buying and selling of various financial instruments, such as treasury bills, repurchase agreements or \"repos\", company bonds, or foreign currencies, in exchange for money on deposit at the central bank. Those deposits are convertible to currency, so all of these purchases or sales result in more or less base currency entering or leaving market circulation. For example, if the central bank wishes to decrease interest rates (executing expansionary monetary policy), it purchases government debt, thereby increasing the amount of cash in circulation or crediting banks' reserve accounts. Commercial banks then have more money to lend, so they reduce lending rates, making loans less expensive. Cheaper credit card interest rates increase consumer spending. Additionally, when business loans are more affordable, companies can expand to keep up with consumer demand. They ultimately hire more workers, whose incomes increase, which in its turn also increases the demand. This method is usually enough to stimulate demand and drive economic growth to a healthy rate. Usually, the short-term goal of open market operations is to achieve a specific short-term interest rate target. In other instances, monetary policy might instead entail the targeting of a specific exchange rate relative to some foreign currency or else relative to gold. For example, in the case of the United States the Federal Reserve targets the federal funds rate, the rate at which member banks lend to one another overnight; however, the monetary policy of China (since 2014) is to target the exchange rate between the Chinese renminbi and a basket of foreign currencies.\nIf the open market operations do not lead to the desired effects, a second tool can be used: the central bank can increase or decrease the interest rate it charges on discounts or overdrafts (loans from the central bank to commercial banks, see discount window). If the interest rate on such transactions is sufficiently low, commercial banks can borrow from the central bank to meet reserve requirements and use the additional liquidity to expand their balance sheets, increasing the credit available to the economy.\nA third alternative is to change the reserve requirements. The reserve requirement refers to the proportion of total liabilities that banks must keep on hand overnight, either in its vaults or at the central bank. Banks only maintain a small portion of their assets as cash available for immediate withdrawal; the rest is invested in illiquid assets like mortgages and loans. Lowering the reserve requirement frees up funds for banks to increase loans or buy other profitable assets. This is expansionary because it creates credit. However, even though this tool immediately increases liquidity, central banks rarely change the reserve requirement because doing so frequently adds uncertainty to banks' planning. The use of open market operations is therefore preferred.\nUnconventional monetary policy.\nOther forms of monetary policy, particularly used when interest rates are at or near 0% and there are concerns about deflation or deflation is occurring, are referred to as unconventional monetary policy. These include credit easing, quantitative easing, forward guidance, and signalling. In credit easing, a central bank purchases private sector assets to improve liquidity and improve access to credit. Signaling can be used to lower market expectations for lower interest rates in the future. For example, during the credit crisis of 2008, the US Federal Reserve indicated rates would be low for an \"extended period\", and the Bank of Canada made a \"conditional commitment\" to keep rates at the lower bound of 25 basis points (0.25%) until the end of the second quarter of 2010.\nSome have envisaged the use of what Milton Friedman once called \"helicopter money\" whereby the central bank would make direct transfers to citizens in order to lift inflation up to the central bank's intended target. Such policy option could be particularly effective at the zero lower bound.\nBanking supervision and other activities.\nIn some countries a central bank, through its subsidiaries, controls and monitors the banking sector. In other countries banking supervision is carried out by a government department such as the UK Treasury, or by an independent government agency, for example, UK's Financial Conduct Authority. It examines the banks' balance sheets and behaviour and policies toward consumers. Apart from refinancing, it also provides banks with services such as transfer of funds, bank notes and coins or foreign currency. Thus it is often described as the \"bank of banks\".\nMany countries will monitor and control the banking sector through several different agencies and for different purposes. The Bank regulation in the United States for example is highly fragmented with 3 federal agencies, the Federal Deposit Insurance Corporation, the Federal Reserve Board, or Office of the Comptroller of the Currency and numerous others on the state and the private level. There is usually significant cooperation between the agencies. For example, money center banks, deposit-taking institutions, and other types of financial institutions may be subject to different (and occasionally overlapping) regulation. Some types of banking regulation may be delegated to other levels of government, such as state or provincial governments.\nAny cartel of banks is particularly closely watched and controlled. Most countries control bank mergers and are wary of concentration in this industry due to the danger of groupthink and runaway lending bubbles based on a single point of failure, the credit culture of the few large banks.\nIndependence.\nNumerous governments have opted to make central banks independent. The economic logic behind central bank independence is that when governments delegate monetary policy to an independent central bank (with an anti-inflationary purpose) and away from elected politicians, monetary policy will not reflect the interests of the politicians. When governments control monetary policy, politicians may be tempted to boost economic activity in advance of an election to the detriment of the long-term health of the economy and the country. As a consequence, financial markets may not consider future commitments to low inflation to be credible when monetary policy is in the hands of elected officials, which increases the risk of capital flight. An alternative to central bank independence is to have fixed exchange rate regimes.\nGovernments generally have some degree of influence over even \"independent\" central banks; the aim of independence is primarily to prevent short-term interference. In 1951, the Deutsche Bundesbank became the first central bank to be given full independence, leading this form of central bank to be referred to as the \"Bundesbank model\", as opposed, for instance, to the New Zealand model, which has a goal (i.e. inflation target) set by the government.\nCentral bank independence is usually guaranteed by legislation and the institutional framework governing the bank's relationship with elected officials, particularly the minister of finance. Central bank legislation will enshrine specific procedures for selecting and appointing the head of the central bank. Often the minister of finance will appoint the governor in consultation with the central bank's board and its incumbent governor. In addition, the legislation will specify banks governor's term of appointment. The most independent central banks enjoy a fixed non-renewable term for the governor in order to eliminate pressure on the governor to please the government in the hope of being re-appointed for a second term. Generally, independent central banks enjoy both goal and instrument independence.\nDespite their independence, central banks are usually accountable at some level to government officials, either to the finance ministry or to parliament. For example, the Board of Governors of the U.S. Federal Reserve are nominated by the U.S. president and confirmed by the Senate, publishes verbatim transcripts, and balance sheets are audited by the Government Accountability Office.\nIn the 1990s there was a trend towards increasing the independence of central banks as a way of improving long-term economic performance. While a large volume of economic research has been done to define the relationship between central bank independence and economic performance, the results are ambiguous.\nThe literature on central bank independence has defined a cumulative and complementary number of aspects:\nThere is very strong consensus among economists that an independent central bank can run a more credible monetary policy, making market expectations more responsive to signals from the central bank. Both the Bank of England (1997) and the European Central Bank have been made independent and follow a set of published inflation targets so that markets know what to expect. Even the People's Bank of China has been accorded great latitude, though in China the official role of the bank remains that of a national bank rather than a central bank, underlined by the official refusal to \"unpeg\" the yuan or to revalue it \"under pressure\". The fact that the Communist Party is not elected also relieves the pressure to please people, increasing its independence. Populism can reduce de facto central bank independence.\nInternational organizations such as the World Bank, the Bank for International Settlements (BIS) and the International Monetary Fund (IMF) strongly support central bank independence. This results, in part, from a belief in the intrinsic merits of increased independence. The support for independence from the international organizations also derives partly from the connection between increased independence for the central bank and increased transparency in the policy-making process. The IMF's Financial Services Action Plan (FSAP) review self-assessment, for example, includes a number of questions about central bank independence in the transparency section. An independent central bank will score higher in the review than one that is not independent.\nCentral bank independence indices.\nCentral bank independence indices allow a quantitative analysis of central bank independence for individual countries over time. One central bank independence index is the Garriga CBI, where a higher index indicates higher central bank independence, shown below for individual countries.\nHistory.\nEarly history.\nThe use of money as a unit of account predates history. Government control of money is documented in the ancient Egyptian economy (2750\u20132150 BCE). The Egyptians measured the value of goods with a central unit called \"shat\". Like many other currencies, the shat was linked to gold. The value of a shat in terms of goods was defined by government administrations. Other cultures in Asia Minor later materialized their currencies in the form of gold and silver coins.\nIn the medieval and the early modern period a network of professional banks was established in Southern and Central Europe. The institutes built a new tier in the financial economy. The monetary system was still controlled by government institutions, mainly through the coinage prerogative. Banks, however, could use book money to create deposits for their customers. Thus, they had the possibility to issue, lend and transfer money autonomously without direct governmental control.\nIn order to consolidate the monetary system, a network of public exchange banks was established at the beginning of the 17th century in main European trade centres. The Amsterdam Wisselbank was founded as a first institute in 1609. Further exchange banks were located in Hamburg, Venice and Nuremberg. The institutes offered a public infrastructure for cashless international payments. They aimed to increase the efficiency of international trade and to safeguard monetary stability. The exchange banks thus fulfilled comparable functions to modern central banks. The institutes even issued their own (book) currency, called \"Mark Banco\".\nThe Bank of Amsterdam established in 1609 is considered to be the precursor to modern central banks. The central bank of Sweden (\"Sveriges Riksbank\" or simply \"Riksbanken\") was founded in Stockholm from the remains of the failed bank Stockholms Banco in 1664 and answered to the parliament (\"Riksdag of the Estates\"). One role of the Swedish central bank was lending money to the government.\nBank of England.\nThe establishment of the Bank of England, the model on which most modern central banks have been based, was devised by Charles Montagu, 1st Earl of Halifax, in 1694, following a proposal by the banker William Paterson three years earlier, which had not been acted upon. In the Kingdom of England in the 1690s, public funds were in short supply, and the credit of William III's government was so low in London that it was impossible for it to borrow the \u00a31,200,000 (at 8 percent) needed to finance the ongoing Nine Years' War with France. In order to induce subscription to the loan, Montagu proposed that the subscribers were to be incorporated as \"The Governor and Company of the Bank of England\" with long-term banking privileges including the issue of notes. The lenders would give the government cash (bullion) and also issue notes against the government bonds, which could be lent again. A royal charter was granted on 27 July through the passage of the Tonnage Act 1694. The bank was given exclusive possession of the government's balances, and was the only limited-liability corporation allowed to issue banknotes. The \u00a31.2\u00a0million was raised in 12 days; half of this was used to rebuild the navy.\nAlthough this establishment of the Bank of England marks the origin of central banking, it did not have the functions of a modern central bank, namely, to regulate the value of the national currency, to finance the government, to be the sole authorized distributor of banknotes, and to function as a 'lender of last resort' to banks suffering a liquidity crisis. These modern central banking functions evolved slowly through the 18th and 19th centuries.\nAlthough the bank was originally a private institution, by the end of the 18th century it was increasingly being regarded as a public authority with civic responsibility toward the upkeep of a healthy financial system. The currency crisis of 1797, caused by panicked depositors withdrawing from the bank led to the government suspending convertibility of notes into specie payment. The bank was soon accused by the bullionists of causing the exchange rate to fall from over issuing banknotes, a charge which the bank denied. Nevertheless, it was clear that the bank was being treated as an organ of the state.\nHenry Thornton, a merchant banker and monetary theorist has been described as the father of the modern central bank. An opponent of the real bills doctrine, he was a defender of the bullionist position and a significant figure in monetary theory. Thornton's process of monetary expansion anticipated the theories of Knut Wicksell regarding the \"cumulative process which restates the Quantity Theory in a theoretically coherent form\". As a response to the 1797 currency crisis, Thornton wrote in 1802 \"An Enquiry into the Nature and Effects of the Paper Credit of Great Britain\", in which he argued that the increase in paper credit did not cause the crisis. The book also gives a detailed account of the British monetary system as well as a detailed examination of the ways in which the Bank of England should act to counteract fluctuations in the value of the pound.\nUntil the mid-nineteenth century, commercial banks were able to issue their own banknotes, and notes issued by provincial banking companies were commonly in circulation. Many consider the origins of the central bank to lie with the passage of the Bank Charter Act 1844. Under the 1844 Act, bullionism was institutionalized in Britain, creating a ratio between the gold reserves held by the Bank of England and the notes that the bank could issue. The Act also placed strict curbs on the issuance of notes by the country banks.\nThe bank accepted the role of 'lender of last resort' in the 1870s after criticism of its lacklustre response to the Overend-Gurney crisis. The journalist Walter Bagehot wrote on the subject in \"\", in which he advocated for the bank to officially become a lender of last resort during a credit crunch, sometimes referred to as \"Bagehot's dictum\". Paul Tucker phrased the dictum in 2009 as follows:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;to avert panic, central banks should lend early and freely (ie without limit), to solvent firms, against good collateral, and at 'high rates'.\nSpread around the world.\nCentral banks were established in many European countries during the 19th century. Napoleon created the Banque de France in 1800, in an attempt to improve the financing of his wars.\nOn the continent of Europe, the Bank of France remained the most important central bank throughout the 19th century. The Bank of Finland was founded in 1812, soon after Finland had been taken over from Sweden by Russia to become its grand duchy. A central banking role was played by a small group of powerful family banking houses, typified by the House of Rothschild, with branches in major cities across Europe, as well as the Hottinguer family in Switzerland and the Oppenheim family in Germany.\nAlthough central banks today are generally associated with fiat money, the 19th and early 20th centuries central banks in most of Europe and Japan developed under the international gold standard. Free banking or currency boards were common at this time. Problems with collapses of banks during downturns, however, led to wider support for central banks in those nations which did not as yet possess them, most notably in Australia.\nAustralia established its first central bank in 1920, Peru in 1922, Colombia in 1923, Mexico and Chile in 1925 and Canada, India and New Zealand in the aftermath of the Great Depression in 1934. By 1935, the only significant independent nation that did not possess a central bank was Brazil, which subsequently developed a precursor thereto in 1945 and the present Central Bank of Brazil twenty years later. After gaining independence, African and Asian countries also established central banks or monetary unions. The Reserve Bank of India, which had been established during British colonial rule as a private company, was nationalized in 1949 following India's independence. The Central Bank of Armenia was founded on April 27, 1993, and the National Bank of Armenia was renamed into the Central Bank of the Republic of Armenia. It was under the governorship of Isahak Isahakyan who was governing the State Bank since 1986.\nThe People's Bank of China evolved its role as a central bank starting in about 1979 with the introduction of market reforms, which accelerated in 1989 when the country adopted a generally capitalist approach to its export economy. Evolving further partly in response to the European Central Bank, the People's Bank of China had by 2000 become a modern central bank. The most recent bank model was introduced together with the euro, and involves coordination of the European national banks, which continue to manage their respective economies separately in all respects other than currency exchange and base interest rates.\nUnited States.\nAlexander Hamilton as Secretary of the Treasury in the 1790s strongly promoted the banking system, and over heavy opposition from Jeffersonian Republicans, set up the First Bank of the United States. Jeffersonians allowed it to lapse, but the overwhelming financial difficulties of funding the War of 1812 without a central bank changed their minds. The Second Bank of the United States (1816\u20131836) under Nicholas Biddle functioned as a central bank, regulating the rapidly growing banking system. The role of a central bank was ended in the Bank War of the 1830s by President Andrew Jackson when he shut down the Second Bank as being too powerful and elitist.\nIn 1913 the United States created the Federal Reserve System through the passing of The Federal Reserve Act.\n21st century.\nAfter the financial crisis of 2007\u20132008 central banks led change, but as of 2015 their ability to boost economic growth has stalled. Central banks debate whether they should experiment with new measures like negative interest rates or direct financing of government, \"lean even more on politicians to do more\". Andy Haldane from the Bank of England said \"central bankers may need to accept that their good old days \u2013 of adjusting interest rates to boost employment or contain inflation \u2013 may be gone for good\". The European Central Bank and the Bank of Japan whose economies are in or close to deflation, continue quantitative easing \u2013 buying securities to encourage more lending.\nSince 2017, prospect of implementing Central Bank Digital Currency (CBDC) has been in discussion. As of the end of 2018, at least 15 central banks were considering to implementing CBDC. Since 2014, the People's Bank of China has been working on a project for digital currency to make its own digital currency and electronic payment systems.\nNaming of central banks.\nThere is no standard terminology for the name of a central bank, but many countries use the \"Bank of [Country]\" form\u2014for example: Bank of Canada, Bank of Mexico, Bank of Thailand. The United Kingdom does not follow this form as its central bank is the Bank of England (which, despite its name, is the central bank of the United Kingdom as a whole). The name's lack of representation of the entire United Kingdom ('Bank of Britain', for example) can be owed to the fact that its establishment occurred when the Kingdoms of England, Scotland and Ireland were separate entities (at least in name), and therefore pre-dates the merger of the Kingdoms of England and Scotland, the Kingdom of Ireland's absorption into the Union and the formation of the present-day United Kingdom.\nThe word \"Reserve\" is also often included, such as the Reserve Bank of India, Reserve Bank of Australia, Reserve Bank of New Zealand, the South African Reserve Bank, and Federal Reserve System (the U.S. central bank). Other central banks are known as monetary authorities such as the Saudi Arabian Monetary Authority, Hong Kong Monetary Authority, Monetary Authority of Singapore, Maldives Monetary Authority and Cayman Islands Monetary Authority. There is an instance where native language was used to name the central bank: in the Philippines the Filipino name Bangko Sentral ng Pilipinas is used even in English.\nSome are styled \"national\" banks, such as the Swiss National Bank, National Bank of Poland and National Bank of Ukraine, although the term national bank is also used for private commercial banks in some countries such as National Bank of Pakistan. In other cases, central banks may incorporate the word \"Central\" (for example, European Central Bank, Central Bank of Ireland, Central Bank of Brazil, Central Bank of Paraguay). In some countries, particularly in formerly Communist ones, the term national bank may be used to indicate both the monetary authority and the leading banking entity, such as the Soviet Union's Gosbank (state bank). In rare cases, central banks are styled \"state\" banks such as the State Bank of Pakistan and State Bank of Vietnam.\nMany countries have state-owned banks or other quasi-government entities that have entirely separate functions, such as financing imports and exports. In other countries, the term national bank may be used to indicate that the central bank's goals are broader than monetary stability, such as full employment, industrial development, or other goals. Some commercial banks have names suggestive of central banks, even if they are not: examples are the State Bank of India and Central Bank of India, National Bank of Greece, Banco do Brasil, Bank of China, Bank of Cyprus, or Bank of Ireland, as well as Deutsche Bank.\nThe chief executive of a central bank is usually known as the Governor, President or chair.\nStatistics.\nTotal assets of central banks worldwide (in trillion U.S. dollars)\nCollectively, central banks purchase less than 500 tonnes of gold each year, on average (out of an annual global production of 2,500-3,000 tonnes). In 2018, central banks collectively hold over 33,000 metric tons of the gold, about a fifth of all the gold ever mined, according to Bloomberg News.\nIn 2016, 75% of the world's central-bank assets were controlled by four centers in China, the United States, Japan and the eurozone. The central banks of Brazil, Switzerland, Saudi Arabia, the U.K., India and Russia, each account for an average of 2.5 percent. The remaining 107 central banks hold less than 13 percent. According to data compiled by Bloomberg News, the top 10 largest central banks owned $21.4\u00a0trillion in assets, a 10 percent increase from 2015.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5667", "revid": "1750837", "url": "https://en.wikipedia.org/wiki?curid=5667", "title": "Chlorine", "text": "Chemical element, symbol Cl and atomic number 17\nChlorine is a chemical element with the symbol Cl and atomic number 17. The second-lightest of the halogens, it appears between fluorine and bromine in the periodic table and its properties are mostly intermediate between them. Chlorine is a yellow-green gas at room temperature. It is an extremely reactive element and a strong oxidising agent: among the elements, it has the highest electron affinity and the third-highest electronegativity on the revised Pauling scale, behind only oxygen and fluorine.\nChlorine played an important role in the experiments conducted by medieval alchemists, which commonly involved the heating of chloride salts like ammonium chloride (sal ammoniac) and sodium chloride (common salt), producing various chemical substances containing chlorine such as hydrogen chloride, mercury(II) chloride (corrosive sublimate), and hydrochloric acid (in the form of ). However, the nature of free chlorine gas as a separate substance was only recognised around 1630 by Jan Baptist van Helmont. Carl Wilhelm Scheele wrote a description of chlorine gas in 1774, supposing it to be an oxide of a new element. In 1809, chemists suggested that the gas might be a pure element, and this was confirmed by Sir Humphry Davy in 1810, who named it after the Ancient Greek (, \"pale green\") because of its colour.\nBecause of its great reactivity, all chlorine in the Earth's crust is in the form of ionic chloride compounds, which includes table salt. It is the second-most abundant halogen (after fluorine) and twenty-first most abundant chemical element in Earth's crust. These crustal deposits are nevertheless dwarfed by the huge reserves of chloride in seawater.\nElemental chlorine is commercially produced from brine by electrolysis, predominantly in the chlor-alkali process. The high oxidising potential of elemental chlorine led to the development of commercial bleaches and disinfectants, and a reagent for many processes in the chemical industry. Chlorine is used in the manufacture of a wide range of consumer products, about two-thirds of them organic chemicals such as polyvinyl chloride (PVC), many intermediates for the production of plastics, and other end products which do not contain the element. As a common disinfectant, elemental chlorine and chlorine-generating compounds are used more directly in swimming pools to keep them sanitary. Elemental chlorine at high concentration is extremely dangerous, and poisonous to most living organisms. As a chemical warfare agent, chlorine was first used in World War\u00a0I as a poison gas weapon.\nIn the form of chloride ions, chlorine is necessary to all known species of life. Other types of chlorine compounds are rare in living organisms, and artificially produced chlorinated organics range from inert to toxic. In the upper atmosphere, chlorine-containing organic molecules such as chlorofluorocarbons have been implicated in ozone depletion. Small quantities of elemental chlorine are generated by oxidation of chloride ions in neutrophils as part of an immune system response against bacteria.\nHistory.\nThe most common compound of chlorine, sodium chloride, has been known since ancient times; archaeologists have found evidence that rock salt was used as early as 3000 BC and brine as early as 6000 BC.\nEarly discoveries.\nAround 900, the authors of the Arabic writings attributed to Jabir ibn Hayyan (Latin: Geber) and the Persian physician and alchemist Abu Bakr al-Razi (c. 865\u2013925, Latin: Rhazes) were experimenting with sal ammoniac (ammonium chloride), which when it was distilled together with vitriol (hydrated sulfates of various metals) produced hydrogen chloride. However, it appears that in these early experiments with chloride salts, the gaseous products were discarded, and hydrogen chloride may have been produced many times before it was discovered that it can be put to chemical use. One of the first such uses was the synthesis of mercury(II) chloride (corrosive sublimate), whose production from the heating of mercury either with alum and ammonium chloride or with vitriol and sodium chloride was first described in the \"De aluminibus et salibus\" (\"On Alums and Salts\", an eleventh- or twelfth century Arabic text falsely attributed to Abu Bakr al-Razi and translated into Latin in the second half of the twelfth century by Gerard of Cremona, 1144\u20131187). Another important development was the discovery by pseudo-Geber (in the \"De inventione veritatis\", \"On the Discovery of Truth\", after c. 1300) that by adding ammonium chloride to nitric acid, a strong solvent capable of dissolving gold (i.e., \"aqua regia\") could be produced. Although \"aqua regia\" is an unstable mixture that continually gives off fumes containing free chlorine gas, this chlorine gas appears to have been ignored until c. 1630, when its nature as a separate gaseous substance was recognised by the Brabantian chemist and physician Jan Baptist van Helmont.\nIsolation.\nThe element was first studied in detail in 1774 by Swedish chemist Carl Wilhelm Scheele, and he is credited with the discovery. Scheele produced chlorine by reacting MnO2 (as the mineral pyrolusite) with HCl:\n4 HCl + MnO2 \u2192 MnCl2 + 2 H2O + Cl2\nScheele observed several of the properties of chlorine: the bleaching effect on litmus, the deadly effect on insects, the yellow-green color, and the smell similar to aqua regia. He called it \"dephlogisticated muriatic acid air\" since it is a gas (then called \"airs\") and it came from hydrochloric acid (then known as \"muriatic acid\"). He failed to establish chlorine as an element.\nCommon chemical theory at that time held that an acid is a compound that contains oxygen (remnants of this survive in the German and Dutch names of oxygen: \"sauerstoff\" or \"zuurstof\", both translating into English as \"acid substance\"), so a number of chemists, including Claude Berthollet, suggested that Scheele's \"dephlogisticated muriatic acid air\" must be a combination of oxygen and the yet undiscovered element, \"muriaticum\".\nIn 1809, Joseph Louis Gay-Lussac and Louis-Jacques Th\u00e9nard tried to decompose \"dephlogisticated muriatic acid air\" by reacting it with charcoal to release the free element \"muriaticum\" (and carbon dioxide). They did not succeed and published a report in which they considered the possibility that \"dephlogisticated muriatic acid air\" is an element, but were not convinced.\nIn 1810, Sir Humphry Davy tried the same experiment again, and concluded that the substance was an element, and not a compound. He announced his results to the Royal Society on 15 November that year. At that time, he named this new element \"chlorine\", from the Greek word \u03c7\u03bb\u03c9\u03c1\u03bf\u03c2 (\"chl\u014dros\", \"green-yellow\"), in reference to its color. The name \"halogen\", meaning \"salt producer\", was originally used for chlorine in 1811 by Johann Salomo Christoph Schweigger. This term was later used as a generic term to describe all the elements in the chlorine family (fluorine, bromine, iodine), after a suggestion by J\u00f6ns Jakob Berzelius in 1826. In 1823, Michael Faraday liquefied chlorine for the first time, and demonstrated that what was then known as \"solid chlorine\" had a structure of chlorine hydrate (Cl2\u00b7H2O).\nLater uses.\nChlorine gas was first used by French chemist Claude Berthollet to bleach textiles in 1785. Modern bleaches resulted from further work by Berthollet, who first produced sodium hypochlorite in 1789 in his laboratory in the town of Javel (now part of Paris, France), by passing chlorine gas through a solution of sodium carbonate. The resulting liquid, known as \"Eau de Javel\" (\"Javel water\"), was a weak solution of sodium hypochlorite. This process was not very efficient, and alternative production methods were sought. Scottish chemist and industrialist Charles Tennant first produced a solution of calcium hypochlorite (\"chlorinated lime\"), then solid calcium hypochlorite (bleaching powder). These compounds produced low levels of elemental chlorine and could be more efficiently transported than sodium hypochlorite, which remained as dilute solutions because when purified to eliminate water, it became a dangerously powerful and unstable oxidizer. Near the end of the nineteenth century, E. S. Smith patented a method of sodium hypochlorite production involving electrolysis of brine to produce sodium hydroxide and chlorine gas, which then mixed to form sodium hypochlorite. This is known as the chloralkali process, first introduced on an industrial scale in 1892, and now the source of most elemental chlorine and sodium hydroxide. In 1884 Chemischen Fabrik Griesheim of Germany developed another chloralkali process which entered commercial production in 1888.\nElemental chlorine solutions dissolved in chemically basic water (sodium and calcium hypochlorite) were first used as anti-putrefaction agents and disinfectants in the 1820s, in France, long before the establishment of the germ theory of disease. This practice was pioneered by Antoine-Germain Labarraque, who adapted Berthollet's \"Javel water\" bleach and other chlorine preparations (for a more complete history, see below). Elemental chlorine has since served a continuous function in topical antisepsis (wound irrigation solutions and the like) and public sanitation, particularly in swimming and drinking water.\nChlorine gas was first used as a weapon on April 22, 1915, at Ypres by the German Army. The effect on the allies was devastating because the existing gas masks were difficult to deploy and had not been broadly distributed.\nProperties.\nChlorine is the second halogen, being a nonmetal in group 17 of the periodic table. Its properties are thus similar to fluorine, bromine, and iodine, and are largely intermediate between those of the first two. Chlorine has the electron configuration [Ne]3s23p5, with the seven electrons in the third and outermost shell acting as its valence electrons. Like all halogens, it is thus one electron short of a full octet, and is hence a strong oxidising agent, reacting with many elements in order to complete its outer shell. Corresponding to periodic trends, it is intermediate in electronegativity between fluorine and bromine (F: 3.98, Cl: 3.16, Br: 2.96, I: 2.66), and is less reactive than fluorine and more reactive than bromine. It is also a weaker oxidising agent than fluorine, but a stronger one than bromine. Conversely, the chloride ion is a weaker reducing agent than bromide, but a stronger one than fluoride. It is intermediate in atomic radius between fluorine and bromine, and this leads to many of its atomic properties similarly continuing the trend from iodine to bromine upward, such as first ionisation energy, electron affinity, enthalpy of dissociation of the X2 molecule (X = Cl, Br, I), ionic radius, and X\u2013X bond length. (Fluorine is anomalous due to its small size.)\nAll four stable halogens experience intermolecular van der Waals forces of attraction, and their strength increases together with the number of electrons among all homonuclear diatomic halogen molecules. Thus, the melting and boiling points of chlorine are intermediate between those of fluorine and bromine: chlorine melts at \u2212101.0\u00a0\u00b0C and boils at \u221234.0\u00a0\u00b0C. As a result of the increasing molecular weight of the halogens down the group, the density and heats of fusion and vaporisation of chlorine are again intermediate between those of bromine and fluorine, although all their heats of vaporisation are fairly low (leading to high volatility) thanks to their diatomic molecular structure. The halogens darken in colour as the group is descended: thus, while fluorine is a pale yellow gas, chlorine is distinctly yellow-green. This trend occurs because the wavelengths of visible light absorbed by the halogens increase down the group. Specifically, the colour of a halogen, such as chlorine, results from the electron transition between the highest occupied antibonding \"\u03c0g\" molecular orbital and the lowest vacant antibonding \"\u03c3u\" molecular orbital. The colour fades at low temperatures, so that solid chlorine at \u2212195\u00a0\u00b0C is almost colourless.\nLike solid bromine and iodine, solid chlorine crystallises in the orthorhombic crystal system, in a layered lattice of Cl2 molecules. The Cl\u2013Cl distance is 198\u00a0pm (close to the gaseous Cl\u2013Cl distance of 199\u00a0pm) and the Cl\u00b7\u00b7\u00b7Cl distance between molecules is 332\u00a0pm within a layer and 382\u00a0pm between layers (compare the van der Waals radius of chlorine, 180\u00a0pm). This structure means that chlorine is a very poor conductor of electricity, and indeed its conductivity is so low as to be practically unmeasurable.\nIsotopes.\nChlorine has two stable isotopes, 35Cl and 37Cl. These are its only two natural isotopes occurring in quantity, with 35Cl making up 76% of natural chlorine and 37Cl making up the remaining 24%. Both are synthesised in stars in the oxygen-burning and silicon-burning processes. Both have nuclear spin 3/2+ and thus may be used for nuclear magnetic resonance, although the spin magnitude being greater than 1/2 results in non-spherical nuclear charge distribution and thus resonance broadening as a result of a nonzero nuclear quadrupole moment and resultant quadrupolar relaxation. The other chlorine isotopes are all radioactive, with half-lives too short to occur in nature primordially. Of these, the most commonly used in the laboratory are 36Cl (\"t\"1/2 = 3.0\u00d7105\u00a0y) and 38Cl (\"t\"1/2 = 37.2\u00a0min), which may be produced from the neutron activation of natural chlorine.\nThe most stable chlorine radioisotope is 36Cl. The primary decay mode of isotopes lighter than 35Cl is electron capture to isotopes of sulfur; that of isotopes heavier than 37Cl is beta decay to isotopes of argon; and 36Cl may decay by either mode to stable 36S or 36Ar. 36Cl occurs in trace quantities in nature as a cosmogenic nuclide in a ratio of about (7\u201310)\u00a0\u00d7\u00a010\u221213 to 1 with stable chlorine isotopes: it is produced in the atmosphere by spallation of 36Ar by interactions with cosmic ray protons. In the top meter of the lithosphere, 36Cl is generated primarily by thermal neutron activation of 35Cl and spallation of 39K and 40Ca. In the subsurface environment, muon capture by 40Ca becomes more important as a way to generate 36Cl.\nChemistry and compounds.\nChlorine is intermediate in reactivity between fluorine and bromine, and is one of the most reactive elements. Chlorine is a weaker oxidising agent than fluorine but a stronger one than bromine or iodine. This can be seen from the standard electrode potentials of the X2/X\u2212 couples (F, +2.866\u2009V; Cl, +1.395\u2009V; Br, +1.087\u2009V; I, +0.615\u2009V; At, approximately +0.3\u00a0\u2009V). However, this trend is not shown in the bond energies because fluorine is singular due to its small size, low polarisability, and inability to show hypervalence. As another difference, chlorine has a significant chemistry in positive oxidation states while fluorine does not. Chlorination often leads to higher oxidation states than bromination or iodination but lower oxidation states than fluorination. Chlorine tends to react with compounds including M\u2013M, M\u2013H, or M\u2013C bonds to form M\u2013Cl bonds.\nGiven that E\u00b0(O2/H2O) = +1.229\u00a0V, which is less than +1.395\u00a0V, it would be expected that chlorine should be able to oxidise water to oxygen and hydrochloric acid. However, the kinetics of this reaction are unfavorable, and there is also a bubble overpotential effect to consider, so that electrolysis of aqueous chloride solutions evolves chlorine gas and not oxygen gas, a fact that is very useful for the industrial production of chlorine.\nHydrogen chloride.\nThe simplest chlorine compound is hydrogen chloride, HCl, a major chemical in industry as well as in the laboratory, both as a gas and dissolved in water as hydrochloric acid. It is often produced by burning hydrogen gas in chlorine gas, or as a byproduct of chlorinating hydrocarbons. Another approach is to treat sodium chloride with concentrated sulfuric acid to produce hydrochloric acid, also known as the \"salt-cake\" process:\nNaCl + H2SO4 150\u2009\u00b0C\u27f6\u2009 NaHSO4 + HCl\nNaCl + NaHSO4 540\u2013600\u2009\u00b0C\u27f6\u2009 Na2SO4 + HCl\nIn the laboratory, hydrogen chloride gas may be made by drying the acid with concentrated sulfuric acid. Deuterium chloride, DCl, may be produced by reacting benzoyl chloride with heavy water (D2O).\nAt room temperature, hydrogen chloride is a colourless gas, like all the hydrogen halides apart from hydrogen fluoride, since hydrogen cannot form strong hydrogen bonds to the larger electronegative chlorine atom; however, weak hydrogen bonding is present in solid crystalline hydrogen chloride at low temperatures, similar to the hydrogen fluoride structure, before disorder begins to prevail as the temperature is raised. Hydrochloric acid is a strong acid (p\"K\"a = \u22127) because the hydrogen bonds to chlorine are too weak to inhibit dissociation. The HCl/H2O system has many hydrates HCl\u00b7\"n\"H2O for \"n\" = 1, 2, 3, 4, and 6. Beyond a 1:1 mixture of HCl and H2O, the system separates completely into two separate liquid phases. Hydrochloric acid forms an azeotrope with boiling point 108.58\u00a0\u00b0C at 20.22\u00a0g HCl per 100\u00a0g solution; thus hydrochloric acid cannot be concentrated beyond this point by distillation.\nUnlike hydrogen fluoride, anhydrous liquid hydrogen chloride is difficult to work with as a solvent, because its boiling point is low, it has a small liquid range, its dielectric constant is low and it does not dissociate appreciably into H2Cl+ and HCl2- ions \u2013 the latter, in any case, are much less stable than the bifluoride ions (HF2-) due to the very weak hydrogen bonding between hydrogen and chlorine, though its salts with very large and weakly polarising cations such as Cs+ and NR4+ (R = Me, Et, Bu\"n\") may still be isolated. Anhydrous hydrogen chloride is a poor solvent, only able to dissolve small molecular compounds such as nitrosyl chloride and phenol, or salts with very low lattice energies such as tetraalkylammonium halides. It readily protonates electrophiles containing lone-pairs or \u03c0 bonds. Solvolysis, ligand replacement reactions, and oxidations are well-characterised in hydrogen chloride solution:\nPh3SnCl + HCl \u27f6 Ph2SnCl2 + PhH (solvolysis)\nPh3COH + 3 HCl \u27f6 Ph3C+HCl2- + H3O+Cl\u2212 (solvolysis)\nMe4N+HCl2- + BCl3 \u27f6 Me4N+BCl4- + HCl (ligand replacement)\nPCl3 + Cl2 + HCl \u27f6 PCl4+HCl2- (oxidation)\nOther binary chlorides.\nNearly all elements in the periodic table form binary chlorides. The exceptions are decidedly in the minority and stem in each case from one of three causes: extreme inertness and reluctance to participate in chemical reactions (the noble gases, with the exception of xenon in the highly unstable XeCl2 and XeCl4); extreme nuclear instability hampering chemical investigation before decay and transmutation (many of the heaviest elements beyond bismuth); and having an electronegativity higher than chlorine's (oxygen and fluorine) so that the resultant binary compounds are formally not chlorides but rather oxides or fluorides of chlorine. Even though nitrogen in NCl3 is bearing a negative charge, the compound is usually called nitrogen trichloride.\nChlorination of metals with Cl2 usually leads to a higher oxidation state than bromination with Br2 when multiple oxidation states are available, such as in MoCl5 and MoBr3. Chlorides can be made by reaction of an element or its oxide, hydroxide, or carbonate with hydrochloric acid, and then dehydrated by mildly high temperatures combined with either low pressure or anhydrous hydrogen chloride gas. These methods work best when the chloride product is stable to hydrolysis; otherwise, the possibilities include high-temperature oxidative chlorination of the element with chlorine or hydrogen chloride, high-temperature chlorination of a metal oxide or other halide by chlorine, a volatile metal chloride, carbon tetrachloride, or an organic chloride. For instance, zirconium dioxide reacts with chlorine at standard conditions to produce zirconium tetrachloride, and uranium trioxide reacts with hexachloropropene when heated under reflux to give uranium tetrachloride. The second example also involves a reduction in oxidation state, which can also be achieved by reducing a higher chloride using hydrogen or a metal as a reducing agent. This may also be achieved by thermal decomposition or disproportionation as follows:\n EuCl3 + H2 \u27f6 EuCl2 + HCl\n ReCl5 at \"bp\"\u27f6\u2009 ReCl3 + Cl2\n AuCl3 160\u2009\u00b0C\u27f6\u2009 AuCl + Cl2\nMost metal chlorides with the metal in low oxidation states (+1 to +3) are ionic. Nonmetals tend to form covalent molecular chlorides, as do metals in high oxidation states from +3 and above. Both ionic and covalent chlorides are known for metals in oxidation state +3 (e.g. scandium chloride is mostly ionic, but aluminium chloride is not). Silver chloride is very insoluble in water and is thus often used as a qualitative test for chlorine.\nPolychlorine compounds.\nAlthough dichlorine is a strong oxidising agent with a high first ionisation energy, it may be oxidised under extreme conditions to form the cation. This is very unstable and has only been characterised by its electronic band spectrum when produced in a low-pressure discharge tube. The yellow cation is more stable and may be produced as follows:\n \u221278 \u00b0C\u27f6 \nThis reaction is conducted in the oxidising solvent arsenic pentafluoride. The trichloride anion, , has also been characterised; it is analogous to triiodide.\nChlorine fluorides.\nThe three fluorides of chlorine form a subset of the interhalogen compounds, all of which are diamagnetic. Some cationic and anionic derivatives are known, such as ClF2-, ClF4-, ClF2+, and Cl2F+. Some pseudohalides of chlorine are also known, such as cyanogen chloride (ClCN, linear), chlorine cyanate (ClNCO), chlorine thiocyanate (ClSCN, unlike its oxygen counterpart), and chlorine azide (ClN3).\nChlorine monofluoride (ClF) is extremely thermally stable, and is sold commercially in 500-gram steel lecture bottles. It is a colourless gas that melts at \u2212155.6\u00a0\u00b0C and boils at \u2212100.1\u00a0\u00b0C. It may be produced by the direction of its elements at 225\u00a0\u00b0C, though it must then be separated and purified from chlorine trifluoride and its reactants. Its properties are mostly intermediate between those of chlorine and fluorine. It will react with many metals and nonmetals from room temperature and above, fluorinating them and liberating chlorine. It will also act as a chlorofluorinating agent, adding chlorine and fluorine across a multiple bond or by oxidation: for example, it will attack carbon monoxide to form carbonyl chlorofluoride, COFCl. It will react analogously with hexafluoroacetone, (CF3)2CO, with a potassium fluoride catalyst to produce heptafluoroisopropyl hypochlorite, (CF3)2CFOCl; with nitriles RCN to produce RCF2NCl2; and with the sulfur oxides SO2 and SO3 to produce ClSO2F and ClOSO2F respectively. It will also react exothermically with compounds containing \u2013OH and \u2013NH groups, such as water:\nH2O + 2 ClF \u27f6 2 HF + Cl2O\nChlorine trifluoride (ClF3) is a volatile colourless molecular liquid which melts at \u221276.3\u00a0\u00b0C and boils at 11.8\u00a0\u2009\u00b0C. It may be formed by directly fluorinating gaseous chlorine or chlorine monofluoride at 200\u2013300\u00a0\u00b0C. One of the most reactive chemical compounds known, the list of elements it sets on fire is diverse, containing hydrogen, potassium, phosphorus, arsenic, antimony, sulfur, selenium, tellurium, bromine, iodine, and powdered molybdenum, tungsten, rhodium, iridium, and iron. It will also ignite water, along with many substances which in ordinary circumstances would be considered chemically inert such as asbestos, concrete, glass, and sand. When heated, it will even corrode noble metals as palladium, platinum, and gold, and even the noble gases xenon and radon do not escape fluorination. An impermeable fluoride layer is formed by sodium, magnesium, aluminium, zinc, tin, and silver, which may be removed by heating. Nickel, copper, and steel containers are usually used due to their great resistance to attack by chlorine trifluoride, stemming from the formation of an unreactive layer of metal fluoride. Its reaction with hydrazine to form hydrogen fluoride, nitrogen, and chlorine gases was used in experimental rocket engine, but has problems largely stemming from its extreme hypergolicity resulting in ignition without any measurable delay. Today, it is mostly used in nuclear fuel processing, to oxidise uranium to uranium hexafluoride for its enriching and to separate it from plutonium, as well as in the semiconductor industry, where it is used to clean chemical vapor deposition chambers. It can act as a fluoride ion donor or acceptor (Lewis base or acid), although it does not dissociate appreciably into ClF2+ and ClF4- ions.\nChlorine pentafluoride (ClF5) is made on a large scale by direct fluorination of chlorine with excess fluorine gas at 350\u00a0\u00b0C and 250\u00a0atm, and on a small scale by reacting metal chlorides with fluorine gas at 100\u2013300\u00a0\u2009\u00b0C. It melts at \u2212103\u00a0\u00b0C and boils at \u221213.1\u00a0\u00b0C. It is a very strong fluorinating agent, although it is still not as effective as chlorine trifluoride. Only a few specific stoichiometric reactions have been characterised. Arsenic pentafluoride and antimony pentafluoride form ionic adducts of the form [ClF4]+[MF6]\u2212 (M = As, Sb) and water reacts vigorously as follows:\n2 H2O + ClF5 \u27f6 4 HF + FClO2\nThe product, chloryl fluoride, is one of the five known chlorine oxide fluorides. These range from the thermally unstable FClO to the chemically unreactive perchloryl fluoride (FClO3), the other three being FClO2, F3ClO, and F3ClO2. All five behave similarly to the chlorine fluorides, both structurally and chemically, and may act as Lewis acids or bases by gaining or losing fluoride ions respectively or as very strong oxidising and fluorinating agents.\nChlorine oxides.\nThe chlorine oxides are well-studied in spite of their instability (all of them are endothermic compounds). They are important because they are produced when chlorofluorocarbons undergo photolysis in the upper atmosphere and cause the destruction of the ozone layer. None of them can be made from directly reacting the elements.\nDichlorine monoxide (Cl2O) is a brownish-yellow gas (red-brown when solid or liquid) which may be obtained by reacting chlorine gas with yellow mercury(II) oxide. It is very soluble in water, in which it is in equilibrium with hypochlorous acid (HOCl), of which it is the anhydride. It is thus an effective bleach and is mostly used to make hypochlorites. It explodes on heating or sparking or in the presence of ammonia gas.\nChlorine dioxide (ClO2) was the first chlorine oxide to be discovered in 1811 by Humphry Davy. It is a yellow paramagnetic gas (deep-red as a solid or liquid), as expected from its having an odd number of electrons: it is stable towards dimerisation due to the delocalisation of the unpaired electron. It explodes above \u221240\u2009\u00b0C as a liquid and under pressure as a gas and therefore must be made at low concentrations for wood-pulp bleaching and water treatment. It is usually prepared by reducing a chlorate as follows:\nClO3- + Cl\u2212 + 2 H+ \u27f6 ClO2 + Cl2 + H2O\nIts production is thus intimately linked to the redox reactions of the chlorine oxoacids. It is a strong oxidising agent, reacting with sulfur, phosphorus, phosphorus halides, and potassium borohydride. It dissolves exothermically in water to form dark-green solutions that very slowly decompose in the dark. Crystalline clathrate hydrates ClO2\u00b7\"n\"H2O (\"n\" \u2248 6\u201310) separate out at low temperatures. However, in the presence of light, these solutions rapidly photodecompose to form a mixture of chloric and hydrochloric acids. Photolysis of individual ClO2 molecules result in the radicals ClO and ClOO, while at room temperature mostly chlorine, oxygen, and some ClO3 and Cl2O6 are produced. Cl2O3 is also produced when photolysing the solid at \u221278\u2009\u00b0C: it is a dark brown solid that explodes below 0\u2009\u00b0C. The ClO radical leads to the depletion of atmospheric ozone and is thus environmentally important as follows:\nCl\u2022 + O3 \u27f6 ClO\u2022 + O2\nClO\u2022 + O\u2022 \u27f6 Cl\u2022 + O2\nChlorine perchlorate (ClOClO3) is a pale yellow liquid that is less stable than ClO2 and decomposes at room temperature to form chlorine, oxygen, and dichlorine hexoxide (Cl2O6). Chlorine perchlorate may also be considered a chlorine derivative of perchloric acid (HOClO3), similar to the thermally unstable chlorine derivatives of other oxoacids: examples include chlorine nitrate (ClONO2, vigorously reactive and explosive), and chlorine fluorosulfate (ClOSO2F, more stable but still moisture-sensitive and highly reactive). Dichlorine hexoxide is a dark-red liquid that freezes to form a solid which turns yellow at \u2212180\u2009\u00b0C: it is usually made by reaction of chlorine dioxide with oxygen. Despite attempts to rationalise it as the dimer of ClO3, it reacts more as though it were chloryl perchlorate, [ClO2]+[ClO4]\u2212, which has been confirmed to be the correct structure of the solid. It hydrolyses in water to give a mixture of chloric and perchloric acids: the analogous reaction with anhydrous hydrogen fluoride does not proceed to completion.\nDichlorine heptoxide (Cl2O7) is the anhydride of perchloric acid (HClO4) and can readily be obtained from it by dehydrating it with phosphoric acid at \u221210\u2009\u00b0C and then distilling the product at \u221235\u2009\u00b0C and 1\u2009mmHg. It is a shock-sensitive, colourless oily liquid. It is the least reactive of the chlorine oxides, being the only one to not set organic materials on fire at room temperature. It may be dissolved in water to regenerate perchloric acid or in aqueous alkalis to regenerate perchlorates. However, it thermally decomposes explosively by breaking one of the central Cl\u2013O bonds, producing the radicals ClO3 and ClO4 which immediately decompose to the elements through intermediate oxides.\nChlorine oxoacids and oxyanions.\nChlorine forms four oxoacids: hypochlorous acid (HOCl), chlorous acid (HOClO), chloric acid (HOClO2), and perchloric acid (HOClO3). As can be seen from the redox potentials given in the adjacent table, chlorine is much more stable towards disproportionation in acidic solutions than in alkaline solutions:\nThe hypochlorite ions also disproportionate further to produce chloride and chlorate (3 ClO\u2212 \u21cc 2 Cl\u2212 + ClO3-) but this reaction is quite slow at temperatures below 70\u00a0\u00b0C in spite of the very favourable equilibrium constant of 1027. The chlorate ions may themselves disproportionate to form chloride and perchlorate (4 ClO3- \u21cc Cl\u2212 + 3 ClO4-) but this is still very slow even at 100\u00a0\u00b0C despite the very favourable equilibrium constant of 1020. The rates of reaction for the chlorine oxyanions increases as the oxidation state of chlorine decreases. The strengths of the chlorine oxyacids increase very quickly as the oxidation state of chlorine increases due to the increasing delocalisation of charge over more and more oxygen atoms in their conjugate bases.\nMost of the chlorine oxoacids may be produced by exploiting these disproportionation reactions. Hypochlorous acid (HOCl) is highly reactive and quite unstable; its salts are mostly used for their bleaching and sterilising abilities. They are very strong oxidising agents, transferring an oxygen atom to most inorganic species. Chlorous acid (HOClO) is even more unstable and cannot be isolated or concentrated without decomposition: it is known from the decomposition of aqueous chlorine dioxide. However, sodium chlorite is a stable salt and is useful for bleaching and stripping textiles, as an oxidising agent, and as a source of chlorine dioxide. Chloric acid (HOClO2) is a strong acid that is quite stable in cold water up to 30% concentration, but on warming gives chlorine and chlorine dioxide. Evaporation under reduced pressure allows it to be concentrated further to about 40%, but then it decomposes to perchloric acid, chlorine, oxygen, water, and chlorine dioxide. Its most important salt is sodium chlorate, mostly used to make chlorine dioxide to bleach paper pulp. The decomposition of chlorate to chloride and oxygen is a common way to produce oxygen in the laboratory on a small scale. Chloride and chlorate may comproportionate to form chlorine as follows:\nClO3- + 5 Cl\u2212 + 6 H+ \u27f6 3 Cl2 + 3 H2O\nPerchlorates and perchloric acid (HOClO3) are the most stable oxo-compounds of chlorine, in keeping with the fact that chlorine compounds are most stable when the chlorine atom is in its lowest (\u22121) or highest (+7) possible oxidation states. Perchloric acid and aqueous perchlorates are vigorous and sometimes violent oxidising agents when heated, in stark contrast to their mostly inactive nature at room temperature due to the high activation energies for these reactions for kinetic reasons. Perchlorates are made by electrolytically oxidising sodium chlorate, and perchloric acid is made by reacting anhydrous sodium perchlorate or barium perchlorate with concentrated hydrochloric acid, filtering away the chloride precipitated and distilling the filtrate to concentrate it. Anhydrous perchloric acid is a colourless mobile liquid that is sensitive to shock that explodes on contact with most organic compounds, sets hydrogen iodide and thionyl chloride on fire and even oxidises silver and gold. Although it is a weak ligand, weaker than water, a few compounds involving coordinated ClO4- are known.\nOrganochlorine compounds.\nLike the other carbon\u2013halogen bonds, the C\u2013Cl bond is a common functional group that forms part of core organic chemistry. Formally, compounds with this functional group may be considered organic derivatives of the chloride anion. Due to the difference of electronegativity between chlorine (3.16) and carbon (2.55), the carbon in a C\u2013Cl bond is electron-deficient and thus electrophilic. Chlorination modifies the physical properties of hydrocarbons in several ways: chlorocarbons are typically denser than water due to the higher atomic weight of chlorine versus hydrogen, and aliphatic organochlorides are alkylating agents because chloride is a leaving group.\nAlkanes and aryl alkanes may be chlorinated under free-radical conditions, with UV light. However, the extent of chlorination is difficult to control: the reaction is not regioselective and often results in a mixture of various isomers with different degrees of chlorination, though this may be permissible if the products are easily separated. Aryl chlorides may be prepared by the Friedel-Crafts halogenation, using chlorine and a Lewis acid catalyst. The haloform reaction, using chlorine and sodium hydroxide, is also able to generate alkyl halides from methyl ketones, and related compounds. Chlorine adds to the multiple bonds on alkenes and alkynes as well, giving di- or tetra-chloro compounds. However, due to the expense and reactivity of chlorine, organochlorine compounds are more commonly produced by using hydrogen chloride, or with chlorinating agents such as phosphorus pentachloride (PCl5) or thionyl chloride (SOCl2). The last is very convenient in the laboratory because all side products are gaseous and do not have to be distilled out.\nMany organochlorine compounds have been isolated from natural sources ranging from bacteria to humans. Chlorinated organic compounds are found in nearly every class of biomolecules including alkaloids, terpenes, amino acids, flavonoids, steroids, and fatty acids. Organochlorides, including dioxins, are produced in the high temperature environment of forest fires, and dioxins have been found in the preserved ashes of lightning-ignited fires that predate synthetic dioxins. In addition, a variety of simple chlorinated hydrocarbons including dichloromethane, chloroform, and carbon tetrachloride have been isolated from marine algae. A majority of the chloromethane in the environment is produced naturally by biological decomposition, forest fires, and volcanoes.\nSome types of organochlorides, though not all, have significant toxicity to plants or animals, including humans. Dioxins, produced when organic matter is burned in the presence of chlorine, and some insecticides, such as DDT, are persistent organic pollutants which pose dangers when they are released into the environment. For example, DDT, which was widely used to control insects in the mid 20th century, also accumulates in food chains, and causes reproductive problems (e.g., eggshell thinning) in certain bird species. Due to the ready homolytic fission of the C\u2013Cl bond to create chlorine radicals in the upper atmosphere, chlorofluorocarbons have been phased out due to the harm they do to the ozone layer.\nOccurrence and production.\nChlorine is too reactive to occur as the free element in nature but is very abundant in the form of its chloride salts. It is the twenty-first most abundant element in Earth's crust and makes up 126\u00a0parts per million of it, through the large deposits of chloride minerals, especially sodium chloride, that have been evaporated from water bodies. All of these pale in comparison to the reserves of chloride ions in seawater: smaller amounts at higher concentrations occur in some inland seas and underground brine wells, such as the Great Salt Lake in Utah and the Dead Sea in Israel.\nSmall batches of chlorine gas are prepared in the laboratory by combining hydrochloric acid and manganese dioxide, but the need rarely arises due to its ready availability. In industry, elemental chlorine is usually produced by the electrolysis of sodium chloride dissolved in water. This method, the chloralkali process industrialized in 1892, now provides most industrial chlorine gas. Along with chlorine, the method yields hydrogen gas and sodium hydroxide, which is the most valuable product. The process proceeds according to the following chemical equation:\n2 NaCl + 2 H2O \u2192 Cl2 + H2 + 2 NaOH\nThe electrolysis of chloride solutions all proceed according to the following equations:\nCathode: 2 H2O + 2 e\u2212 \u2192 H2 + 2 OH\u2212\nAnode: 2 Cl\u2212 \u2192 Cl2 + 2 e\u2212\nIn diaphragm cell electrolysis, an asbestos (or polymer-fiber) diaphragm separates a cathode and an anode, preventing the chlorine forming at the anode from re-mixing with the sodium hydroxide and the hydrogen formed at the cathode. The salt solution (brine) is continuously fed to the anode compartment and flows through the diaphragm to the cathode compartment, where the caustic alkali is produced and the brine is partially depleted. Diaphragm methods produce dilute and slightly impure alkali, but they are not burdened with the problem of mercury disposal and they are more energy efficient.\nMembrane cell electrolysis employs permeable membrane as an ion exchanger. Saturated sodium (or potassium) chloride solution is passed through the anode compartment, leaving at a lower concentration. This method also produces very pure sodium (or potassium) hydroxide but has the disadvantage of requiring very pure brine at high concentrations.\nIn the Deacon process, hydrogen chloride recovered from the production of organochlorine compounds is recovered as chlorine. The process relies on oxidation using oxygen:\n 4 HCl + O2 \u2192 2 Cl2 + 2 H2O\nThe reaction requires a catalyst. As introduced by Deacon, early catalysts were based on copper. Commercial processes, such as the Mitsui MT-Chlorine Process, have switched to chromium and ruthenium-based catalysts. The chlorine produced is available in cylinders from sizes ranging from 450\u00a0g to 70\u00a0kg, as well as drums (865\u00a0kg), tank wagons (15\u00a0tonnes on roads; 27\u201390\u00a0tonnes by rail), and barges (600\u20131200\u00a0tonnes).\nApplications.\nSodium chloride is the most common chlorine compound, and is the main source of chlorine for the demand by the chemical industry. About 15000 chlorine-containing compounds are commercially traded, including such diverse compounds as chlorinated methane, ethanes, vinyl chloride, polyvinyl chloride (PVC), aluminium trichloride for catalysis, the chlorides of magnesium, titanium, zirconium, and hafnium which are the precursors for producing the pure form of those elements.\nQuantitatively, of all elemental chlorine produced, about 63% is used in the manufacture of organic compounds, and 18% in the manufacture of inorganic chlorine compounds. About 15,000 chlorine compounds are used commercially. The remaining 19% of chlorine produced is used for bleaches and disinfection products. The most significant of organic compounds in terms of production volume are 1,2-dichloroethane and vinyl chloride, intermediates in the production of PVC. Other particularly important organochlorines are methyl chloride, methylene chloride, chloroform, vinylidene chloride, trichloroethylene, perchloroethylene, allyl chloride, epichlorohydrin, chlorobenzene, dichlorobenzenes, and trichlorobenzenes. The major inorganic compounds include HCl, Cl2O, HOCl, NaClO3, chlorinated isocyanurates, AlCl3, SiCl4, SnCl4, PCl3, PCl5, POCl3, AsCl3, SbCl3, SbCl5, BiCl3, and ZnCl2.\nSanitation, disinfection, and antisepsis.\nCombating putrefaction.\nIn France (as elsewhere), animal intestines were processed to make musical instrument strings, Goldbeater's skin and other products. This was done in \"gut factories\" (\"boyauderies\"), and it was an odiferous and unhealthy process. In or about 1820, the Soci\u00e9t\u00e9 d'encouragement pour l'industrie nationale offered a prize for the discovery of a method, chemical or mechanical, for separating the peritoneal membrane of animal intestines without putrefaction. The prize was won by Antoine-Germain Labarraque, a 44-year-old French chemist and pharmacist who had discovered that Berthollet's chlorinated bleaching solutions (\"Eau de Javel\") not only destroyed the smell of putrefaction of animal tissue decomposition, but also actually retarded the decomposition.\nLabarraque's research resulted in the use of chlorides and hypochlorites of lime (calcium hypochlorite) and of sodium (sodium hypochlorite) in the \"boyauderies.\" The same chemicals were found to be useful in the routine disinfection and deodorization of latrines, sewers, markets, abattoirs, anatomical theatres, and morgues. They were successful in hospitals, lazarets, prisons, infirmaries (both on land and at sea), magnaneries, stables, cattle-sheds, etc.; and they were beneficial during exhumations, embalming, outbreaks of epidemic disease, fever, and blackleg in cattle.\nDisinfection.\nLabarraque's chlorinated lime and soda solutions have been advocated since 1828 to prevent infection (called \"contagious infection\", presumed to be transmitted by \"miasmas\"), and to treat putrefaction of existing wounds, including septic wounds. In his 1828 work, Labarraque recommended that doctors breathe chlorine, wash their hands in chlorinated lime, and even sprinkle chlorinated lime about the patients' beds in cases of \"contagious infection\". In 1828, the contagion of infections was well known, even though the agency of the microbe was not discovered until more than half a century later.\nDuring the Paris cholera outbreak of 1832, large quantities of so-called \"chloride of lime\" were used to disinfect the capital. This was not simply modern calcium chloride, but chlorine gas dissolved in lime-water (dilute calcium hydroxide) to form calcium hypochlorite (chlorinated lime). Labarraque's discovery helped to remove the terrible stench of decay from hospitals and dissecting rooms, and by doing so, effectively deodorised the Latin Quarter of Paris. These \"putrid miasmas\" were thought by many to cause the spread of \"contagion\" and \"infection\" \u2013 both words used before the germ theory of infection. Chloride of lime was used for destroying odors and \"putrid matter\". One source claims chloride of lime was used by Dr. John Snow to disinfect water from the cholera-contaminated well that was feeding the Broad Street pump in 1854 London, though three other reputable sources that describe that famous cholera epidemic do not mention the incident. One reference makes it clear that chloride of lime was used to disinfect the offal and filth in the streets surrounding the Broad Street pump \u2013 a common practice in mid-nineteenth century England.\nSemmelweis and experiments with antisepsis.\nPerhaps the most famous application of Labarraque's chlorine and chemical base solutions was in 1847, when Ignaz Semmelweis used chlorine-water (chlorine dissolved in pure water, which was cheaper than chlorinated lime solutions) to disinfect the hands of Austrian doctors, which Semmelweis noticed still carried the stench of decomposition from the dissection rooms to the patient examination rooms. Long before the germ theory of disease, Semmelweis theorized that \"cadaveric particles\" were transmitting decay from fresh medical cadavers to living patients, and he used the well-known \"Labarraque's solutions\" as the only known method to remove the smell of decay and tissue decomposition (which he found that soap did not). The solutions proved to be far more effective antiseptics than soap (Semmelweis was also aware of their greater efficacy, but not the reason), and this resulted in Semmelweis's celebrated success in stopping the transmission of childbed fever (\"puerperal fever\") in the maternity wards of Vienna General Hospital in Austria in 1847.\nMuch later, during World War I in 1916, a standardized and diluted modification of Labarraque's solution containing hypochlorite (0.5%) and boric acid as an acidic stabilizer was developed by Henry Drysdale Dakin (who gave full credit to Labarraque's prior work in this area). Called Dakin's solution, the method of wound irrigation with chlorinated solutions allowed antiseptic treatment of a wide variety of open wounds, long before the modern antibiotic era. A modified version of this solution continues to be employed in wound irrigation in modern times, where it remains effective against bacteria that are resistant to multiple antibiotics (see Century Pharmaceuticals).\nPublic sanitation.\nThe first continuous application of chlorination to drinking U.S. water was installed in Jersey City, New Jersey, in 1908. By 1918, the US Department of Treasury called for all drinking water to be disinfected with chlorine. Chlorine is presently an important chemical for water purification (such as in water treatment plants), in disinfectants, and in bleach. Even small water supplies are now routinely chlorinated.\nChlorine is usually used (in the form of hypochlorous acid) to kill bacteria and other microbes in drinking water supplies and public swimming pools. In most private swimming pools, chlorine itself is not used, but rather sodium hypochlorite, formed from chlorine and sodium hydroxide, or solid tablets of chlorinated isocyanurates. The drawback of using chlorine in swimming pools is that the chlorine reacts with the amino acids in proteins in human hair and skin. Contrary to popular belief, the distinctive \"chlorine aroma\" associated with swimming pools is not the result of elemental chlorine itself, but of chloramine, a chemical compound produced by the reaction of free dissolved chlorine with amines in organic substances including those in urine and sweat. As a disinfectant in water, chlorine is more than three times as effective against \"Escherichia coli\" as bromine, and more than six times as effective as iodine. Increasingly, monochloramine itself is being directly added to drinking water for purposes of disinfection, a process known as chloramination.\nIt is often impractical to store and use poisonous chlorine gas for water treatment, so alternative methods of adding chlorine are used. These include hypochlorite solutions, which gradually release chlorine into the water, and compounds like sodium dichloro-s-triazinetrione (dihydrate or anhydrous), sometimes referred to as \"dichlor\", and trichloro-s-triazinetrione, sometimes referred to as \"trichlor\". These compounds are stable while solid and may be used in powdered, granular, or tablet form. When added in small amounts to pool water or industrial water systems, the chlorine atoms hydrolyze from the rest of the molecule, forming hypochlorous acid (HOCl), which acts as a general biocide, killing germs, microorganisms, algae, and so on.\nUse as a weapon.\nWorld War I.\nChlorine gas, also known as bertholite, was first used as a weapon in World War I by Germany on April 22, 1915, in the Second Battle of Ypres. As described by the soldiers, it had the distinctive smell of a mixture of pepper and pineapple. It also tasted metallic and stung the back of the throat and chest. Chlorine reacts with water in the mucosa of the lungs to form hydrochloric acid, destructive to living tissue and potentially lethal. Human respiratory systems can be protected from chlorine gas by gas masks with activated charcoal or other filters, which makes chlorine gas much less lethal than other chemical weapons. It was pioneered by a German scientist later to be a Nobel laureate, Fritz Haber of the Kaiser Wilhelm Institute in Berlin, in collaboration with the German chemical conglomerate IG Farben, which developed methods for discharging chlorine gas against an entrenched enemy. After its first use, both sides in the conflict used chlorine as a chemical weapon, but it was soon replaced by the more deadly phosgene and mustard gas.\nMiddle east.\nChlorine gas was also used during the Iraq War in Anbar Province in 2007, with insurgents packing truck bombs with mortar shells and chlorine tanks. The attacks killed two people from the explosives and sickened more than 350. Most of the deaths were caused by the force of the explosions rather than the effects of chlorine since the toxic gas is readily dispersed and diluted in the atmosphere by the blast. In some bombings, over a hundred civilians were hospitalized due to breathing difficulties. The Iraqi authorities tightened security for elemental chlorine, which is essential for providing safe drinking water to the population.\nOn 23 October 2014, it was reported that the Islamic State of Iraq and the Levant had used chlorine gas in the town of Duluiyah, Iraq. Laboratory analysis of clothing and soil samples confirmed the use of chlorine gas against Kurdish Peshmerga Forces in a vehicle-borne improvised explosive device attack on 23 January 2015 at the Highway 47 Kiske Junction near Mosul.\nAnother country in the middle east, Syria, has used chlorine as a chemical weapon delivered from barrel bombs and rockets. In 2016, the OPCW-UN Joint Investigative Mechanism concluded that the Syrian government used chlorine as a chemical weapon in three separate attacks. Later investigations from the OPCW's Investigation and Identification Team concluded that the Syrian Air Force was responsible for chlorine attacks in 2017 and 2018.\nBiological role.\nThe chloride anion is an essential nutrient for metabolism. Chlorine is needed for the production of hydrochloric acid in the stomach and in cellular pump functions. The main dietary source is table salt, or sodium chloride. Overly low or high concentrations of chloride in the blood are examples of electrolyte disturbances. Hypochloremia (having too little chloride) rarely occurs in the absence of other abnormalities. It is sometimes associated with hypoventilation. It can be associated with chronic respiratory acidosis. Hyperchloremia (having too much chloride) usually does not produce symptoms. When symptoms do occur, they tend to resemble those of hypernatremia (having too much sodium). Reduction in blood chloride leads to cerebral dehydration; symptoms are most often caused by rapid rehydration which results in cerebral edema. Hyperchloremia can affect oxygen transport.\nHazards.\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nChlorine is a toxic gas that attacks the respiratory system, eyes, and skin. Because it is denser than air, it tends to accumulate at the bottom of poorly ventilated spaces. Chlorine gas is a strong oxidizer, which may react with flammable materials.\nChlorine is detectable with measuring devices in concentrations as low as 0.2 parts per million (ppm), and by smell at 3\u00a0ppm. Coughing and vomiting may occur at 30\u00a0ppm and lung damage at 60\u00a0ppm. About 1000\u00a0ppm can be fatal after a few deep breaths of the gas. The IDLH (immediately dangerous to life and health) concentration is 10\u00a0ppm. Breathing lower concentrations can aggravate the respiratory system and exposure to the gas can irritate the eyes. When chlorine is inhaled at concentrations greater than 30\u00a0ppm, it reacts with water within the lungs, producing hydrochloric acid (HCl) and hypochlorous acid (HClO).\nWhen used at specified levels for water disinfection, the reaction of chlorine with water is not a major concern for human health. Other materials present in the water may generate disinfection by-products that are associated with negative effects on human health.\nIn the United States, the Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for elemental chlorine at 1\u00a0ppm, or 3\u00a0mg/m3. The National Institute for Occupational Safety and Health has designated a recommended exposure limit of 0.5\u00a0ppm over 15 minutes.\nIn the home, accidents occur when hypochlorite bleach solutions come into contact with certain acidic drain-cleaners to produce chlorine gas. Hypochlorite bleach (a popular laundry additive) combined with ammonia (another popular laundry additive) produces chloramines, another toxic group of chemicals.\nChlorine-induced cracking in structural materials.\nChlorine is widely used for purifying water, especially potable water supplies and water used in swimming pools. Several catastrophic collapses of swimming pool ceilings have occurred from chlorine-induced stress corrosion cracking of stainless steel suspension rods. Some polymers are also sensitive to attack, including acetal resin and polybutene. Both materials were used in hot and cold water domestic plumbing, and stress corrosion cracking caused widespread failures in the US in the 1980s and 1990s.\nChlorine-iron fire.\nThe element iron can combine with chlorine at high temperatures in a strong exothermic reaction, creating a \"chlorine-iron fire\". Chlorine-iron fires are a risk in chemical process plants, where much of the pipework that carries chlorine gas is made of steel.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExplanatory notes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5668", "revid": "155158", "url": "https://en.wikipedia.org/wiki?curid=5668", "title": "Calcium", "text": "Chemical element, symbol Ca and atomic number 20\nCalcium is a chemical element with the symbol Ca and atomic number 20. As an alkaline earth metal, calcium is a reactive metal that forms a dark oxide-nitride layer when exposed to air. Its physical and chemical properties are most similar to its heavier homologues strontium and barium. It is the fifth most abundant element in Earth's crust, and the third most abundant metal, after iron and aluminium. The most common calcium compound on Earth is calcium carbonate, found in limestone and the fossilised remnants of early sea life; gypsum, anhydrite, fluorite, and apatite are also sources of calcium. The name derives from Latin \"calx\" \"lime\", which was obtained from heating limestone.\nSome calcium compounds were known to the ancients, though their chemistry was unknown until the seventeenth century. Pure calcium was isolated in 1808 via electrolysis of its oxide by Humphry Davy, who named the element. Calcium compounds are widely used in many industries: in foods and pharmaceuticals for calcium supplementation, in the paper industry as bleaches, as components in cement and electrical insulators, and in the manufacture of soaps. On the other hand, the metal in pure form has few applications due to its high reactivity; still, in small quantities it is often used as an alloying component in steelmaking, and sometimes, as a calcium\u2013lead alloy, in making automotive batteries.\nCalcium is the most abundant metal and the fifth-most abundant element in the human body. As electrolytes, calcium ions (Ca2+) play a vital role in the physiological and biochemical processes of organisms and cells: in signal transduction pathways where they act as a second messenger; in neurotransmitter release from neurons; in contraction of all muscle cell types; as cofactors in many enzymes; and in fertilization. Calcium ions outside cells are important for maintaining the potential difference across excitable cell membranes, protein synthesis, and bone formation.\nCharacteristics.\nClassification.\nCalcium is a very ductile silvery metal (sometimes described as pale yellow) whose properties are very similar to the heavier elements in its group, strontium, barium, and radium. A calcium atom has twenty electrons, arranged in the electron configuration [Ar]4s2. Like the other elements placed in group 2 of the periodic table, calcium has two valence electrons in the outermost s-orbital, which are very easily lost in chemical reactions to form a dipositive ion with the stable electron configuration of a noble gas, in this case argon.\nHence, calcium is almost always divalent in its compounds, which are usually ionic. Hypothetical univalent salts of calcium would be stable with respect to their elements, but not to disproportionation to the divalent salts and calcium metal, because the enthalpy of formation of MX2 is much higher than those of the hypothetical MX. This occurs because of the much greater lattice energy afforded by the more highly charged Ca2+ cation compared to the hypothetical Ca+ cation.\nCalcium, strontium, barium, and radium are always considered to be alkaline earth metals; the lighter beryllium and magnesium, also in group 2 of the periodic table, are often included as well. Nevertheless, beryllium and magnesium differ significantly from the other members of the group in their physical and chemical behaviour: they behave more like aluminium and zinc respectively and have some of the weaker metallic character of the post-transition metals, which is why the traditional definition of the term \"alkaline earth metal\" excludes them.\nPhysical properties.\nCalcium metal melts at 842\u00a0\u00b0C and boils at 1494\u00a0\u00b0C; these values are higher than those for magnesium and strontium, the neighbouring group 2 metals. It crystallises in the face-centered cubic arrangement like strontium; above 450\u00a0\u00b0C, it changes to an anisotropic hexagonal close-packed arrangement like magnesium. Its density of 1.55\u00a0g/cm3 is the lowest in its group.\nCalcium is harder than lead but can be cut with a knife with effort. While calcium is a poorer conductor of electricity than copper or aluminium by volume, it is a better conductor by mass than both due to its very low density. While calcium is infeasible as a conductor for most terrestrial applications as it reacts quickly with atmospheric oxygen, its use as such in space has been considered.\nChemical properties.\nThe chemistry of calcium is that of a typical heavy alkaline earth metal. For example, calcium spontaneously reacts with water more quickly than magnesium and less quickly than strontium to produce calcium hydroxide and hydrogen gas. It also reacts with the oxygen and nitrogen in the air to form a mixture of calcium oxide and calcium nitride. When finely divided, it spontaneously burns in air to produce the nitride. In bulk, calcium is less reactive: it quickly forms a hydration coating in moist air, but below 30% relative humidity it may be stored indefinitely at room temperature.\nBesides the simple oxide CaO, the peroxide CaO2 can be made by direct oxidation of calcium metal under a high pressure of oxygen, and there is some evidence for a yellow superoxide Ca(O2)2. Calcium hydroxide, Ca(OH)2, is a strong base, though it is not as strong as the hydroxides of strontium, barium or the alkali metals. All four dihalides of calcium are known. Calcium carbonate (CaCO3) and calcium sulfate (CaSO4) are particularly abundant minerals. Like strontium and barium, as well as the alkali metals and the divalent lanthanides europium and ytterbium, calcium metal dissolves directly in liquid ammonia to give a dark blue solution.\nDue to the large size of the calcium ion (Ca2+), high coordination numbers are common, up to 24 in some intermetallic compounds such as CaZn13. Calcium is readily complexed by oxygen chelates such as EDTA and polyphosphates, which are useful in analytic chemistry and removing calcium ions from hard water. In the absence of steric hindrance, smaller group 2 cations tend to form stronger complexes, but when large polydentate macrocycles are involved the trend is reversed.\nAlthough calcium is in the same group as magnesium and organomagnesium compounds are very commonly used throughout chemistry, organocalcium compounds are not similarly widespread because they are more difficult to make and more reactive, although they have recently been investigated as possible catalysts. Organocalcium compounds tend to be more similar to organoytterbium compounds due to the similar ionic radii of Yb2+ (102\u00a0pm) and Ca2+ (100\u00a0pm).\nMost of these compounds can only be prepared at low temperatures; bulky ligands tend to favor stability. For example, calcium dicyclopentadienyl, Ca(C5H5)2, must be made by directly reacting calcium metal with mercurocene or cyclopentadiene itself; replacing the C5H5 ligand with the bulkier C5(CH3)5 ligand on the other hand increases the compound's solubility, volatility, and kinetic stability.\nIsotopes.\nNatural calcium is a mixture of five stable isotopes (40Ca, 42Ca, 43Ca, 44Ca, and 46Ca) and one isotope with a half-life so long that it can be considered stable for all practical purposes (48Ca, with a half-life of about 4.3\u00a0\u00d7\u00a01019\u00a0years). Calcium is the first (lightest) element to have six naturally occurring isotopes.\nBy far the most common isotope of calcium in nature is 40Ca, which makes up 96.941% of all natural calcium. It is produced in the silicon-burning process from fusion of alpha particles and is the heaviest stable nuclide with equal proton and neutron numbers; its occurrence is also supplemented slowly by the decay of primordial 40K. Adding another alpha particle leads to unstable 44Ti, which quickly decays via two successive electron captures to stable 44Ca; this makes up 2.806% of all natural calcium and is the second-most common isotope.\nThe other four natural isotopes, 42Ca, 43Ca, 46Ca, and 48Ca, are significantly rarer, each comprising less than 1% of all natural calcium. The four lighter isotopes are mainly products of the oxygen-burning and silicon-burning processes, leaving the two heavier ones to be produced via neutron capture processes. 46Ca is mostly produced in a \"hot\" s-process, as its formation requires a rather high neutron flux to allow short-lived 45Ca to capture a neutron. 48Ca is produced by electron capture in the r-process in type Ia supernovae, where high neutron excess and low enough entropy ensures its survival.\n46Ca and 48Ca are the first \"classically stable\" nuclides with a six-neutron or eight-neutron excess respectively. Although extremely neutron-rich for such a light element, 48Ca is very stable because it is a doubly magic nucleus, having 20 protons and 28 neutrons arranged in closed shells. Its beta decay to 48Sc is very hindered because of the gross mismatch of nuclear spin: 48Ca has zero nuclear spin, being even\u2013even, while 48Sc has spin 6+, so the decay is forbidden by the conservation of angular momentum. While two excited states of 48Sc are available for decay as well, they are also forbidden due to their high spins. As a result, when 48Ca does decay, it does so by double beta decay to 48Ti instead, being the lightest nuclide known to undergo double beta decay.\nThe heavy isotope 46Ca can also theoretically undergo double beta decay to 46Ti as well, but this has never been observed. The lightest and most common isotope 40Ca is also doubly magic and could undergo double electron capture to 40Ar, but this has likewise never been observed. Calcium is the only element to have two primordial doubly magic isotopes. The experimental lower limits for the half-lives of 40Ca and 46Ca are 5.9\u00a0\u00d7\u00a01021\u00a0years and 2.8\u00a0\u00d7\u00a01015\u00a0years respectively.\nApart from the practically stable 48Ca, the longest lived radioisotope of calcium is 41Ca. It decays by electron capture to stable 41K with a half-life of about a hundred thousand years. Its existence in the early Solar System as an extinct radionuclide has been inferred from excesses of 41K: traces of 41Ca also still exist today, as it is a cosmogenic nuclide, continuously reformed through neutron activation of natural 40Ca.\nMany other calcium radioisotopes are known, ranging from 35Ca to 60Ca. They are all much shorter-lived than 41Ca, the most stable among them being 45Ca (half-life 163\u00a0days) and 47Ca (half-life 4.54\u00a0days). The isotopes lighter than 42Ca usually undergo beta plus decay to isotopes of potassium, and those heavier than 44Ca usually undergo beta minus decay to isotopes of scandium, although near the nuclear drip lines, proton emission and neutron emission begin to be significant decay modes as well.\nLike other elements, a variety of processes alter the relative abundance of calcium isotopes. The best studied of these processes is the mass-dependent fractionation of calcium isotopes that accompanies the precipitation of calcium minerals such as calcite, aragonite and apatite from solution. Lighter isotopes are preferentially incorporated into these minerals, leaving the surrounding solution enriched in heavier isotopes at a magnitude of roughly 0.025% per atomic mass unit (amu) at room temperature. Mass-dependent differences in calcium isotope composition are conventionally expressed by the ratio of two isotopes (usually 44Ca/40Ca) in a sample compared to the same ratio in a standard reference material. 44Ca/40Ca varies by about 1% among common earth materials.\nHistory.\nCalcium compounds were known for millennia, although their chemical makeup was not understood until the 17th century. Lime as a building material and as plaster for statues was used as far back as around 7000\u00a0BC. The first dated lime kiln dates back to 2500\u00a0BC and was found in Khafajah, Mesopotamia.\nAt about the same time, dehydrated gypsum (CaSO4\u00b72H2O) was being used in the Great Pyramid of Giza. This material would later be used for the plaster in the tomb of Tutankhamun. The ancient Romans instead used lime mortars made by heating limestone (CaCO3). The name \"calcium\" itself derives from the Latin word \"calx\" \"lime\".\nVitruvius noted that the lime that resulted was lighter than the original limestone, attributing this to the boiling of the water. In 1755, Joseph Black proved that this was due to the loss of carbon dioxide, which as a gas had not been recognised by the ancient Romans.\nIn 1789, Antoine Lavoisier suspected that lime might be an oxide of a fundamental chemical element. In his table of the elements, Lavoisier listed five \"salifiable earths\" (i.e., ores that could be made to react with acids to produce salts (\"salis\" = salt, in Latin): \"chaux\" (calcium oxide), \"magn\u00e9sie\" (magnesia, magnesium oxide), \"baryte\" (barium sulfate), \"alumine\" (alumina, aluminium oxide), and \"silice\" (silica, silicon dioxide)). About these \"elements\", Lavoisier reasoned: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;We are probably only acquainted as yet with a part of the metallic substances existing in nature, as all those which have a stronger affinity to oxygen than carbon possesses, are incapable, hitherto, of being reduced to a metallic state, and consequently, being only presented to our observation under the form of oxyds, are confounded with earths. It is extremely probable that barytes, which we have just now arranged with earths, is in this situation; for in many experiments it exhibits properties nearly approaching to those of metallic bodies. It is even possible that all the substances we call earths may be only metallic oxyds, irreducible by any hitherto known process.\nCalcium, along with its congeners magnesium, strontium, and barium, was first isolated by Humphry Davy in 1808. Following the work of J\u00f6ns Jakob Berzelius and Magnus Martin af Pontin on electrolysis, Davy isolated calcium and magnesium by putting a mixture of the respective metal oxides with mercury(II) oxide on a platinum plate which was used as the anode, the cathode being a platinum wire partially submerged into mercury. Electrolysis then gave calcium\u2013mercury and magnesium\u2013mercury amalgams, and distilling off the mercury gave the metal. However, pure calcium cannot be prepared in bulk by this method and a workable commercial process for its production was not found until over a century later.\nOccurrence and production.\nAt 3%, calcium is the fifth most abundant element in the Earth's crust, and the third most abundant metal behind aluminium and iron. It is also the fourth most abundant element in the lunar highlands. Sedimentary calcium carbonate deposits pervade the Earth's surface as fossilized remains of past marine life; they occur in two forms, the rhombohedral calcite (more common) and the orthorhombic aragonite (forming in more temperate seas). Minerals of the first type include limestone, dolomite, marble, chalk, and iceland spar; aragonite beds make up the Bahamas, the Florida Keys, and the Red Sea basins. Corals, sea shells, and pearls are mostly made up of calcium carbonate. Among the other important minerals of calcium are gypsum (CaSO4\u00b72H2O), anhydrite (CaSO4), fluorite (CaF2), and apatite ([Ca5(PO4)3F]).\nThe major producers of calcium are China (about 10000 to 12000 tonnes per year), Russia (about 6000 to 8000 tonnes per year), and the United States (about 2000 to 4000 tonnes per year). Canada and France are also among the minor producers. In 2005, about 24000 tonnes of calcium were produced; about half of the world's extracted calcium is used by the United States, with about 80% of the output used each year.\nIn Russia and China, Davy's method of electrolysis is still used, but is instead applied to molten calcium chloride. Since calcium is less reactive than strontium or barium, the oxide\u2013nitride coating that results in air is stable and lathe machining and other standard metallurgical techniques are suitable for calcium. In the United States and Canada, calcium is instead produced by reducing lime with aluminium at high temperatures.\nGeochemical cycling.\nCalcium cycling provides a link between tectonics, climate, and the carbon cycle. In the simplest terms, uplift of mountains exposes calcium-bearing rocks such as some granites to chemical weathering and releases Ca2+ into surface water. These ions are transported to the ocean where they react with dissolved CO2 to form limestone (CaCO3), which in turn settles to the sea floor where it is incorporated into new rocks. Dissolved CO2, along with carbonate and bicarbonate ions, are termed \"dissolved inorganic carbon\" (DIC).\nThe actual reaction is more complicated and involves the bicarbonate ion (HCO) that forms when CO2 reacts with water at seawater pH:\n&lt;chem&gt;Ca^2+ + 2 HCO3- -&gt; CaCO3_v + CO2 + H2O&lt;/chem&gt;\nAt seawater pH, most of the CO2 is immediately converted back into HCO3-. The reaction results in a net transport of one molecule of CO2 from the ocean/atmosphere into the lithosphere. The result is that each Ca2+ ion released by chemical weathering ultimately removes one CO2 molecule from the surficial system (atmosphere, ocean, soils and living organisms), storing it in carbonate rocks where it is likely to stay for hundreds of millions of years. The weathering of calcium from rocks thus scrubs CO2 from the ocean and atmosphere, exerting a strong long-term effect on climate.\nUses.\nThe largest use of metallic calcium is in steelmaking, due to its strong chemical affinity for oxygen and sulfur. Its oxides and sulfides, once formed, give liquid lime aluminate and sulfide inclusions in steel which float out; on treatment, these inclusions disperse throughout the steel and become small and spherical, improving castability, cleanliness and general mechanical properties. Calcium is also used in maintenance-free automotive batteries, in which the use of 0.1% calcium\u2013lead alloys instead of the usual antimony\u2013lead alloys leads to lower water loss and lower self-discharging.\nDue to the risk of expansion and cracking, aluminium is sometimes also incorporated into these alloys. These lead\u2013calcium alloys are also used in casting, replacing lead\u2013antimony alloys. Calcium is also used to strengthen aluminium alloys used for bearings, for the control of graphitic carbon in cast iron, and to remove bismuth impurities from lead. Calcium metal is found in some drain cleaners, where it functions to generate heat and calcium hydroxide that saponifies the fats and liquefies the proteins (for example, those in hair) that block drains.\nBesides metallurgy, the reactivity of calcium is exploited to remove nitrogen from high-purity argon gas and as a getter for oxygen and nitrogen. It is also used as a reducing agent in the production of chromium, zirconium, thorium, and uranium. It can also be used to store hydrogen gas, as it reacts with hydrogen to form solid calcium hydride, from which the hydrogen can easily be re-extracted.\nCalcium isotope fractionation during mineral formation has led to several applications of calcium isotopes. In particular, the 1997 observation by Skulan and DePaolo that calcium minerals are isotopically lighter than the solutions from which the minerals precipitate is the basis of analogous applications in medicine and in paleoceanography. In animals with skeletons mineralized with calcium, the calcium isotopic composition of soft tissues reflects the relative rate of formation and dissolution of skeletal mineral.\nIn humans, changes in the calcium isotopic composition of urine have been shown to be related to changes in bone mineral balance. When the rate of bone formation exceeds the rate of bone resorption, the 44Ca/40Ca ratio in soft tissue rises and vice versa. Because of this relationship, calcium isotopic measurements of urine or blood may be useful in the early detection of metabolic bone diseases like osteoporosis.\nA similar system exists in seawater, where 44Ca/40Ca tends to rise when the rate of removal of Ca2+ by mineral precipitation exceeds the input of new calcium into the ocean. In 1997, Skulan and DePaolo presented the first evidence of change in seawater 44Ca/40Ca over geologic time, along with a theoretical explanation of these changes. More recent papers have confirmed this observation, demonstrating that seawater Ca2+ concentration is not constant, and that the ocean is never in a \"steady state\" with respect to calcium input and output. This has important climatological implications, as the marine calcium cycle is closely tied to the carbon cycle.\nMany calcium compounds are used in food, as pharmaceuticals, and in medicine, among others. For example, calcium and phosphorus are supplemented in foods through the addition of calcium lactate, calcium diphosphate, and tricalcium phosphate. The last is also used as a polishing agent in toothpaste and in antacids. Calcium lactobionate is a white powder that is used as a suspending agent for pharmaceuticals. In baking, calcium phosphate is used as a leavening agent. Calcium sulfite is used as a bleach in papermaking and as a disinfectant, calcium silicate is used as a reinforcing agent in rubber, and calcium acetate is a component of liming rosin and is used to make metallic soaps and synthetic resins.\nCalcium is on the World Health Organization's List of Essential Medicines.\nFood sources.\nFoods rich in calcium include dairy products, such as yogurt and cheese, sardines, salmon, soy products, kale, and fortified breakfast cereals.\nBecause of concerns for long-term adverse side effects, including calcification of arteries and kidney stones, both the U.S. Institute of Medicine (IOM) and the European Food Safety Authority (EFSA) set Tolerable Upper Intake Levels (ULs) for combined dietary and supplemental calcium. From the IOM, people of ages 9\u201318 years are not to exceed 3\u00a0g/day combined intake; for ages 19\u201350, not to exceed 2.5\u00a0g/day; for ages 51 and older, not to exceed 2\u00a0g/day. EFSA set the UL for all adults at 2.5\u00a0g/day, but decided the information for children and adolescents was not sufficient to determine ULs.\nBiological and pathological role.\nFunction.\nCalcium is an essential element needed in large quantities. The Ca2+ ion acts as an electrolyte and is vital to the health of the muscular, circulatory, and digestive systems; is indispensable to the building of bone; and supports synthesis and function of blood cells. For example, it regulates the contraction of muscles, nerve conduction, and the clotting of blood. As a result, intra- and extracellular calcium levels are tightly regulated by the body. Calcium can play this role because the Ca2+ ion forms stable coordination complexes with many organic compounds, especially proteins; it also forms compounds with a wide range of solubilities, enabling the formation of the skeleton.\nBinding.\nCalcium ions may be complexed by proteins through binding the carboxyl groups of glutamic acid or aspartic acid residues; through interacting with phosphorylated serine, tyrosine, or threonine residues; or by being chelated by \u03b3-carboxylated amino acid residues. Trypsin, a digestive enzyme, uses the first method; osteocalcin, a bone matrix protein, uses the third.\nSome other bone matrix proteins such as osteopontin and bone sialoprotein use both the first and the second. Direct activation of enzymes by binding calcium is common; some other enzymes are activated by noncovalent association with direct calcium-binding enzymes. Calcium also binds to the phospholipid layer of the cell membrane, anchoring proteins associated with the cell surface.\nSolubility.\nAs an example of the wide range of solubility of calcium compounds, monocalcium phosphate is very soluble in water, 85% of extracellular calcium is as dicalcium phosphate with a solubility of 2.0\u00a0mM and the hydroxyapatite of bones in an organic matrix is tricalcium phosphate at 100\u00a0\u03bcM.\nNutrition.\nCalcium is a common constituent of multivitamin dietary supplements, but the composition of calcium complexes in supplements may affect its bioavailability which varies by solubility of the salt involved: calcium citrate, malate, and lactate are highly bioavailable, while the oxalate is less. Other calcium preparations include calcium carbonate, calcium citrate malate, and calcium gluconate. The intestine absorbs about one-third of calcium eaten as the free ion, and plasma calcium level is then regulated by the kidneys.\nHormonal regulation of bone formation and serum levels.\nParathyroid hormone and vitamin D promote the formation of bone by allowing and enhancing the deposition of calcium ions there, allowing rapid bone turnover without affecting bone mass or mineral content. When plasma calcium levels fall, cell surface receptors are activated and the secretion of parathyroid hormone occurs; it then proceeds to stimulate the entry of calcium into the plasma pool by taking it from targeted kidney, gut, and bone cells, with the bone-forming action of parathyroid hormone being antagonised by calcitonin, whose secretion increases with increasing plasma calcium levels.\nAbnormal serum levels.\nExcess intake of calcium may cause hypercalcemia. However, because calcium is absorbed rather inefficiently by the intestines, high serum calcium is more likely caused by excessive secretion of parathyroid hormone (PTH) or possibly by excessive intake of vitamin D, both of which facilitate calcium absorption. All these conditions result in excess calcium salts being deposited in the heart, blood vessels, or kidneys. Symptoms include anorexia, nausea, vomiting, memory loss, confusion, muscle weakness, increased urination, dehydration, and metabolic bone disease.\nChronic hypercalcaemia typically leads to calcification of soft tissue and its serious consequences: for example, calcification can cause loss of elasticity of vascular walls and disruption of laminar blood flow\u2014and thence to plaque rupture and thrombosis. Conversely, inadequate calcium or vitamin D intakes may result in hypocalcemia, often caused also by inadequate secretion of parathyroid hormone or defective PTH receptors in cells. Symptoms include neuromuscular excitability, which potentially causes tetany and disruption of conductivity in cardiac tissue.\nBone disease.\nAs calcium is required for bone development, many bone diseases can be traced to the organic matrix or the hydroxyapatite in molecular structure or organization of bone. Osteoporosis is a reduction in mineral content of bone per unit volume, and can be treated by supplementation of calcium, vitamin D, and bisphosphonates. Inadequate amounts of calcium, vitamin D, or phosphates can lead to softening of bones, called osteomalacia.\nSafety.\nMetallic calcium.\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nBecause calcium reacts exothermically with water and acids, calcium metal coming into contact with bodily moisture results in severe corrosive irritation. When swallowed, calcium metal has the same effect on the mouth, oesophagus, and stomach, and can be fatal. However, long-term exposure is not known to have distinct adverse effects.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5669", "revid": "46063458", "url": "https://en.wikipedia.org/wiki?curid=5669", "title": "Chromium", "text": "Chemical element, symbol Cr and atomic number 24\nChromium is a chemical element with the symbol Cr and atomic number 24. It is the first element in group 6. It is a steely-grey, lustrous, hard, and brittle transition metal.\nChromium metal is valued for its high corrosion resistance and hardness. A major development in steel production was the discovery that steel could be made highly resistant to corrosion and discoloration by adding metallic chromium to form stainless steel. Stainless steel and chrome plating (electroplating with chromium) together comprise 85% of the commercial use. Chromium is also greatly valued as a metal that is able to be highly polished while resisting tarnishing. Polished chromium reflects almost 70% of the visible spectrum, and almost 90% of infrared light. The name of the element is derived from the Greek word \u03c7\u03c1\u1ff6\u03bc\u03b1, \"chr\u014dma\", meaning color, because many chromium compounds are intensely colored.\nIndustrial production of chromium proceeds from chromite ore (mostly FeCr2O4) to produce ferrochromium, an iron-chromium alloy, by means of aluminothermic or silicothermic reactions. Ferrochromium is then used to produce alloys such as stainless steel. Pure chromium metal is produced by a different process: roasting and leaching of chromite to separate it from iron, followed by reduction with carbon and then aluminium.\nIn the United States, trivalent chromium (Cr(III)) ion is considered an essential nutrient in humans for insulin, sugar, and lipid metabolism. However, in 2014, the European Food Safety Authority, acting for the European Union, concluded that there was insufficient evidence for chromium to be recognized as essential.\nWhile chromium metal and Cr(III) ions are considered non-toxic, hexavalent chromium, Cr(VI), is toxic and carcinogenic. According to the European Chemicals Agency (ECHA), chromium trioxide that is used in industrial electroplating processes is a \"substance of very high concern\" (SVHC).\nAbandoned chromium production sites often require environmental cleanup.\nPhysical properties.\nAtomic.\nChromium is the fourth transition metal found on the periodic table, and has an electron configuration of [Ar] 3d5 4s1. It is also the first element in the periodic table whose ground-state electron configuration violates the Aufbau principle. This occurs again later in the periodic table with other elements and their electron configurations, such as copper, niobium, and molybdenum. This occurs because electrons in the same orbital repel each other due to their like charges. In the previous elements, the energetic cost of promoting an electron to the next higher energy level is too great to compensate for that released by lessening inter-electronic repulsion. However, in the 3d transition metals, the energy gap between the 3d and the next-higher 4s subshell is very small, and because the 3d subshell is more compact than the 4s subshell, inter-electron repulsion is smaller between 4s electrons than between 3d electrons. This lowers the energetic cost of promotion and increases the energy released by it, so that the promotion becomes energetically feasible and one or even two electrons are always promoted to the 4s subshell. (Similar promotions happen for every transition metal atom but one, palladium.)\nChromium is the first element in the 3d series where the 3d electrons start to sink into the nucleus; they thus contribute less to metallic bonding, and hence the melting and boiling points and the enthalpy of atomisation of chromium are lower than those of the preceding element vanadium. Chromium(VI) is a strong oxidising agent in contrast to the molybdenum(VI) and tungsten(VI) oxides.\nBulk.\nChromium is extremely hard, and is the third hardest element behind carbon (diamond) and boron. Its Mohs hardness is 8.5, which means that it can scratch samples of quartz and topaz, but can be scratched by corundum. Chromium is highly resistant to tarnishing, which makes it useful as a metal that preserves its outermost layer from corroding, unlike other metals such as copper, magnesium, and aluminium.\nChromium has a melting point of 1907\u00a0\u00b0C (3465\u00a0\u00b0F), which is relatively low compared to the majority of transition metals. However, it still has the second highest melting point out of all the Period 4 elements, being topped by vanadium by 3\u00a0\u00b0C (5\u00a0\u00b0F) at 1910\u00a0\u00b0C (3470\u00a0\u00b0F). The boiling point of 2671\u00a0\u00b0C (4840\u00a0\u00b0F), however, is comparatively lower, having the fourth lowest boiling point out of the Period 4 transition metals alone behind copper, manganese and zinc. The electrical resistivity of chromium at 20\u00a0\u00b0C is 125 nanoohm-meters.\nChromium has a high specular reflection in comparison to other transition metals. In infrared, at 425 \u03bcm, chromium has a maximum reflectance of about 72%, reducing to a minimum of 62% at 750 \u03bcm before rising again to 90% at 4000 \u03bcm. When chromium is used in stainless steel alloys and polished, the specular reflection decreases with the inclusion of additional metals, yet is still high in comparison with other alloys. Between 40% and 60% of the visible spectrum is reflected from polished stainless steel. The explanation on why chromium displays such a high turnout of reflected photon waves in general, especially the 90% in infrared, can be attributed to chromium's magnetic properties. Chromium has unique magnetic properties - chromium is the only elemental solid that shows antiferromagnetic ordering at room temperature and below. Above 38\u00a0\u00b0C, its magnetic ordering becomes paramagnetic. The antiferromagnetic properties, which cause the chromium atoms to temporarily ionize and bond with themselves, are present because the body-centric cubic's magnetic properties are disproportionate to the lattice periodicity. This is due to the magnetic moments at the cube's corners and the unequal, but antiparallel, cube centers. From here, the frequency-dependent relative permittivity of chromium, deriving from Maxwell's equations and chromium's antiferromagnetism, leaves chromium with a high infrared and visible light reflectance.\nPassivation.\nChromium metal left standing in air is passivated - it forms a thin, protective, surface layer of oxide. This layer has a spinel structure a few atomic layers thick; it is very dense and inhibits the diffusion of oxygen into the underlying metal. In contrast, iron forms a more porous oxide through which oxygen can migrate, causing continued rusting. Passivation can be enhanced by short contact with oxidizing acids like nitric acid. Passivated chromium is stable against acids. Passivation can be removed with a strong reducing agent that destroys the protective oxide layer on the metal. Chromium metal treated in this way readily dissolves in weak acids.\nChromium, unlike iron and nickel, does not suffer from hydrogen embrittlement. However, it does suffer from nitrogen embrittlement, reacting with nitrogen from air and forming brittle nitrides at the high temperatures necessary to work the metal parts.\nIsotopes.\nNaturally occurring chromium is composed of four stable isotopes; 50Cr, 52Cr, 53Cr and 54Cr, with 52Cr being the most abundant (83.789% natural abundance). 50Cr is observationally stable, as it is theoretically capable of decaying to 50Ti via double electron capture with a half-life of no less than 1.3\u00d71018 years. Twenty-five radioisotopes have been characterized, ranging from 42Cr to 70Cr; the most stable radioisotope is 51Cr with a half-life of 27.7 days. All of the remaining radioactive isotopes have half-lives that are less than 24 hours and the majority less than 1 minute. Chromium also has two metastable nuclear isomers.\n53Cr is the radiogenic decay product of 53Mn (half-life 3.74 million years). Chromium isotopes are typically collocated (and compounded) with manganese isotopes. This circumstance is useful in isotope geology. Manganese-chromium isotope ratios reinforce the evidence from 26Al and 107Pd concerning the early history of the Solar System. Variations in 53Cr/52Cr and Mn/Cr ratios from several meteorites indicate an initial 53Mn/55Mn ratio that suggests Mn-Cr isotopic composition must result from in-situ decay of 53Mn in differentiated planetary bodies. Hence 53Cr provides additional evidence for nucleosynthetic processes immediately before coalescence of the Solar System.\nThe isotopes of chromium range in atomic mass from 43\u00a0u (43Cr) to 67\u00a0u (67Cr). The primary decay mode before the most abundant stable isotope, 52Cr, is electron capture and the primary mode after is beta decay. 53Cr has been posited as a proxy for atmospheric oxygen concentration.\nChemistry and compounds.\nChromium is a member of group 6, of the transition metals. The +3 and +6 states occur most commonly within chromium compounds, followed by +2; charges of +1, +4 and +5 for chromium are rare, but do nevertheless occasionally exist.\nCommon oxidation states.\nChromium(0).\nMany Cr(0) complexes are known. Bis(benzene)chromium and chromium hexacarbonyl are highlights in organochromium chemistry.\nChromium(II).\nChromium(II) compounds are uncommon, in part because they readily oxidize to chromium(III) derivatives in air. Water-stable chromium(II) chloride CrCl2 that can be made by reducing chromium(III) chloride with zinc. The resulting bright blue solution created from dissolving chromium(II) chloride is stable at neutral pH. Some other notable chromium(II) compounds include chromium(II) oxide CrO, and chromium(II) sulfate CrSO4. Many chromium(II) carboxylates are known. The red chromium(II) acetate (Cr2(O2CCH3)4) is somewhat famous. It features a Cr-Cr quadruple bond.\nChromium(III).\nA large number of chromium(III) compounds are known, such as chromium(III) nitrate, chromium(III) acetate, and chromium(III) oxide. Chromium(III) can be obtained by dissolving elemental chromium in acids like hydrochloric acid or sulfuric acid, but it can also be formed through the reduction of chromium(VI) by cytochrome c7. The Cr3+ ion has a similar radius (63\u00a0pm) to Al3+ (radius 50\u00a0pm), and they can replace each other in some compounds, such as in chrome alum and alum.\nChromium(III) tends to form octahedral complexes. Commercially available chromium(III) chloride hydrate is the dark green complex [CrCl2(H2O)4]Cl. Closely related compounds are the pale green [CrCl(H2O)5]Cl2 and violet [Cr(H2O)6]Cl3. If anhydrous violet chromium(III) chloride is dissolved in water, the violet solution turns green after some time as the chloride in the inner coordination sphere is replaced by water. This kind of reaction is also observed with solutions of chrome alum and other water-soluble chromium(III) salts. A tetrahedral coordination of chromium(III) has been reported for the Cr-centered Keggin anion [\u03b1-CrW12O40]5\u2013.\nChromium(III) hydroxide (Cr(OH)3) is amphoteric, dissolving in acidic solutions to form [Cr(H2O)6]3+, and in basic solutions to form [Cr(OH)6]3-. It is dehydrated by heating to form the green chromium(III) oxide (Cr2O3), a stable oxide with a crystal structure identical to that of corundum.\nChromium(VI).\nChromium(VI) compounds are oxidants at low or neutral pH. Chromate anions (CrO42-) and dichromate (Cr2O72\u2212) anions are the principal ions at this oxidation state. They exist at an equilibrium, determined by pH:\n2 [CrO4]2\u2212 + 2 H+ \u21cc [Cr2O7]2\u2212 + H2O\nChromium(VI) oxyhalides are known also and include chromyl fluoride (CrO2F2) and chromyl chloride (CrO2Cl2). However, despite several erroneous claims, chromium hexafluoride (as well as all higher hexahalides) remains unknown, as of 2020.\nSodium chromate is produced industrially by the oxidative roasting of chromite ore with sodium carbonate. The change in equilibrium is visible by a change from yellow (chromate) to orange (dichromate), such as when an acid is added to a neutral solution of potassium chromate. At yet lower pH values, further condensation to more complex oxyanions of chromium is possible.\nBoth the chromate and dichromate anions are strong oxidizing reagents at low pH:\nCr2O72- + 14 H3O+ + 6 e\u2212 \u2192 2 Cr3+ + 21 H2O (\u03b50 = 1.33\u00a0V)\nThey are, however, only moderately oxidizing at high pH:\nCrO42- + 4 H2O + 3 e\u2212 \u2192 Cr(OH)3 + 5 OH- (\u03b50 = \u22120.13\u00a0V)\nChromium(VI) compounds in solution can be detected by adding an acidic hydrogen peroxide solution. The unstable dark blue chromium(VI) peroxide (CrO5) is formed, which can be stabilized as an ether adduct CrO5\u00b7OR2.\nChromic acid has the hypothetical formula H2CrO4. It is a vaguely described chemical, despite many well-defined chromates and dichromates being known. The dark red chromium(VI) oxide CrO3, the acid anhydride of chromic acid, is sold industrially as \"chromic acid\". It can be produced by mixing sulfuric acid with dichromate and is a strong oxidizing agent.\nOther oxidation states.\nCompounds of chromium(V) are rather rare; the oxidation state +5 is only realized in few compounds but are intermediates in many reactions involving oxidations by chromate. The only binary compound is the volatile chromium(V) fluoride (CrF5). This red solid has a melting point of 30\u00a0\u00b0C and a boiling point of 117\u00a0\u00b0C. It can be prepared by treating chromium metal with fluorine at 400\u00a0\u00b0C and 200 bar pressure. The peroxochromate(V) is another example of the +5 oxidation state. Potassium peroxochromate (K3[Cr(O2)4]) is made by reacting potassium chromate with hydrogen peroxide at low temperatures. This red brown compound is stable at room temperature but decomposes spontaneously at 150\u2013170\u00a0\u00b0C.\nCompounds of chromium(IV) are slightly more common than those of chromium(V). The tetrahalides, CrF4, CrCl4, and CrBr4, can be produced by treating the trihalides (CrX3) with the corresponding halogen at elevated temperatures. Such compounds are susceptible to disproportionation reactions and are not stable in water. Organic compounds containing Cr(IV) state such as chromium tetra \"t\"-butoxide are also known.\nMost chromium(I) compounds are obtained solely by oxidation of electron-rich, octahedral chromium(0) complexes. Other chromium(I) complexes contain cyclopentadienyl ligands. As verified by X-ray diffraction, a Cr-Cr quintuple bond (length 183.51(4) \u00a0pm) has also been described. Extremely bulky monodentate ligands stabilize this compound by shielding the quintuple bond from further reactions.\nOccurrence.\nChromium is the 21st most abundant element in Earth's crust with an average concentration of 100\u00a0ppm. Chromium compounds are found in the environment from the erosion of chromium-containing rocks, and can be redistributed by volcanic eruptions. Typical background concentrations of chromium in environmental media are: atmosphere &lt;10\u00a0ng/m3; soil &lt;500\u00a0mg/kg; vegetation &lt;0.5\u00a0mg/kg; freshwater &lt;10\u00a0\u03bcg/L; seawater &lt;1\u00a0\u03bcg/L; sediment &lt;80\u00a0mg/kg. Chromium is mined as chromite (FeCr2O4) ore.\nAbout two-fifths of the chromite ores and concentrates in the world are produced in South Africa, about a third in Kazakhstan, while India, Russia, and Turkey are also substantial producers. Untapped chromite deposits are plentiful, but geographically concentrated in Kazakhstan and southern Africa. Although rare, deposits of native chromium exist. The Udachnaya Pipe in Russia produces samples of the native metal. This mine is a kimberlite pipe, rich in diamonds, and the reducing environment helped produce both elemental chromium and diamonds.\nThe relation between Cr(III) and Cr(VI) strongly depends on pH and oxidative properties of the location. In most cases, Cr(III) is the dominating species, but in some areas, the ground water can contain up to 39\u00a0\u00b5g/L of total chromium, of which 30\u00a0\u00b5g/L is Cr(VI).\nHistory.\nEarly applications.\nChromium minerals as pigments came to the attention of the west in the eighteenth century. On 26 July 1761, Johann Gottlob Lehmann found an orange-red mineral in the Beryozovskoye mines in the Ural Mountains which he named \"Siberian red lead\". Though misidentified as a lead compound with selenium and iron components, the mineral was in fact crocoite with a formula of PbCrO4. In 1770, Peter Simon Pallas visited the same site as Lehmann and found a red lead mineral that was discovered to possess useful properties as a pigment in paints. After Pallas, the use of Siberian red lead as a paint pigment began to develop rapidly throughout the region. Crocoite would be the principal source of chromium in pigments until the discovery of chromite many years later.\nIn 1794, Louis Nicolas Vauquelin received samples of crocoite ore. He produced chromium trioxide (CrO3) by mixing crocoite with hydrochloric acid. In 1797, Vauquelin discovered that he could isolate metallic chromium by heating the oxide in a charcoal oven, for which he is credited as the one who truly discovered the element. Vauquelin was also able to detect traces of chromium in precious gemstones, such as ruby and emerald.\nDuring the nineteenth century, chromium was primarily used not only as a component of paints, but in tanning salts as well. For quite some time, the crocoite found in Russia was the main source for such tanning materials. In 1827, a larger chromite deposit was discovered near Baltimore, United States, which quickly met the demand for tanning salts much more adequately than the crocoite that had been used previously. This made the United States the largest producer of chromium products until the year 1848, when larger deposits of chromite were uncovered near the city of Bursa, Turkey. With the development of metallurgy and chemical industries in the Western world, the need for chromium increased.\nChromium is also famous for its reflective, metallic luster when polished. It is used as a protective and decorative coating on car parts, plumbing fixtures, furniture parts and many other items, usually applied by electroplating. Chromium was used for electroplating as early as 1848, but this use only became widespread with the development of an improved process in 1924.\nProduction.\nApproximately 28.8 million metric tons (Mt) of marketable chromite ore was produced in 2013, and converted into 7.5 Mt of ferrochromium. According to John F. Papp, writing for the USGS, \"Ferrochromium is the leading end use of chromite ore, [and] stainless steel is the leading end use of ferrochromium.\"\nThe largest producers of chromium ore in 2013 have been South Africa (48%), Kazakhstan (13%), Turkey (11%), and India (10%), with several other countries producing the rest of about 18% of the world production.\nThe two main products of chromium ore refining are ferrochromium and metallic chromium. For those products the ore smelter process differs considerably. For the production of ferrochromium, the chromite ore (FeCr2O4) is reduced in large scale in electric arc furnace or in smaller smelters with either aluminium or silicon in an aluminothermic reaction.\nFor the production of pure chromium, the iron must be separated from the chromium in a two step roasting and leaching process. The chromite ore is heated with a mixture of calcium carbonate and sodium carbonate in the presence of air. The chromium is oxidized to the hexavalent form, while the iron forms the stable Fe2O3. The subsequent leaching at higher elevated temperatures dissolves the chromates and leaves the insoluble iron oxide. The chromate is converted by sulfuric acid into the dichromate.\n4 FeCr2O4 + 8 Na2CO3 + 7 O2 \u2192 8 Na2CrO4 + 2 Fe2O3 + 8 CO2\n2 Na2CrO4 + H2SO4 \u2192 Na2Cr2O7 + Na2SO4 + H2O\nThe dichromate is converted to the chromium(III) oxide by reduction with carbon and then reduced in an aluminothermic reaction to chromium.\nNa2Cr2O7 + 2 C \u2192 Cr2O3 + Na2CO3 + CO\nCr2O3 + 2 Al \u2192 Al2O3 + 2 Cr\nApplications.\nThe creation of metal alloys account for 85% of the available chromium's usage. The remainder of chromium is used in the chemical, refractory, and foundry industries.\nMetallurgy.\nThe strengthening effect of forming stable metal carbides at grain boundaries, and the strong increase in corrosion resistance made chromium an important alloying material for steel. High-speed tool steels contain between 3 and 5% chromium. Stainless steel, the primary corrosion-resistant metal alloy, is formed when chromium is introduced to iron in concentrations above 11%. For stainless steel's formation, ferrochromium is added to the molten iron. Also, nickel-based alloys have increased strength due to the formation of discrete, stable, metal, carbide particles at the grain boundaries. For example, Inconel 718 contains 18.6% chromium. Because of the excellent high-temperature properties of these nickel superalloys, they are used in jet engines and gas turbines in lieu of common structural materials. ASTM B163 relies on Chromium for condenser and heat-exchanger tubes, while castings with high strength at elevated temperatures that contain Chromium are standardised with ASTM A567. AISI type 332 is used where high temperature would normally cause carburization, oxidation or corrosion. Incoloy 800 \"is capable of remaining stable and maintaining its austenitic structure even after long time exposures to high temperatures\". Nichrome is used as resistance wire for heating elements in things like toasters and space heaters. These uses make chromium a strategic material. Consequently, during World War II, U.S. road engineers were instructed to avoid chromium in yellow road paint, as it \"may become a critical material during the emergency.\" The United States likewise considered chromium \"essential for the German war industry\" and made intense diplomatic efforts to keep it out of the hands of Nazi Germany.\nThe high hardness and corrosion resistance of unalloyed chromium makes it a reliable metal for surface coating; it is still the most popular metal for sheet coating, with its above-average durability, compared to other coating metals. A layer of chromium is deposited on pretreated metallic surfaces by electroplating techniques. There are two deposition methods: thin, and thick. Thin deposition involves a layer of chromium below 1\u00a0\u00b5m thickness deposited by chrome plating, and is used for decorative surfaces. Thicker chromium layers are deposited if wear-resistant surfaces are needed. Both methods use acidic chromate or dichromate solutions. To prevent the energy-consuming change in oxidation state, the use of chromium(III) sulfate is under development; for most applications of chromium, the previously established process is used.\nIn the chromate conversion coating process, the strong oxidative properties of chromates are used to deposit a protective oxide layer on metals like aluminium, zinc, and cadmium. This passivation and the self-healing properties of the chromate stored in the chromate conversion coating, which is able to migrate to local defects, are the benefits of this coating method. Because of environmental and health regulations on chromates, alternative coating methods are under development.\nChromic acid anodizing (or Type I anodizing) of aluminium is another electrochemical process that does not lead to the deposition of chromium, but uses chromic acid as an electrolyte in the solution. During anodization, an oxide layer is formed on the aluminium. The use of chromic acid, instead of the normally used sulfuric acid, leads to a slight difference of these oxide layers.\nThe high toxicity of Cr(VI) compounds, used in the established chromium electroplating process, and the strengthening of safety and environmental regulations demand a search for substitutes for chromium, or at least a change to less toxic chromium(III) compounds.\nPigment.\nThe mineral crocoite (which is also lead chromate PbCrO4) was used as a yellow pigment shortly after its discovery. After a synthesis method became available starting from the more abundant chromite, chrome yellow was, together with cadmium yellow, one of the most used yellow pigments. The pigment does not photodegrade, but it tends to darken due to the formation of chromium(III) oxide. It has a strong color, and was used for school buses in the United States and for the postal services (for example, the Deutsche Post) in Europe. The use of chrome yellow has since declined due to environmental and safety concerns and was replaced by organic pigments or other alternatives that are free from lead and chromium. Other pigments that are based around chromium are, for example, the deep shade of red pigment chrome red, which is simply lead chromate with lead(II) hydroxide (PbCrO4\u00b7Pb(OH)2). A very important chromate pigment, which was used widely in metal primer formulations, was zinc chromate, now replaced by zinc phosphate. A wash primer was formulated to replace the dangerous practice of pre-treating aluminium aircraft bodies with a phosphoric acid solution. This used zinc tetroxychromate dispersed in a solution of polyvinyl butyral. An 8% solution of phosphoric acid in solvent was added just before application. It was found that an easily oxidized alcohol was an essential ingredient. A thin layer of about 10\u201315\u00a0\u00b5m was applied, which turned from yellow to dark green when it was cured. There is still a question as to the correct mechanism. Chrome green is a mixture of Prussian blue and chrome yellow, while the chrome oxide green is chromium(III) oxide.\nChromium oxides are also used as a green pigment in the field of glassmaking and also as a glaze for ceramics. Green chromium oxide is extremely lightfast and as such is used in cladding coatings. It is also the main ingredient in infrared reflecting paints, used by the armed forces to paint vehicles and to give them the same infrared reflectance as green leaves.\nOther uses.\nChromium(III) ions present in corundum crystals (aluminium oxide) cause them to be colored red; when corundum appears as such, it is known as a ruby. If the corundum is lacking in chromium(III) ions, it is known as a sapphire. A red-colored artificial ruby may also be achieved by doping chromium(III) into artificial corundum crystals, thus making chromium a requirement for making synthetic rubies. Such a synthetic ruby crystal was the basis for the first laser, produced in 1960, which relied on stimulated emission of light from the chromium atoms in such a crystal. Ruby has a laser transition at 694.3 nanometers, in a deep red color.\nBecause of their toxicity, chromium(VI) salts are used for the preservation of wood. For example, chromated copper arsenate (CCA) is used in timber treatment to protect wood from decay fungi, wood-attacking insects, including termites, and marine borers. The formulations contain chromium based on the oxide CrO3 between 35.3% and 65.5%. In the United States, 65,300 metric tons of CCA solution were used in 1996.\nChromium(III) salts, especially chrome alum and chromium(III) sulfate, are used in the tanning of leather. The chromium(III) stabilizes the leather by cross linking the collagen fibers. Chromium tanned leather can contain between 4 and 5% of chromium, which is tightly bound to the proteins. Although the form of chromium used for tanning is not the toxic hexavalent variety, there remains interest in management of chromium in the tanning industry. Recovery and reuse, direct/indirect recycling, and \"chrome-less\" or \"chrome-free\" tanning are practiced to better manage chromium usage.\nThe high heat resistivity and high melting point makes chromite and chromium(III) oxide a material for high temperature refractory applications, like blast furnaces, cement kilns, molds for the firing of bricks and as foundry sands for the casting of metals. In these applications, the refractory materials are made from mixtures of chromite and magnesite. The use is declining because of the environmental regulations due to the possibility of the formation of chromium(VI). \nSeveral chromium compounds are used as catalysts for processing hydrocarbons. For example, the Phillips catalyst, prepared from chromium oxides, is used for the production of about half the world's polyethylene. Fe-Cr mixed oxides are employed as high-temperature catalysts for the water gas shift reaction. Copper chromite is a useful hydrogenation catalyst.\nChromates of metals are used in humistor.\nBiological role.\nThe biologically beneficial effects of chromium(III) are debated. Chromium is accepted by the U.S. National Institutes of Health as a trace element for its roles in the action of insulin, a hormone that mediates the metabolism and storage of carbohydrate, fat, and protein. The mechanism of its actions in the body, however, have not been defined, leaving in question the essentiality of chromium.\nIn contrast, hexavalent chromium (Cr(VI) or Cr6+) is highly toxic and mutagenic. Ingestion of chromium(VI) in water has been linked to stomach tumors, and it may also cause allergic contact dermatitis (ACD).\n\"Chromium deficiency\", involving a lack of Cr(III) in the body, or perhaps some complex of it, such as glucose tolerance factor, is controversial. Some studies suggest that the biologically active form of chromium (III) is transported in the body via an oligopeptide called low-molecular-weight chromium-binding substance (LMWCr), which might play a role in the insulin signaling pathway.\nThe chromium content of common foods is generally low (1\u201313 micrograms per serving). The chromium content of food varies widely, due to differences in soil mineral content, growing season, plant cultivar, and contamination during processing. Chromium (and nickel) leach into food cooked in stainless steel, with the effect being largest when the cookware is new. Acidic foods that are cooked for many hours also exacerbate this effect.\nDietary recommendations.\nThere is disagreement on chromium's status as an essential nutrient. Governmental departments from Australia, New Zealand, India, Japan, and the United States consider chromium essential while the European Food Safety Authority (EFSA) of the European Union does not.\nThe U.S. National Academy of Medicine (NAM) updated the Estimated Average Requirements (EARs) and the Recommended Dietary Allowances (RDAs) for chromium in 2001. For chromium, there was insufficient information to set EARs and RDAs, so its needs are described as estimates for Adequate Intakes (AIs). The current AIs of chromium for women ages 14 through 50 is 25 \u03bcg/day, and the AIs for women ages 50 and above is 20 \u03bcg/day. The AIs for women who are pregnant are 30 \u03bcg/day, and for women who are lactating, the set AIs are 45 \u03bcg/day. The AIs for men ages 14 through 50 are 35 \u03bcg/day, and the AIs for men ages 50 and above are 30 \u03bcg/day. For children ages 1 through 13, the AIs increase with age from 0.2 \u03bcg/day up to 25 \u03bcg/day. As for safety, the NAM sets Tolerable Upper Intake Levels (ULs) for vitamins and minerals when the evidence is sufficient. In the case of chromium, there is not yet enough information, hence no UL has been established. Collectively, the EARs, RDAs, AIs, and ULs are the parameters for the nutrition recommendation system known as Dietary Reference Intake (DRI). Australia and New Zealand consider chromium to be an essential nutrient, with an AI of 35 \u03bcg/day for men, 25 \u03bcg/day for women, 30 \u03bcg/day for women who are pregnant, and 45 \u03bcg/day for women who are lactating. A UL has not been set due to the lack of sufficient data. India considers chromium to be an essential nutrient, with an adult recommended intake of 33 \u03bcg/day. Japan also considers chromium to be an essential nutrient, with an AI of 10 \u03bcg/day for adults, including women who are pregnant or lactating. A UL has not been set. The EFSA of the European Union however, does not consider chromium to be an essential nutrient; chromium is the only mineral for which the United States and the European Union disagree.\nLabeling.\nFor U.S. food and dietary supplement labeling purposes, the amount of the substance in a serving is expressed as a percent of the Daily Value (%DV). For chromium labeling purposes, 100% of the Daily Value was 120 \u03bcg. As of May 27, 2016, the percentage of daily value was revised to 35 \u03bcg to bring the chromium intake into a consensus with the official Recommended Dietary Allowance. A table of the old and new adult daily values is provided at Reference Daily Intake.\nFood sources.\nFood composition databases such as those maintained by the U.S. Department of Agriculture do not contain information on the chromium content of foods. A wide variety of animal and vegetable foods contain chromium. Content per serving is influenced by the chromium content of the soil in which the plants are grown, by foodstuffs fed to animals, and by processing methods, as chromium is leached into foods if processed or cooked in stainless steel equipment. One diet analysis study conducted in Mexico reported an average daily chromium intake of 30 micrograms. An estimated 31% of adults in the United States consume multi-vitamin/mineral dietary supplements, which often contain 25 to 60 micrograms of chromium.\nSupplementation.\nChromium is an ingredient in total parenteral nutrition (TPN), because deficiency can occur after months of intravenous feeding with chromium-free TPN. It is also added to nutritional products for preterm infants. Although the mechanism of action in biological roles for chromium is unclear, in the United States chromium-containing products are sold as non-prescription dietary supplements in amounts ranging from 50 to 1,000 \u03bcg. Lower amounts of chromium are also often incorporated into multi-vitamin/mineral supplements consumed by an estimated 31% of adults in the United States. Chemical compounds used in dietary supplements include chromium chloride, chromium citrate, chromium(III) picolinate, chromium(III) polynicotinate, and other chemical compositions. The benefit of supplements has not been proven.\nApproved and disapproved health claims.\nIn 2005, the U.S. Food and Drug Administration had approved a qualified health claim for chromium picolinate with a requirement for very specific label wording: \"One small study suggests that chromium picolinate may reduce the risk of insulin resistance, and therefore possibly may reduce the risk of type 2 diabetes. FDA concludes, however, that the existence of such a relationship between chromium picolinate and either insulin resistance or type 2 diabetes is highly uncertain.\" At the same time, in answer to other parts of the petition, the FDA rejected claims for chromium picolinate and cardiovascular disease, retinopathy or kidney disease caused by abnormally high blood sugar levels. In 2010, chromium(III) picolinate was approved by Health Canada to be used in dietary supplements. Approved labeling statements include: a factor in the maintenance of good health, provides support for healthy glucose metabolism, helps the body to metabolize carbohydrates and helps the body to metabolize fats. The European Food Safety Authority (EFSA) approved claims in 2010 that chromium contributed to normal macronutrient metabolism and maintenance of normal blood glucose concentration, but rejected claims for maintenance or achievement of a normal body weight, or reduction of tiredness or fatigue.\nGiven the evidence for chromium deficiency causing problems with glucose management in the context of intravenous nutrition products formulated without chromium, research interest turned to whether chromium supplementation would benefit people who have type 2 diabetes but are not chromium deficient. Looking at the results from four meta-analyses, one reported a statistically significant decrease in fasting plasma glucose levels (FPG) and a non-significant trend in lower hemoglobin A1C. A second reported the same, a third reported significant decreases for both measures, while a fourth reported no benefit for either. A review published in 2016 listed 53 randomized clinical trials that were included in one or more of six meta-analyses. It concluded that whereas there may be modest decreases in FPG and/or HbA1C that achieve statistical significance in some of these meta-analyses, few of the trials achieved decreases large enough to be expected to be relevant to clinical outcome.\nTwo systematic reviews looked at chromium supplements as a mean of managing body weight in overweight and obese people. One, limited to chromium picolinate, a popular supplement ingredient, reported a statistically significant \u22121.1\u00a0kg (2.4\u00a0lb) weight loss in trials longer than 12 weeks. The other included all chromium compounds and reported a statistically significant \u22120.50\u00a0kg (1.1\u00a0lb) weight change. Change in percent body fat did not reach statistical significance. Authors of both reviews considered the clinical relevance of this modest weight loss as uncertain/unreliable. The European Food Safety Authority reviewed the literature and concluded that there was insufficient evidence to support a claim.\nChromium is promoted as a sports performance dietary supplement, based on the theory that it potentiates insulin activity, with anticipated results of increased muscle mass, and faster recovery of glycogen storage during post-exercise recovery. A review of clinical trials reported that chromium supplementation did not improve exercise performance or increase muscle strength. The International Olympic Committee reviewed dietary supplements for high-performance athletes in 2018 and concluded there was no need to increase chromium intake for athletes, nor support for claims of losing body fat.\nFresh-water fish.\nChromium is naturally present in the environment in trace amounts, but industrial use in rubber and stainless steel manufacturing, chrome plating, dyes for textiles, tanneries and other uses contaminates aquatic systems. In Bangladesh, rivers in or downstream from industrialized areas exhibit heavy metal contamination. Irrigation water standards for chromium are 0.1\u00a0mg/L, but some rivers are more than five times that amount. The standard for fish for human consumption is less than 1\u00a0mg/kg, but many tested samples were more than five times that amount. Chromium, especially hexavalent chromium, is highly toxic to fish because it is easily absorbed across the gills, readily enters blood circulation, crosses cell membranes and bioconcentrates up the food chain. In contrast, the toxicity of trivalent chromium is very low, attributed to poor membrane permeability and little biomagnification.\nAcute and chronic exposure to chromium(VI) affects fish behavior, physiology, reproduction and survival. Hyperactivity and erratic swimming have been reported in contaminated environments. Egg hatching and fingerling survival are affected. In adult fish there are reports of histopathological damage to liver, kidney, muscle, intestines, and gills. Mechanisms include mutagenic gene damage and disruptions of enzyme functions.\nThere is evidence that fish may not require chromium, but benefit from a measured amount in diet. In one study, juvenile fish gained weight on a zero chromium diet, but the addition of 500 \u03bcg of chromium in the form of chromium chloride or other supplement types, per kilogram of food (dry weight), increased weight gain. At 2,000 \u03bcg/kg the weight gain was no better than with the zero chromium diet, and there were increased DNA strand breaks.\nPrecautions.\nWater-insoluble chromium(III) compounds and chromium metal are not considered a health hazard, while the toxicity and carcinogenic properties of chromium(VI) have been known for a long time. Because of the specific transport mechanisms, only limited amounts of chromium(III) enter the cells. Acute oral toxicity ranges between 50 and 150\u00a0mg/kg. A 2008 review suggested that moderate uptake of chromium(III) through dietary supplements poses no genetic-toxic risk. In the US, the Occupational Safety and Health Administration (OSHA) has designated an air permissible exposure limit (PEL) in the workplace as a time-weighted average (TWA) of 1\u00a0mg/m3. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.5\u00a0mg/m3, time-weighted average. The IDLH (immediately dangerous to life and health) value is 250\u00a0mg/m3.\nChromium(VI) toxicity.\nThe acute oral toxicity for chromium(VI) ranges between 1.5 and 3.3\u00a0mg/kg. In the body, chromium(VI) is reduced by several mechanisms to chromium(III) already in the blood before it enters the cells. The chromium(III) is excreted from the body, whereas the chromate ion is transferred into the cell by a transport mechanism, by which also sulfate and phosphate ions enter the cell. The acute toxicity of chromium(VI) is due to its strong oxidant properties. After it reaches the blood stream, it damages the kidneys, the liver and blood cells through oxidation reactions. Hemolysis, renal, and liver failure result. Aggressive dialysis can be therapeutic.\nThe carcinogenity of chromate dust has been known for a long time, and in 1890 the first publication described the elevated cancer risk of workers in a chromate dye company. Three mechanisms have been proposed to describe the genotoxicity of chromium(VI). The first mechanism includes highly reactive hydroxyl radicals and other reactive radicals which are by products of the reduction of chromium(VI) to chromium(III). The second process includes the direct binding of chromium(V), produced by reduction in the cell, and chromium(IV) compounds to the DNA. The last mechanism attributed the genotoxicity to the binding to the DNA of the end product of the chromium(III) reduction.\nChromium salts (chromates) are also the cause of allergic reactions in some people. Chromates are often used to manufacture, amongst other things, leather products, paints, cement, mortar and anti-corrosives. Contact with products containing chromates can lead to allergic contact dermatitis and irritant dermatitis, resulting in ulceration of the skin, sometimes referred to as \"chrome ulcers\". This condition is often found in workers that have been exposed to strong chromate solutions in electroplating, tanning and chrome-producing manufacturers.\nEnvironmental issues.\nBecause chromium compounds were used in dyes, paints, and leather tanning compounds, these compounds are often found in soil and groundwater at active and abandoned industrial sites, needing environmental cleanup and remediation. Primer paint containing hexavalent chromium is still widely used for aerospace and automobile refinishing applications.\nIn 2010, the Environmental Working Group studied the drinking water in 35 American cities in the first nationwide study. The study found measurable hexavalent chromium in the tap water of 31 of the cities sampled, with Norman, Oklahoma, at the top of list; 25 cities had levels that exceeded California's proposed limit.\nThe more toxic hexavalent chromium form can be reduced to the less soluble trivalent oxidation state in soils by organic matter, ferrous iron, sulfides, and other reducing agents, with the rates of such reduction being faster under more acidic conditions than under more alkaline ones. In contrast, trivalent chromium can be oxidized to hexavalent chromium in soils by manganese oxides, such as Mn(III) and Mn(IV) compounds. Since the solubility and toxicity of chromium (VI) are greater that those of chromium (III), the oxidation-reduction conversions between the two oxidation states have implications for movement and bioavailability of chromium in soils, groundwater, and plants.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5671", "revid": "1157543899", "url": "https://en.wikipedia.org/wiki?curid=5671", "title": "Cymbal", "text": "Unpitched percussion instrument\nA cymbal is a common percussion instrument. Often used in pairs, cymbals consist of thin, normally round plates of various alloys. The majority of cymbals are of indefinite pitch, although small disc-shaped cymbals based on ancient designs sound a definite note (such as crotales). Cymbals are used in many ensembles ranging from the orchestra, percussion ensembles, jazz bands, heavy metal bands, and marching groups. Drum kits usually incorporate at least a crash, ride, or crash/ride, and a pair of hi-hat cymbals. A player of cymbals is known as a cymbalist.\nEtymology and names.\nThe word cymbal is derived from the Latin \"cymbalum\", which is the latinisation of the Greek word \"kymbalon\", \"cymbal\", which in turn derives from \"kymb\u0113\", \"cup, bowl\".\nIn orchestral scores, cymbals may be indicated by the French \"cymbales\"; German \"Becken\", \"Schellbecken\", \"Teller\", or \"Tschinellen\"; Italian \"piatti\" or \"cinelli\"; and Spanish \"platillos\". Many of these derive from the word for plates.\nHistory.\nCymbals have existed since ancient times. Representations of cymbals may be found in reliefs and paintings from Armenian Highlands (7th century BC), Larsa, Babylon, Assyria, ancient Egypt, ancient Greece, and ancient Rome. References to cymbals also appear throughout the Bible, through many Psalms and songs of praise to God. Cymbals may have been introduced to China from Central Asia in the 3rd or 4th century AD.\nIndia.\nIn India, cymbals have been in use since ancient times and are still used across almost all major temples and Buddhist sites. Gigantic aartis along the Ganges, which are revered by Hindus all over the world, are incomplete without large cymbals.\nCentral Asia and Iran.\nThe Shahnameh (circa 977 and 1010 CE) mentions the use of cymbals at least 14 times in its text, most in the context of creating a loud din in war, to frighten the enemy or to celebrate. The Persian word is \"sanj\" or \"senj\" (Persian ), but the Shahnameh does not claim these to be Persian in origin. Several times it calls then \"Indian cymbals.\" Other adjectives to describe them include \"golden\" and \"brass,\" and to play them is to \"clash\" them.\nA different form is called sanj angshati () or finger cymbals. These are zill.\nAshura ceremony.\nBesides the original use in war, another use in Persian culture was the Ashura ceremony. \nOriginally in the ceremony, two pieces of stone were beaten on the sides of the mourner with special movements accompanied by a lamentation song. This has been replaced by beating \"Karbzani\" or \"Karebzani\" and playing \"sanj \"and ratchets. Cities where this has been performed include Lahijan and Aran of Kashan, as well as Semnan and Sabzevar.\n\"See Zang\"\nEtymology.\nAll theories about the etymology of the word Sanj, identify it as a Pahlavi word. By some accounts means \"weight\"; and it is possible that the original term was \"sanjk\u016bb\" meaning \u201dstriking weights\u201d [against each other]. By some accounts the word is reform version of \"Zang\" (bell), referring to its bell-shaped plate.\nTurkey.\nCymbals were employed by Turkish janissaries in the 14th century or earlier. By the 17th century, such cymbals were used in European music, and more commonly played in military bands and orchestras by the mid 18th century. Since the 19th century, some composers have called for larger roles for cymbals in musical works, and a variety of cymbal shapes, techniques, and hardware have been developed in response.\nAnatomy.\nThe anatomy of the cymbal plays a large part in the sound it creates. A hole is drilled in the center of the cymbal, which is used to either mount the cymbal on a stand or for tying straps through (for hand playing). The bell, dome, or cup is the raised section immediately surrounding the hole. The bell produces a higher \"pinging\" pitch than the rest of the cymbal. The bow is the rest of the surface surrounding the bell. The bow is sometimes described in two areas: the ride and crash area. The ride area is the thicker section closer to the bell while the crash area is the thinner tapering section near the edge. The edge or rim is the immediate circumference of the cymbal.\nCymbals are measured by their diameter either in inches or centimeters. The size of the cymbal affects its sound, larger cymbals usually being louder and having longer sustain. The weight describes how thick the cymbal is. Cymbal weights are important to the sound they produce and how they play. Heavier cymbals have a louder volume, more cut, and better stick articulation (when using drum sticks). Thin cymbals have a fuller sound, lower pitch, and faster response.\nThe profile of the cymbal is the vertical distance of the bow from the bottom of the bell to the cymbal edge (higher profile cymbals are more bowl-shaped). The profile affects the pitch of the cymbal: higher profile cymbals have higher pitch.\nTypes.\nOrchestral cymbals.\nCymbals offer a composer nearly endless amounts of color and effect. Their unique timbre allows them to project even against a full orchestra and through the heaviest of orchestrations and enhance articulation and nearly any dynamic. Cymbals have been utilized historically to suggest frenzy, fury or bacchanalian revels, as seen in the Venus music in Wagner's \"Tannh\u00e4user\", Grieg's \"Peer Gynt suite\", and Osmin's aria \"O wie will ich triumphieren\" from Mozart's \"Die Entf\u00fchrung aus dem Serail\".\nClash cymbals.\nOrchestral clash cymbals are traditionally used in pairs, each one having a strap set in the bell of the cymbal by which they are held. Such a pair is known as clash cymbals, crash cymbals, hand cymbals, or plates. Certain sounds can be obtained by rubbing their edges together in a sliding movement for a \"sizzle\", striking them against each other in what is called a \"crash\", tapping the edge of one against the body of the other in what is called a \"tap-crash\", scraping the edge of one from the inside of the bell to the edge for a \"scrape\" or \"zischen\", or shutting the cymbals together and choking the sound in what is called a \"hi-hat\" or \"crush\". A skilled percussionist can obtain an enormous dynamic range from such cymbals. For example, in Beethoven's Symphony No. 9, the percussionist is employed to first play cymbals pianissimo, adding a touch of colour rather than loud crash.\nCrash cymbals are usually damped by pressing them against the percussionist's body. A composer may write \"laissez vibrer\", or, \"let vibrate\" (usually abbreviated l.v.), \"secco\" (dry), or equivalent indications on the score; more usually, the percussionist must judge when to damp based on the written duration of a crash and the context in which it occurs. Crash cymbals have traditionally been accompanied by the bass drum playing an identical part. This combination, played loudly, is an effective way to accentuate a note since it contributes to both very low and very high-frequency ranges and provides a satisfying \"crash-bang-wallop\". In older music the composer sometimes provided one part for this pair of instruments, writing \"senza piatti\" or \"piatti soli\" () if only one is needed. This came from the common practice of having one percussionist play using one cymbal mounted to the shell of the bass drum. The percussionist would crash the cymbals with the left hand and use a mallet to strike the bass drum with the right. This method is nowadays often employed in pit orchestras and called for specifically by composers who desire a certain effect. Stravinsky calls for this in his ballet Petrushka, and Mahler calls for this in his Titan Symphony. The modern convention is for the instruments to have independent parts. However, in kit drumming, a cymbal crash is still most often accompanied by a simultaneous kick to the bass drum, which provides a musical effect and support to the crash.\nHi hats.\nCrash cymbals evolved into the low-sock and from this to the modern hi-hat. Even in a modern drum kit, they remain paired with the bass drum as the two instruments which are played with the player's feet. However, hi-hat cymbals tend to be heavy with little taper, more similar to a ride cymbal than to a clash cymbal as found in a drum kit, and perform a ride rather than a crash function.\nSuspended cymbal.\nAnother use of cymbals is the suspended cymbal. This instrument takes its name from the traditional method of suspending the cymbal by means of a leather strap or rope, thus allowing the cymbal to vibrate as freely as possible for maximum musical effect. Early jazz drumming pioneers borrowed this style of cymbal mounting during the early 1900s and later drummers further developed this instrument into the mounted horizontal or nearly horizontally mounted \"crash\" cymbals of a modern drum kit instead of a leather strap suspension system. Many modern drum kits use a mount with felt or otherwise dampening fabric to act as a barrier to hold the cymbals between metal clamps: thus forming the modern-day ride cymbal. Suspended cymbals can be played with yarn-, sponge-, or cord wrapped mallets. The first known instance of using a sponge-headed mallet on a cymbal is the final chord of Hector Berlioz' Symphonie Fantastique. Composers sometimes specifically request other types of mallets like felt mallets or timpani mallets for different attack and sustain qualities. Suspended cymbals can produce bright and slicing tones when forcefully struck, and give an eerie transparent \"windy\" sound when played quietly. A tremolo, or roll (played with two mallets alternately striking on opposing sides of the cymbal) can build in volume from almost inaudible to an overwhelming climax in a satisfyingly smooth manner (as in Humperdinck's Mother Goose Suite). The edge of a suspended cymbal may be hit with the shoulder of a drum stick to obtain a sound somewhat akin to that of clash cymbals. Other methods of playing include scraping a coin or triangle beater rapidly across the ridges on the top of the cymbal, giving a \"zing\" sound (as some percussionists do in the fourth movement of Dvo\u0159\u00e1k's Symphony No. 9). Other effects that can be used include drawing a bass bow across the edge of the cymbal for a sound like squealing car brakes.\nAncient cymbals.\nAncient, antique or tuned cymbals are much more rarely called for. Their timbre is entirely different, more like that of small hand-bells or of the notes of the keyed harmonica. They are not struck full against each other, but by one of their edges, and the note given in by them is higher in proportion as they are thicker and smaller. Berlioz's \"Romeo and Juliet\" calls for two pairs of cymbals, modeled on some old Pompeian instruments no larger than the hand (some are no larger than a large coin), and tuned to F and B flat. The modern instruments descended from this line are the crotales.\nList of cymbal types.\nCymbal types include:\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5672", "revid": "5350778", "url": "https://en.wikipedia.org/wiki?curid=5672", "title": "Cadmium", "text": "Chemical element, symbol Cd and atomic number 48\nCadmium is a chemical element with the symbol Cd and atomic number 48. This soft, silvery-white metal is chemically similar to the two other stable metals in group 12, zinc and mercury. Like zinc, it demonstrates oxidation state +2 in most of its compounds, and like mercury, it has a lower melting point than the transition metals in groups 3 through 11. Cadmium and its congeners in group 12 are often not considered transition metals, in that they do not have partly filled \"d\" or \"f\" electron shells in the elemental or common oxidation states. The average concentration of cadmium in Earth's crust is between 0.1 and 0.5 parts per million (ppm). It was discovered in 1817 simultaneously by Stromeyer and Hermann, both in Germany, as an impurity in zinc carbonate.\nCadmium occurs as a minor component in most zinc ores and is a byproduct of zinc production. Cadmium was used for a long time as a corrosion-resistant plating on steel, and cadmium compounds are used as red, orange, and yellow pigments, to color glass, and to stabilize plastic. Cadmium use is generally decreasing because it is toxic (it is specifically listed in the European Restriction of Hazardous Substances Directive) and nickel-cadmium batteries have been replaced with nickel-metal hydride and lithium-ion batteries. One of its few new uses is in cadmium telluride solar panels.\nAlthough cadmium has no known biological function in higher organisms, a cadmium-dependent carbonic anhydrase has been found in marine diatoms.\nCharacteristics.\nPhysical properties.\nCadmium is a soft, malleable, ductile, silvery-white divalent metal. It is similar in many respects to zinc but forms complex compounds. Unlike most other metals, cadmium is resistant to corrosion and is used as a protective plate on other metals. As a bulk metal, cadmium is insoluble in water and is not flammable; however, in its powdered form it may burn and release toxic fumes.\nChemical properties.\nAlthough cadmium usually has an oxidation state of +2, it also exists in the +1 state. Cadmium and its congeners are not always considered transition metals, in that they do not have partly filled d or f electron shells in the elemental or common oxidation states. Cadmium burns in air to form brown amorphous cadmium oxide (CdO); the crystalline form of this compound is a dark red which changes color when heated, similar to zinc oxide. Hydrochloric acid, sulfuric acid, and nitric acid dissolve cadmium by forming cadmium chloride (CdCl2), cadmium sulfate (CdSO4), or cadmium nitrate (Cd(NO3)2). The oxidation state +1 can be produced by dissolving cadmium in a mixture of cadmium chloride and aluminium chloride, forming the Cd22+ cation, which is similar to the Hg22+ cation in mercury(I) chloride.\nCd + CdCl2 + 2 AlCl3 \u2192 Cd2(AlCl4)2\nThe structures of many cadmium complexes with nucleobases, amino acids, and vitamins have been determined.\nIsotopes.\nNaturally occurring cadmium is composed of eight isotopes. Two of them are radioactive, and three are expected to decay but have not done so under laboratory conditions. The two natural radioactive isotopes are 113Cd (beta decay, half-life is ) and 116Cd (two-neutrino double beta decay, half-life is ). The other three are 106Cd, 108Cd (both double electron capture), and 114Cd (double beta decay); only lower limits on these half-lives have been determined. At least three isotopes\u00a0\u2013 110Cd, 111Cd, and 112Cd\u00a0\u2013 are stable. Among the isotopes that do not occur naturally, the most long-lived are 109Cd with a half-life of 462.6\u00a0days, and 115Cd with a half-life of 53.46\u00a0hours. All of the remaining radioactive isotopes have half-lives of less than 2.5\u00a0hours, and the majority have half-lives of less than 5\u00a0minutes. Cadmium has 8 known meta states, with the most stable being 113mCd (\"t\"1\u20442\u00a0= 14.1\u00a0years), 115mCd (\"t\"1\u20442\u00a0= 44.6\u00a0days), and 117mCd (\"t\"1\u20442\u00a0= 3.36\u00a0hours).\nThe known isotopes of cadmium range in atomic mass from 94.950\u00a0u (95Cd) to 131.946\u00a0u (132Cd). For isotopes lighter than 112\u00a0u, the primary decay mode is electron capture and the dominant decay product is element\u00a047 (silver). Heavier isotopes decay mostly through beta emission producing element\u00a049 (indium).\nOne isotope of cadmium, 113Cd, absorbs neutrons with high selectivity: With very high probability, neutrons with energy below the \"cadmium cut-off\" will be absorbed; those higher than the \"cut-off will be transmitted\". The cadmium cut-off is about 0.5\u00a0eV, and neutrons below that level are deemed slow neutrons, distinct from intermediate and fast neutrons.\nCadmium is created via the s-process in low- to medium-mass stars with masses of 0.6\u00a0to 10\u00a0solar masses, over thousands of years. In that process, a silver atom captures a neutron and then undergoes beta decay.\nHistory.\nCadmium (Latin \"cadmia\", Greek \"\u03ba\u03b1\u03b4\u03bc\u03b5\u03af\u03b1\" meaning \"calamine\", a cadmium-bearing mixture of minerals that was named after the Greek mythological character \u039a\u03ac\u03b4\u03bc\u03bf\u03c2, Cadmus, the founder of Thebes) was discovered in contaminated zinc compounds sold in pharmacies in Germany in 1817 by Friedrich Stromeyer. Karl Samuel Leberecht Hermann simultaneously investigated the discoloration in zinc oxide and found an impurity, first suspected to be arsenic, because of the yellow precipitate with hydrogen sulfide. Additionally Stromeyer discovered that one supplier sold zinc carbonate instead of zinc oxide. Stromeyer found the new element as an impurity in zinc carbonate (calamine), and, for 100 years, Germany remained the only important producer of the metal. The metal was named after the Latin word for calamine, because it was found in this zinc ore. Stromeyer noted that some impure samples of calamine changed color when heated but pure calamine did not. He was persistent in studying these results and eventually isolated cadmium metal by roasting and reducing the sulfide. The potential for cadmium yellow as pigment was recognized in the 1840s, but the lack of cadmium limited this application.\nEven though cadmium and its compounds are toxic in certain forms and concentrations, the British Pharmaceutical Codex from 1907 states that cadmium iodide was used as a medication to treat \"enlarged joints, scrofulous glands, and chilblains\".\nIn 1907, the International Astronomical Union defined the international \u00e5ngstr\u00f6m in terms of a red cadmium spectral line (1 wavelength = 6438.46963\u00a0\u00c5). This was adopted by the 7th General Conference on Weights and Measures in 1927. In 1960, the definitions of both the metre and \u00e5ngstr\u00f6m were changed to use krypton.\nAfter the industrial scale production of cadmium started in the 1930s and 1940s, the major application of cadmium was the coating of iron and steel to prevent corrosion; in 1944, 62% and in 1956, 59% of the cadmium in the United States was used for plating. In 1956, 24% of the cadmium in the United States was used for a second application in red, orange and yellow pigments from sulfides and selenides of cadmium.\nThe stabilizing effect of cadmium chemicals like the carboxylates cadmium laurate and cadmium stearate on PVC led to an increased use of those compounds in the 1970s and 1980s. The demand for cadmium in pigments, coatings, stabilizers, and alloys declined as a result of environmental and health regulations in the 1980s and 1990s; in 2006, only 7% of to total cadmium consumption was used for plating, and only 10% was used for pigments.\nAt the same time, these decreases in consumption were compensated by a growing demand for cadmium for nickel-cadmium batteries, which accounted for 81% of the cadmium consumption in the United States in 2006.\nOccurrence.\nCadmium makes up about 0.1\u00a0ppm of Earth's crust. It is much rarer than zinc, which makes up about 65\u00a0ppm. No significant deposits of cadmium-containing ores are known. The only cadmium mineral of importance, greenockite (CdS), is nearly always associated with sphalerite (ZnS). This association is caused by geochemical similarity between zinc and cadmium, with no geological process likely to separate them. Thus, cadmium is produced mainly as a byproduct of mining, smelting, and refining sulfidic ores of zinc, and, to a lesser degree, lead and copper. Small amounts of cadmium, about 10% of consumption, are produced from secondary sources, mainly from dust generated by recycling iron and steel scrap. Production in the United States began in 1907, but wide use began after World War I.\nMetallic cadmium can be found in the Vilyuy River basin in Siberia.\nRocks mined for phosphate fertilizers contain varying amounts of cadmium, resulting in a cadmium concentration of as much as 300\u00a0mg/kg in the fertilizers and a high cadmium content in agricultural soils. Coal can contain significant amounts of cadmium, which ends up mostly in coal fly ash.\nCadmium in soil can be absorbed by crops such as rice and cocoa. Chinese ministry of agriculture measured in 2002 that 28% of rice it sampled had excess lead and 10% had excess cadmium above limits defined by law. \"Consumer Reports\" tested 28 brands of dark chocolate sold in the United States in 2022, and found cadmium in all of them, with 13 exceeding the California Maximum Allowable Dose level.\nSome plants such as willow trees and poplars have been found to clean both lead and cadmium from soil.\nTypical background concentrations of cadmium do not exceed 5\u00a0ng/m3 in the atmosphere; 2\u00a0mg/kg in soil; 1\u00a0\u03bcg/L in freshwater and 50\u00a0ng/L in seawater. Concentrations of cadmium above 10 \u03bcg/L may be stable in water having low total solute concentrations and \"p\" H and can be difficult to remove by conventional water treatment processes.\nProduction.\nCadmium is a common impurity in zinc ores, and it is most often isolated during the production of zinc. Some zinc ores concentrates from zinc sulfate ores contain up to 1.4% of cadmium. In the 1970s, the output of cadmium was per ton of zinc. Zinc sulfide ores are roasted in the presence of oxygen, converting the zinc sulfide to the oxide. Zinc metal is produced either by smelting the oxide with carbon or by electrolysis in sulfuric acid. Cadmium is isolated from the zinc metal by vacuum distillation if the zinc is smelted, or cadmium sulfate is precipitated from the electrolysis solution.\nThe British Geological Survey reports that in 2001, China was the top producer of cadmium with almost one-sixth of the world's production, closely followed by South Korea and Japan.\nApplications.\nCadmium is a common component of electric batteries, pigments, coatings, and electroplating.\nBatteries.\nIn 2009, 86% of cadmium was used in batteries, predominantly in rechargeable nickel-cadmium batteries. Nickel-cadmium cells have a nominal cell potential of 1.2\u00a0V. The cell consists of a positive nickel hydroxide electrode and a negative cadmium electrode plate separated by an alkaline electrolyte (potassium hydroxide). The European Union put a limit on cadmium in electronics in 2004 of 0.01%, with some exceptions, and in 2006 reduced the limit on cadmium content to 0.002%. Another type of battery based on cadmium is the silver-cadmium battery.\nElectroplating.\nCadmium electroplating, consuming 6% of the global production, is used in the aircraft industry to reduce corrosion of steel components. This coating is passivated by chromate salts. A limitation of cadmium plating is hydrogen embrittlement of high-strength steels from the electroplating process. Therefore, steel parts heat-treated to tensile strength above 1300 MPa (200 ksi) should be coated by an alternative method (such as special low-embrittlement cadmium electroplating processes or physical vapor deposition).\nTitanium embrittlement from cadmium-plated tool residues resulted in banishment of those tools (and the implementation of routine tool testing to detect cadmium contamination) in the A-12/SR-71, U-2, and subsequent aircraft programs that use titanium.\nNuclear fission.\nCadmium is used in the control rods of nuclear reactors, acting as a very effective neutron poison to control neutron flux in nuclear fission. When cadmium rods are inserted in the core of a nuclear reactor, cadmium absorbs neutrons, preventing them from creating additional fission events, thus controlling the amount of reactivity. The pressurized water reactor designed by Westinghouse Electric Company uses an alloy consisting of 80% silver, 15% indium, and 5% cadmium.\nTelevisions.\nQLED TVs have been starting to include cadmium in construction. Some companies have been looking to reduce the environmental impact of human exposure and pollution of the material in televisions during production.\nAnticancer drugs.\nComplexes based on heavy metals have great potential for the treatment of a wide variety of cancers but their use is often limited due to toxic side effects. However, scientists are advancing in the field and new promising cadmium complex compounds with reduced toxicity have been discovered.\nCompounds.\nCadmium oxide was used in black and white television phosphors and in the blue and green phosphors of color television cathode ray tubes. Cadmium sulfide (CdS) is used as a photoconductive surface coating for photocopier drums.\nVarious cadmium salts are used in paint pigments, with CdS as a yellow pigment being the most common. Cadmium selenide is a red pigment, commonly called \"cadmium red\". To painters who work with the pigment, cadmium provides the most brilliant and durable yellows, oranges, and reds\u00a0\u2013 so much so that during production, these colors are significantly toned down before they are ground with oils and binders or blended into watercolors, gouaches, acrylics, and other paint and pigment formulations. Because these pigments are potentially toxic, users should use a barrier cream on the hands to prevent absorption through the skin even though the amount of cadmium absorbed into the body through the skin is reported to be less than 1%.\nIn PVC, cadmium was used as heat, light, and weathering stabilizers. Currently, cadmium stabilizers have been completely replaced with barium-zinc, calcium-zinc and organo-tin stabilizers. Cadmium is used in many kinds of solder and bearing alloys, because it has a low coefficient of friction and fatigue resistance. It is also found in some of the lowest-melting alloys, such as Wood's metal.\nSemiconductors.\nCadmium is an element in some semiconductor materials. Cadmium sulfide, cadmium selenide, and cadmium telluride are used in some photodetectors and solar cells. HgCdTe detectors are sensitive to mid-infrared light and used in some motion detectors.\nLaboratory uses.\nHelium\u2013cadmium lasers are a common source of blue or ultraviolet laser light. Lasers at wavelengths of 325, 354 and 442\u00a0nm are made using this gain medium; some models can switch between these wavelengths. They are notably used in fluorescence microscopy as well as various laboratory uses requiring laser light at these wavelengths.\nCadmium selenide quantum dots emit bright luminescence under UV excitation (He-Cd laser, for example). The color of this luminescence can be green, yellow or red depending on the particle size. Colloidal solutions of those particles are used for imaging of biological tissues and solutions with a fluorescence microscope.\nIn molecular biology, cadmium is used to block voltage-dependent calcium channels from fluxing calcium ions, as well as in hypoxia research to stimulate proteasome-dependent degradation of Hif-1\u03b1.\nCadmium-selective sensors based on the fluorophore BODIPY have been developed for imaging and sensing of cadmium in cells. One powerful method for monitoring cadmium in aqueous environments involves electrochemistry. By employing a self-assembled monolayer one can obtain a cadmium selective electrode with a ppt-level sensitivity.\nBiological role and research.\nCadmium has no known function in higher organisms and is considered toxic. Cadmium is considered an environmental pollutant that causes health hazard to living organisms. Administration of cadmium to cells causes oxidative stress and increases the levels of antioxidants produced by cells to protect against macro molecular damage.\nHowever a cadmium-dependent carbonic anhydrase has been found in some marine diatoms. The diatoms live in environments with very low zinc concentrations and cadmium performs the function normally carried out by zinc in other anhydrases. This was discovered with X-ray absorption near edge structure (XANES) spectroscopy.\nCadmium is preferentially absorbed in the kidneys of humans. Up to about 30\u00a0mg of cadmium is commonly inhaled throughout human childhood and adolescence. Cadmium is under research regarding its toxicity in humans, potentially elevating risks of cancer, cardiovascular disease, and osteoporosis.\nEnvironment.\nThe biogeochemistry of cadmium and its release to the environment has been the subject of review, as has the speciation of cadmium in the environment.\nSafety.\n&lt;templatestyles src=\"Chembox/styles.css\"/&gt;\nChemical compound\nIndividuals and organizations have been reviewing cadmium's bioinorganic aspects for its toxicity. The most dangerous form of occupational exposure to cadmium is inhalation of fine dust and fumes, or ingestion of highly soluble cadmium compounds. Inhalation of cadmium fumes can result initially in metal fume fever, but may progress to chemical pneumonitis, pulmonary edema, and death.\nCadmium is also an environmental hazard. Human exposure is primarily from fossil fuel combustion, phosphate fertilizers, natural sources, iron and steel production, cement production and related activities, nonferrous metals production, and municipal solid waste incineration. Other sources of cadmium include bread, root crops, and vegetables.\nThere have been a few instances of general population poisoning as the result of long-term exposure to cadmium in contaminated food and water. Research into an estrogen mimicry that may induce breast cancer is ongoing, as of 2012[ [update]]. In the decades leading up to World War II, mining operations contaminated the Jinz\u016b River in Japan with cadmium and traces of other toxic metals. As a consequence, cadmium accumulated in the rice crops along the riverbanks downstream of the mines. Some members of the local agricultural communities consumed the contaminated rice and developed itai-itai disease and renal abnormalities, including proteinuria and glucosuria. The victims of this poisoning were almost exclusively post-menopausal women with low iron and low body stores of other minerals. Similar general population cadmium exposures in other parts of the world have not resulted in the same health problems because the populations maintained sufficient iron and other mineral levels. Thus, although cadmium is a major factor in the itai-itai disease in Japan, most researchers have concluded that it was one of several factors.\nCadmium is one of six substances banned by the European Union's Restriction of Hazardous Substances (RoHS) directive, which regulates hazardous substances in electrical and electronic equipment, but allows for certain exemptions and exclusions from the scope of the law.\nThe International Agency for Research on Cancer has classified cadmium and cadmium compounds as carcinogenic to humans. Although occupational exposure to cadmium is linked to lung and prostate cancer, there is still uncertainty about the carcinogenicity of cadmium in low environmental exposure. Recent data from epidemiological studies suggest that intake of cadmium through diet is associated with a higher risk of endometrial, breast, and prostate cancer as well as with osteoporosis in humans. A recent study has demonstrated that endometrial tissue is characterized by higher levels of cadmium in current and former smoking females.\nCadmium exposure is associated with a large number of illnesses including kidney disease, early atherosclerosis, hypertension, and cardiovascular diseases. Although studies show a significant correlation between cadmium exposure and occurrence of disease in human populations, a molecular mechanism has not yet been identified. One hypothesis holds that cadmium is an endocrine disruptor and some experimental studies have shown that it can interact with different hormonal signaling pathways. For example, cadmium can bind to the estrogen receptor alpha, and affect signal transduction along the estrogen and MAPK signaling pathways at low doses.\nThe tobacco plant absorbs and accumulates heavy metals such as cadmium from the surrounding soil into its leaves. Following tobacco smoke inhalation, these are readily absorbed into the body of users. Tobacco smoking is the most important single source of cadmium exposure in the general population. An estimated 10% of the cadmium content of a cigarette is inhaled through smoking. Absorption of cadmium through the lungs is more effective than through the gut. As much as 50% of the cadmium inhaled in cigarette smoke may be absorbed.\nOn average, cadmium concentrations in the blood of smokers is 4\u00a0to 5 times greater than non-smokers and in the kidney, 2\u20133 times greater than in non-smokers. Despite the high cadmium content in cigarette smoke, there seems to be little exposure to cadmium from passive smoking.\nIn a non-smoking population, food is the greatest source of exposure. High quantities of cadmium can be found in crustaceans, mollusks, offal, frog legs, cocoa solids, bitter and semi-bitter chocolate, seaweed, fungi and algae products. However, grains, vegetables, and starchy roots and tubers are consumed in much greater quantity in the U.S., and are the source of the greatest dietary exposure there. Most plants bio-accumulate metal toxins such as cadmium and when composted to form organic fertilizers, yield a product that often can contain high amounts (e.g., over 0.5\u00a0mg) of metal toxins for every kilogram of fertilizer. Fertilizers made from animal dung (e.g., cow dung) or urban waste can contain similar amounts of cadmium. The cadmium added to the soil from fertilizers (rock phosphates or organic fertilizers) become bio-available and toxic only if the soil pH is low (i.e., acidic soils).\nZinc, copper, calcium, and iron ions, and selenium with vitamin C are used to treat cadmium intoxication, though it is not easily reversed.\nRegulations.\nBecause of the adverse effects of cadmium on the environment and human health, the supply and use of cadmium is restricted in Europe under the REACH Regulation.\nThe EFSA Panel on Contaminants in the Food Chain specifies that 2.5 \u03bcg/kg body weight is a tolerable weekly intake for humans. The Joint FAO/WHO Expert Committee on Food Additives has declared 7 \u03bcg/kg body weight to be the provisional tolerable weekly intake level. The state of California requires a food label to carry a warning about potential exposure to cadmium on products such as cocoa powder.\nThe U.S. Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit (PEL) for cadmium at a time-weighted average (TWA) of 0.005 ppm. The National Institute for Occupational Safety and Health (NIOSH) has not set a recommended exposure limit (REL) and has designated cadmium as a known human carcinogen. The IDLH (immediately dangerous to life and health) level for cadmium is 9\u00a0mg/m3.\nIn addition to mercury, the presence of cadmium in some batteries has led to the requirement of proper disposal (or recycling) of batteries.\nProduct recalls.\nIn May 2006, a sale of the seats from Arsenal F.C.'s old stadium, Highbury in London, England was cancelled when the seats were discovered to contain trace amounts of cadmium. Reports of high levels of cadmium use in children's jewelry in 2010 led to a US Consumer Product Safety Commission investigation. The U.S. CPSC issued specific recall notices for cadmium content in jewelry sold by Claire's and Wal-Mart stores.\nIn June 2010, McDonald's voluntarily recalled more than 12\u00a0million promotional \"Shrek Forever After 3D\" Collectible Drinking Glasses because of the cadmium levels in paint pigments on the glassware. The glasses were manufactured by Arc International, of Millville, New Jersey, USA.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5675", "revid": "3240788", "url": "https://en.wikipedia.org/wiki?curid=5675", "title": "Curium", "text": "Chemical element, symbol Cm and atomic number 96\nCurium is a transuranic, radioactive chemical element with the symbol Cm and atomic number 96. This actinide element was named after eminent scientists Marie and Pierre Curie, both known for their research on radioactivity. Curium was first intentionally made by the team of Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso in 1944, using the cyclotron at Berkeley. They bombarded the newly discovered element plutonium (the isotope 239Pu) with alpha particles. This was then sent to the Metallurgical Laboratory at University of Chicago where a tiny sample of curium was eventually separated and identified. The discovery was kept secret until after the end of World War II. The news was released to the public in November 1947. Most curium is produced by bombarding uranium or plutonium with neutrons in nuclear reactors \u2013 one tonne of spent nuclear fuel contains ~20 grams of curium.\nCurium is a hard, dense, silvery metal with a high melting and boiling point for an actinide. It is paramagnetic at ambient conditions, but becomes antiferromagnetic upon cooling, and other magnetic transitions are also seen in many curium compounds. In compounds, curium usually has valence +3 and sometimes +4; the +3 valence is predominant in solutions. Curium readily oxidizes, and its oxides are a dominant form of this element. It forms strongly fluorescent complexes with various organic compounds, but there is no evidence of its incorporation into bacteria and archaea. If it gets into the human body, curium accumulates in bones, lungs, and liver, where it promotes cancer.\nAll known isotopes of curium are radioactive and have small critical mass for a nuclear chain reaction. They mostly emit \u03b1-particles; radioisotope thermoelectric generators can use the heat from this process, but this is hindered by the rarity and high cost of curium. Curium is used in making heavier actinides and the 238Pu radionuclide for power sources in artificial cardiac pacemakers and RTGs for spacecraft. It served as the \u03b1-source in the alpha particle X-ray spectrometers of several space probes, including the \"Sojourner\", \"Spirit\", \"Opportunity\", and \"Curiosity\" Mars rovers and the Philae lander on comet 67P/Churyumov\u2013Gerasimenko, to analyze the composition and structure of the surface.\nHistory.\nThough curium had likely been produced in previous nuclear experiments as well as the natural nuclear fission reactor at Oklo, Gabon, it was first intentionally synthesized, isolated and identified in 1944, at University of California, Berkeley, by Glenn T. Seaborg, Ralph A. James, and Albert Ghiorso. In their experiments, they used a cyclotron.\nCurium was chemically identified at the Metallurgical Laboratory (now Argonne National Laboratory), University of Chicago. It was the third transuranium element to be discovered even though it is the fourth in the series \u2013 the lighter element americium was still unknown.\nThe sample was prepared as follows: first plutonium nitrate solution was coated on a platinum foil of ~0.5\u00a0cm2 area, the solution was evaporated and the residue was converted into plutonium(IV) oxide (PuO2) by annealing. Following cyclotron irradiation of the oxide, the coating was dissolved with nitric acid and then precipitated as the hydroxide using concentrated aqueous ammonia solution. The residue was dissolved in perchloric acid, and further separation was done by ion exchange to yield a certain isotope of curium. The separation of curium and americium was so painstaking that the Berkeley group initially called those elements \"pandemonium\" (from Greek for \"all demons\" or \"hell\") and \"delirium\" (from Latin for \"madness\").\nCurium-242 was made in July\u2013August 1944 by bombarding 239Pu with \u03b1-particles to produce curium with the release of a neutron:\n &lt;chem&gt;^{239}_{94}Pu + ^{4}_{2}He -&gt; ^{242}_{96}Cm + ^{1}_{0}n&lt;/chem&gt;\nCurium-242 was unambiguously identified by the characteristic energy of the \u03b1-particles emitted during the decay:\n &lt;chem&gt;^{242}_{96}Cm -&gt; ^{238}_{94}Pu + ^{4}_{2}He&lt;/chem&gt;\nThe half-life of this alpha decay was first measured as 150 days and then corrected to 162.8 days.\nAnother isotope 240Cm was produced in a similar reaction in March 1945:\n &lt;chem&gt;^{239}_{94}Pu + ^{4}_{2}He -&gt; ^{240}_{96}Cm + 3^{1}_{0}n&lt;/chem&gt;\nThe \u03b1-decay half-life of 240Cm was correctly determined as 26.7 days.\nThe discovery of curium and americium in 1944 was closely related to the Manhattan Project, so the results were confidential and declassified only in 1945. Seaborg leaked the synthesis of the elements 95 and 96 on the U.S. radio show for children, the Quiz Kids, five days before the official presentation at an American Chemical Society meeting on November 11, 1945, when one listener asked if any new transuranic element beside plutonium and neptunium had been discovered during the war. The discovery of curium (242Cm and 240Cm), its production, and its compounds was later patented listing only Seaborg as the inventor.\nThe element was named after Marie Curie and her husband Pierre Curie, who are known for discovering radium and for their work in radioactivity. It followed the example of gadolinium, a lanthanide element above curium in the periodic table, which was named after the explorer of rare-earth elements Johan Gadolin:\n\"\"As the name for the element of atomic number 96 we should like to propose \"curium\", with symbol Cm. The evidence indicates that element 96 contains seven 5f electrons and is thus analogous to the element gadolinium, with its seven 4f electrons in the regular rare earth series. On this basis element 96 is named after the Curies in a manner analogous to the naming of gadolinium, in which the chemist Gadolin was honored.\"\"\nThe first curium samples were barely visible, and were identified by their radioactivity. Louis Werner and Isadore Perlman made the first substantial sample of 30\u00a0\u00b5g curium-242 hydroxide at University of California, Berkeley in 1947 by bombarding americium-241 with neutrons. Macroscopic amounts of curium(III) fluoride were obtained in 1950 by W. W. T. Crane, J. C. Wallmann and B. B. Cunningham. Its magnetic susceptibility was very close to that of GdF3 providing the first experimental evidence for the +3 valence of curium in its compounds. Curium metal was produced only in 1951 by reduction of CmF3 with barium.\nCharacteristics.\nPhysical.\nA synthetic, radioactive element, curium is a hard, dense metal with a silvery-white appearance and physical and chemical properties resembling gadolinium. Its melting point of 1344\u00a0\u00b0C is significantly higher than that of the previous elements neptunium (637\u00a0\u00b0C), plutonium (639\u00a0\u00b0C) and americium (1176\u00a0\u00b0C). In comparison, gadolinium melts at 1312\u00a0\u00b0C. Curium boils at 3556\u00a0\u00b0C. With a density of 13.52\u00a0g/cm3, curium is lighter than neptunium (20.45\u00a0g/cm3) and plutonium (19.8\u00a0g/cm3), but heavier than most other metals. Of two crystalline forms of curium, \u03b1-Cm is more stable at ambient conditions. It has a hexagonal symmetry, space group P63/mmc, lattice parameters \"a\"\u00a0=\u00a0365\u00a0pm and \"c\"\u00a0=\u00a01182\u00a0pm, and four formula units per unit cell. The crystal consists of double-hexagonal close packing with the layer sequence ABAC and so is isotypic with \u03b1-lanthanum. At pressure &gt;23\u00a0GPa, at room temperature, \u03b1-Cm becomes \u03b2-Cm, which has face-centered cubic symmetry, space group Fm3m and lattice constant \"a\"\u00a0=\u00a0493\u00a0pm. On further compression to 43\u00a0GPa, curium becomes an orthorhombic \u03b3-Cm structure similar to \u03b1-uranium, with no further transitions observed up to 52\u00a0GPa. These three curium phases are also called Cm I, II and III.\nCurium has peculiar magnetic properties. Its neighbor element americium shows no deviation from Curie-Weiss paramagnetism in the entire temperature range, but \u03b1-Cm transforms to an antiferromagnetic state upon cooling to 65\u201352\u00a0K, and \u03b2-Cm exhibits a ferrimagnetic transition at ~205\u00a0K. Curium pnictides show ferromagnetic transitions upon cooling: 244CmN and 244CmAs at 109\u00a0K, 248CmP at 73\u00a0K and 248CmSb at 162\u00a0K. The lanthanide analog of curium, gadolinium, and its pnictides, also show magnetic transitions upon cooling, but the transition character is somewhat different: Gd and GdN become ferromagnetic, and GdP, GdAs and GdSb show antiferromagnetic ordering.\nIn accordance with magnetic data, electrical resistivity of curium increases with temperature \u2013 about twice between 4 and 60\u00a0K \u2013 and then is nearly constant up to room temperature. There is a significant increase in resistivity over time (~) due to self-damage of the crystal lattice by alpha decay. This makes uncertain the true resistivity of curium (~). Curium's resistivity is similar to that of gadolinium, and the actinides plutonium and neptunium, but significantly higher than that of americium, uranium, polonium and thorium.\nUnder ultraviolet illumination, curium(III) ions show strong and stable yellow-orange fluorescence with a maximum in the range of 590\u2013640\u00a0nm depending on their environment. The fluorescence originates from the transitions from the first excited state 6D7/2 and the ground state 8S7/2. Analysis of this fluorescence allows monitoring interactions between Cm(III) ions in organic and inorganic complexes.\nChemical.\nCurium ion in solution almost always has a +3 oxidation state, the most stable oxidation state for curium. A +4 oxidation state is seen mainly in a few solid phases, such as CmO2 and CmF4. Aqueous curium(IV) is only known in the presence of strong oxidizers such as potassium persulfate, and is easily reduced to curium(III) by radiolysis and even by water itself. Chemical behavior of curium is different from the actinides thorium and uranium, and is similar to americium and many lanthanides. In aqueous solution, the Cm3+ ion is colorless to pale green; Cm4+ ion is pale yellow. The optical absorption of Cm3+ ion contains three sharp peaks at 375.4, 381.2 and 396.5 nm and their strength can be directly converted into the concentration of the ions. The +6 oxidation state has only been reported once in solution in 1978, as the curyl ion (CmO22+): this was prepared from beta decay of americium-242 in the americium(V) ion 242AmO2+. Failure to get Cm(VI) from oxidation of Cm(III) and Cm(IV) may be due to the high Cm4+/Cm3+ ionization potential and the instability of Cm(V).\nCurium ions are hard Lewis acids and thus form most stable complexes with hard bases. The bonding is mostly ionic, with a small covalent component. Curium in its complexes commonly exhibits a 9-fold coordination environment, with a tricapped trigonal prismatic molecular geometry.\nIsotopes.\nAbout 19 radioisotopes and 7 nuclear isomers, 233Cm to 251Cm, are known; none are stable. The longest half-lives are 15.6 million years (247Cm) and 348,000 years (248Cm). Other long-lived ones are 245Cm (8500 years), 250Cm (8300 years) and 246Cm (4760 years). Curium-250 is unusual: it mostly (~86%) decays by spontaneous fission. The most commonly used isotopes are 242Cm and 244Cm with the half-lives 162.8 days and 18.1 years, respectively.\nAll isotopes 242Cm-248Cm, and 250Cm, undergo a self-sustaining nuclear chain reaction and thus in principle can be a nuclear fuel in a reactor. As in most transuranic elements, nuclear fission cross section is especially high for the odd-mass curium isotopes 243Cm, 245Cm and 247Cm. These can be used in thermal-neutron reactors, whereas a mixture of curium isotopes is only suitable for fast breeder reactors since the even-mass isotopes are not fissile in a thermal reactor and accumulate as burn-up increases. The mixed-oxide (MOX) fuel, which is to be used in power reactors, should contain little or no curium because neutron activation of 248Cm will create californium. Californium is a strong neutron emitter, and would pollute the back end of the fuel cycle and increase the dose to reactor personnel. Hence, if minor actinides are to be used as fuel in a thermal neutron reactor, the curium should be excluded from the fuel or placed in special fuel rods where it is the only actinide present.\nThe adjacent table lists the critical masses for curium isotopes for a sphere, without moderator or reflector. With a metal reflector (30\u00a0cm of steel), the critical masses of the odd isotopes are about 3\u20134\u00a0kg. When using water (thickness ~20\u201330\u00a0cm) as the reflector, the critical mass can be as small as 59\u00a0gram for 245Cm, 155\u00a0gram for 243Cm and 1550\u00a0gram for 247Cm. There is significant uncertainty in these critical mass values. While it is usually on the order of 20%, the values for 242Cm and 246Cm were listed as large as 371\u00a0kg and 70.1\u00a0kg, respectively, by some research groups.\nCurium is not currently used as nuclear fuel due to its low availability and high price. 245Cm and 247Cm have very small critical mass and so could be used in tactical nuclear weapons, but none are known to have been made. Curium-243 is not suitable for such, due to its short half-life and strong \u03b1 emission, which would cause excessive heat. Curium-247 would be highly suitable due to its long half-life, which is 647 times longer than plutonium-239 (used in many existing nuclear weapons).\nOccurrence.\nThe longest-lived isotope, 247Cm, has half-life 15.6 million years; so any primordial curium, that is, present on Earth when it formed, should have decayed by now. Its past presence as an extinct radionuclide is detectable as an excess of its primordial, long-lived daughter 235U. Traces of curium may occur naturally in uranium minerals due to neutron capture and beta decay, though this has not been confirmed. Traces of 247Cm are also probably brought to Earth in cosmic rays, but again this has not been confirmed.\nCurium is made artificially in small amounts for research purposes. It also occurs as one of the waste products in spent nuclear fuel. Curium is present in nature in some areas used for nuclear weapons testing. Analysis of the debris at the test site of the United States' first thermonuclear weapon, Ivy Mike, (1 November 1952, Enewetak Atoll), besides einsteinium, fermium, plutonium and americium also revealed isotopes of berkelium, californium and curium, in particular 245Cm, 246Cm and smaller quantities of 247Cm, 248Cm and 249Cm.\nAtmospheric curium compounds are poorly soluble in common solvents and mostly adhere to soil particles. Soil analysis revealed about 4,000 times higher concentration of curium at the sandy soil particles than in water present in the soil pores. An even higher ratio of about 18,000 was measured in loam soils.\nThe transuranium elements from americium to fermium, including curium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\nCurium, and other non-primordial actinides, have also been detected in the spectrum of Przybylski's Star.\nSynthesis.\nIsotope preparation.\nCurium is made in small amounts in nuclear reactors, and by now only kilograms of 242Cm and 244Cm have been accumulated, and grams or even milligrams for heavier isotopes. Hence the high price of curium, which has been quoted at 160\u2013185 USD per milligram, with a more recent estimate at US$2,000/g for 242Cm and US$170/g for 244Cm. In nuclear reactors, curium is formed from 238U in a series of nuclear reactions. In the first chain, 238U captures a neutron and converts into 239U, which via \u03b2\u2212 decay transforms into 239Np and 239Pu.\nFurther neutron capture followed by \u03b2\u2212-decay gives americium (241Am) which further becomes 242Cm:\nFor research purposes, curium is obtained by irradiating not uranium but plutonium, which is available in large amounts from spent nuclear fuel. A much higher neutron flux is used for the irradiation that results in a different reaction chain and formation of 244Cm:\nCurium-244 alpha decays to 240Pu, but it also absorbs neutrons, hence a small amount of heavier curium isotopes. Of those, 247Cm and 248Cm are popular in scientific research due to their long half-lives. But the production rate of 247Cm in thermal neutron reactors is low because it is prone to fission due to thermal neutrons. Synthesis of 250Cm by neutron capture is unlikely due to the short half-life of the intermediate 249Cm (64 min), which \u03b2\u2212 decays to the berkelium isotope 249Bk.\nThe above cascade of (n,\u03b3) reactions gives a mix of different curium isotopes. Their post-synthesis separation is cumbersome, so a selective synthesis is desired. Curium-248 is favored for research purposes due to its long half-life. The most efficient way to prepare this isotope is by \u03b1-decay of the californium isotope 252Cf, which is available in relatively large amounts due to its long half-life (2.65 years). About 35\u201350\u00a0mg of 248Cm is produced thus, per year. The associated reaction produces 248Cm with isotopic purity of 97%.\nAnother isotope, 245Cm, can be obtained for research, from \u03b1-decay of 249Cf; the latter isotope is produced in small amounts from \u03b2\u2212-decay of 249Bk.\nMetal preparation.\nMost synthesis routines yield a mix of actinide isotopes as oxides, from which a given isotope of curium needs to be separated. An example procedure could be to dissolve spent reactor fuel (e.g. MOX fuel) in nitric acid, and remove the bulk of the uranium and plutonium using a PUREX (Plutonium \u2013 URanium EXtraction) type extraction with tributyl phosphate in a hydrocarbon. The lanthanides and the remaining actinides are then separated from the aqueous residue (raffinate) by a diamide-based extraction to give, after stripping, a mixture of trivalent actinides and lanthanides. A curium compound is then selectively extracted using multi-step chromatographic and centrifugation techniques with an appropriate reagent. \"Bis\"-triazinyl bipyridine complex has been recently proposed as such reagent which is highly selective to curium. Separation of curium from the very chemically similar americium can also be done by treating a slurry of their hydroxides in aqueous sodium bicarbonate with ozone at elevated temperature. Both americium and curium are present in solutions mostly in the +3 valence state; americium oxidizes to soluble Am(IV) complexes, but curium stays unchanged and so can be isolated by repeated centrifugation.\nMetallic curium is obtained by reduction of its compounds. Initially, curium(III) fluoride was used for this purpose. The reaction was done in an environment free of water and oxygen, in an apparatus made of tantalum and tungsten, using elemental barium or lithium as reducing agents.\nformula_1\nAnother possibility is reduction of curium(IV) oxide using a magnesium-zinc alloy in a melt of magnesium chloride and magnesium fluoride.\nCompounds and reactions.\nOxides.\nCurium readily reacts with oxygen forming mostly Cm2O3 and CmO2 oxides, but the divalent oxide CmO is also known. Black CmO2 can be obtained by burning curium oxalate (Cm2(C2O4)3), nitrate (Cm(NO3)3), or hydroxide in pure oxygen. Upon heating to 600\u2013650\u00a0\u00b0C in vacuum (about 0.01 Pa), it transforms into the whitish Cm2O3:\n &lt;chem&gt;4CmO2 -&gt;[\\Delta T] 2Cm2O3 + O2&lt;/chem&gt;.\nOr, Cm2O3 can be obtained by reducing CmO2 with molecular hydrogen:\n &lt;chem&gt;2CmO2 + H2 -&gt; Cm2O3 + H2O&lt;/chem&gt;\nAlso, a number of ternary oxides of the type M(II)CmO3 are known, where M stands for a divalent metal, such as barium.\nThermal oxidation of trace quantities of curium hydride (CmH2\u20133) has been reported to give a volatile form of CmO2 and the volatile trioxide CmO3, one of two known examples of the very rare +6 state for curium. Another observed species was reported to behave similar to a supposed plutonium tetroxide and was tentatively characterized as CmO4, with curium in the extremely rare +8 state; but new experiments seem to indicate that CmO4 does not exist, and have cast doubt on the existence of PuO4 as well.\nHalides.\nThe colorless curium(III) fluoride (CmF3) can be made by adding fluoride ions into curium(III)-containing solutions. The brown tetravalent curium(IV) fluoride (CmF4) on the other hand is only obtained by reacting curium(III) fluoride with molecular fluorine:\n formula_2\nA series of ternary fluorides are known of the form A7Cm6F31 (A = alkali metal).\nThe colorless curium(III) chloride (CmCl3) is made by reacting curium hydroxide (Cm(OH)3) with anhydrous hydrogen chloride gas. It can be further turned into other halides such as curium(III) bromide (colorless to light green) and curium(III) iodide (colorless), by reacting it with the ammonia salt of the corresponding halide at temperatures of ~400\u2013450\u00b0C:\n formula_3\nOr, one can heat curium oxide to ~600\u00b0C with the corresponding acid (such as hydrobromic for curium bromide). Vapor phase hydrolysis of curium(III) chloride gives curium oxychloride:\n formula_4\nChalcogenides and pnictides.\nSulfides, selenides and tellurides of curium have been obtained by treating curium with gaseous sulfur, selenium or tellurium in vacuum at elevated temperature. Curium pnictides of the type CmX are known for nitrogen, phosphorus, arsenic and antimony. They can be prepared by reacting either curium(III) hydride (CmH3) or metallic curium with these elements at elevated temperature.\nOrganocurium compounds and biological aspects.\nOrganometallic complexes analogous to uranocene are known also for other actinides, such as thorium, protactinium, neptunium, plutonium and americium. Molecular orbital theory predicts a stable \"curocene\" complex (\u03b78-C8H8)2Cm, but it has not been reported experimentally yet.\nFormation of the complexes of the type Cm(n-C3H7-BTP)3 (BTP = 2,6-di(1,2,4-triazin-3-yl)pyridine), in solutions containing n-C3H7-BTP and Cm3+ ions has been confirmed by EXAFS. Some of these BTP-type complexes selectively interact with curium and thus are useful for separating it from lanthanides and another actinides. Dissolved Cm3+ ions bind with many organic compounds, such as hydroxamic acid, urea, fluorescein and adenosine triphosphate. Many of these compounds are related to biological activity of various microorganisms. The resulting complexes show strong yellow-orange emission under UV light excitation, which is convenient not only for their detection, but also for studying interactions between the Cm3+ ion and the ligands via changes in the half-life (of the order ~0.1 ms) and spectrum of the fluorescence.\nCurium has no biological significance. There are a few reports on biosorption of Cm3+ by bacteria and archaea, but no evidence for incorporation of curium into them.\nApplications.\nRadionuclides.\nCurium is one of the most radioactive isolable elements. Its two most common isotopes 242Cm and 244Cm are strong alpha emitters (energy 6\u00a0MeV); they have fairly short half-lives, 162.8 days and 18.1 years, and give as much as 120\u00a0W/g and 3\u00a0W/g of heat, respectively. Therefore, curium can be used in its common oxide form in radioisotope thermoelectric generators like those in spacecraft. This application has been studied for the 244Cm isotope, while 242Cm was abandoned due to its prohibitive price, around 2000\u00a0USD/g. 243Cm with a ~30-year half-life and good energy yield of ~1.6\u00a0W/g could be a suitable fuel, but it gives significant amounts of harmful gamma and beta rays from radioactive decay products. As an \u03b1-emitter, 244Cm needs much less radiation shielding, but it has a high spontaneous fission rate, and thus a lot of neutron and gamma radiation. Compared to a competing thermoelectric generator isotope such as 238Pu, 244Cm emits 500 times more neutrons, and its higher gamma emission requires a shield that is 20 times thicker\u2014 of lead for a 1\u00a0kW source, compared to for 238Pu. Therefore, this use of curium is currently considered impractical.\nA more promising use of 242Cm is for making 238Pu, a better radioisotope for thermoelectric generators such as in heart pacemakers. The alternate routes to 238Pu use the (n,\u03b3) reaction of 237Np, or deuteron bombardment of uranium, though both reactions always produce 236Pu as an undesired by-product since the latter decays to 232U with strong gamma emission. Curium is a common starting material for making higher transuranic and superheavy elements. Thus, bombarding 248Cm with neon (22Ne), magnesium (26Mg), or calcium (48Ca) yields isotopes of seaborgium (265Sg), hassium (269Hs and 270Hs), and livermorium (292Lv, 293Lv, and possibly 294Lv). Californium was discovered when a microgram-sized target of curium-242 was irradiated with 35\u00a0MeV alpha particles using the cyclotron at Berkeley:\nCm + He \u2192 Cf + n\nOnly about 5,000 atoms of californium were produced in this experiment.\nThe odd-mass curium isotopes 243Cm, 245Cm, and 247Cm are all highly fissile and can release additional energy in a thermal spectrum nuclear reactor. All curium isotopes are fissionable in fast-neutron reactors. This is one of the motives for minor actinide separation and transmutation in the nuclear fuel cycle, helping to reduce the long-term radiotoxicity of used, or spent nuclear fuel.\nX-ray spectrometer.\nThe most practical application of 244Cm\u2014though rather limited in total volume\u2014is as \u03b1-particle source in alpha particle X-ray spectrometers (APXS). These instruments were installed on the Sojourner, Mars, Mars 96, Mars Exploration Rovers and Philae comet lander, as well as the Mars Science Laboratory to analyze the composition and structure of the rocks on the surface of planet Mars. APXS was also used in the Surveyor 5\u20137 moon probes but with a 242Cm source.\nAn elaborate APXS setup has a sensor head containing six curium sources with a total decay rate of several tens of millicuries (roughly one gigabecquerel). The sources are collimated on a sample, and the energy spectra of the alpha particles and protons scattered from the sample are analyzed (proton analysis is done only in some spectrometers). These spectra contain quantitative information on all major elements in the sample except for hydrogen, helium and lithium.\nSafety.\nDue to its radioactivity, curium and its compounds must be handled in appropriate labs under special arrangements. While curium itself mostly emits \u03b1-particles which are absorbed by thin layers of common materials, some of its decay products emit significant fractions of beta and gamma rays, which require a more elaborate protection. If consumed, curium is excreted within a few days and only 0.05% is absorbed in the blood. From there, ~45% goes to the liver, 45% to the bones, and the remaining 10% is excreted. In bone, curium accumulates on the inside of the interfaces to the bone marrow and does not significantly redistribute with time; its radiation destroys bone marrow and thus stops red blood cell creation. The biological half-life of curium is about 20 years in the liver and 50 years in the bones. Curium is absorbed in the body much more strongly via inhalation, and the allowed total dose of 244Cm in soluble form is 0.3 \u03bcCi. Intravenous injection of 242Cm- and 244Cm-containing solutions to rats increased the incidence of bone tumor, and inhalation promoted lung and liver cancer.\nCurium isotopes are inevitably present in spent nuclear fuel (about 20 g/tonne). The isotopes 245Cm\u2013248Cm have decay times of thousands of years and must be removed to neutralize the fuel for disposal. Such a procedure involves several steps, where curium is first separated and then converted by neutron bombardment in special reactors to short-lived nuclides. This procedure, nuclear transmutation, while well documented for other elements, is still being developed for curium.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5676", "revid": "42089642", "url": "https://en.wikipedia.org/wiki?curid=5676", "title": "Californium", "text": "Chemical element, symbol Cf and atomic number 98\nCalifornium is a radioactive chemical element with the symbol Cf and atomic number 98. The element was first synthesized in 1950 at Lawrence Berkeley National Laboratory (then the University of California Radiation Laboratory), by bombarding curium with alpha particles (helium-4 ions). It is an actinide element, the sixth transuranium element to be synthesized, and has the second-highest atomic mass of all elements that have been produced in amounts large enough to see with the naked eye (after einsteinium). The element was named after the university and the U.S. state of California.\nTwo crystalline forms exist for californium at normal pressure: one above and one below . A third form exists at high pressure. Californium slowly tarnishes in air at room temperature. Californium compounds are dominated by the +3 oxidation state. The most stable of californium's twenty known isotopes is californium-251, with a half-life of 898 years. This short half-life means the element is not found in significant quantities in the Earth's crust. 252Cf, with a half-life of about 2.645 years, is the most common isotope used and is produced at Oak Ridge National Laboratory in the United States and Research Institute of Atomic Reactors in Russia.\nCalifornium is one of the few transuranium elements with practical applications. Most of these applications exploit the property of certain isotopes of californium to emit neutrons. For example, californium can be used to help start up nuclear reactors, and it is employed as a source of neutrons when studying materials using neutron diffraction and neutron spectroscopy. Californium can also be used in nuclear synthesis of higher mass elements; oganesson (element 118) was synthesized by bombarding californium-249 atoms with calcium-48 ions. Users of californium must take into account radiological concerns and the element's ability to disrupt the formation of red blood cells by bioaccumulating in skeletal tissue.\nCharacteristics.\nPhysical properties.\nCalifornium is a silvery-white actinide metal with a melting point of and an estimated boiling point of . The pure metal is malleable and is easily cut with a razor blade. Californium metal starts to vaporize above when exposed to a vacuum. Below californium metal is either ferromagnetic or ferrimagnetic (it acts like a magnet), between 48 and 66\u00a0K it is antiferromagnetic (an intermediate state), and above it is paramagnetic (external magnetic fields can make it magnetic). It forms alloys with lanthanide metals but little is known about the resulting materials.\nThe element has two crystalline forms at standard atmospheric pressure: a double-hexagonal close-packed form dubbed alpha (\u03b1) and a face-centered cubic form designated beta (\u03b2). The \u03b1 form exists below 600\u2013800\u00a0\u00b0C with a density of 15.10\u00a0g/cm3 and the \u03b2 form exists above 600\u2013800\u00a0\u00b0C with a density of 8.74\u00a0g/cm3. At 48\u00a0GPa of pressure the \u03b2 form changes into an orthorhombic crystal system due to delocalization of the atom's 5f electrons, which frees them to bond.\nThe bulk modulus of a material is a measure of its resistance to uniform pressure. Californium's bulk modulus is , which is similar to trivalent lanthanide metals but smaller than more familiar metals, such as aluminium (70\u00a0GPa).\nChemical properties and compounds.\nCalifornium exhibits oxidation states of 4, 3, or 2. It typically forms eight or nine bonds to surrounding atoms or ions. Its chemical properties are predicted to be similar to other primarily 3+ valence actinide elements and the element dysprosium, which is the lanthanide above californium in the periodic table. Compounds in the +4 oxidation state are strong oxidizing agents and those in the +2 state are strong reducing agents.\nThe element slowly tarnishes in air at room temperature, with the rate increasing when moisture is added. Californium reacts when heated with hydrogen, nitrogen, or a chalcogen (oxygen family element); reactions with dry hydrogen and aqueous mineral acids are rapid. \nCalifornium is only water-soluble as the californium(III) cation. Attempts to reduce or oxidize the +3 ion in solution have failed. The element forms a water-soluble chloride, nitrate, perchlorate, and sulfate and is precipitated as a fluoride, oxalate, or hydroxide. Californium is the heaviest actinide to exhibit covalent properties, as is observed in the californium borate.\nIsotopes.\nTwenty isotopes of californium are known (mass number ranging from 237 to 256); the most stable are 251Cf with half-life 898 years, 249Cf with half-life 351 years, 250Cf with half-life 13.08 years, and 252Cf with half-life 2.645 years. All other isotopes have half-life shorter than a year, and most of these have half-life less than 20 minutes.\n249Cf is formed from beta decay of berkelium-249, and most other californium isotopes are made by subjecting berkelium to intense neutron radiation in a nuclear reactor. Though californium-251 has the longest half-life, its production yield is only 10% due to its tendency to collect neutrons (high neutron capture) and its tendency to interact with other particles (high neutron cross section).\nCalifornium-252 is a very strong neutron emitter, which makes it extremely radioactive and harmful. 252Cf, 96.9% of the time, alpha decays to curium-248; the other 3.1% of decays are spontaneous fission. One microgram (\u03bcg) of 252Cf emits 2.3\u00a0million neutrons per second, an average of 3.7 neutrons per spontaneous fission. Most other isotopes of californium, alpha decay to curium (atomic number 96).\nHistory.\nCalifornium was first made at University of California Radiation Laboratory, Berkeley, by physics researchers Stanley Gerald Thompson, Kenneth Street Jr., Albert Ghiorso, and Glenn T. Seaborg, about February 9, 1950. It was the sixth transuranium element to be discovered; the team announced its discovery on March 17, 1950.\nTo produce californium, a microgram-size target of curium-242 (Cm) was bombarded with 35\u00a0MeV alpha particles (He) in the cyclotron at Berkeley, which produced californium-245 (Cf) plus one free neutron ().\n Cm + He \u2192 Cf + \nTo identify and separate out the element, ion exchange and adsorsion methods were undertaken. Only about 5,000 atoms of californium were produced in this experiment, and these atoms had a half-life of 44\u00a0minutes.\nThe discoverers named the new element after the university and the state. This was a break from the convention used for elements 95 to 97, which drew inspiration from how the elements directly above them in the periodic table were named. However, the element directly above #98 in the periodic table, dysprosium, has a name that means \"hard to get at\", so the researchers decided to set aside the informal naming convention. They added that \"the best we can do is to point out [that] ... searchers a century ago found it difficult to get to California\".\nWeighable amounts of californium were first produced by the irradiation of plutonium targets at Materials Testing Reactor at National Reactor Testing Station, eastern Idaho; these findings were reported in 1954. The high spontaneous fission rate of californium-252 was observed in these samples. The first experiment with californium in concentrated form occurred in 1958. The isotopes 249Cf to 252Cf were isolated that same year from a sample of plutonium-239 that had been irradiated with neutrons in a nuclear reactor for five years. Two years later, in 1960, Burris Cunningham and James Wallman of Lawrence Radiation Laboratory of the University of California created the first californium compounds\u2014californium trichloride, californium(III) oxychloride, and californium oxide\u2014by treating californium with steam and hydrochloric acid.\nThe High Flux Isotope Reactor (HFIR) at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, started producing small batches of californium in the 1960s. By 1995, HFIR nominally produced of californium annually. Plutonium supplied by the United Kingdom to the United States under the 1958 US\u2013UK Mutual Defence Agreement was used for making californium.\nThe Atomic Energy Commission sold 252Cf to industrial and academic customers in the early 1970s for $10 per microgram, and an average of of 252Cf were shipped each year from 1970 to 1990. Californium metal was first prepared in 1974 by Haire and Baybarz, who reduced californium(III) oxide with lanthanum metal to obtain microgram amounts of sub-micrometer thick films.\nOccurrence.\nTraces of californium can be found near facilities that use the element in mineral prospecting and in medical treatments. The element is fairly insoluble in water, but it adheres well to ordinary soil; and concentrations of it in the soil can be 500 times higher than in the water surrounding the soil particles.\nNuclear fallout from atmospheric nuclear weapons testing prior to 1980 contributed a small amount of californium to the environment. Californium isotopes with mass numbers 249, 252, 253, and 254 have been observed in the radioactive dust collected from the air after a nuclear explosion. Californium is not a major radionuclide at United States Department of Energy legacy sites since it was not produced in large quantities.\nCalifornium was once believed to be produced in supernovas, as their decay matches the 60-day half-life of 254Cf. However, subsequent studies failed to demonstrate any californium spectra, and supernova light curves are now thought to follow the decay of nickel-56.\nThe transuranium elements from americium to fermium, including californium, occurred naturally in the natural nuclear fission reactor at Oklo, but no longer do so.\nSpectral lines of californium, along with those of several other non-primordial elements, were detected in Przybylski's Star in 2008.\nProduction.\nCalifornium is produced in nuclear reactors and particle accelerators. Californium-250 is made by bombarding berkelium-249 (Bk) with neutrons, forming berkelium-250 (Bk) via neutron capture (n,\u03b3) which, in turn, quickly beta decays (\u03b2\u2212) to californium-250 (Cf) in the following reaction:\nBk(n,\u03b3)Bk \u2192 Cf + \u03b2\u2212\nBombardment of californium-250 with neutrons produces californium-251 and californium-252.\nProlonged irradiation of americium, curium, and plutonium with neutrons produces milligram amounts of californium-252 and microgram amounts of californium-249. As of 2006, curium isotopes 244 to 248 are irradiated by neutrons in special reactors to produce primarily californium-252 with lesser amounts of isotopes 249 to 255.\nMicrogram quantities of californium-252 are available for commercial use through the U.S. Nuclear Regulatory Commission. Only two sites produce californium-252: the Oak Ridge National Laboratory in the United States, and the Research Institute of Atomic Reactors in Dimitrovgrad, Russia. As of 2003, the two sites produce 0.25 grams and 0.025 grams of californium-252 per year, respectively.\nThree californium isotopes with significant half-lives are produced, requiring a total of 15 neutron captures by uranium-238 without nuclear fission or alpha decay occurring during the process. Californium-253 is at the end of a production chain that starts with uranium-238, includes several isotopes of plutonium, americium, curium, berkelium, and the californium isotopes 249 to 253 (see diagram).\nApplications.\nCalifornium-252 has a number of specialized uses as a strong neutron emitter; it produces 139\u00a0million neutrons per microgram per minute. This property makes it useful as a startup neutron source for some nuclear reactors and as a portable (non-reactor based) neutron source for neutron activation analysis to detect trace amounts of elements in samples. Neutrons from californium are used as a treatment of certain cervical and brain cancers where other radiation therapy is ineffective. It has been used in educational applications since 1969 when Georgia Institute of Technology got a loan of 119\u00a0\u03bcg of 252Cf from the Savannah River Site. It is also used with online elemental coal analyzers and bulk material analyzers in the coal and cement industries.\nNeutron penetration into materials makes californium useful in detection instruments such as fuel rod scanners; neutron radiography of aircraft and weapons components to detect corrosion, bad welds, cracks and trapped moisture; and in portable metal detectors. Neutron moisture gauges use 252Cf to find water and petroleum layers in oil wells, as a portable neutron source for gold and silver prospecting for on-the-spot analysis, and to detect ground water movement. The main uses of 252Cf in 1982 were, reactor start-up (48.3%), fuel rod scanning (25.3%), and activation analysis (19.4%). By 1994, most 252Cf was used in neutron radiography (77.4%), with fuel rod scanning (12.1%) and reactor start-up (6.9%) as important but secondary uses. In 2021, fast neutrons from 252Cf were used for wireless data transmission.\n251Cf has a very small calculated critical mass of about , high lethality, and a relatively short period of toxic environmental irradiation. The low critical mass of californium led to some exaggerated claims about possible uses for the element.\nIn October 2006, researchers announced that three atoms of oganesson (element 118) had been identified at Joint Institute for Nuclear Research in Dubna, Russia, from bombarding 249Cf with calcium-48, making it the heaviest element ever made. The target contained about 10\u00a0mg of 249Cf deposited on a titanium foil of 32\u00a0cm2 area. Californium has also been used to produce other transuranium elements; for example, lawrencium was first synthesized in 1961 by bombarding californium with boron nuclei.\nPrecautions.\nCalifornium that bioaccumulates in skeletal tissue releases radiation that disrupts the body's ability to form red blood cells. The element plays no natural biological role in any organism due to its intense radioactivity and low concentration in the environment.\nCalifornium can enter the body from ingesting contaminated food or drinks or by breathing air with suspended particles of the element. Once in the body, only 0.05% of the californium will reach the bloodstream. About 65% of that californium will be deposited in the skeleton, 25% in the liver, and the rest in other organs, or excreted, mainly in urine. Half of the californium deposited in the skeleton and liver are gone in 50 and 20 years, respectively. Californium in the skeleton adheres to bone surfaces before slowly migrating throughout the bone.\nThe element is most dangerous if taken into the body. In addition, californium-249 and californium-251 can cause tissue damage externally, through gamma ray emission. Ionizing radiation emitted by californium on bone and in the liver can cause cancer.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nExternal links.\n Media related to at Wikimedia Commons"}
{"id": "5677", "revid": "212624", "url": "https://en.wikipedia.org/wiki?curid=5677", "title": "Cerebral Spinal Fluid", "text": ""}
{"id": "5679", "revid": "43296941", "url": "https://en.wikipedia.org/wiki?curid=5679", "title": "Christian Social Union in Bavaria", "text": "Conservative party in Bavaria, Germany\nThe Christian Social Union in Bavaria (German: , CSU) is a Christian democratic and conservative political party in Germany. Having a regionalist identity, the CSU operates only in Bavaria while its larger counterpart, the Christian Democratic Union (CDU), operates in the other fifteen states of Germany. It differs from the CDU by being somewhat more conservative in social matters, following Catholic social teaching. The CSU is considered the \"de facto\" successor of the Weimar-era Catholic Bavarian People's Party.\nAt the federal level, the CSU forms a common faction in the Bundestag with the CDU which is frequently referred to as the Union Faction (\"die Unionsfraktion\") or simply CDU/CSU. The CSU has 45 seats in the Bundestag since the 2021 federal election, making it currently the second smallest of the seven parties represented. The CSU is a member of the European People's Party and the International Democrat Union.\nParty leader Markus S\u00f6der serves as Minister-President of Bavaria, a position that CSU representatives have held from 1946 to 1954 and again since 1957.\nHistory.\nFranz Josef Strau\u00df (1915\u20131988) had left behind the strongest legacy as a leader of the party, having led the party from 1961 until his death in 1988. His political career in the federal cabinet was unique in that he had served in four ministerial posts in the years between 1953 and 1969. From 1978 until his death in 1988, Strau\u00df served as the Minister-President of Bavaria. Strau\u00df was the first leader of the CSU to be a candidate for the German chancellery in 1980. In the 1980 federal election, Strau\u00df ran against the incumbent Helmut Schmidt of the Social Democratic Party of Germany (SPD) but lost thereafter as the SPD and the Free Democratic Party (FDP) managed to secure an absolute majority together, forming a social-liberal coalition.\nThe CSU has led the Bavarian state government since it came into existence in 1946, save from 1954 to 1957 when the SPD formed a state government in coalition with the Bavaria Party and the state branches of the GB/BHE and FDP.\nInitially, the separatist Bavaria Party (BP) successfully competed for the same electorate as the CSU, as both parties saw and presented themselves as successors to the BVP. The CSU was ultimately able to win this power struggle for itself. Among other things, the BP was involved in the \"casino affair\" under dubious circumstances by the CSU at the end of the 1950s and lost considerable prestige and votes. In the 1966 state election, the BP finally left the state parliament.\nBefore the 2008 elections in Bavaria, the CSU perennially achieved absolute majorities at the state level by itself. This level of dominance is unique among Germany's 16 states. Edmund Stoiber took over the CSU leadership in 1999. He ran for Chancellor of Germany in 2002, but his preferred CDU/CSU\u2013FDP coalition lost against the SPD candidate Gerhard Schr\u00f6der's SPD\u2013Green alliance.\nIn the 2003 Bavarian state election, the CSU won 60.7% of the vote and 124 of 180 seats in the state parliament. This was the first time any party had won a two-thirds majority in a German state parliament. \"The Economist\" later suggested that this exceptional result was due to a backlash against Schr\u00f6der's government in Berlin. The CSU's popularity declined in subsequent years. Stoiber stepped down from the posts of Minister-President and CSU chairman in September 2007. A year later, the CSU lost its majority in the 2008 Bavarian state election, with its vote share dropping from 60.7% to 43.4%. The CSU remained in power by forming a coalition with the FDP. In the 2009 general election, the CSU received only 42.5% of the vote in Bavaria in the 2009 election, which by then constituted its weakest showing in the party's history.\nThe CSU made gains in the 2013 Bavarian state election and the 2013 federal election, which were held a week apart in September 2013. The CSU regained their majority in the Bavarian Landtag and remained in government in Berlin. They had three ministers in the Fourth Merkel cabinet, namely Horst Seehofer (Minister of the Interior, Building and Community), Andreas Scheuer (Minister of Transport and Digital Infrastructure) and Gerd M\u00fcller (Minister for Economic Cooperation and Development).\nThe 2018 Bavarian state election yielded the worst result for the CSU in the state elections (top candidate Markus S\u00f6der) since 1950 with 37.2% of votes, a decline of over ten percentage points compared to the last result in 2013. After that, the CSU had to form a new coalition government with the minor partner Free Voters of Bavaria.\nThe 2021 German federal election saw the worst election result ever for the Union. The CSU also had a weak showing with 5.2% of votes nationally and 31.7% of the total in Bavaria.\nRelationship with the CDU.\nThe CSU is the sister party of the Christian Democratic Union (CDU). Together, they are called the Union. The CSU operates only within Bavaria, and the CDU operates in all states other than Bavaria. While virtually independent, at the federal level the parties form a common CDU/CSU faction. No Chancellor has ever come from the CSU, although Strau\u00df and Edmund Stoiber were CDU/CSU candidates for Chancellor in the 1980 federal election and the 2002 federal election, respectively, which were both won by the Social Democratic Party of Germany (SPD). Below the federal level, the parties are entirely independent.\nSince its formation, the CSU has been more conservative than the CDU. CSU and the state of Bavaria decided not to sign the \"Grundgesetz\" of the Federal Republic of Germany as they could not agree with the division of Germany into two states after World War II. Although Bavaria like all German states has a separate police and justice system (distinctive and non-federal), the CSU has actively participated in all political affairs of the German Parliament, the German government, the German Bundesrat, the parliamentary elections of the German President, the European Parliament and meetings with Mikhail Gorbachev in Russia.\nLike the CDU, the CSU is pro-European, although some Eurosceptic tendencies were shown in the past.\nLeaders.\nMinisters-president.\nThe CSU has contributed eleven of the twelve Ministers-President of Bavaria since 1945, with only Wilhelm Hoegner (1945\u20131946, 1954\u20131957) of the SPD also holding the office.\nNotes and references.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5680", "revid": "13560851", "url": "https://en.wikipedia.org/wiki?curid=5680", "title": "CEO", "text": ""}
{"id": "5681", "revid": "11288132", "url": "https://en.wikipedia.org/wiki?curid=5681", "title": "Corporate title", "text": "Titles given in an organization to show what duties and responsibilities a person has\nCorporate titles or business titles are given to corporate officers to show what duties and responsibilities they have in the organization. Such titles are used by publicly and privately held for-profit corporations, cooperatives, non-profit organizations, educational institutions, partnerships, and sole proprietorships also confer corporate titles.\nVariations.\nThere are considerable variations in the composition and responsibilities of corporate title.\nWithin the corporate office or corporate center of a corporation, some corporations have a chairman and chief executive officer (CEO) as the top-ranking executive, while the number two is the president and chief operating officer (COO); other corporations have a president and CEO but no official deputy. Typically, senior managers are \"higher\" than vice presidents, although many times a senior officer may also hold a vice president title, such as executive vice president and chief financial officer (CFO). The board of directors is technically not part of management itself, although its chairman may be considered part of the corporate office if he or she is an executive chairman.\nA corporation often consists of different businesses, whose senior executives report directly to the CEO or COO, but that depends on the form of the business. If organized as a division then the top manager is often known as an executive vice president (EVP). If that business is a subsidiary which has considerably more independence, then the title might be chairman and CEO.\nIn many countries, particularly in Europe and Asia, there is a separate executive board for day-to-day business and supervisory board (elected by shareholders) for control purposes. In these countries, the CEO presides over the executive board and the chairman presides over the supervisory board, and these two roles will always be held by different people. This ensures a distinction between management by the executive board and governance by the supervisory board. This seemingly allows for clear lines of authority. There is a strong parallel here with the structure of government, which tends to separate the political cabinet from the management civil service.\nIn the United States and other countries that follow a single-board corporate structure, the board of directors (elected by the shareholders) is often equivalent to the European or Asian supervisory board, while the functions of the executive board may be vested either in the board of directors or in a separate committee, which may be called an operating committee (J.P. Morgan Chase), management committee (Goldman Sachs), executive committee (Lehman Brothers), executive council (Hewlett-Packard), or executive board (HeiG) composed of the division/subsidiary heads and senior officers that report directly to the CEO.\nUnited States.\nState laws in the United States traditionally required certain positions to be created within every corporation, such as president, secretary and treasurer. Today, the approach under the \"Model Business Corporation Act\", which is employed in many states, is to grant corporations discretion in determining which titles to have, with the only mandated organ being the board of directors.\nSome states that do not employ the MBCA continue to require that certain offices be established. Under the law of Delaware, where most large US corporations are established, stock certificates must be signed by two officers with titles specified by law (e.g. a president and secretary or a president and treasurer). Every corporation incorporated in California must have a chairman of the board or a president (or both), as well as a secretary and a chief financial officer.\nLimited liability company (LLC)-structured companies are generally run directly by their members, but the members can agree to appoint officers such as a CEO or to appoint \"managers\" to operate the company.\nAmerican companies are generally led by a CEO. In some companies, the CEO also has the title of \"president\". In other companies, a president is a different person, and the primary duties of the two positions are defined in the company's bylaws (or the laws of the governing legal jurisdiction). Many companies also have a CFO, a chief operating officer (COO) and other senior positions such as chief legal officer (CLO), chief strategy officer (CSO), chief marketing officer (CMO), etc. that report to the president and CEO. The next level, which are not executive positions, is middle management and may be called \"vice presidents\", \"directors\" or \"managers\", depending on the size and required managerial depth of the company.\nUnited Kingdom.\nIn British English, the title of managing director is generally synonymous with that of chief executive officer. Managing directors do not have any particular authority under the \"Companies Act\" in the UK, but do have implied authority based on the general understanding of what their position entails, as well as any authority expressly delegated by the board of directors.\nJapan and South Korea.\nIn Japan, corporate titles are roughly standardized across companies and organizations; although there is variation from company to company, corporate titles within a company are always consistent, and the large companies in Japan generally follow the same outline. These titles are the formal titles that are used on business cards. Korean corporate titles are similar to those of Japan.\nLegally, Japanese and Korean companies are only required to have a board of directors with at least one representative director. In Japanese, a company director is called a \"torishimariyaku\" (\u53d6\u7de0\u5f79) and a representative director is called a \"daihy\u014d torishimariyaku\" (\u4ee3\u8868\u53d6\u7de0\u5f79). The equivalent Korean titles are \"isa\" (\uc774\uc0ac, \u7406\u4e8b) and \"daepyo-isa\" (\ub300\ud45c\uc774\uc0ac, \u4ee3\u8868\u7406\u4e8b). These titles are often combined with lower titles, e.g. \"senmu torishimariyaku\" or \"j\u014dmu torishimariyaku\" for Japanese executives who are also board members. Most Japanese companies also have statutory auditors, who operate alongside the board of directors in supervisory roles.\nUnder the commercial code in Japan, \"Jugy\u014din\" (\u5f93\u696d\u54e1) meaning the \"employee\", is different from \"Kaishain\" (\u4f1a\u793e\u54e1), meaning the \"stockholders\".\nThe typical structure of executive titles in large companies includes the following:\nThe top management group, comprising \"jomu\"/\"sangmu\" and above, is often referred to collectively as \"cadre\" or \"senior management\" (\u5e79\u90e8 or \u91cd\u5f79; \"kambu\" or \"juyaku\" in Japanese; \"ganbu\" or \"jungy\u014fk\" in Korean).\nSome Japanese and Korean companies have also adopted American-style titles, but these are not yet widespread and their usage varies. For example, although there is a Korean translation for \"chief operating officer\" (\"\ucd5c\uace0\uc6b4\uc601\ucc45\uc784\uc790, choego uny\u014fng chaegimja\"), not many companies have yet adopted it with the exception of a few multi-national companies such as Samsung and CJ (a spin-off from Samsung), while the CFO title is often used alongside other titles such as \"bu-sajang\" (SEVP) or \"J\u014fnmu\" (EVP).\nSince the late 1990s, many Japanese companies have introduced the title of \"shikk\u014d yakuin\" (\u57f7\u884c\u5f79\u54e1) or 'officer', seeking to emulate the separation of directors and officers found in American companies. In 2002, the statutory title of \"shikk\u014d yaku\" (\u57f7\u884c\u5f79) was introduced for use in companies that introduced a three-committee structure in their board of directors. The titles are frequently given to \"buch\u014d\" and higher-level personnel. Although the two titles are very similar in intent and usage, there are several legal distinctions: \"shikk\u014d yaku\" make their own decisions in the course of performing work delegated to them by the board of directors, and are considered managers of the company rather than employees, with a legal status similar to that of directors. \"Shikk\u014d yakuin\" are considered employees of the company that follow the decisions of the board of directors, although in some cases directors may have the \"shikk\u014d yakuin\" title as well.\nSenior management.\nThe highest-level executives in senior management usually have titles beginning with \"chief\" and ending with \"officer\", forming what is often called the \"C-suite\", or \"CxO\", where \"x\" is a variable that could be any functional area (not to be confused with CXO). The traditional three such officers are CEO, COO, and CFO. Depending on the management structure, titles may exist instead of, or be blended/overlapped with, other traditional executive titles, such as \"president\", various designations of \"vice presidents\" (e.g. VP of marketing), and \"general managers\" or \"directors\" of various divisions (such as director of marketing); the latter may or may not imply membership of the \"board of directors\".\nCertain other prominent positions have emerged, some of which are sector-specific. For example, chief audit executive (CAE), chief procurement officer (CPO) and chief risk officer (CRO) positions are often found in many types of financial services companies. Technology companies of all sorts now tend to have a chief technology officer (CTO) to manage technology development. A chief information officer (CIO) oversees information technology (IT) matters, either in companies that specialize in IT or in any kind of company that relies on it for supporting infrastructure.\nMany companies now also have a chief marketing officer (CMO), particularly mature companies in competitive sectors, where brand management is a high priority. A chief value officer (CVO) is introduced in companies where business processes and organizational entities are focused on the creation and maximization of value. Approximately 50% of the S&amp;P 500 companies have created a chief strategy officer (CSO) in their top management team to lead strategic planning and manage inorganic growth, which provides a long range perspective versus the tactical view of the COO or CFO. This function often replaces a COO on the C-Suite team, in cases where the company wants to focus on growth rather than efficiency and cost containment. A chief administrative officer (CAO) may be found in many large complex organizations that have various departments or divisions. Additionally, many companies now call their top diversity leadership position the chief diversity officer (CDO). However, this and many other nontraditional and lower-ranking titles are not universally recognized as corporate officers, and they tend to be specific to particular organizational cultures or the preferences of employees.\nSpecific corporate officer positions.\nChairman of the board \u2013 presiding officer of the corporate board of directors. The chairman influences the board of directors, which in turn elects and removes the officers of a corporation and oversees the human, financial, environmental and technical operations of a corporation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5683", "revid": "8066546", "url": "https://en.wikipedia.org/wiki?curid=5683", "title": "Computer expo", "text": ""}
{"id": "5685", "revid": "1175300", "url": "https://en.wikipedia.org/wiki?curid=5685", "title": "Cambridge, Massachusetts", "text": "Cambridge ( ) is a city in Middlesex County, Massachusetts, in the United States. It is a major suburb in the Greater Boston metropolitan area, located directly across the Charles River from Boston. The city's population as of the 2020 U.S. census was 118,403, making it the largest city in the county, the fourth most populous city in the state, behind Boston, Worcester, and Springfield, and ninth most populous city in New England. It was named in honor of the University of Cambridge in England, which was an important center of the Puritan theology that was embraced by the town's founders.\nCambridge is known globally as home to two of the world's most prestigious universities. Harvard University, an Ivy League university founded in Cambridge in 1636, is the oldest institution of higher learning in the nation and has routinely been ranked as one of the best universities in the world. The Massachusetts Institute of Technology (MIT), founded in 1861, is also located in Cambridge and has been similarly ranked highly among the world's best universities. Lesley University and Hult International Business School also are based in Cambridge. Radcliffe College, an elite women's liberal arts college, also was based in Cambridge from its 1879 founding until its assimiliation into Harvard in 1999.\nKendall Square, near MIT in the eastern part of Cambridge, has been called \"the most innovative square mile on the planet\" due to the high concentration of startup companies that have emerged there since 2010.\nHistory.\nPre-colonization.\nNative Americans inhabited the area that would become Cambridge for thousands of years prior to European colonization of the Americas. At the time of European contact and exploration, the area was inhabited by Naumkeag or Pawtucket to the north and Massachusett to the south, and may have been inhabited by other groups such as the Totant not well described in later European narratives. The contact period introduced a number of European infectious diseases which would decimate native populations in virgin soil epidemics, leaving the area uncontested upon the arrival of large groups of English settlers in 1630.\n17th century and colonialism.\nIn December 1630, the site of present-day Cambridge was chosen for settlement because it was safely upriver from Boston Harbor, making it easily defensible from attacks by enemy ships. The city was founded by Thomas Dudley, his daughter Anne Bradstreet, and his son-in-law Simon Bradstreet. The first houses were built in the spring of 1631. The settlement was initially referred to as \"the newe towne\". Official Massachusetts records show the name rendered as Newe Towne by 1632, and as Newtowne by 1638.\nLocated at the first convenient Charles River crossing west of Boston, Newtowne was one of several towns, including Boston, Dorchester, Watertown, and Weymouth, founded by the 700 original Puritan colonists of the Massachusetts Bay Colony under Governor John Winthrop. Its first preacher was Thomas Hooker, who led many of its original inhabitants west in 1636 to found Hartford and the Connecticut Colony; before leaving, they sold their plots to more recent immigrants from England. The original village site is now within Harvard Square. The marketplace where farmers sold crops from surrounding towns at the edge of a salt marsh (since filled) remains within a small park at the corner of John F. Kennedy and Winthrop Streets.\nIn 1636, Newe College, later renamed Harvard College after benefactor John Harvard, was founded as North America's first institution of higher learning. Its initial purpose was training ministers. According to Cotton Mather, Newtowne was chosen for the site of the college by the Great and General Court, then the legislature of Massachusetts Bay Colony, primarily for its proximity to the popular and highly respected Puritan preacher Thomas Shepard. In May 1638, the settlement's name was changed to Cambridge in honor of the University of Cambridge in Cambridge, England.\nIn 1639, the Massachusetts General Court purchased the land that became present-day Cambridge from the Naumkeag Squaw Sachem of Mistick.\nThe town comprised a much larger area than the present city, with various outlying parts becoming independent towns over the years: Cambridge Village (later Newtown and now Newton) in 1688, Cambridge Farms (now Lexington) in 1712 or 1713, and Little or South Cambridge (now Brighton) and Menotomy or West Cambridge (now Arlington) in 1807. In the late 19th century, various schemes for annexing Cambridge to Boston were pursued and rejected.\nNewtowne's ministers, Hooker and Shepard, the college's first president, the college's major benefactor, and the first schoolmaster Nathaniel Eaton were all Cambridge alumni, as was the colony's governor John Winthrop. In 1629, Winthrop had led the signing of the founding document of the city of Boston, which was known as the Cambridge Agreement, after the university. In 1650, Governor Thomas Dudley signed the charter creating the corporation that still governs Harvard College.\nCambridge grew slowly as an agricultural village by road from Boston, the colony's capital. By the American Revolution, most residents lived near the Common and Harvard College, with most of the town comprising farms and estates. Most inhabitants were descendants of the original Puritan colonists, but there was also a small elite of Anglican \"worthies\" who were not involved in village life, made their livings from estates, investments, and trade, and lived in mansions along \"the Road to Watertown\", present-day Brattle Street, which is still known as Tory Row.\n18th century and Revolutionary War.\nComing south from Virginia, George Washington took command of the force of Patriot soldiers camped on Cambridge Common on July 3, 1775, which is now considered the birthplace of the Continental Army.\nOn January 24, 1776, Henry Knox arrived with an artillery train captured from Fort Ticonderoga, which allowed Washington to force the British Army to evacuate Boston. Most of the Loyalist estates in Cambridge were confiscated after the Revolutionary War.\n19th century and industrialization.\nBetween 1790 and 1840, Cambridge grew rapidly with the construction of West Boston Bridge in 1792 connecting Cambridge directly to Boston, making it no longer necessary to travel through the Boston Neck, Roxbury, and Brookline to cross the Charles River. A second bridge, the Canal Bridge, opened in 1809 alongside the new Middlesex Canal. The new bridges and roads made what were formerly estates and marshland into prime industrial and residential districts.\nIn the mid-19th century, Cambridge was the center of a literary revolution. It was home to some of the famous Fireside poets, named because their poems would often be read aloud by families in front of their evening fires. The Fireside poets, including Henry Wadsworth Longfellow, James Russell Lowell, and Oliver Wendell Holmes, were highly popular and influential in this era.\nSoon after, turnpikes were built: the Cambridge and Concord Turnpike (today's Broadway and Concord Ave.), the Middlesex Turnpike (Hampshire St. and Massachusetts Ave. northwest of Porter Square), and what are today's Cambridge, Main, and Harvard Streets connected various areas of Cambridge to the bridges. In addition, the town was connected to the Boston &amp; Maine Railroad, leading to the development of Porter Square as well as the creation of neighboring Somerville from the formerly rural parts of Charlestown.\nCambridge was incorporated as a city in 1846. The city's commercial center began to shift from Harvard Square to Central Square, which became the city's downtown around that time.\nBetween 1850 and 1900, Cambridge took on much of its present character, featuring streetcar suburban development along the turnpikes and working class and industrial neighborhoods focused on East Cambridge, comfortable middle-class housing on the old Cambridgeport, and Mid-Cambridge estates and upper-class enclaves near Harvard University and on the minor hills. The arrival of the railroad in North Cambridge and Northwest Cambridge led to three major changes: the development of massive brickyards and brickworks between Massachusetts Avenue, Concord Avenue, and Alewife Brook; the ice-cutting industry launched by Frederic Tudor on Fresh Pond; and the carving up of the last estates into residential subdivisions to house the thousands of immigrants who arrived to work in the new industries.\nFor much of the 19th and early 20th centuries, the city's largest employer was the New England Glass Company, founded in 1818. By the middle of the 19th century, it was the world's largest and most modern glassworks. In 1888, Edward Drummond Libbey moved all production to Toledo, Ohio, where it continues today under the name Owens-Illinois. The company's flint glassware with heavy lead content is prized by antique glass collectors, and the Toledo Museum of Art has a large collection. The Museum of Fine Arts in Boston and the Sandwich Glass Museum on Cape Cod also house several pieces.\nIn 1895, Edwin Ginn, founder of Ginn and Company, built the Athenaeum Press Building for his publishing textbook empire.\n20th century.\nBy 1920, Cambridge was one of New England's main industrial cities, with nearly 120,000 residents. Among the largest businesses in Cambridge during the period of industrialization was Carter's Ink Company, whose neon sign long adorned the Charles River and which was for many years the world's largest ink manufacturer. Next door was the Athenaeum Press. Confectionery and snack manufacturers in the Cambridgeport-Area 4-Kendall corridor included Kennedy Biscuit Factory, later part of Nabisco and originator of the Fig Newton, Necco, Squirrel Brands, George Close Company (1861\u20131930s), Page &amp; Shaw, Daggett Chocolate (1892\u20131960s, recipes bought by Necco), Fox Cross Company (1920\u20131980, originator of the Charleston Chew, and now part of Tootsie Roll Industries), Kendall Confectionery Company, and James O. Welch (1927\u20131963, originator of Junior Mints, Sugar Daddies, Sugar Mamas, and Sugar Babies, now part of Tootsie Roll Industries). Main Street was nicknamed \"Confectioner's Row\".\nOnly the Cambridge Brands subsidiary of Tootsie Roll Industries remains in town, still manufacturing Junior Mints in the old Welch factory on Main Street. The Blake and Knowles Steam Pump Company (1886), the Kendall Boiler and Tank Company (1880, now in Chelmsford, Massachusetts), and the New England Glass Company (1818\u20131878) were among the industrial manufacturers in what are now Kendall Square and East Cambridge.\nIn 1935, the Cambridge Housing Authority and the Public Works Administration demolished an integrated low-income tenement neighborhood with African Americans and European immigrants. In its place, it built the whites-only \"Newtowne Court\" public housing development and the adjoining, blacks-only \"Washington Elms\" project in 1940; the city required segregation in its other public housing projects as well.\nAs industry in New England began to decline during the Great Depression and after World War II, Cambridge lost much of its industrial base. It also began to become an intellectual, rather than an industrial, center. Harvard University, which had always been important as both a landowner and an institution, began to play a more dominant role in the city's life and culture. When Radcliffe College was established in 1879, the town became a mecca for some of the nation's most academically talented female students. MIT's move from Boston to Cambridge in 1916 reinforced Cambridge's status as an intellectual center of the United States.\nAfter the 1950s, the city's population began to decline slowly as families tended to be replaced by single people and young couples. In Cambridge Highlands, the technology company Bolt, Beranek, &amp; Newman produced the first network router in 1969 and hosted the invention of computer-to-computer email in 1971. The 1980s brought a wave of high technology startups. Those selling advanced minicomputers were overtaken by the microcomputer. Cambridge-based VisiCorp made the first spreadsheet software for personal computers, Visicalc, and helped propel the Apple II to major consumer success. It was overtaken and purchased by Cambridge-based Lotus Development, maker of Lotus 1-2-3 (which was, in turn, replaced in by Microsoft Excel).\nThe city continues to be home to many startups. Kendall Square was a major software hub through the dot-com boom and today hosts offices of such technology companies as Google, Microsoft, and Amazon. The Square also now houses the headquarters of Akamai.\nIn 1976, Harvard's plans to start experiments with recombinant DNA led to a three-month moratorium and a citizen review panel. In the end, Cambridge decided to allow such experiments but passed safety regulations in 1977. This led to regulatory certainty and acceptance when Biogen opened a lab in 1982, in contrast to the hostility that caused the Genetic Institute, a Harvard spinoff, to abandon Somerville and Boston for Cambridge. The biotech and pharmaceutical industries have since thrived in Cambridge, which now includes headquarters for Biogen and Genzyme; laboratories for Novartis, Teva, Takeda, Alnylam, Ironwood, Catabasis, Moderna Therapeutics, Editas Medicine; support companies such as Cytel; and many smaller companies.\nBy the end of the 20th century, Cambridge had one of the most costly housing markets in the Northeastern United States. While considerable class, race, and age diversity existed, it became more challenging for those who grew up in the city to afford to remain. The end of rent control in 1994 prompted many Cambridge renters to move to more affordable housing in Somerville and other Massachusetts cities and towns.\n21st century.\nCambridge's mix of amenities and proximity to Boston kept housing prices relatively stable despite the bursting of the United States housing bubble in 2008 and 2009. Cambridge has been a sanctuary city since 1985 and reaffirmed its status as such in 2006.\nGeography.\nAccording to the U.S. Census Bureau, Cambridge has a total area of , of which is land and (9.82%) is water.\nAdjacent municipalities.\nCambridge is located in eastern Massachusetts, bordered by:\nThe border between Cambridge and the neighboring city of Somerville passes through densely populated neighborhoods, which are connected by the MBTA Red Line. Some of the main squares, Inman, Porter, and to a lesser extent, Harvard and Lechmere, are very close to the city line, as are Somerville's Union and Davis Squares.\nThrough the City of Cambridge's exclusive municipal water system, the city further controls two exclave areas, one being Payson Park Reservoir and Gatehouse, a 2009 listed American Water Landmark located roughly one mile west of Fresh Pond and surrounded by the town of Belmont. The second area is the larger Hobbs Brook and Stony Brook watersheds, which share borders with neighboring towns and cities including Lexington, Lincoln, Waltham and Weston.\nNeighborhoods.\nSquares.\nCambridge has been called the \"City of Squares\", as most of its commercial districts are major street intersections known as squares. Each square acts as a neighborhood center. These include:\nOther neighborhoods.\nCambridge's residential neighborhoods border but are not defined by the squares.\nThe Avon Hill sub-neighborhood consists of the higher elevations within the area bounded by Upland Road, Raymond Street, Linnaean Street and Massachusetts Avenue.\nClimate.\nIn the K\u00f6ppen-Geiger classification, Cambridge has a hot-summer humid continental climate (Dfa) with hot summers and cold winters, that can appear in the southern end of New England's interior. Abundant rain falls on the city (and in the winter often as snow); it has no dry season. The average January temperature is 26.6\u00a0\u00b0F (\u20133\u00a0\u00b0C), making Cambridge part of Group D, independent of the isotherm. There are four well-defined seasons.\nDemographics.\nAs of the census of 2010, there were 105,162 people, 44,032 households, and 17,420 families residing in the city. The population density was . There were 47,291 housing units at an average density of . The racial makeup of the city was 66.60% White, 11.70% Black or African American, 0.20% Native American, 15.10% Asian (3.7% Chinese, 1.4% Asian Indian, 1.2% Korean, 1.0% Japanese), 0.01% Pacific Islander, 2.10% from other races, and 4.30% from two or more races. 7.60% of the population were Hispanic or Latino of any race (1.6% Puerto Rican, 1.4% Mexican, 0.6% Dominican, 0.5% Colombian &amp; Salvadoran, 0.4% Spaniard). Non-Hispanic Whites were 62.1% of the population in 2010, down from 89.7% in 1970. An individual resident of Cambridge is known as a Cantabrigian.\nIn 2010, there were 44,032 households, out of which 16.9% had children under the age of 18 living with them, 28.9% were married couples living together, 8.4% had a female householder with no husband present, and 60.4% were non-families. 40.7% of all households were made up of individuals, and 9.6% had someone living alone who was 65 years of age or older. The average household size was 2.00 and the average family size was 2.76.\nIn the city, the population was spread out, with 13.3% of the population under the age of 18, 21.2% from 18 to 24, 38.6% from 25 to 44, 17.8% from 45 to 64, and 9.2% who were 65 years of age or older. The median age was 30.5 years. For every 100 females, there were 96.1 males. For every 100 females age 18 and over, there were 94.7 males.\nThe median income for a household in the city was $47,979, and the median income for a family was $59,423 (these figures had risen to $58,457 and $79,533 respectively as of a 2007 estimate[ [update]]). Males had a median income of $43,825 versus $38,489 for females. The per capita income for the city was $31,156. About 8.7% of families and 12.9% of the population were below the poverty line, including 15.1% of those under age 18 and 12.9% of those age 65 or over.\nCambridge has been ranked as one of the most liberal cities in America. Locals living in and near the city jokingly refer to it as \"The People's Republic of Cambridge.\" For 2016, the residential property tax rate in Cambridge was $6.99 per $1,000. Cambridge enjoys the highest possible bond credit rating, AAA, with all three Wall Street rating agencies.\nIn 2000, 11.0% of city residents were of Irish ancestry; 7.2% were of English, 6.9% Italian, 5.5% West Indian and 5.3% German ancestry. 69.4% spoke only English at home, while 6.9% spoke Spanish, 3.2% Chinese or Mandarin, 3.0% Portuguese, 2.9% French Creole, 2.3% French, 1.5% Korean, and 1.0% Italian.\nIncome.\nData is from the 2009\u20132013 American Community Survey 5-Year Estimates.\nEconomy.\nManufacturing was an important part of Cambridge's economy in the late 19th and early 20th century, but educational institutions are its biggest employers today. Harvard and MIT together employ about 20,000. As a cradle of technological innovation, Cambridge was home to technology firms Analog Devices, Akamai, Bolt, Beranek, and Newman (BBN Technologies) (now part of Raytheon), General Radio (later GenRad), Lotus Development Corporation (now part of IBM), Polaroid, Symbolics, and Thinking Machines.\nIn 1996, Polaroid, Arthur D. Little, and Lotus were Cambridge's top employers, with over 1,000 employees, but they faded out a few years later. Health care and biotechnology firms such as Genzyme, Biogen Idec, bluebird bio, Millennium Pharmaceuticals, Sanofi, Pfizer and Novartis have significant presences in the city. Though headquartered in Switzerland, Novartis continues to expand its operations in Cambridge.\nOther major biotech and pharmaceutical firms expanding their presence in Cambridge include GlaxoSmithKline, AstraZeneca, Shire, and Pfizer. Most of Cambridge's biotech firms are in Kendall Square and East Cambridge, which decades ago were the city's center of manufacturing. Some others are in University Park at MIT, a new development in another former manufacturing area.\nNone of the high technology firms that once dominated the economy was among the 25 largest employers in 2005, but by 2008 Akamai and ITA Software were. Google, IBM Research, Microsoft Research, and Philips Research maintain offices in Cambridge. In late January 2012\u2014less than a year after acquiring Billerica-based analytic database management company, Vertica\u2014Hewlett-Packard announced it would also be opening its first offices in Cambridge. Also around that time, e-commerce giants Staples and Amazon.com said they would be opening research and innovation centers in Kendall Square. And LabCentral provides a shared laboratory facility for approximately 25 emerging biotech companies.\nThe proximity of Cambridge's universities has also made the city a center for nonprofit groups and think tanks, including the National Bureau of Economic Research, the Smithsonian Astrophysical Observatory, the Lincoln Institute of Land Policy, Cultural Survival, and One Laptop per Child.\nIn September 2011, Cambridge launched its Entrepreneur Walk of Fame initiative, recognizing people who have made contributions to innovation in global business.\nIn 2021, Cambridge was one of approximately 27 US cities to receive a AAA rating from each of the three major credit rating agencies in the nation, Moody's Investors Service, Standard &amp; Poor's and Fitch Ratings. 2021 marked the 22nd consecutive year that Cambridge had retained this distinction.\nTop employers.\nAs of 2019[ [update]], the city's ten largest employers are:\nArts and culture.\nPublic art.\nCambridge has a large and varied collection of permanent public art, on both city property, managed by the Cambridge Arts Council, and the Harvard and MIT campuses. Temporary public artworks are displayed as part of the annual Cambridge River Festival on the banks of the Charles River during winter celebrations in Harvard and Central Squares and at Harvard University campus sites. Experimental forms of public artistic and cultural expression include the Central Square World's Fair, the annual Somerville-based Honk! Festival, and If This House Could Talk, a neighborhood art and history event.\nStreet musicians and other performers entertain tourists and locals in Harvard Square during the warmer months. The performances are coordinated through a public process that has been developed collaboratively by the performers, city administrators, private organizations and business groups. The Cambridge public library contains four Works Progress Administration murals completed in 1935 by Elizabeth Tracy Montminy: \"Religion\", \"Fine Arts\", \"History of Books and Paper\", and \"The Development of the Printing Press\".\nArchitecture.\nDespite intensive urbanization during the late 19th century and the 20th century, Cambridge has several historic buildings, including some from the 17th century. The city also has abundant contemporary architecture, largely built by Harvard and MIT.\nNotable historic buildings in the city include:\nContemporary architecture:\nMusic.\nThe city has an active music scene, from classical performances to the latest popular bands. Beyond its colleges and universities, Cambridge has many music venues, including The Middle East, Club Passim, The Plough and Stars, The Lizard Lounge and the Nameless Coffeehouse.\nParks and recreation.\nConsisting largely of densely built residential space, Cambridge lacks significant tracts of public parkland. Easily accessible open space on the university campuses, including Harvard Yard, Radcliffe Yard, and MIT's Great Lawn, as well as the considerable open space of Mount Auburn Cemetery and Fresh Pond Reservation, partly compensates for this. At Cambridge's western edge, the cemetery is known as a garden cemetery because of its landscaping (the oldest planned landscape in the country) and arboretum. Although known as a Cambridge landmark, much of the cemetery lies within Watertown. It is also an Important Bird Area (IBA) in the Greater Boston area. Fresh Pond Reservation is the largest open green space in Cambridge with 162 acres (656,000 m2) of land around a 155-acre (627,000 m2) kettle hole lake. This land includes a 2.25-mile walking trail around the reservoir and a public 9-hole golf course.\nPublic parkland includes the esplanade along the Charles River, which mirrors its Boston counterpart, Cambridge Common, Danehy Park, and Alewife Brook Reservation.\nGovernment.\nFederal and state representation.\nCambridge is split between Massachusetts's 5th and 7th U.S. congressional districts. The 5th district seat is held by Democrat Katherine Clark, who replaced now-Senator Ed Markey in a 2013 special election; the 7th is represented by Democrat Ayanna Pressley, elected in 2018. The state's senior United States senator is Democrat Elizabeth Warren, elected in 2012, who lives in Cambridge. The governor of Massachusetts is Democrat Maura Healey, elected in 2022.\nCambridge is represented in six districts in the Massachusetts House of Representatives: the 24th Middlesex (which includes parts of Belmont and Arlington), the 25th and 26th Middlesex (the latter of which includes a portion of Somerville), the 29th Middlesex (which includes a small part of Watertown), and the Eighth and Ninth Suffolk (both including parts of the City of Boston). The city is represented in the Massachusetts Senate as a part of the 2nd Middlesex, Middlesex and Suffolk, and 1st Suffolk and Middlesex districts.\nPolitics.\nFrom 1860 to 1880, Republicans Abraham Lincoln, Ulysses S. Grant, Rutherford B. Hayes, and James Garfield each won Cambridge, Grant doing so by margins of over 20 points in both of his campaigns. Following that, from 1884\u20131892, Grover Cleveland won Cambridge in all three of his presidential campaigns, by less than ten points each time.\nThen from 1896 to 1924, Cambridge became something of a \"swing\" city with a slight Republican lean. GOP nominees carried the city in five of the eight presidential elections during that time frame, with five of the elections resulting in either a plurality or a margin of victory of fewer than ten points.\nThe city of Cambridge is extremely Democratic in modern times, however. In the last 23 presidential elections dating back to the nomination of Al Smith in 1928, the Democratic nominee has carried Cambridge in every election. Every Democratic nominee since Massachusetts native John F. Kennedy in 1960 has received at least 70% of the vote, except for Jimmy Carter in 1976 and 1980. Since 1928, the only Republican nominee to come within ten points of carrying Cambridge is Dwight Eisenhower in his 1956 re-election bid.\nCity government.\nCambridge has a city government led by a mayor and a nine-member city council. There is also a six-member school committee that functions alongside the superintendent of public schools. The councilors and school committee members are elected every two years using proportional representation.\nThe mayor is elected by the city councilors from among themselves and serves as the chair of city council meetings. The mayor also sits on the school committee. The mayor is not the city's chief executive. Rather, the city manager, who is appointed by the city council, serves in that capacity.\nUnder the city's Plan E form of government, the city council does not have the power to appoint or remove city officials who are under the direction of the city manager. The city council and its members are also forbidden from giving orders to any subordinate of the city manager.\nYi-An Huang is the City Manager as of September 6, 2022, succeeding Owen O'Riordan (now the Deputy City Manager) who briefly served as the Acting City Manager after Louis DePasquale resigned on July 5, 2022, after six years in office.\n\"* = current mayor\"\n\"** = former mayor\"\nOn March 8, 2021, Cambridge City Council voted to recognize polyamorous domestic partnerships, becoming the second city in the United States following neighboring Somerville, which had done so in 2020.\nCounty government.\nCambridge was a county seat of Middlesex County, along with Lowell, until the abolition of county government. Though the county government was abolished in 1997, the county still exists as a geographical and political region. The employees of Middlesex County courts, jails, registries, and other county agencies now work directly for the state. The county's registrars of Deeds and Probate remain in Cambridge, but the Superior Court and District Attorney have had their operations transferred to Woburn. Third District Court has shifted operations to Medford, and the county Sheriff's office awaits near-term relocation.\nEducation.\nHigher education.\nCambridge is perhaps best known as an academic and intellectual center. Its colleges and universities include:\nAt least 258 of the world's total 962 Nobel Prize winners have at some point in their careers been affiliated with universities in Cambridge.\nThe American Academy of Arts and Sciences, one of the nation's oldest learned societies founded in 1780, is based in Cambridge.\nPrimary and secondary public education.\nThe city's schools constitute the Cambridge Public School District. Schools include:\nFive upper schools offer grades 6\u20138 in some of the same buildings as the elementary schools:\nCambridge has three district public high school programs, including Cambridge Rindge and Latin School (CRLS).\nOther public charter schools include Benjamin Banneker Charter School, which serves grades K\u20136; Community Charter School of Cambridge in Kendall Square, which serves grades 7\u201312; and Prospect Hill Academy, a charter school whose upper school is in Central Square though it is not a part of the Cambridge Public School District.\nPrimary and secondary private education.\nCambridge also has several private schools, including:\nMedia.\nNewspapers.\nCambridge is served by a single online newspaper, Cambridge Day. The last physical newspaper in the city, \"Cambridge Chronicle\", ceased publication in 2022 and today only cross-posts regional stories from other Gannett properties.\nRadio.\nCambridge is home to the following commercially licensed and student-run radio stations:\nTelevision and broadband.\nCambridge Community Television (CCTV) has served the city since its inception in 1988. CCTV operates Cambridge's public access television facility and three television channels, 8, 9, and 96, on the Cambridge cable system (Comcast). The city has invited tenders from other cable providers, but Comcast remains its only fixed television and broadband utility, though services from American satellite TV providers are available. In October 2014, Cambridge City Manager Richard Rossi appointed a citizen Broadband Task Force to \"examine options to increase competition, reduce pricing, and improve speed, reliability and customer service for both residents and businesses.\"\nInfrastructure.\nWater department.\nCambridge obtains water from Hobbs Brook (in Lincoln and Waltham) and Stony Brook (Waltham and Weston), as well as an emergency connection to the Massachusetts Water Resources Authority. The city owns over of land in other towns that includes these reservoirs and portions of their watershed. Water from these reservoirs flows by gravity through an aqueduct to Fresh Pond in Cambridge. It is then treated in an adjacent plant and pumped uphill to an elevation of above sea level at the Payson Park Reservoir (Belmont). The water is then redistributed downhill via gravity to individual users in the city. A new water treatment plant opened in 2001.\nIn October 2016, the city announced that, owing to drought conditions, they would begin buying water from the MWRA. On January 3, 2017, Cambridge announced that \"As a result of continued rainfall each month since October 2016, we have been able to significantly reduce the need to use MWRA water. We have not purchased any MWRA water since December 12, 2016 and if 'average' rainfall continues this could continue for several months.\"\nTransportation.\nRoad.\nSeveral major roads lead to Cambridge, including Route 2, Route 16, and the McGrath Highway (Route 28). The Massachusetts Turnpike does not pass through Cambridge but provides access by an exit in nearby Allston. Both U.S. Route 1 and Interstate 93 also provide additional access on the eastern end of Cambridge at Leverett Circle in Boston. Route 2A runs the length of the city, chiefly along Massachusetts Avenue. The Charles River forms the southern border of Cambridge and is crossed by 11 bridges connecting Cambridge to Boston, including the Longfellow Bridge and the Harvard Bridge, eight of which are open to motorized road traffic.\nCambridge has an irregular street network because many of the roads date from the colonial era. Contrary to popular belief, the road system did not evolve from longstanding cow-paths. Roads connected various village settlements with each other and nearby towns and were shaped by geographic features, most notably streams, hills, and swampy areas. Today, the major \"squares\" are typically connected by long, mostly straight roads, such as Massachusetts Avenue between Harvard Square and Central Square or Hampshire Street between Kendall Square and Inman Square.\nMass transit.\nCambridge is served by the Massachusetts Bay Transportation Authority, including Porter station on the regional Commuter Rail, Lechmere station on the Green Line, and Alewife, Porter, Harvard, Central, and Kendall Square/MIT stations on the Red Line. Alewife station, the terminus of the Red Line, has a large multi-story parking garage at a rate of $7 per day (as of 2015[ [update]]).\nThe Harvard bus tunnel under Harvard Square connects to the Red Line underground. This tunnel was originally opened for streetcars in 1912 and served trackless trolleys, trolleybuses, and buses as the routes were converted; four lines of the MBTA trolleybus system continued to use it until their conversion to diesel in 2022. The tunnel was partially reconfigured when the Red Line was extended to Alewife in the early 1980s.\nBoth Union Square station in Somerville on the Green Line and Community College station in Charlestown on the Orange Line are located just outside of Cambridge.\nBesides the state-owned transit agency, the city is also served by the Charles River Transportation Management Agency (CRTMA) shuttles which are supported by some of the largest companies operating in the city, in addition to the municipal government itself.\nCycling.\nCambridge has several bike paths, including one along the Charles River, and the Linear Park connecting the Minuteman Bikeway at Alewife with the Somerville Community Path. A connection to Watertown opened in 2022. Bike parking is common and there are bike lanes on many streets, although concerns have been expressed regarding the suitability of many of the lanes. On several central MIT streets, bike lanes transfer onto the sidewalk. Cambridge bans cycling on certain sections of sidewalk where pedestrian traffic is heavy.\n\"Bicycling Magazine\" in 2006 rated Boston as one of the worst cities in the nation for bicycling, but it has given Cambridge honorable mention as one of the best and was called \"Boston's great hope\" by the magazine. Boston has since then followed the example of Cambridge and made considerable efforts to improve bicycling safety and convenience.\nWalking.\nWalking is a popular activity in Cambridge. In 2000, among U.S. cities with more than 100,000 residents, Cambridge had the highest percentage of commuters who walked to work. Cambridge's major historic squares have changed into modern walking neighborhoods, including traffic calming features based on the needs of pedestrians rather than of motorists.\nIntercity.\nThe Boston intercity bus and train stations at South Station in Boston, and Logan International Airport in East Boston, both of which are accessible by subway. The Fitchburg Line rail service from Porter Square connects to some western suburbs. Since October 2010, there has also been intercity bus service between Alewife Station (Cambridge) and New York City.\nPolice department.\nIn addition to the Cambridge Police Department, the city is patrolled by the Fifth (Brighton) Barracks of Troop H of the Massachusetts State Police. Owing, however, to proximity, the city also practices functional cooperation with the Fourth (Boston) Barracks of Troop H, as well. The campuses of Harvard and MIT are patrolled by the Harvard University Police Department and MIT Police Department, respectively.\nFire department.\nThe city of Cambridge is protected by the Cambridge Fire Department. Established in 1832, the CFD operates eight engine companies, four ladder companies, one rescue company, and two paramedic squad companies from eight fire stations located throughout the city. The Acting Chief is Thomas F. Cahill, Jr.\nEmergency medical services (EMS).\nThe city of Cambridge receives emergency medical services from PRO EMS, a privately contracted ambulance service.\nPublic library services.\nFurther educational services are provided at the Cambridge Public Library. The large modern main building was built in 2009, and connects to the restored 1888 Richardson Romanesque building. It was founded as the private Cambridge Athenaeum in 1849 and was acquired by the city in 1858, and became the Dana Library. The 1888 building was a donation of Frederick H. Rindge.\nSister cities and twin towns.\nCambridge's sister cities with active relationships are:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nCambridge has ten additional inactive sister city relationships:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5686", "revid": "6056090", "url": "https://en.wikipedia.org/wiki?curid=5686", "title": "Cambridge (disambiguation)", "text": "Cambridge is a city and the county town of Cambridgeshire, United Kingdom, famous for being the location of the University of Cambridge.\nCambridge may also refer to:\n&lt;templatestyles src=\"Template:TOC_right/styles.css\" /&gt;\nOther uses.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5687", "revid": "9784415", "url": "https://en.wikipedia.org/wiki?curid=5687", "title": "Cambridge University", "text": ""}
{"id": "5688", "revid": "3556578", "url": "https://en.wikipedia.org/wiki?curid=5688", "title": "Colin Dexter", "text": "English writer (1930\u20132017)\nNorman Colin Dexter (29 September 1930 \u2013 21 March 2017) was an English crime writer known for his \"Inspector Morse\" series of novels, which were written between 1975 and 1999 and adapted as an ITV television series, \"Inspector Morse\", from 1987 to 2000. His characters have spawned a sequel series, \"Lewis\" from 2006 to 2015, and a prequel series, \"Endeavour\" from 2012 to 2023.\nEarly life and career.\nDexter was born in Stamford, Lincolnshire, to Alfred and Dorothy Dexter. He had an elder brother, John, a fellow classicist, who taught Classics at The King's School, Peterborough, and a sister, Avril. Alfred ran a small garage and taxi company from premises in Scotgate, Stamford. Dexter was educated at St John's Infants School and Bluecoat Junior School, from which he gained a scholarship to Stamford School, a boys' grammar school, where a younger contemporary was England cricket captain and England rugby player M. J. K. Smith.\nAfter leaving school, Dexter completed his national service with the Royal Corps of Signals and then read Classics at Christ's College, Cambridge, graduating in 1953 and receiving a master's degree in 1958.\nIn 1954, Dexter began his teaching career as assistant Classics master at Wyggeston Grammar School for Boys in Leicester. There he helped the school's Christian Union. However, in 2000 he stated that he shared the same views on politics and religion as Inspector Morse, who was portrayed in the final Morse novel, \"The Remorseful Day\", as an atheist. A post at Loughborough Grammar School followed in 1957, then he took up the position of senior Classics teacher at Corby Grammar School, Northamptonshire, in 1959.\nIn 1966, he was forced by the onset of deafness to retire from teaching and took up the post of senior assistant secretary at the University of Oxford Delegacy of Local Examinations (UODLE) in Oxford, a job he held until his retirement in 1988.\nIn November 2008, Dexter featured prominently in the BBC Four programme \"How to Solve a Cryptic Crossword\" as part of the \"Timeshift\" series, in which he recounted some of the crossword clues solved by Morse.\nWriting career.\nThe initial books written by Dexter were general studies textbooks. He began writing mysteries in 1972 during a family holiday. \"Last Bus to Woodstock\" was published in 1975 and introduced the character of Inspector Morse, the irascible detective whose penchants for cryptic crosswords, English literature, cask ale, and music by Wagner reflected Dexter's own enthusiasms. Dexter's plots used false leads and other red herrings, \"presenting Morse, and his readers, with fiendishly difficult puzzles to solve\".\nThe success of the 33 two-hour episodes of the ITV television series \"Inspector Morse\", produced between 1987 and 2000, brought further attention to Dexter's writings. The show featured Inspector Morse, played by John Thaw, and his assistant Sergeant Robert Lewis, played by Kevin Whately. In the manner of Alfred Hitchcock, Dexter made a cameo appearance in almost all episodes.\nFrom 2006 to 2015, Morse's assistant Lewis was featured in a 33-episode ITV series titled \"Lewis\" (\"Inspector Lewis\" in the United States). Lewis is assisted by DS James Hathaway, played by Laurence Fox. A prequel series, \"Endeavour\", features a young Morse and stars Shaun Evans and Roger Allam. \"Endeavour\" was first broadcast on the ITV network in 2012, ending with the ninth series in 2023, taking young Morse's career into 1972. Dexter was a consultant for \"Lewis\" and the first few years of \"Endeavour\". As with \"Morse\", Dexter occasionally made cameo appearances in both \"Lewis\" and \"Endeavour\".\nAlthough Dexter's military service was as a Morse code operator in the Royal Corps of Signals, the character was named after his friend Sir Jeremy Morse, a crossword devotee like Dexter. The music for the television series, written by Barrington Pheloung, used a motif based on the Morse code for Morse's name.\nAwards and honours.\nDexter received several Crime Writers' Association awards: two Silver Daggers for \"Service of All the Dead\" in 1979 and \"The Dead of Jericho\" in 1981; two Gold Daggers for \"The Wench is Dead\" in 1989 and \"The Way Through the Woods\" in 1992; and a Cartier Diamond Dagger for lifetime achievement in 1997. In 1996, Dexter received a Macavity Award for his short story \"Evans Tries an O-Level\". In 1980, he was elected a member of the by-invitation-only Detection Club. In 2005 Dexter became a Fellow by Special Election of St Cross College, Oxford.\nIn the 2000 Birthday Honours Dexter was appointed an Officer of the Order of the British Empire for services to literature. In 2001 he was awarded the Freedom of the City of Oxford. In September 2011, the University of Lincoln awarded Dexter an honorary Doctor of Letters degree.\nPersonal life.\nIn 1956 he married Dorothy Cooper. They had a daughter, Sally, and a son, Jeremy.\nDeath.\nOn 21 March 2017 Dexter's publisher, Macmillan, said in a statement \"With immense sadness, Macmillan announces the death of Colin Dexter who died peacefully at his home in Oxford this morning.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5689", "revid": "1160828252", "url": "https://en.wikipedia.org/wiki?curid=5689", "title": "College", "text": "Higher education institution\n \nA college (Latin: \"collegium\") is an educational institution or a constituent part of one. A college may be a degree-awarding tertiary educational institution, a part of a collegiate or federal university, an institution offering vocational education, or a secondary school. \nIn most of the world, a college may be a high school or secondary school, a college of further education, a training institution that awards trade qualifications, a higher-education provider that does not have university status (often without its own degree-awarding powers), or a constituent part of a university. In the United States, a college may offer undergraduate programs \u2013 either as an independent institution or as the undergraduate program of a university \u2013 or it may be a residential college of a university or a community college, referring to (primarily public) higher education institutions that aim to provide affordable and accessible education, usually limited to two-year associate degrees. The word is generally also used as a synonym for a university in the US. Colleges in countries such as France, Belgium, and Switzerland provide secondary education.\nEtymology.\nThe word \"college\" is from the Latin verb \"lego, legere, legi, lectum\", \"to collect, gather together, pick\", plus the preposition \"cum\", \"with\", thus meaning \"selected together\". Thus \"colleagues\" are literally \"persons who have been selected to work together\". In ancient Rome a \"collegium\" was a \"body, guild, corporation united in colleagueship; of magistrates, praetors, tribunes, priests, augurs; a political club or trade guild\". Thus a college was a form of corporation or corporate body, an artificial legal person (body/corpus) with its own legal personality, with the capacity to enter into legal contracts, to sue and be sued. In mediaeval England there were colleges of priests, for example in chantry chapels; modern survivals include the Royal College of Surgeons in England (originally the Guild of Surgeons Within the City of London), the College of Arms in London (a body of heralds enforcing heraldic law), an electoral college (to elect representatives), etc., all groups of persons \"selected in common\" to perform a specified function and appointed by a monarch, founder or other person in authority. As for the modern \"college of education\", it was a body created for that purpose, for example Eton College was founded in 1440 by letters patent of King Henry VI for the constitution of a college \"of Fellows, priests, clerks, choristers, poor scholars, and old poor men, with one master or governor\", whose duty it shall be to instruct these scholars and any others who may resort thither from any part of England in the knowledge of letters, and especially of grammar, without payment\".\nOverview.\nHigher education.\nWithin higher education, the term can be used to refer to: \nFurther education.\nA sixth form college or college of further education is an educational institution in England, Wales, Northern Ireland, Belize, the Caribbean, Malta, Norway, Brunei, or Southern Africa, among others, where students aged 16 to 19 typically study for advanced school-level qualifications, such as A-levels, BTEC, HND or its equivalent and the International Baccalaureate Diploma, or school-level qualifications such as GCSEs. In Singapore and India, this is known as a junior college. The municipal government of the city of Paris uses the phrase \"sixth form college\" as the English name for a lyc\u00e9e.\nSecondary education.\nIn some national education systems, secondary schools may be called \"colleges\" or have \"college\" as part of their title.\nIn Australia the term \"college\" is applied to any private or independent (non-government) primary and, especially, secondary school as distinct from a state school. Melbourne Grammar School, Cranbrook School, Sydney and The King's School, Parramatta are considered colleges.\nThere has also been a recent trend to rename or create government secondary schools as \"colleges\". In the state of Victoria, some state high schools are referred to as \"secondary colleges\", although the pre-eminent government secondary school for boys in Melbourne is still named Melbourne High School. In Western Australia, South Australia and the Northern Territory, \"college\" is used in the name of all state high schools built since the late 1990s, and also some older ones. In New South Wales, some high schools, especially multi-campus schools resulting from mergers, are known as \"secondary colleges\". In Queensland some newer schools which accept primary and high school students are styled \"state college\", but state schools offering only secondary education are called \"State High School\". In Tasmania and the Australian Capital Territory, \"college\" refers to the final two years of high school (years 11 and 12), and the institutions which provide this. In this context, \"college\" is a system independent of the other years of high school. Here, the expression is a shorter version of \"matriculation college\".\nIn a number of Canadian cities, many government-run secondary schools are called \"collegiates\" or \"collegiate institutes\" (C.I.), a complicated form of the word \"college\" which avoids the usual \"post-secondary\" connotation. This is because these secondary schools have traditionally focused on academic, rather than vocational, subjects and ability levels (for example, collegiates offered Latin while vocational schools offered technical courses). Some private secondary schools (such as Upper Canada College, Vancouver College) choose to use the word \"college\" in their names nevertheless. Some secondary schools elsewhere in the country, particularly ones within the separate school system, may also use the word \"college\" or \"collegiate\" in their names.\nIn New Zealand the word \"college\" normally refers to a secondary school for ages 13 to 17 and \"college\" appears as part of the name especially of private or integrated schools. \"Colleges\" most frequently appear in the North Island, whereas \"high schools\" are more common in the South Island.\nIn the Netherlands, \"college\" is equivalent to HBO (Higher professional education). It is oriented towards professional training with clear occupational outlook, unlike universities which are scientifically oriented.\nIn South Africa, some secondary schools, especially private schools on the English public school model, have \"college\" in their title, including six of South Africa's Elite Seven high schools. A typical example of this category would be St John's College.\nPrivate schools that specialize in improving children's marks through intensive focus on examination needs are informally called \"cram-colleges\".\nIn Sri Lanka the word \"college\" (known as \"Vidyalaya\" in \"Sinhala\") normally refers to a secondary school, which usually signifies above the 5th standard. During the British colonial period a limited number of exclusive secondary schools were established based on English public school model (Royal College Colombo, S. Thomas' College, Mount Lavinia, Trinity College, Kandy) these along with several Catholic schools (St. Joseph's College, Colombo, St Anthony's College) traditionally carry their name as colleges. Following the start of free education in 1931 large group of central colleges were established to educate the rural masses. Since Sri Lanka gained Independence in 1948, many schools that have been established have been named as \"college\".\nOther.\nAs well as an educational institution, the term, in accordance with its etymology, may also refer to any formal group of colleagues set up under statute or regulation; often under a Royal Charter. Examples include an electoral college, the College of Arms, a college of canons, and the College of Cardinals. Other collegiate bodies include professional associations, particularly in medicine and allied professions. In the UK these include the Royal College of Nursing and the Royal College of Physicians. Examples in the United States include the American College of Physicians, the American College of Surgeons, and the American College of Dentists. An example in Australia is the Royal Australian College of General Practitioners.\nCollege by country.\nThe different ways in which the term \"College\" is used to describe educational institutions in various regions of the world is listed below: \nAmericas.\nCanada.\nIn Canadian English, the term \"college\" usually refers to a trades school, applied arts/science/technology/business/health school or community college. These are post-secondary institutions granting certificates, diplomas, associate degrees and (in some cases) bachelor's degrees. The French acronym specific to public institutions within Quebec's particular system of pre-university and technical education is CEGEP (\"Coll\u00e8ge d'enseignement g\u00e9n\u00e9ral et professionnel\", \"college of general and professional education\"). They are collegiate-level institutions that a student typically enrols in if they wish to continue onto university in the Quebec education system, or to learn a trade. In Ontario and Alberta, there are also institutions that are designated university colleges, which only grant undergraduate degrees. This is to differentiate between universities, which have both undergraduate and graduate programs and those that do not.\nIn Canada, there is a strong distinction between \"college\" and \"university\". In conversation, one specifically would say either \"they are going to university\" (i.e., studying for a three- or four-year degree at a university) or \"they are going to college\" (i.e., studying at a technical/career training).\nUsage in a university setting.\nThe term \"college\" also applies to distinct entities that formally act as an affiliated institution of the university, formally referred to as federated college, or affiliated colleges. A university may also formally include several constituent colleges, forming a collegiate university. Examples of collegiate universities in Canada include Trent University, and the University of Toronto. These types of institutions act independently, maintaining their own endowments, and properties. However, they remain either affiliated, or federated with the overarching university, with the overarching university being the institution that formally grants the degrees. For example, Trinity College was once an independent institution, but later became federated with the University of Toronto. Several centralized universities in Canada have mimicked the collegiate university model; although constituent colleges in a centralized university remains under the authority of the central administration. Centralized universities that have adopted the collegiate model to a degree includes the University of British Columbia, with Green College and St. John's College; and the Memorial University of Newfoundland, with Sir Wilfred Grenfell College.\nOccasionally, \"college\" refers to a subject specific faculty within a university that, while distinct, are neither \"federated\" nor \"affiliated\"\u2014College of Education, College of Medicine, College of Dentistry, College of Biological Science among others.\nThe Royal Military College of Canada is a military college which trains officers for the Canadian Armed Forces. The institution is a full-fledged university, with the authority to issue graduate degrees, although it continues to word the term \"college\" in its name. The institution's sister schools, Royal Military College Saint-Jean also uses the term college in its name, although it academic offering is akin to a CEGEP institution in Quebec. A number of post-secondary art schools in Canada formerly used the word \"college\" in their names, despite formally being universities. However, most of these institutions were renamed, or re-branded in the early 21st century, omitting the word \"college\" from its name.\nUsage in secondary education.\nThe word \"college\" continues to be used in the names public separate secondary schools in Ontario. A number of independent schools across Canada also use the word \"college\" in its name.\nPublic secular school boards in Ontario also refer to their secondary schools as \"collegiate institutes\". However, usage of the word \"collegiate institute\" varies between school boards. \"Collegiate institute\" is the predominant name for secondary schools in Lakehead District School Board, and Toronto District School Board, although most school boards in Ontario use \"collegiate institute\" alongside \"high school\", and \"secondary school\" in the names of their institutions. Similarly, secondary schools in Regina, and Saskatoon are referred to as \"Collegiate\".\nChile.\nIn Chile, the term \"college\" is usually used in the name of some bilingual schools, like Santiago College, Saint George's College etc.\nSince 2009 the Pontifical Catholic University of Chile incorporated college as a bachelor's degree, it has a Bachelor of Natural Sciences and Mathematics, a Bachelor of Social Science and a Bachelor of Arts and Humanities. It has the same system as the American universities, it combines majors and minors. And it let the students continue a higher degree in the same university once finished.\nUnited States.\nIn the United States, there were 5,916 post-secondary institutions (universities and colleges) as of 2020\u201321,[ [update]] having peaked at 7,253 in 2012\u201313 and fallen every year since. A \"college\" in the US can refer to a constituent part of a university (which can be a residential college, the sub-division of the university offering undergraduate courses, or a school of the university offering particular specialized courses), an independent institution offering bachelor's-level courses, or an institution offering instruction in a particular professional, technical or vocational field. In popular usage, the word \"college\" is the generic term for any post-secondary undergraduate education. Americans \"go to college\" after high school, regardless of whether the specific institution is formally a college or a university. Some students choose to dual-enroll, by taking college classes while still in high school. The word and its derivatives are the standard terms used to describe the institutions and experiences associated with American post-secondary undergraduate education.\nStudents must pay for college before taking classes. Some borrow the money via loans, and some students fund their educations with cash, scholarships, grants, or some combination of these payment methods. In 2011, the state or federal government subsidized $8,000 to $100,000 for each undergraduate degree. For state-owned schools (called \"public\" universities), the subsidy was given to the college, with the student benefiting from lower tuition. The state subsidized on average 50% of public university tuition.\nColleges vary in terms of size, degree, and length of stay. Two-year colleges, also known as junior or community colleges, usually offer an associate degree, and four-year colleges usually offer a bachelor's degree. Often, these are entirely undergraduate institutions, although some have graduate school programs.\nFour-year institutions in the U.S. that emphasize a liberal arts curriculum are known as liberal arts colleges. Until the 20th century, liberal arts, law, medicine, theology, and divinity were about the only form of higher education available in the United States. These schools have traditionally emphasized instruction at the undergraduate level, although advanced research may still occur at these institutions.\nWhile there is no national standard in the United States, the term \"university\" primarily designates institutions that provide undergraduate and graduate education. A university typically has as its core and its largest internal division an undergraduate college teaching a liberal arts curriculum, also culminating in a bachelor's degree. What often distinguishes a university is having, in addition, one or more graduate schools engaged in both teaching graduate classes and in research. Often these would be called a School of Law or School of Medicine, (but may also be called a college of law, or a faculty of law). An exception is Vincennes University, Indiana, which is styled and chartered as a \"university\" even though almost all of its academic programs lead only to two-year associate degrees. Some institutions, such as Dartmouth College and The College of William &amp; Mary, have retained the term \"college\" in their names for historical reasons. In one unique case, Boston College and Boston University, the former located in Chestnut Hill, Massachusetts and the latter located in Boston, Massachusetts, are completely separate institutions.\nUsage of the terms varies among the states. In 1996, for example, Georgia changed all of its four-year institutions previously designated as colleges to universities, and all of its vocational technology schools to technical colleges.\nThe terms \"university\" and \"college\" do not exhaust all possible titles for an American institution of higher education. Other options include \"institute\" (Worcester Polytechnic Institute and Massachusetts Institute of Technology), \"academy\" (United States Military Academy), \"union\" (Cooper Union), \"conservatory\" (New England Conservatory), and \"school\" (Juilliard School). In colloquial use, they are still referred to as \"college\" when referring to their undergraduate studies.\nThe term \"college\" is also, as in the United Kingdom, used for a constituent semi-autonomous part of a larger university but generally organized on academic rather than residential lines. For example, at many institutions, the undergraduate portion of the university can be briefly referred to as the college (such as The College of the University of Chicago, Harvard College at Harvard, or Columbia College at Columbia) while at others, such as the University of California, Berkeley, \"colleges\" are collections of academic programs and other units that share some common characteristics, mission, or disciplinary focus (the \"college of engineering\", the \"college of nursing\", and so forth). There exist other variants for historical reasons, including some uses that exist because of mergers and acquisitions; for example, Duke University, which was called Trinity College until the 1920s, still calls its main undergraduate subdivision Trinity College of Arts and Sciences.\nResidential colleges.\nSome American universities, such as Princeton, Rice, and Yale have established residential colleges (sometimes, as at Harvard, the first to establish such a system in the 1930s, known as houses) along the lines of Oxford or Cambridge. Unlike the Oxbridge colleges, but similarly to Durham, these residential colleges are not autonomous legal entities nor are they typically much involved in education itself, being primarily concerned with room, board, and social life. At the University of Michigan, University of California, San Diego and the University of California, Santa Cruz, each residential college teaches its own core writing courses and has its own distinctive set of graduation requirements.\nMany U.S. universities have placed increased emphasis on their residential colleges in recent years. This is exemplified by the creation of new colleges at Ivy League schools such as Yale University and Princeton University, and efforts to strengthen the contribution of the residential colleges to student education, including through a 2016 taskforce at Princeton on residential colleges.\nOrigin of the U.S. usage.\nThe founders of the first institutions of higher education in the United States were graduates of the University of Oxford and the University of Cambridge. The small institutions they founded would not have seemed to them like universities \u2013 they were tiny and did not offer the higher degrees in medicine and theology. Furthermore, they were not composed of several small colleges. Instead, the new institutions felt like the Oxford and Cambridge colleges they were used to \u2013 small communities, housing and feeding their students, with instruction from residential tutors (as in the United Kingdom, described above). When the first students graduated, these \"colleges\" assumed the right to confer degrees upon them, usually with authority\u2014for example, The College of William &amp; Mary has a royal charter from the British monarchy allowing it to confer degrees while Dartmouth College has a charter permitting it to award degrees \"as are usually granted in either of the universities, or any other college in our realm of Great Britain.\"\nThe leaders of Harvard College (which granted America's first degrees in 1642) might have thought of their college as the first of many residential colleges that would grow up into a New Cambridge university. However, over time, few new colleges were founded there, and Harvard grew and added higher faculties. Eventually, it changed its title to university, but the term \"college\" had stuck and \"colleges\" have arisen across the United States.\nIn U.S. usage, the word \"college\" not only embodies a particular type of school, but has historically been used to refer to the general concept of higher education when it is not necessary to specify a school, as in \"going to college\" or \"college savings accounts\" offered by banks.\nIn a survey of more than 2,000 college students in 33 states and 156 different campuses, the U.S. Public Interest Research Group found the average student spends as much as $1,200 each year on textbooks and supplies alone. By comparison, the group says that's the equivalent of 39 percent of tuition and fees at a community college, and 14 percent of tuition and fees at a four-year public university.\nMorrill Land-Grant Act.\nIn addition to private colleges and universities, the U.S. also has a system of government funded, public universities. Many were founded under the Morrill Land-Grant Colleges Act of 1862. A movement had arisen to bring a form of more practical higher education to the masses, as \"...many politicians and educators wanted to make it possible for all young Americans to receive some sort of advanced education.\" The Morrill Act \"...made it possible for the new western states to establish colleges for the citizens.\" Its goal was to make higher education more easily accessible to the citizenry of the country, specifically to improve agricultural systems by providing training and scholarship in the production and sales of agricultural products, and to provide formal education in \"...agriculture, home economics, mechanical arts, and other professions that seemed practical at the time.\"\nThe act was eventually extended to allow all states that had remained with the Union during the American Civil War, and eventually all states, to establish such institutions. Most of the colleges established under the Morrill Act have since become full universities, and some are among the elite of the world.\nBenefits of college.\nSelection of a four-year college as compared to a two-year junior college, even by marginal students such as those with a C+ grade average in high school and SAT scores in the mid 800s, increases the probability of graduation and confers substantial economic and social benefits.\nAsia.\nBangladesh.\nIn Bangladesh, educational institutions offering higher secondary (11th\u201312th grade) education are known as colleges.\nHong Kong.\nIn Hong Kong, the term 'college' is used by tertiary institutions as either part of their names or to refer to a constituent part of the university, such as the colleges in the collegiate The Chinese University of Hong Kong; or to a residence hall of a university, such as St. John's College, University of Hong Kong. Many older secondary schools have the term 'college' as part of their names.\nIndia.\nThe modern system of education was heavily influenced by the British starting in 1835.\nIn India, the term \"college\" is commonly reserved for institutions that offer high school diplomas at year 12 (\"Junior College\", similar to American \"high schools\"), and those that offer the bachelor's degree; some colleges, however, offer programmes up to PhD level. Generally, colleges are located in different parts of a state and all of them are affiliated to a regional university. The colleges offer programmes leading to degrees of that university. Colleges may be either Autonomous or non-autonomous. Autonomous Colleges are empowered to establish their own syllabus, and conduct and assess their own examinations; in non-autonomous colleges, examinations are conducted by the university, at the same time for all colleges under its affiliation. There are several hundred universities and each university has affiliated colleges, often a large number.\nThe first liberal arts and sciences college in India was \"Cottayam College\" or the \"Syrian College\", Kerala in 1815. The First inter linguistic residential education institution in Asia was started at this college. At present it is a Theological seminary which is popularly known as Orthodox Theological Seminary or Old Seminary. After that, CMS College, Kottayam, established in 1817, and the Presidency College, Kolkata, also 1817, initially known as Hindu College. The first college for the study of Christian theology and ecumenical enquiry was Serampore College (1818). The first Missionary institution to impart Western style education in India was the Scottish Church College, Calcutta (1830). The first commerce and economics college in India was Sydenham College, Mumbai (1913).\nIn India a new term has been introduced that is Autonomous Institutes &amp; Colleges. An autonomous Colleges are colleges which\u00a0need to be affiliated to a certain university. These colleges can conduct their own admission procedure, examination syllabus, fees structure etc. However, at the end of course completion, they cannot issue their own degree or diploma. The final degree or diploma is issued by the affiliated university. \nAlso, some significant changes can pave way under the NEP (New Education Policy 2020) which may affect the present guidelines for universities and colleges.\nIsrael.\nIn Israel, any non-university higher-learning facility is called a college. Institutions accredited by the Council for Higher Education in Israel (CHE) to confer a bachelor's degree are called \"Academic Colleges\" (; plural ). These colleges (at least 4 for 2012) may also offer master's degrees and act as Research facilities. There are also over twenty teacher training colleges or seminaries, most of which may award only a Bachelor of Education (BEd) degree.\nMacau.\nFollowing the Portuguese usage, the term \"college\" (\"col\u00e9gio\") in Macau has traditionally been used in the names for private (and non-governmental) pre-university educational institutions, which correspond to form one to form six level tiers. Such schools are usually run by the Roman Catholic church or missionaries in Macau. Examples include Chan Sui Ki Perpetual Help College, Yuet Wah College, and Sacred Heart Canossian College.\nPhilippines.\nIn the Philippines, colleges usually refer to institutions of learning that grant degrees but whose scholastic fields are not as diverse as that of a university (University of Santo Tomas, University of the Philippines, Ateneo de Manila University, De La Salle University, Far Eastern University, and AMA University), such as the San Beda College which specializes in law, AMA Computer College whose campuses are spread all over the Philippines which specializes in information and computing technologies, and the Map\u00faa Institute of Technology which specializes in engineering, or to component units within universities that do not grant degrees but rather facilitate the instruction of a particular field, such as a College of Science and College of Engineering, among many other colleges of the University of the Philippines.\nA state college may not have the word \"college\" on its name, but may have several component colleges, or departments. Thus, the Eulogio Amang Rodriguez Institute of Science and Technology is a state college by classification.\nUsually, the term \"college\" is also thought of as a hierarchical demarcation between the term \"university\", and quite a number of colleges seek to be recognized as universities as a sign of improvement in academic standards (Colegio de San Juan de Letran, San Beda College), and increase in the diversity of the offered degree programs (called \"courses\"). For private colleges, this may be done through a survey and evaluation by the Commission on Higher Education and accrediting organizations, as was the case of Urios College which is now the Fr. Saturnino Urios University. For state colleges, it is usually done by a legislation by the Congress or Senate. In common usage, \"going to college\" simply means attending school for an undergraduate degree, whether it's from an institution recognized as a college or a university.\nWhen it comes to referring to the level of education, \"college\" is the term more used to be synonymous to tertiary or higher education. A student who is or has studied his/her undergraduate degree at either an institution with \"college\" or \"university\" in its name is considered to be going to or have gone to \"college\".\nSingapore.\nThe term \"college\" in Singapore is generally only used for pre-university educational institutions called \"Junior Colleges\", which provide the final two years of secondary education (equivalent to sixth form in British terms or grades 11\u201312 in the American system). Since 1 January 2005, the term also refers to the three campuses of the Institute of Technical Education with the introduction of the \"collegiate system\", in which the three institutions are called ITE College East, ITE College Central, and ITE College West respectively.\nThe term \"university\" is used to describe higher-education institutions offering locally conferred degrees. Institutions offering diplomas are called \"polytechnics\", while other institutions are often referred to as \"institutes\" and so forth.\nSri Lanka.\nThere are several professional and vocational institutions that offer post-secondary education without granting degrees that are referred to as \"colleges\". This includes the Sri Lanka Law College, the many Technical Colleges and Teaching Colleges.\nTurkey.\nIn Turkey, the term \"kolej\" (college) refers to a private high school, typically preceded by one year of preparatory language education. Notable Turkish colleges include Robert College, Uskudar American Academy, American Collegiate Institute and Tarsus American College.\nAfrica.\nSouth Africa.\nAlthough the term \"college\" is hardly used in any context at any university in South Africa, some non-university tertiary institutions call themselves colleges. These include teacher training colleges, business colleges and wildlife management colleges. See: List of universities in South Africa#Private colleges and universities; List of post secondary institutions in South Africa.\nZimbabwe.\nThe term college is mainly used by private or independent secondary schools with Advanced Level (Upper 6th formers) and also Polytechnic Colleges which confer diplomas only. A student can complete secondary education (International General Certificate of Secondary Education, IGCSE) at 16 years and proceed straight to a poly-technical college or they can proceed to Advanced level (16 to 19 years) and obtain a General Certificate of Education (GCE) certificate which enables them to enroll at a university, provided they have good grades. Alternatively, with lower grades, the GCE certificate holders will have an added advantage over their GCSE counterparts if they choose to enroll at a polytechnical college. Some schools in Zimbabwe choose to offer the International Baccalaureate studies as an alternative to the IGCSE and GCE.\nEurope.\nGreece.\nKollegio (in Greek \u039a\u03bf\u03bb\u03bb\u03ad\u03b3\u03b9\u03bf) refers to the Centers of Post-Lyceum Education (in Greek \u039a\u03ad\u03bd\u03c4\u03c1\u03bf \u039c\u03b5\u03c4\u03b1\u03bb\u03c5\u03ba\u03b5\u03b9\u03b1\u03ba\u03ae\u03c2 \u0395\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7\u03c2, abbreviated as KEME), which are principally private and belong to the Greek post-secondary education system. Some of them have links to EU or US higher education institutions or accreditation organizations, such as the NEASC. \"Kollegio\" (or \"Kollegia\" in plural) may also refer to private non-tertiary schools, such as the Athens College.\nIreland.\nIn Ireland the term \"college\" is normally used to describe an institution of tertiary education. University students often say they attend \"college\" rather than \"university\". Until 1989, no university provided teaching or research directly; they were formally offered by a constituent college of the university.\nThere are number of secondary education institutions that traditionally used the word \"college\" in their names: these are either older, private schools (such as Belvedere College, Gonzaga College, Castleknock College, and St. Michael's College) or what were formerly a particular kind of secondary school. These secondary schools, formerly known as \"technical colleges,\" were renamed \"community colleges,\" but remain secondary schools.\nThe country's only ancient university is the University of Dublin. Created during the reign of Elizabeth I, it is modelled on the collegiate universities of Cambridge and Oxford. However, only one constituent college was ever founded, hence the curious position of Trinity College Dublin today; although both are usually considered one and the same, the university and college are completely distinct corporate entities with separate and parallel governing structures.\nAmong more modern foundations, the National University of Ireland, founded in 1908, consisted of constituent colleges and recognised colleges until 1997. The former are now referred to as constituent universities \u2013 institutions that are essentially universities in their own right. The National University can trace its existence back to 1850 and the creation of the Queen's University of Ireland and the creation of the Catholic University of Ireland in 1854. From 1880, the degree awarding roles of these two universities was taken over by the Royal University of Ireland, which remained until the creation of the National University in 1908 and Queen's University Belfast.\nThe state's two new universities, Dublin City University and University of Limerick, were initially National Institute for Higher Education institutions. These institutions offered university level academic degrees and research from the start of their existence and were awarded university status in 1989 in recognition of this.\nThird level technical education in the state has been carried out in the Institutes of Technology, which were established from the 1970s as Regional Technical Colleges. These institutions have \"delegated authority\" which entitles them to give degrees and diplomas from Quality and Qualifications Ireland (QQI) in their own names.\nA number of private colleges exist such as Dublin Business School, providing undergraduate and postgraduate courses validated by QQI and in some cases by other universities.\nOther types of college include colleges of education, such as the Church of Ireland College of Education. These are specialist institutions, often linked to a university, which provide both undergraduate and postgraduate academic degrees for people who want to train as teachers.\nA number of state-funded further education colleges exist \u2013 which offer vocational education and training in a range of areas from business studies and information and communications technology to sports injury therapy. These courses are usually one, two or less often three years in duration and are validated by QQI at Levels 5 or 6, or for the BTEC Higher National Diploma award, which is a Level 6/7 qualification, validated by Edexcel. There are numerous private colleges (particularly in Dublin and Limerick) which offer both further and higher education qualifications. These degrees and diplomas are often certified by foreign universities/international awarding bodies and are aligned to the National Framework of Qualifications at Levels 6, 7 and 8.\nNetherlands.\nIn the Netherlands there are 3 main educational routes after high school.\nHBO graduates can be awarded two titles, which are Baccalaureus (bc.) and Ingenieur (ing.). At a WO institution, many more bachelor's and master's titles can be awarded. Bachelor's degrees: Bachelor of Arts (BA), Bachelor of Science (BSc) and Bachelor of Laws (LLB). Master's degrees: Master of Arts (MA), Master of Laws (LLM) and Master of Science (MSc). The PhD title is a research degree awarded upon completion and defense of a doctoral thesis.\nPortugal.\nPresently in Portugal, the term \"col\u00e9gio\" (college) is normally used as a generic reference to a private (non-government) school that provides from basic to secondary education. Many of the private schools include the term \"col\u00e9gio\" in their name. Some special public schools \u2013 usually of the boarding school type \u2013 also include the term in their name, with a notable example being the \"Col\u00e9gio Militar\" (Military College). The term \"col\u00e9gio interno\" (literally \"internal college\") is used specifically as a generic reference to a boarding school.\nUntil the 19th century, a \"col\u00e9gio\" was usually a secondary or pre-university school, of public or religious nature, where the students usually lived together. A model for these colleges was the Royal College of Arts and Humanities, founded in Coimbra by King John III of Portugal in 1542.\nUnited Kingdom.\nSecondary education and further education.\nFurther education (FE) colleges and sixth form colleges are institutions providing further education to students over 16. Some of these also provide higher education courses (see below). In the context of secondary education, 'college' is used in the names of some private schools, e.g. Eton College and Winchester College.\nHigher education.\nIn higher education, a college is normally a provider that does not hold university status, although it can also refer to a constituent part of a collegiate or federal university or a grouping of academic faculties or departments within a university. Traditionally the distinction between colleges and universities was that colleges did not award degrees while universities did, but this is no longer the case with NCG having gained taught degree awarding powers (the same as some universities) on behalf of its colleges, and many of the colleges of the University of London holding full degree awarding powers and being effectively universities. Most colleges, however, do not hold their own degree awarding powers and continue to offer higher education courses that are validated by universities or other institutions that can award degrees.\nIn England, as of August\u00a02016[ [update]], over 60% of the higher education providers directly funded by HEFCE (208/340) are sixth-form or further education colleges, often termed colleges of further and higher education, along with 17 colleges of the University of London, one university college, 100 universities, and 14 other providers (six of which use 'college' in their name). Overall, this means over two-thirds of state-supported higher education providers in England are colleges of one form or another. Many private providers are also called colleges, e.g. the New College of the Humanities and St Patrick's College, London.\nColleges within universities vary immensely in their responsibilities. The large constituent colleges of the University of London are effectively universities in their own right; colleges in some universities, including those of the University of the Arts London and smaller colleges of the University of London, run their own degree courses but do not award degrees; those at the University of Roehampton provide accommodation and pastoral care as well as delivering the teaching on university courses; those at Oxford and Cambridge deliver some teaching on university courses as well as providing accommodation and pastoral care; and those in Durham, Kent, Lancaster and York provide accommodation and pastoral care but do not normally participate in formal teaching. The legal status of these colleges also varies widely, with University of London colleges being independent corporations and recognised bodies, Oxbridge colleges, colleges of the University of the Highlands and Islands (UHI) and some Durham colleges being independent corporations and listed bodies, most Durham colleges being owned by the university but still listed bodies, and those of other collegiate universities not having formal recognition. When applying for undergraduate courses through UCAS, University of London colleges are treated as independent providers, colleges of Oxford, Cambridge, Durham and UHI are treated as locations within the universities that can be selected by specifying a 'campus code' in addition to selecting the university, and colleges of other universities are not recognised.\nThe UHI and the University of Wales Trinity Saint David (UWTSD) both include further education colleges. However, while the UHI colleges integrate FE and HE provision, UWTSD maintains a separation between the university campuses (Lampeter, Carmarthen and Swansea) and the two colleges (\"Coleg Sir G\u00e2r\" and \"Coleg Ceredigion\"; n.b. \"coleg\" is Welsh for college), which although part of the same group are treated as separate institutions rather than colleges within the university.\nA university college is an independent institution with the power to award taught degrees, but which has not been granted university status. University College is a protected title that can only be used with permission, although note that University College London, University College, Oxford and University College, Durham are colleges within their respective universities and not university colleges (in the case of UCL holding full degree awarding powers that set it above a university college), while University College Birmingham is a university in its own right and also not a university college.\nOceania.\nAustralia.\nIn Australia a college may be an institution of tertiary education that is smaller than a university, run independently or as part of a university. Following a reform in the 1980s many of the formerly independent colleges now belong to a larger universities.\nReferring to parts of a university, there are \"residential colleges\" which provide residence for students, both undergraduate and postgraduate, called university colleges. These colleges often provide additional tutorial assistance, and some host theological study. Many colleges have strong traditions and rituals, so are a combination of dormitory style accommodation and fraternity or sorority culture.\nMost technical and further education institutions (TAFEs), which offer certificate and diploma vocational courses, are styled \"TAFE colleges\" or \"Colleges of TAFE\". In some places, such as Tasmania, college refers to a type of school for Year 11 and 12 students, e.g. Don College.\nNew Zealand.\nThe constituent colleges of the former University of New Zealand (such as Canterbury University College) have become independent universities. Some halls of residence associated with New Zealand universities retain the name of \"college\", particularly at the University of Otago (which although brought under the umbrella of the University of New Zealand, already possessed university status and degree awarding powers). The institutions formerly known as \"Teacher-training colleges\" now style themselves \"College of education\".\nSome universities, such as the University of Canterbury, have divided their university into constituent administrative \"Colleges\" \u2013 the College of Arts containing departments that teach Arts, Humanities and Social Sciences, College of Science containing Science departments, and so on. This is largely modelled on the Cambridge model, discussed above.\nLike the United Kingdom some professional bodies in New Zealand style themselves as \"colleges\", for example, the Royal Australasian College of Surgeons, the Royal Australasian College of Physicians.\nIn some parts of the country, secondary school is often referred to as college and the term is used interchangeably with high school. This sometimes confuses people from other parts of New Zealand. But in all parts of the country many secondary schools have \"College\" in their name, such as Rangitoto College, New Zealand's largest secondary.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5690", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5690", "title": "Chalmers University of Technology", "text": "University in Gothenburg, Sweden\nChalmers University of Technology (, often shortened to Chalmers) is a Swedish university located in Gothenburg that conducts research and education in technology and natural sciences. The university has approximately 3100 employees and 10,000 students, and offers education in engineering, science, shipping, architecture and other management areas.\nChalmers is a highly-reputed university in education \nand research worldwide. It is considered as one of Europe\u2019s leading technical and engineering universities and it is consistently ranked among the top engineering universities in the world.\nChalmers is coordinating the Graphene Flagship, the European Union's biggest research initiative to bring graphene innovation out of the lab and into commercial applications, and leading the development of a Swedish quantum computer.\nHistory.\nChalmers was founded in 1829 following a donation by William Chalmers, a director of the Swedish East India Company. He donated part of his fortune for the establishment of an \"industrial school\". The university was run as a private institution until 1937 when it became the second state-owned technical university. In 1994 the government of Sweden reorganised Chalmers into a private company (aktiebolag) owned by a government-controlled foundation.\nChalmers is one of only three universities in Sweden which are named after a person, the other two being Karolinska Institutet and Linnaeus University.\nDepartments.\nChalmers University of Technology has the following 13 departments:\nFurthermore, Chalmers is home to six Areas of Advance and six national competence centers in key fields such as materials, mathematical modelling, environmental science, and vehicle safety.\nResearch infrastructure.\nChalmers University of Technology's research infrastructure includes everything from advanced real or virtual labs to large databases, computer capacity for large-scale calculations and research facilities.\nRankings and reputation.\nSince 2012, Chalmers has achieved the highest reputation for Swedish Universities by the Kantar Sifo's Reputation Index. According to the survey, Chalmers is the most well-known university in Sweden regarded as a successful and competitive high-class institution with a large contribution to society and credibility in media.\nMoreover, the European Commission has recognized Chalmers as one of Europe's top universities, while based on the U-Multirank 2022, Chalmers characterized as a top performing university across various indicators (i.e., teaching &amp; learning, research, knowledge transfer and international orientation) with the highest number of \u2018A\u2019 (very good) scores on the institutional level for Sweden.\nAdditionally, in 2018, a benchmarking report from MIT ranked Chalmers top 10 in the world of engineering education, while in 2020, the World University Research Rankings placed Chalmers 12th in the world based on the evaluation of three key research aspects, namely research multi-disciplinarity, research impact, and research cooperativeness.\nFurthermore, the QS World University Rankings 2023, placed Chalmers among the top 100 universities in the world in most engineering subjects. For example, Chalmers is ranked 46th in the world in the field of architecture &amp; built environment, 47th in materials sciences, 57th in Mechanical, Aeronautical &amp; Manufacturing Engineering, 61st in electrical &amp; electronic engineering and 93rd in Chemical Engineering. \nFinally, in 2011, the International Professional Ranking of Higher Education Institutions, which is established on the basis of the number of alumni holding a post of Chief Executive Officer (CEO) or equivalent in one of the Fortune Global 500 companies, Chalmers ranked 38th in the world, ranking 1st in Sweden and 15th in Europe.\nTies and partnerships.\nChalmers is a member of the IDEA League network, a strategic alliance between five leading European universities of science and technology. The scope of the network is to provide the environment for students, researchers and staff to share knowledge, experience and resources.\nMoreover, Chalmers is a partner of the UNITECH International, an organization consisting of distinguished technical universities and multinational companies across Europe. UNITECH helps bridge the gap between the industrial and academic world offering exchange programs consisting of studies as well as an integrated internship at one of the corporate partners.\nChalmers is also a member of the Nordic Five Tech network, a strategic alliance of the five leading technical universities in Denmark, Finland, Norway and Sweden. The Nordic Five Tech universities are amongst the top international technical universities with the goal of creating synergies within education, research and innovation.\nAdditionally, Chalmers is a member of the ENHANCE, an alliance of ten leading Universities of Technology shaping the future of Europe and driving transformation in science and society. The partner institutions have a history of solid cooperation in EU programmes and joint research projects.\nFurthermore, Chalmers is a member of CESAER, a European association of universities of science and technology. Among others, the requirements for a university to be a member of CESAER is to provide excellent science and technology research, education and innovation as well as to have a leading position in their region, their country and beyond.\nAdditionally, Chalmers has established formal agreements with three leading materials science centers: University of California, Santa Barbara, ETH Zurich and Stanford University. Within the framework of the agreements, a yearly bilateral workshop is organized, and exchange of researchers is supported.\nChalmers has general exchange agreements with many European and U.S. universities and maintains a special exchange program agreement with National Chiao Tung University (NCTU) in Taiwan where the exchange students from the two universities maintain offices for, among other things, helping local students with applying and preparing for an exchange year as well as acting as representatives.\nFinally, Chalmers has strong partnerships with major industries such as Ericsson, Volvo, Saab AB and AstraZeneca.\nStudents.\nApproximately 40% of Sweden's graduate engineers and architects are educated at Chalmers. Each year, around 250 postgraduate degrees are awarded as well as 850 graduate degrees. About 1,000 post-graduate students attend programmes at the university, and many students are taking Master of Science engineering programmes and the Master of Architecture programme. Since 2007, all master's programmes are taught in English for both national and international students. This was a result of the adaptation to the Bologna process that started in 2004 at Chalmers (as the first technical university in Sweden).\nCurrently, about 10% of all students at Chalmers come from countries outside Sweden to enrol in a master's or PhD program.\nAround 2,700 students also attend Bachelor of Science engineering programmes, merchant marine and other undergraduate courses at Campus Lindholmen. Chalmers also shares some students with Gothenburg University in the joint IT University project. The IT University focuses exclusively on information technology and offers bachelor's and master's programmes with degrees issued from either Chalmers or Gothenburg University, depending on the programme.\nChalmers confers honorary doctoral degrees to people outside the university who have shown great merit in their research or in society.\nOrganization.\nChalmers is an aktiebolag with 100 shares \u00e0 1,000 SEK, all of which are owned by the Chalmers University of Technology Foundation, a private foundation, which appoints the university board and the president. The foundation has its members appointed by the Swedish government (4 to 8 seats), the departments appoint one member, the student union appoints one member and the president automatically gains one chair. Each department is led by a department head, usually a member of the faculty of that department. The faculty senate represents members of the faculty when decisions are taken.\nCampuses.\nIn 1937, the school moved from the city centre to the new Gibraltar Campus, named after the mansion which owned the grounds, where it is now located. The Lindholmen College Campus was created in the early 1990s and is located on the island Hisingen. Campus Johanneberg and Campus Lindholmen, as they are now called, are connected by bus lines.\nStudent societies and traditions.\nTraditions include the graduation ceremony and the Cort\u00e8ge procession, an annual public event.\nPresidents.\nAlthough the official Swedish title for the head is \"rektor\", the university now uses \"President\" as the English translation.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5691", "revid": "160367", "url": "https://en.wikipedia.org/wiki?curid=5691", "title": "Codex", "text": "Historical ancestor of the modern book\nThe codex (plural codices ) was the historical ancestor of the modern book. Instead of being composed of sheets of paper, it used sheets of vellum, papyrus, or other materials. The term \"codex\" is often used for ancient manuscript books, with handwritten contents. A codex, much like the modern book, is bound by stacking the pages and securing one set of edges by a variety of methods over the centuries, yet in a form analogous to modern bookbinding. Modern books are divided into paperback (or softback) and those bound with stiff boards, called hardbacks. Elaborate historical bindings are called treasure bindings. At least in the Western world, the main alternative to the paged codex format for a long document was the continuous scroll, which was the dominant form of document in the ancient world. Some codices are continuously folded like a concertina, in particular the Maya codices and Aztec codices, which are actually long sheets of paper or animal skin folded into pages.\nThe Ancient Romans developed the form from wax tablets. The gradual replacement of the scroll by the codex has been called the most important advance in book making before the invention of the printing press. The codex transformed the shape of the book itself, and offered a form that has lasted ever since. The spread of the codex is often associated with the rise of Christianity, which early on adopted the format for the Bible. First described in the 1st century of the Common Era, when the Roman poet Martial praised its convenient use, the codex achieved numerical parity with the scroll around 300 CE, and had completely replaced it throughout what was by then a Christianized Greco-Roman world by the 6th century.\nEtymology and origins.\nThe word codex comes from the Latin word \"caudex\", meaning \"trunk of a tree\", \"block of wood\" or \"book\". The codex began to replace the scroll almost as soon as it was invented. In Egypt, by the fifth century, the codex outnumbered the scroll by ten to one based on surviving examples. By the sixth century, the scroll had almost vanished as a medium for literature. The change from rolls to codices roughly coincides with the transition from papyrus to parchment as the preferred writing material, but the two developments are unconnected. In fact, any combination of codices and scrolls with papyrus and parchment is technically feasible and common in the historical record.\nTechnically, even modern notebooks and paperbacks are codices, but publishers and scholars reserve the term for manuscript (hand-written) books produced from Late antiquity until the Middle Ages. The scholarly study of these manuscripts is sometimes called codicology. The study of ancient documents in general is called paleography.\nThe codex provided considerable advantages over other book formats, primarily its compactness, sturdiness, economic use of materials by using both sides (recto and verso), and ease of reference (a codex accommodates random access, as opposed to a scroll, which uses sequential access).\nHistory.\nThe Romans used precursors made of reusable wax-covered tablets of wood for taking notes and other informal writings. Two ancient polyptychs, a \"pentaptych\" and \"octoptych\" excavated at Herculaneum, used a unique connecting system that presages later sewing on of thongs or cords. Julius Caesar may have been the first Roman to reduce scrolls to bound pages in the form of a note-book, possibly even as a papyrus codex. At the turn of the 1st century AD, a kind of folded parchment notebook called \"pugillares membranei\" in Latin became commonly used for writing in the Roman Empire. Theodore Cressy Skeat theorized that this form of notebook was invented in Rome and then spread rapidly to the Near East.\nCodices are described in certain works by the Classical Latin poet, Martial. He wrote a series of five couplets meant to accompany gifts of literature that Romans exchanged during the festival of Saturnalia. Three of these books are specifically described by Martial as being in the form of a codex; the poet praises the compendiousness of the form (as opposed to the scroll), as well as the convenience with which such a book can be read on a journey. In another poem by Martial, the poet advertises a new edition of his works, specifically noting that it is produced as a codex, taking less space than a scroll and being more comfortable to hold in one hand. According to Theodore Cressy Skeat, this might be the first recorded known case of an entire edition of a literary work (not just a single copy) being published in codex form, though it was likely an isolated case and was not a common practice until a much later time.\nIn his discussion of one of the earliest parchment codices to survive from Oxyrhynchus in Egypt, Eric Turner seems to challenge Skeat's notion when stating, \"its mere existence is evidence that this book form had a prehistory\", and that \"early experiments with this book form may well have taken place outside of Egypt.\" Early codices of parchment or papyrus appear to have been widely used as personal notebooks, for instance in recording copies of letters sent (Cicero \"Fam.\" 9.26.1). Early codices weren't always cohesive. They often contained multiple languages, various topics and even multiple authors. \"Such codices formed libraries in their own right.\" The parchment notebook pages were \"more durable, and could withstand being folded and stitched to other sheets\". Parchments whose writing was no longer needed were commonly washed or scraped for re-use, creating a palimpsest; the erased text, which can often be recovered, is older and usually more interesting than the newer text which replaced it. Consequently, writings in a codex were often considered informal and impermanent. Parchment (animal skin) was expensive, and therefore it was used primarily by the wealthy and powerful, who were also able to pay for textual design and color. \"Official documents and deluxe manuscripts [in the late Middle Ages] were written in gold and silver ink on parchment...dyed or painted with costly purple pigments as an expression of imperial power and wealth.\"\nAs early as the early 2nd century, there is evidence that a codex\u2014usually of papyrus\u2014was the preferred format among Christians. In the library of the Villa of the Papyri, Herculaneum (buried in AD 79), all the texts (of Greek literature) are scrolls (see Herculaneum papyri). However, in the Nag Hammadi library, hidden about AD 390, all texts (Gnostic) are codices. Despite this comparison, a fragment of a non-Christian parchment codex of Demosthenes' \"De Falsa Legatione\" from Oxyrhynchus in Egypt demonstrates that the surviving evidence is insufficient to conclude whether Christians played a major or central role in the development of early codices\u2014or if they simply adopted the format to distinguish themselves from Jews.\nThe earliest surviving fragments from codices come from Egypt, and are variously dated (always tentatively) towards the end of the 1st century or in the first half of the 2nd. This group includes the Rylands Library Papyrus P52, containing part of St John's Gospel, and perhaps dating from between 125 and 160.\nIn Western culture, the codex gradually replaced the scroll. Between the 4th century, when the codex gained wide acceptance, and the Carolingian Renaissance in the 8th century, many works that were not converted from scroll to codex were lost. The codex improved on the scroll in several ways. It could be opened flat at any page for easier reading, pages could be written on both front and back (recto and verso), and the protection of durable covers made it more compact and easier to transport.\nThe ancients stored codices with spines facing inward, and not always vertically. The spine could be used for the incipit, before the concept of a proper title developed in medieval times. Though most early codices were made of papyrus, papyrus was fragile and supplied from Egypt, the only place where papyrus grew. The more durable parchment and vellum gained favor, despite the cost.\nThe codices of pre-Columbian Mesoamerica (Mexico and Central America) had a similar appearance when closed to the European codex, but were instead made with long folded strips of either fig bark (amatl) or plant fibers, often with a layer of whitewash applied before writing. New World codices were written as late as the 16th century (see Maya codices and Aztec codices). Those written before the Spanish conquests seem all to have been single long sheets folded concertina-style, sometimes written on both sides of the \"amatl\" paper. There are significant codices produced in the colonial era, with pictorial and alphabetic texts in Spanish or an indigenous language such as Nahuatl.\nIn East Asia, the scroll remained standard for far longer than in the Mediterranean world. There were intermediate stages, such as scrolls folded concertina-style and pasted together at the back and books that were printed only on one side of the paper. This replaced traditional Chinese writing mediums such as bamboo and wooden slips, as well as silk and paper scrolls. The evolution of the codex in China began with folded-leaf pamphlets in the 9th century, during the late Tang dynasty (618\u2013907), improved by the 'butterfly' bindings of the Song dynasty (960\u20131279), the wrapped back binding of the Yuan dynasty (1271\u20131368), the stitched binding of the Ming (1368\u20131644) and Qing dynasties (1644\u20131912), and finally the adoption of Western-style bookbinding in the 20th century. The initial phase of this evolution, the accordion-folded palm-leaf-style book, most likely came from India and was introduced to China via Buddhist missionaries and scriptures.\nJudaism still retains the Torah scroll, at least for ceremonial use.\nPreparation.\nThe first stage in creating a codex is to prepare the animal skin. The skin is washed with water and lime but not together. The skin is soaked in the lime for a couple of days. The hair is removed, and the skin is dried by attaching it to a frame, called a herse. The parchment maker attaches the skin at points around the circumference. The skin attaches to the herse by cords. To prevent it from being torn, the maker wraps the area of the skin attached to the cord around a pebble called a pippin. After completing that, the maker uses a crescent shaped knife called a \"lunarium\" or \"lunellum\" to remove any remaining hairs. Once the skin completely dries, the maker gives it a deep clean and processes it into sheets. The number of sheets from a piece of skin depends on the size of the skin and the final product dimensions. For example, the average calfskin can provide three-and-a-half medium sheets of writing material, which can be doubled when they are folded into two conjoint leaves, also known as a \"bifolium\". Historians have found evidence of manuscripts in which the scribe wrote down the medieval instructions now followed by modern membrane makers. Defects can often be found in the membrane, whether they are from the original animal, human error during the preparation period, or from when the animal was killed. Defects can also appear during the writing process. Unless the manuscript is kept in perfect condition, defects can also appear later in its life.\nPreparation of pages for writing.\nFirstly, the membrane must be prepared. The first step is to set up the quires. The quire is a group of several sheets put together. Raymond Clemens and Timothy Graham point out, in \"Introduction to Manuscript Studies\", that \"the quire was the scribe's basic writing unit throughout the Middle Ages\":\nPricking is the process of making holes in a sheet of parchment (or membrane) in preparation of it ruling. The lines were then made by ruling between the prick marks... The process of entering ruled lines on the page to serve as a guide for entering text. Most manuscripts were ruled with horizontal lines that served as the baselines on which the text was entered and with vertical bounding lines that marked the boundaries of the columns.\nForming quire.\nFrom the Carolingian period to the end of the Middle Ages, different styles of folding the quire came about. For example, in continental Europe throughout the Middle Ages, the quire was put into a system in which each side folded on to the same style. The hair side met the hair side and the flesh side to the flesh side. This was not the same style used in the British Isles, where the membrane was folded so that it turned out an eight-leaf quire, with single leaves in the third and sixth positions. The next stage was tacking the quire. Tacking is when the scribe would hold together the leaves in quire with thread. Once threaded together, the scribe would then sew a line of parchment up the \"spine\" of the manuscript to protect the tacking.\nMaterials.\nThe materials codices are made with are their support, and include papyrus, parchment (sometimes referred to as membrane or vellum), and paper. They are written and drawn on with metals, pigments and ink. The quality, size, and choice of support determine the status of a codex. Papyrus is found only in late antiquity and the early Middle Ages. Codices intended for display were bound with more durable materials than vellum. Parchment varied widely due to animal species and finish, and identification of animals used to make it has only begun to be studied in the 21st century. How manufacturing influenced the final products, technique, and style, is little understood. However, changes in style are underpinned more by variation in technique. Before the 14th and 15th century, paper was expensive, and its use may mark off the deluxe copy.\nStructure.\nThe structure of a codex includes its size, format/\"ordinatio\"(its quires or gatherings), consisting of sheets folded a number of times, often twice- a \"bifolio\"), sewing, bookbinding and rebinding. A quire consisted of a number of folded sheets inserting into one another- at least three, but most commonly four bifolia, that is eight sheets and sixteen pages: Latin quaternio or Greek tetradion, which became a synonym for quires. Unless an exemplar (text to be copied) was copied exactly, format differed. In preparation for writing codices, ruling patterns were used that determined the layout of each page. Holes were prickled with a spiked lead wheel and a circle. Ruling was then applied separately on each page or once through the top folio. Ownership markings, decorations and illumination are also a part of it. They are specific to the scriptoria, or any production center, and libraries of codices.\nPages.\nWatermarks may provide, although often approximate, dates for when the copying occurred. The layout\u2013 size of the margin and the number of lines\u2013 is determined. There may be textual articulations, running heads, openings, chapters and paragraphs. Space was reserved for illustrations and decorated guide letters. The apparatus of books for scholars became more elaborate during the 13th and 14th centuries when chapter, verse, page numbering, marginalia finding guides, indexes, glossaries and tables of contents were developed.\nThe \"libraire\".\nBy a close examination of the physical attributes of a codex, it is sometimes possible to match up long-separated elements originally from the same book. In 13th-century book publishing, due to secularization, stationers or \"libraires\" emerged. They would receive commissions for texts, which they would contract out to scribes, illustrators, and binders, to whom they supplied materials. Due to the systematic format used for assembly by the \"libraire\", the structure can be used to reconstruct the original order of a manuscript. However, complications can arise in the study of a codex. Manuscripts were frequently rebound, and this resulted in a particular codex incorporating works of different dates and origins, thus different internal structures. Additionally, a binder could alter or unify these structures to ensure a better fit for the new binding. Completed quires or books of quires might constitute independent book units- booklets, which could be returned to the stationer, or combined with other texts to make anthologies or miscellanies. Exemplars were sometimes divided into quires for simultaneous copying and loaned out to students for study. To facilitate this, catchwords were used- a word at the end of a page providing the next page's first word.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5692", "revid": "25046916", "url": "https://en.wikipedia.org/wiki?curid=5692", "title": "Calf (animal)", "text": "Young of domestic cattle\nA calf (PL: calves) is a young domestic cow or bull. Calves are reared to become adult cattle or are slaughtered for their meat, called veal, and hide.\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\nTerminology.\n\"Calf\" is the term used from birth to weaning, when it becomes known as a \"weaner\" or \"weaner calf\", though in some areas the term \"calf\" may be used until the animal is a yearling. The birth of a calf is known as \"calving\". A calf that has lost its mother is an orphan calf, also known as a \"poddy\" or \"poddy-calf\" in British. \"Bobby calves\" are young calves which are to be slaughtered for human consumption. A \"vealer\" is a calf weighing less than about which is at about eight to nine months of age. A young female calf from birth until she has had a calf of her own is called a \"heifer\"\n(). In the American Old West, a motherless or small, runty calf was sometimes referred to as a dodie.\nThe term \"calf\" is also used for some other species. See \"Other animals\" below.\nEarly development.\nCalves may be produced by natural means, or by artificial breeding using artificial insemination or embryo transfer.\nCalves are born after nine months. They usually stand within a few minutes of calving, and suckle within an hour. However, for the first few days they are not easily able to keep up with the rest of the herd, so young calves are often left hidden by their mothers, who visit them several times a day to suckle them. By a week old the calf is able to follow the mother all the time.\nSome calves are ear tagged soon after birth, especially those that are stud cattle in order to correctly identify their dams (mothers), or in areas (such as the EU) where tagging is a legal requirement for cattle. Typically when the calves are about two months old they are branded, ear marked, castrated and vaccinated.\nCalf rearing systems.\nThe \"single suckler\" system of rearing calves is similar to that occurring naturally in wild cattle, where each calf is suckled by its own mother until it is weaned at about nine months old. This system is commonly used for rearing beef cattle throughout the world.\nCows kept on poor forage (as is typical in subsistence farming) produce a limited amount of milk. A calf left with such a mother all the time can easily drink all the milk, leaving none for human consumption. For dairy production under such circumstances, the calf's access to the cow must be limited, for example by penning the calf and bringing the mother to it once a day after partly milking her. The small amount of milk available for the calf under such systems may mean that it takes a longer time to rear, and in subsistence farming it is therefore common for cows to calve only in alternate years.\nIn more intensive dairy farming, cows can easily be bred and fed to produce far more milk than one calf can drink. In the \"multi-suckler\" system, several calves are fostered onto one cow in addition to her own, and these calves' mothers can then be used wholly for milk production. More commonly, calves of dairy cows are fed formula milk from soon after birth, usually from a bottle or bucket.\nPurebred female calves of dairy cows are reared as replacement dairy cows. Most purebred dairy calves are produced by artificial insemination (AI). By this method each bull can serve many cows, so only a very few of the purebred dairy male calves are needed to provide bulls for breeding. The remainder of the male calves may be reared for beef or veal; Only a proportion of purebred heifers are needed to provide replacement cows, so often some of the cows in dairy herds are put to a beef bull to produce crossbred calves suitable for rearing as beef.\nVeal calves may be reared entirely on milk formula and killed at about 18 or 20 weeks as \"white\" veal, or fed on grain and hay and killed at 22 to 35 weeks to produce red or pink veal.\nGrowth.\nA commercial steer or bull calf is expected to put on about per month. A nine-month-old steer or bull is therefore expected to weigh about . Heifers will weigh at least at eight months of age.\nCalves are usually weaned at about eight to nine months of age, but depending on the season and condition of the dam, they might be weaned earlier. They may be paddock weaned, often next to their mothers, or weaned in stockyards. The latter system is preferred by some as it accustoms the weaners to the presence of people and they are trained to take feed other than grass. Small numbers may also be weaned with their dams with the use of weaning nose rings or nosebands which results in the mothers rejecting the calves' attempts to suckle. Many calves are also weaned when they are taken to the large weaner auction sales that are conducted in the south eastern states of Australia. Victoria and New South Wales have yardings (sale yard numbers) of up to 8,000 weaners (calves) for auction sale in one day. The best of these weaners may go to the butchers. Others will be purchased by re-stockers to grow out and fatten on grass or as potential breeders. In the United States these weaners may be known as \"feeders\" and would be placed directly into feedlots.\nAt about 12 months old a beef heifer reaches puberty if she is well grown.\nDiseases.\nCalves suffer from few congenital abnormalities but the Akabane virus is widely distributed in temperate to tropical regions of the world. The virus is a teratogenic pathogen which causes abortions, stillbirths, premature births and congenital abnormalities, but occurs only during some years.\nCalves commonly face on-farm acquired diseases, often of infectious nature. Preweaned calves most commonly experience conditions such as diarrhea, omphalitis, lameness and respiratory diseases. Diarrhea, Omphalitis and Lameness are most common in calves aged up to two weeks, while the frequency of respiratory diseases tends to increase with age. These conditions also display seasonal patterns, with omphalitis being more common in the summer months, and respiratory diseases and diarrhea occurring more frequently in the fall.\nUses.\nCalf meat for human consumption is called veal, and is usually produced from the male calves of Dairy cattle. Also eaten are calf's brains and calf liver. The hide is used to make calfskin, or tanned into leather and called calf leather, or sometimes in the US \"novillo\", the Spanish term. The fourth compartment of the stomach of slaughtered milk-fed calves is the source of rennet. The intestine is used to make Goldbeater's skin, and is the source of Calf Intestinal Alkaline Phosphatase (CIP).\nDairy cows can only produce milk after having calved, and dairy cows need to produce one calf each year in order to remain in production. Female calves will become a replacement dairy cow. Male dairy calves are generally reared for beef or veal; relatively few are kept for breeding purposes.\nOther animals.\nIn English the term \"calf\" is used by extension for the young of various other large species of mammal. In addition to other bovid species (such as bison, yak and water buffalo), these include the young of camels, dolphins, elephants, giraffes, hippopotamuses, deer (such as moose, elk (wapiti) and red deer), rhinoceroses, porpoises, whales, walruses and larger seals. (Generally, the adult males of these same species are called \"bulls\" and the adult females \"cows\".) However, common domestic species tend to have their own specific names, such as lamb, foal used for all \"Equidae\", or piglet used for all suidae.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5693", "revid": "35296831", "url": "https://en.wikipedia.org/wiki?curid=5693", "title": "Claude Shannon", "text": "American mathematician and information theorist (1916\u20132001)\nClaude Elwood Shannon (April 30, 1916 \u2013 February 24, 2001) was an American mathematician, electrical engineer, computer scientist and cryptographer known as the \"father of information theory\". \nAs a 21-year-old master's degree student at the Massachusetts Institute of Technology (MIT), he wrote his thesis demonstrating that electrical applications of Boolean algebra could construct any logical numerical relationship. Shannon contributed to the field of cryptanalysis for national defense of the United States during World War II, including his fundamental work on codebreaking and secure telecommunications.\nBiography.\nChildhood.\nThe Shannon family lived in Gaylord, Michigan, and Claude was born in a hospital in nearby Petoskey. His father, Claude Sr. (1862\u20131934), was a businessman and for a while, a judge of probate in Gaylord. His mother, Mabel Wolf Shannon (1890\u20131945), was a language teacher, who also served as the principal of Gaylord High School. Claude Sr. was a descendant of New Jersey settlers, while Mabel was a child of German immigrants. Shannon's family was active in their Methodist Church during his youth.\nMost of the first 16 years of Shannon's life were spent in Gaylord, where he attended public school, graduating from Gaylord High School in 1932. Shannon showed an inclination towards mechanical and electrical things. His best subjects were science and mathematics. At home, he constructed such devices as models of planes, a radio-controlled model boat and a barbed-wire telegraph system to a friend's house a half-mile away. While growing up, he also worked as a messenger for the Western Union company.\nShannon's childhood hero was Thomas Edison, who he later learned was a distant cousin. Both Shannon and Edison were descendants of John Ogden (1609\u20131682), a colonial leader and an ancestor of many distinguished people.\nLogic circuits.\nIn 1932, Shannon entered the University of Michigan, where he was introduced to the work of George Boole. He graduated in 1936 with two bachelor's degrees: one in electrical engineering and the other in mathematics.\nIn 1936, Shannon began his graduate studies in electrical engineering at MIT, where he worked on Vannevar Bush's differential analyzer, an early analog computer. While studying the complicated \"ad hoc\" circuits of this analyzer, Shannon designed switching circuits based on Boole's concepts. In 1937, he wrote his master's degree thesis, \"A Symbolic Analysis of Relay and Switching Circuits\". A paper from this thesis was published in 1938. In this work, Shannon proved that his switching circuits could be used to simplify the arrangement of the electromechanical relays that were used during that time in telephone call routing switches. Next, he expanded this concept, proving that these circuits could solve all problems that Boolean algebra could solve. In the last chapter, he presented diagrams of several circuits, including a 4-bit full adder.\nUsing this property of electrical switches to implement logic is the fundamental concept that underlies all electronic digital computers. Shannon's work became the foundation of digital circuit design, as it became widely known in the electrical engineering community during and after World War II. The theoretical rigor of Shannon's work superseded the \"ad hoc\" methods that had prevailed previously. Howard Gardner called Shannon's thesis \"possibly the most important, and also the most noted, master's thesis of the century.\"\nShannon received his PhD in mathematics from MIT in 1940. Vannevar Bush had suggested that Shannon should work on his dissertation at the Cold Spring Harbor Laboratory, in order to develop a mathematical formulation for Mendelian genetics. This research resulted in Shannon's PhD thesis, called \"An Algebra for Theoretical Genetics\".\nIn 1940, Shannon became a National Research Fellow at the Institute for Advanced Study in Princeton, New Jersey. In Princeton, Shannon had the opportunity to discuss his ideas with influential scientists and mathematicians such as Hermann Weyl and John von Neumann, and he also had occasional encounters with Albert Einstein and Kurt G\u00f6del. Shannon worked freely across disciplines, and this ability may have contributed to his later development of mathematical information theory.\nWartime research.\nShannon then joined Bell Labs to work on fire-control systems and cryptography during World War II, under a contract with section D-2 (Control Systems section) of the National Defense Research Committee (NDRC).\nShannon is credited with the invention of signal-flow graphs, in 1942. He discovered the topological gain formula while investigating the functional operation of an analog computer.\nFor two months early in 1943, Shannon came into contact with the leading British mathematician Alan Turing. Turing had been posted to Washington to share with the U.S. Navy's cryptanalytic service the methods used by the British Government Code and Cypher School at Bletchley Park to break the ciphers used by the Kriegsmarine U-boats in the north Atlantic Ocean. He was also interested in the encipherment of speech and to this end spent time at Bell Labs. Shannon and Turing met at teatime in the cafeteria. Turing showed Shannon his 1936 paper that defined what is now known as the \"Universal Turing machine\". This impressed Shannon, as many of its ideas complemented his own.\nIn 1945, as the war was coming to an end, the NDRC was issuing a summary of technical reports as a last step prior to its eventual closing down. Inside the volume on fire control, a special essay titled \"Data Smoothing and Prediction in Fire-Control Systems\", coauthored by Shannon, Ralph Beebe Blackman, and Hendrik Wade Bode, formally treated the problem of smoothing the data in fire-control by analogy with \"the problem of separating a signal from interfering noise in communications systems.\" In other words, it modeled the problem in terms of data and signal processing and thus heralded the coming of the Information Age.\nShannon's work on cryptography was even more closely related to his later publications on communication theory. At the close of the war, he prepared a classified memorandum for Bell Telephone Labs entitled \"A Mathematical Theory of Cryptography\", dated September 1945. A declassified version of this paper was published in 1949 as \"Communication Theory of Secrecy Systems\" in the \"Bell System Technical Journal\". This paper incorporated many of the concepts and mathematical formulations that also appeared in his \"A Mathematical Theory of Communication\". Shannon said that his wartime insights into communication theory and cryptography developed simultaneously and that \"they were so close together you couldn't separate them\". In a footnote near the beginning of the classified report, Shannon announced his intention to \"develop these results \u2026 in a forthcoming memorandum on the transmission of information.\"\nWhile he was at Bell Labs, Shannon proved that the cryptographic one-time pad is unbreakable in his classified research that was later published 1949. The same article also proved that any unbreakable system must have essentially the same characteristics as the one-time pad: the key must be truly random, as large as the plaintext, never reused in whole or part, and kept secret.\nInformation theory.\nIn 1948, the promised memorandum appeared as \"A Mathematical Theory of Communication\", an article in two parts in the July and October issues of the \"Bell System Technical Journal\". This work focuses on the problem of how best to encode the message a sender wants to transmit. Shannon developed information entropy as a measure of the information content in a message, which is a measure of uncertainty reduced by the message. In so doing, he essentially invented the field of information theory.\nThe book \"The Mathematical Theory of Communication\" reprints Shannon's 1948 article and Warren Weaver's popularization of it, which is accessible to the non-specialist. Weaver pointed out that the word \"information\" in communication theory is not related to what you do say, but to what you could say. That is, information is a measure of one's freedom of choice when one selects a message. Shannon's concepts were also popularized, subject to his own proofreading, in John Robinson Pierce's \"Symbols, Signals, and Noise\".\nInformation theory's fundamental contribution to natural language processing and computational linguistics was further established in 1951, in his article \"Prediction and Entropy of Printed English\", showing upper and lower bounds of entropy on the statistics of English \u2013 giving a statistical foundation to language analysis. In addition, he proved that treating whitespace as the 27th letter of the alphabet actually lowers uncertainty in written language, providing a clear quantifiable link between cultural practice and probabilistic cognition.\nAnother notable paper published in 1949 is \"Communication Theory of Secrecy Systems\", a declassified version of his wartime work on the mathematical theory of cryptography, in which he proved that all theoretically unbreakable ciphers must have the same requirements as the one-time pad. He is also credited with the introduction of sampling theory, which is concerned with representing a continuous-time signal from a (uniform) discrete set of samples. This theory was essential in enabling telecommunications to move from analog to digital transmissions systems in the 1960s and later.\nHe returned to MIT to hold an endowed chair in 1956.\nTeaching at MIT.\nIn 1956 Shannon joined the MIT faculty to work in the Research Laboratory of Electronics (RLE). He continued to serve on the MIT faculty until 1978.\nLater life.\nShannon developed Alzheimer's disease and spent the last few years of his life in a nursing home; he died in 2001, survived by his wife, a son and daughter, and two granddaughters.\nHobbies and inventions.\nOutside of Shannon's academic pursuits, he was interested in juggling, unicycling, and chess. He also invented many devices, including a Roman numeral computer called THROBAC, and juggling machines. He built a device that could solve the Rubik's Cube puzzle.\nShannon designed the Minivac 601, a digital computer trainer to teach business people about how computers functioned. It was sold by the Scientific Development Corp starting in 1961.\nHe is also considered the co-inventor of the first wearable computer along with Edward O. Thorp. The device was used to improve the odds when playing roulette.\nPersonal life.\nShannon married Norma Levor, a wealthy, Jewish, left-wing intellectual in January 1940. The marriage ended in divorce after about a year. Levor later married Ben Barzman.\nShannon met his second wife, Betty Shannon (n\u00e9e Mary Elizabeth Moore), when she was a numerical analyst at Bell Labs. They were married in 1949. Betty assisted Claude in building some of his most famous inventions. They had three children.\nShannon presented himself as apolitical and an atheist.\nTributes.\nThere are six statues of Shannon sculpted by Eugene Daub: one at the University of Michigan; one at MIT in the Laboratory for Information and Decision Systems; one in Gaylord, Michigan; one at the University of California, San Diego; one at Bell Labs; and another at AT&amp;T Shannon Labs. The statue in Gaylord is located in the Claude Shannon Memorial Park. After the breakup of the Bell System, the part of Bell Labs that remained with AT&amp;T Corporation was named Shannon Labs in his honor.\nAccording to Neil Sloane, an AT&amp;T Fellow who co-edited Shannon's large collection of papers in 1993, the perspective introduced by Shannon's communication theory (now called information theory) is the foundation of the digital revolution, and every device containing a microprocessor or microcontroller is a conceptual descendant of Shannon's publication in 1948: \"He's one of the great men of the century. Without him, none of the things we know today would exist. The whole digital revolution started with him.\" The cryptocurrency unit shannon (a synonym for gwei) is named after him.\n\"A Mind at Play\", a biography of Shannon written by Jimmy Soni and Rob Goodman, was published in 2017.\nOn April 30, 2016, Shannon was honored with a Google Doodle to celebrate his life on what would have been his 100th birthday.\n\"The Bit Player\", a feature film about Shannon directed by Mark Levinson premiered at the World Science Festival in 2019. Drawn from interviews conducted with Shannon in his house in the 1980s, the film was released on Amazon Prime in August 2020.\nThe Mathematical Theory of Communication.\nWeaver's Contribution.\nShannon's \"The Mathematical Theory of Communication,\" begins with an interpretation of his own work by Warren Weaver. Although Shannon's entire work is about communication itself, Warren Weaver communicated his ideas in such a way that those not acclimated to complex theory and mathematics could comprehend the fundamental laws he put forth. The coupling of their unique communicational abilities and ideas generated the Shannon-Weaver model, although the mathematical and theoretical underpinnings emanate entirely from Shannon's work after Weaver's introduction. For layman Weaver's introduction better communicates \"The Mathematical Theory of Communication\", but Shannon's subsequent logic, mathematics, and expressive precision was responsible for defining the problem itself.\nOther work.\nShannon's mouse.\n\"Theseus\", created in 1950, was a mechanical mouse controlled by an electromechanical relay circuit that enabled it to move around a labyrinth of 25 squares. The maze configuration was flexible and it could be modified arbitrarily by rearranging movable partitions. The mouse was designed to search through the corridors until it found the target. Having travelled through the maze, the mouse could then be placed anywhere it had been before, and because of its prior experience it could go directly to the target. If placed in unfamiliar territory, it was programmed to search until it reached a known location and then it would proceed to the target, adding the new knowledge to its memory and learning new behavior. Shannon's mouse appears to have been the first artificial learning device of its kind.\nShannon's estimate for the complexity of chess.\nIn 1949 Shannon completed a paper (published in March 1950) which estimates the game-tree complexity of chess, which is approximately 10120. This number is now often referred to as the \"Shannon number\", and is still regarded today as an accurate estimate of the game's complexity. The number is often cited as one of the barriers to solving the game of chess using an exhaustive analysis (i.e. brute force analysis).\nShannon's computer chess program.\nOn March 9, 1949, Shannon presented a paper called \"Programming a Computer for playing Chess\". The paper was presented at the National Institute for Radio Engineers Convention in New York. He described how to program a computer to play chess based on position scoring and move selection. He proposed basic strategies for restricting the number of possibilities to be considered in a game of chess. In March 1950 it was published in \"Philosophical Magazine\", and is considered one of the first articles published on the topic of programming a computer for playing chess, and using a computer to solve the game.\nHis process for having the computer decide on which move to make was a minimax procedure, based on an evaluation function of a given chess position. Shannon gave a rough example of an evaluation function in which the value of the black position was subtracted from that of the white position. \"Material\" was counted according to the usual chess piece relative value (1 point for a pawn, 3 points for a knight or bishop, 5 points for a rook, and 9 points for a queen). He considered some positional factors, subtracting \u00bd point for each doubled pawn, backward pawn, and isolated pawn; \"mobility\" was incorporated by adding 0.1 point for each legal move available.\nShannon's maxim.\nShannon formulated a version of Kerckhoffs' principle as \"The enemy knows the system\". In this form it is known as \"Shannon's maxim\".\nCommemorations.\nShannon centenary.\nThe Shannon centenary, 2016, marked the life and influence of Claude Elwood Shannon on the hundredth anniversary of his birth on April 30, 1916. It was inspired in part by the Alan Turing Year. An ad hoc committee of the IEEE Information Theory Society including Christina Fragouli, R\u00fcdiger Urbanke, Michelle Effros, Lav Varshney and Sergio Verd\u00fa, coordinated worldwide events. The initiative was announced in the History Panel at the 2015 IEEE Information Theory Workshop Jerusalem and the IEEE Information Theory Society Newsletter.\nA detailed listing of confirmed events was available on the website of the IEEE Information Theory Society.\nSome of the planned activities included:\nAwards and honors list.\nThe Claude E. Shannon Award was established in his honor; he was also its first recipient, in 1972.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5694", "revid": "19404073", "url": "https://en.wikipedia.org/wiki?curid=5694", "title": "Cracking", "text": "Cracking may refer to:\nIn computing:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5695", "revid": "7997953", "url": "https://en.wikipedia.org/wiki?curid=5695", "title": "Community", "text": "Social unit which shares commonality\nA community is a social unit (a group of living things) with commonality such as place, norms, religion, values, customs, or identity. Communities may share a sense of place situated in a given geographical area (e.g. a country, village, town, or neighbourhood) or in virtual space through communication platforms. Durable good relations that extend beyond immediate genealogical ties also define a sense of community, important to their identity, practice, and roles in social institutions such as family, home, work, government, society, or humanity at large. Although communities are usually small relative to personal social ties, \"community\" may also refer to large group affiliations such as national communities, international communities, and virtual communities.\nThe English-language word \"community\" derives from the Old French (Modern French: \"\"), which comes from the Latin \"communitas\" \"community\", \"public spirit\" (from Latin \"communis\", \"common\").\nHuman communities may have intent, belief, resources, preferences, needs, and risks in common, affecting the identity of the participants and their degree of cohesiveness.\nPerspectives of various disciplines.\nArchaeology.\nArchaeological studies of social communities use the term \"community\" in two ways, paralleling usage in other areas. The first is an informal definition of community as a place where people used to live. In this sense it is synonymous with the concept of an ancient settlement\u2014whether a hamlet, village, town, or city. The second meaning resembles the usage of the term in other social sciences: a community is a group of people living near one another who interact socially. Social interaction on a small scale can be difficult to identify with archaeological data. Most reconstructions of social communities by archaeologists rely on the principle that social interaction in the past was conditioned by physical distance. Therefore, a small village settlement likely constituted a social community and spatial subdivisions of cities and other large settlements may have formed communities. Archaeologists typically use similarities in material culture\u2014from house types to styles of pottery\u2014to reconstruct communities in the past. This classification method relies on the assumption that people or households will share more similarities in the types and styles of their material goods with other members of a social community than they will with outsiders.\nEcology.\nIn ecology, a community is an assemblage of populations\u2014potentially of different species\u2014interacting with one another. Community ecology is the branch of ecology that studies interactions between and among species. It considers how such interactions, along with interactions between species and the abiotic environment, affect social structure and species richness, diversity and patterns of abundance. Species interact in three ways: competition, predation and mutualism:\nThe two main types of ecological communities are major communities, which are self-sustaining and self-regulating (such as a forest or a lake), and minor communities, which rely on other communities (like fungi decomposing a log) and are the building blocks of major communities.\nSemantics.\nThe concept of \"community\" often has a positive semantic connotation, exploited rhetorically by populist politicians and by advertisers\nto promote feelings and associations of mutual well-being, happiness and togetherness\u2014veering towards an almost-achievable utopian community.\nIn contrast, the epidemiological term \"community transmission\" can have negative implications, and instead of a \"criminal community\" one often speaks of a \"criminal underworld\" or of the \"criminal fraternity\".\nKey concepts.\n\"Gemeinschaft and Gesellschaft\".\nIn (1887), German sociologist Ferdinand T\u00f6nnies described two types of human association: (usually translated as \"community\") and (\"society\" or \"association\"). T\u00f6nnies proposed the \"\u2013\" dichotomy as a way to think about social ties. No group is exclusively one or the other. stress personal social interactions, and the roles, values, and beliefs based on such interactions. stress indirect interactions, impersonal roles, formal values, and beliefs based on such interactions.\nSense of community.\nIn a seminal 1986 study, McMillan and Chavis identify four elements of \"sense of community\":\nA \"sense of community index\" (SCI) was developed by Chavis and colleagues, and revised and adapted by others. Although originally designed to assess sense of community in neighborhoods, the index has been adapted for use in schools, the workplace, and a variety of types of communities.\nStudies conducted by the APPA indicate that young adults who feel a sense of belonging in a community, particularly small communities, develop fewer psychiatric and depressive disorders than those who do not have the feeling of love and belonging.\nSocialization.\nThe process of learning to adopt the behavior patterns of the community is called socialization. The most fertile time of socialization is usually the early stages of life, during which individuals develop the skills and knowledge and learn the roles necessary to function within their culture and social environment. For some psychologists, especially those in the psychodynamic tradition, the most important period of socialization is between the ages of one and ten. But socialization also includes adults moving into a significantly different environment where they must learn a new set of behaviors.\nSocialization is influenced primarily by the family, through which children first learn community norms. Other important influences include schools, peer groups, people, mass media, the workplace, and government. The degree to which the norms of a particular society or community are adopted determines one's willingness to engage with others. The norms of tolerance, reciprocity, and trust are important \"habits of the heart\", as de Tocqueville put it, in an individual's involvement in community.\nCommunity development.\nCommunity development is often linked with community work or community planning, and may involve stakeholders, foundations, governments, or contracted entities including non-government organisations (NGOs), universities or government agencies to progress the social well-being of local, regional and, sometimes, national communities. More grassroots efforts, called community building or community organizing, seek to empower individuals and groups of people by providing them with the skills they need to effect change in their own communities. These skills often assist in building political power through the formation of large social groups working for a common agenda. Community development practitioners must understand both how to work with individuals and how to affect communities' positions within the context of larger social institutions. Public administrators, in contrast, need to understand community development in the context of rural and urban development, housing and economic development, and community, organizational and business development.\nFormal accredited programs conducted by universities, as part of degree granting institutions, are often used to build a knowledge base to drive curricula in public administration, sociology and community studies. The General Social Survey from the National Opinion Research Center at the University of Chicago and the Saguaro Seminar at the Harvard Kennedy School are examples of national community development in the United States. The Maxwell School of Citizenship and Public Affairs at Syracuse University in New York State offers core courses in community and economic development, and in areas ranging from non-profit development to US budgeting (federal to local, community funds). In the United Kingdom, the University of Oxford has led in providing extensive research in the field through its \"Community Development Journal,\" used worldwide by sociologists and community development practitioners.\nAt the intersection between community \"development\" and community \"building\" are a number of programs and organizations with community development tools. One example of this is the program of the Asset Based Community Development Institute of Northwestern University. The institute makes available downloadable tools to assess community assets and make connections between non-profit groups and other organizations that can help in community building. The Institute focuses on helping communities develop by \"mobilizing neighborhood assets\"\u00a0\u2013 building from the inside out rather than the outside in. In the disability field, community building was prevalent in the 1980s and 1990s with roots in John McKnight's approaches.\nCommunity building and organizing.\nIn \"The Different Drum: Community-Making and Peace\" (1987) Scott Peck argues that the almost accidental sense of community that exists at times of crisis can be consciously built. Peck believes that conscious community building is a process of deliberate design based on the knowledge and application of certain rules. He states that this process goes through four stages:\nIn 1991, Peck remarked that building a sense of community is easy but maintaining this sense of community is difficult in the modern world. An interview with M. Scott Peck by Alan Atkisson. \"In Context\" #29, p. 26.\nThe three basic types of community organizing are grassroots organizing, coalition building, and \"institution-based community organizing\", (also called \"broad-based community organizing\", an example of which is faith-based community organizing, or Congregation-based Community Organizing).\nCommunity building can use a wide variety of practices, ranging from simple events (e.g., potlucks, small book clubs) to larger-scale efforts (e.g., mass festivals, construction projects that involve local participants rather than outside contractors).\nCommunity building that is geared toward citizen action is usually termed \"community organizing\". In these cases, organized community groups seek accountability from elected officials and increased direct representation within decision-making bodies. Where good-faith negotiations fail, these constituency-led organizations seek to pressure the decision-makers through a variety of means, including picketing, boycotting, sit-ins, petitioning, and electoral politics. \nCommunity organizing can focus on more than just resolving specific issues. Organizing often means building a widely accessible power structure, often with the end goal of distributing power equally throughout the community. Community organizers generally seek to build groups that are open and democratic in governance. Such groups facilitate and encourage consensus decision-making with a focus on the general health of the community rather than a specific interest group.\nIf communities are developed based on something they share in common, whether location or values, then one challenge for developing communities is how to incorporate individuality and differences. Rebekah Nathan suggests in her book, \"My Freshman Year\", we are drawn to developing communities totally based on sameness, despite stated commitments to diversity, such as those found on university websites.\nTypes of community.\nA number of ways to categorize types of community have been proposed. One such breakdown is as follows:\nThe usual categorizations of community relations have a number of problems: (1) they tend to give the impression that a particular community can be defined as just this kind or another; (2) they tend to conflate modern and customary community relations; (3) they tend to take sociological categories such as ethnicity or race as given, forgetting that different ethnically defined persons live in different kinds of communities\u2014grounded, interest-based, diasporic, etc.\nIn response to these problems, Paul James and his colleagues have developed a taxonomy that maps community relations, and recognizes that actual communities can be characterized by different kinds of relations at the same time:\nIn these terms, communities can be nested and/or intersecting; one community can contain another\u2014for example a location-based community may contain a number of ethnic communities. Both lists above can used in a cross-cutting matrix in relation to each other.\nInternet communities.\nIn general, virtual communities value knowledge and information as currency or social resource. What differentiates virtual communities from their physical counterparts is the extent and impact of \"weak ties\", which are the relationships acquaintances or strangers form to acquire information through online networks. Relationships among members in a virtual community tend to focus on information exchange about specific topics. A survey conducted by Pew Internet and The American Life Project in 2001 found those involved in entertainment, professional, and sports virtual-groups focused their activities on obtaining information.\nAn epidemic of bullying and harassment has arisen from the exchange of information between strangers, especially among teenagers, in virtual communities. Despite attempts to implement anti-bullying policies, Sheri Bauman, professor of counselling at the University of Arizona, claims the \"most effective strategies to prevent bullying\" may cost companies revenue.\nVirtual Internet-mediated communities can interact with offline real-life activity, potentially forming strong and tight-knit groups such as QAnon.\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5696", "revid": "217538", "url": "https://en.wikipedia.org/wiki?curid=5696", "title": "Community college", "text": "Educational institution\nA community college is a type of undergraduate higher education institution, generally leading to an associate degree, certificate, or diploma. The term can have different meanings in different countries: many community colleges have an \"open enrollment\" for students who have graduated from high school (also known as senior secondary school or upper secondary school). The term usually refers to a higher educational institution that provides workforce education and college transfer academic programs. Some institutions maintain athletic teams and dormitories similar to their university counterparts.\nAustralia.\nIn Australia, the term \"community college\" refers to small private businesses running short (e.g. 6 weeks) courses generally of a self-improvement or hobbyist nature. Equivalent to the American notion of community colleges are Tertiary and Further Education colleges or TAFEs; these are institutions regulated mostly at state and territory level. There are also an increasing number of private providers colloquially called \"colleges\".\nTAFEs and other providers carry on the tradition of adult education, which was established in Australia around the mid-19th century, when evening classes were held to help adults enhance their numeracy and literacy skills. Most Australian universities can also be traced back to such forerunners, although obtaining a university charter has always changed their nature. In TAFEs and colleges today, courses are designed for personal development of an individual or for employment outcomes. Educational programs cover a variety of topics such as arts, languages, business and lifestyle. They usually are scheduled to run two, three or four days of the week, depending on the level of the course undertaken. A Certificate I may only run for 4 hours twice a week for a term of 9 weeks. A full-time Diploma course might have classes 4 days per week for a year (36 weeks). Some courses may be offered in the evenings or weekends to accommodate people working full-time. Funding for colleges may come from government grants and course fees. Many are not-for-profit organisations. Such TAFES are located in metropolitan, regional and rural locations of Australia.\nEducation offered by TAFEs and colleges has changed over the years. By the 1980s, many colleges had recognised a community need for computer training. Since then thousands of people have increased skills through IT courses. The majority of colleges by the late 20th century had also become Registered Training Organisations. They offer individuals a nurturing, non-traditional education venue to gain skills that better prepare them for the workplace and potential job openings. TAFEs and colleges have not traditionally offered bachelor's degrees, instead providing pathway arrangements with universities to continue towards degrees. The American innovation of the associate degree is being developed at some institutions. Certificate courses I to IV, diplomas and advanced diplomas are typically offered, the latter deemed equivalent to an undergraduate qualification, albeit typically in more vocational areas. Recently, some TAFE institutes (and private providers) have also become higher education providers in their own right and are now starting to offer bachelor's degree programs.\nCanada.\nIn Canada, colleges are adult educational institutions that provide higher education and tertiary education, and grant certificates and diplomas. Alternatively, Canadian colleges are often called \u201cinstitutes\u201d or \u201cpolytechnic institutes\u201d. As well, in Ontario, the 24 colleges of applied arts and technology have been mandated to offer their own stand-alone degrees as well as to offer joint degrees with universities through \"articulation agreements\" that often result in students emerging with both a diploma and a degree. Thus, for example, the University of Guelph \"twins\" with Humber College and York University does the same with Seneca College. More recently, however, colleges have been offering a variety of their own degrees, often in business, technology, science, and other technical fields. Each province has its own educational system, as prescribed by the Canadian federalism model of governance. In the mid-1960s and early 1970s, most Canadian colleges began to provide practical education and training for the emerging and booming generation, and for immigrants from around the world who were entering Canada in increasing numbers at that time. A formative trend was the merging of the then separate vocational training and adult education (night school) institutions.\nCanadian colleges are either publicly funded or private post-secondary institutions (run for profit).\nIn terms of academic pathways, Canadian colleges and universities collaborate with each other with the purpose of providing college students the opportunity to academically upgrade their education. Students can transfer their diplomas and earn transfer credits through their completed college credits towards undergraduate university degrees.\nThe term associate degree is used in western Canada to refer to a two-year college arts or science degree, similar to how the term is used in the United States. In other parts of Canada, the term advanced degree is used to indicate a three- or four-year college program.\nIn Quebec, three years is the norm for a university degree because a year of credit is earned in the C\u00c9GEP (college) system. Even when speaking in English, people often refer to all colleges as C\u00e9geps; however, the term is an acronym more correctly applied specifically to the French-language public system: Coll\u00e8ge d'enseignement g\u00e9n\u00e9ral et professionnel (CEGEP); in English: College of General and Vocational Education. The word \"college\" can also refer to a private high school in Quebec.\nIndia.\nIn India, 98 community colleges are recognized by the University Grants Commission. The courses offered by these colleges are diplomas, advance diplomas and certificate courses. The duration of these courses usually ranges from six months to two years.\nMalaysia.\nCommunity colleges in Malaysia are a network of educational institutions whereby vocational and technical skills training could be provided at all levels for school leavers before they entered the workforce. The community colleges also provide an infrastructure for rural communities to gain skills training through short courses as well as providing access to a post-secondary education.\nAt the moment, most community colleges award qualifications up to Level 3 in the Malaysian Qualifications Framework (Certificate 3) in both the Skills sector (Sijil Kemahiran Malaysia or the Malaysian Skills Certificate) as well as the Vocational and Training sector but the number of community colleges that are starting to award Level 4 qualifications (Diploma) are increasing. This is two levels below a bachelor's degree (Level 6 in the MQF) and students within the system who intend to further their studies to that level will usually seek entry into Advanced Diploma programs in public universities, polytechnics or accredited private providers.\nPhilippines.\nIn the Philippines, a community school functions as elementary or secondary school at daytime and towards the end of the day convert into a community college. This type of institution offers night classes under the supervision of the same principal, and the same faculty members who are given part-time college teaching load.\nThe concept of community college dates back to the time of the former Minister of Education, Culture and Sports (MECS) that had under its wings the Bureaus of Elementary Education, Secondary Education, Higher Education and Vocational-Technical Education. MECS Secretary, Dr. Cecilio Putong, who in 1971 wrote that a community school is a school established in the community, by the community, and for the community itself. Dr. Pedro T. Orata of Pangasinan shared the same idea, hence the establishment of a community college, now called the City College of Urdaneta.\nA community college like the one in Abuyog, Leyte can operate with only a PHP 124,000 annual budget in a two-story structure housing more than 700 students.\nSpain.\nUniversidades populares are not-for-profit organizations established by charitable organizations, cultural groups and municipal councils to facilitate access to continuing education as well as combatting employment discrimination for anyone, but particularly for those who may not have formal access to further education such as refugees, immigrants and senior citizens.\nUnited Kingdom.\nExcept for Scotland, this term is rarely used in the United Kingdom. When it is, a community college is a school which not only provides education for the school-age population (11\u201318) of the locality, but also additional services and education to adults and other members of the community. This education includes but is not limited to sports, adult literacy and lifestyle education. Usually when students finish their secondary school studies at age 16, they move on to a sixth form college where they study for their A-levels (although some secondary schools have integrated sixth forms). After the two-year A-level period, they may proceed to a college of further education or a university. The former is also known as a technical college.\nUnited States.\nIn the United States, community colleges, sometimes called junior colleges, technical colleges, two-year colleges, or city colleges, are primarily public institutions providing tertiary education, also known as continuing education, that focuses on certificates, diplomas, and associate degrees. After graduating from a community college, some students transfer to a liberal arts college or university for two to three years to complete a bachelor's degree.\nBefore the 1970s, community colleges in the United States were more commonly referred to as junior colleges. That term is still used at some institutions. Public community colleges primarily attract and accept students from the local community and are usually supported by local tax revenue. They usually work with local and regional businesses to ensure students are being prepared for the local workforce.\nResearch.\nSome research organizations and publications focus upon the activities of community college, junior college, and technical college institutions. Many of these institutions and organizations present the most current research and practical outcomes at annual community college conferences.\nSeveral peer-reviewed journals extensively publish research on community colleges:\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5697", "revid": "33642442", "url": "https://en.wikipedia.org/wiki?curid=5697", "title": "Civil Rights Memorial", "text": "American memorial in Montgomery, Alabama\nThe Civil Rights Memorial is an American memorial in Montgomery, Alabama, created by Maya Lin. The names of 41 people are inscribed on the granite fountain as martyrs who were killed in the civil rights movement. The memorial is sponsored by the Southern Poverty Law Center.\nDesign.\nThe names included in the memorial belong to those who were killed between 1955 and 1968. Those dates were chosen because in 1956 the U.S. Supreme Court ruled that racial segregation in schools was unlawful and 1968 is the year of the assassination of Martin Luther King Jr. The monument was created by Maya Lin, who is best known for creating the Vietnam Veterans Memorial in Washington, D.C. The Civil Rights Memorial was dedicated in 1989.\nThe concept of Lin's design is based on the soothing and healing effect of water. It was inspired by a passage from King's \"I Have a Dream\" speech \"...we will not be satisfied \"until justice rolls down like waters and righteousness like a mighty stream...\" The quotation in the passage, which is inscribed on the memorial, is a direct paraphrase of , as translated in the American Standard Version of the Bible. The memorial is a fountain in the form of an asymmetric inverted stone cone. A film of water flows over the base of the cone, which contains the 41 names included. It is possible to touch the smooth film of water and to alter it temporarily, which quickly returns to smoothness. As such, the memorial represents the aspirations of the civil rights movement to end legal racial segregation.\nTours and location.\nThe memorial is in downtown Montgomery, at 400 Washington Avenue, in an open plaza in front of the Civil Rights Memorial Center, which was the offices of the Southern Poverty Law Center until it moved across the street into a new building in 2001. The memorial may be visited freely 24 hours a day, 7 days a week.\nThe Civil Rights Memorial Center offers guided group tours, lasting approximately one hour. Tours are available by appointment, Monday to Saturday.\nThe memorial is only a few blocks from other historic sites, including the Dexter Avenue King Memorial Baptist Church, the Alabama State Capitol, the Alabama Department of Archives and History, the corners where Claudette Colvin and Rosa Parks boarded buses in 1955 on which they would later refuse to give up their seats, and the Rosa Parks Library and Museum.\nNames included.\n\"Civil Rights Martyrs\".\nThe 41 names included in the Civil Rights Memorial are those of:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\n\"The Forgotten\".\n\"The Forgotten\" are 74 people who are identified in a display at the Civil Rights Memorial Center. These names were not inscribed on the Memorial because there was insufficient information about their deaths at the time the Memorial was created. However, it is thought that these people were killed as a result of racially motivated violence between 1952 and 1968.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5698", "revid": "39368017", "url": "https://en.wikipedia.org/wiki?curid=5698", "title": "Charles Babbage", "text": "English mathematician, philosopher, and engineer (1791\u20131871)\nCharles Babbage (; 26 December 1791\u00a0\u2013 18 October 1871) was an English polymath. A mathematician, philosopher, inventor and mechanical engineer, Babbage originated the concept of a digital programmable computer.\nBabbage is considered by some to be \"father of the computer\". Babbage is credited with inventing the first mechanical computer, the Difference Engine, that eventually led to more complex electronic designs, though all the essential ideas of modern computers are to be found in Babbage's Analytical Engine, programmed using a principle openly borrowed from the Jacquard loom. Babbage had a broad range of interests in addition to his work on computers covered in his book \"Economy of Manufactures and Machinery\". His varied work in other fields has led him to be described as \"pre-eminent\" among the many polymaths of his century.\nBabbage, who died before the complete successful engineering of many of his designs, including his Difference Engine and Analytical Engine, remained a prominent figure in the ideating of computing. Parts of Babbage's incomplete mechanisms are on display in the Science Museum in London. In 1991, a functioning difference engine was constructed from Babbage's original plans. Built to tolerances achievable in the 19th century, the success of the finished engine indicated that Babbage's machine would have worked.\nEarly life.\nBabbage's birthplace is disputed, but according to the \"Oxford Dictionary of National Biography\" he was most likely born at 44 Crosby Row, Walworth Road, London, England. A blue plaque on the junction of Larcom Street and Walworth Road commemorates the event.\nHis date of birth was given in his obituary in \"The Times\" as 26 December 1792; but then a nephew wrote to say that Babbage was born one year earlier, in 1791. The parish register of St. Mary's, Newington, London, shows that Babbage was baptised on 6 January 1792, supporting a birth year of 1791.\nBabbage was one of four children of Benjamin Babbage and Betsy Plumleigh Teape. His father was a banking partner of William Praed in founding Praed's &amp; Co. of Fleet Street, London, in 1801. In 1808, the Babbage family moved into the old Rowdens house in East Teignmouth. Around the age of eight, Babbage was sent to a country school in Alphington near Exeter to recover from a life-threatening fever. For a short time, he attended King Edward VI Grammar School in Totnes, South Devon, but his health forced him back to private tutors for a time.\nBabbage then joined the 30-student Holmwood Academy, in Baker Street, Enfield, Middlesex, under the Reverend Stephen Freeman. The academy had a library that prompted Babbage's love of mathematics. He studied with two more private tutors after leaving the academy. The first was a clergyman near Cambridge; through him Babbage encountered Charles Simeon and his evangelical followers, but the tuition was not what he needed. He was brought home, to study at the Totnes school: this was at age 16 or 17. The second was an Oxford tutor, under whom Babbage reached a level in Classics sufficient to be accepted by the University of Cambridge.\nAt the University of Cambridge.\nBabbage arrived at Trinity College, Cambridge, in October 1810. He was already self-taught in some parts of contemporary mathematics; he had read Robert Woodhouse, Joseph Louis Lagrange, and Marie Agnesi. As a result, he was disappointed in the standard mathematical instruction available at the university.\nBabbage, John Herschel, George Peacock, and several other friends formed the Analytical Society in 1812; they were also close to Edward Ryan. As a student, Babbage was also a member of other societies such as The Ghost Club, concerned with investigating supernatural phenomena, and the Extractors Club, dedicated to liberating its members from the madhouse, should any be committed to one.\nIn 1812, Babbage transferred to Peterhouse, Cambridge. He was the top mathematician there, but did not graduate with honours. He instead received a degree without examination in 1814. He had defended a thesis that was considered blasphemous in the preliminary public disputation, but it is not known whether this fact is related to his not sitting the examination.\nAfter Cambridge.\nConsidering his reputation, Babbage quickly made progress. He lectured to the Royal Institution on astronomy in 1815, and was elected a Fellow of the Royal Society in 1816. After graduation, on the other hand, he applied for positions unsuccessfully, and had little in the way of a career. In 1816 he was a candidate for a teaching job at Haileybury College; he had recommendations from James Ivory and John Playfair, but lost out to Henry Walter. In 1819, Babbage and Herschel visited Paris and the Society of Arcueil, meeting leading French mathematicians and physicists. That year Babbage applied to be professor at the University of Edinburgh, with the recommendation of Pierre Simon Laplace; the post went to William Wallace.\nWith Herschel, Babbage worked on the electrodynamics of Arago's rotations, publishing in 1825. Their explanations were only transitional, being picked up and broadened by Michael Faraday. The phenomena are now part of the theory of eddy currents, and Babbage and Herschel missed some of the clues to unification of electromagnetic theory, staying close to Amp\u00e8re's force law.\nBabbage purchased the actuarial tables of George Barrett, who died in 1821 leaving unpublished work, and surveyed the field in 1826 in \"Comparative View of the Various Institutions for the Assurance of Lives\". This interest followed a project to set up an insurance company, prompted by Francis Baily and mooted in 1824, but not carried out. Babbage did calculate actuarial tables for that scheme, using Equitable Society mortality data from 1762 onwards.\nDuring this whole period, Babbage depended awkwardly on his father's support, given his father's attitude to his early marriage, of 1814: he and Edward Ryan wedded the Whitmore sisters. He made a home in Marylebone in London and established a large family. On his father's death in 1827, Babbage inherited a large estate (value around \u00a3100,000, equivalent to \u00a3 or $ today), making him independently wealthy. After his wife's death in the same year he spent time travelling. In Italy he met Leopold II, Grand Duke of Tuscany, foreshadowing a later visit to Piedmont. In April 1828 he was in Rome, and relying on Herschel to manage the difference engine project, when he heard that he had become a professor at Cambridge, a position he had three times failed to obtain (in 1820, 1823 and 1826).\nRoyal Astronomical Society.\nBabbage was instrumental in founding the Royal Astronomical Society in 1820, initially known as the Astronomical Society of London. Its original aims were to reduce astronomical calculations to a more standard form, and to circulate data. These directions were closely connected with Babbage's ideas on computation, and in 1824 he won its Gold Medal, cited \"for his invention of an engine for calculating mathematical and astronomical tables\".\nBabbage's motivation to overcome errors in tables by mechanisation had been a commonplace since Dionysius Lardner wrote about it in 1834 in the \"Edinburgh Review\" (under Babbage's guidance). The context of these developments is still debated. Babbage's own account of the origin of the difference engine begins with the Astronomical Society's wish to improve \"The Nautical Almanac\". Babbage and Herschel were asked to oversee a trial project, to recalculate some part of those tables. With the results to hand, discrepancies were found. This was in 1821 or 1822, and was the occasion on which Babbage formulated his idea for mechanical computation. The issue of the \"Nautical Almanac\" is now described as a legacy of a polarisation in British science caused by attitudes to Sir Joseph Banks, who had died in 1820.\nBabbage studied the requirements to establish a modern postal system, with his friend Thomas Frederick Colby, concluding there should be a uniform rate that was put into effect with the introduction of the Uniform Fourpenny Post supplanted by the Uniform Penny Post in 1839 and 1840. Colby was another of the founding group of the Society. He was also in charge of the Survey of Ireland. Herschel and Babbage were present at a celebrated operation of that survey, the remeasuring of the Lough Foyle baseline.\nBritish Lagrangian School.\nThe Analytical Society had initially been no more than an undergraduate provocation. During this period it had some more substantial achievements. In 1816 Babbage, Herschel and Peacock published a translation from French of the lectures of Sylvestre Lacroix, which was then the state-of-the-art calculus textbook.\nReference to Lagrange in calculus terms marks out the application of what are now called formal power series. British mathematicians had used them from about 1730 to 1760. As re-introduced, they were not simply applied as notations in differential calculus. They opened up the fields of functional equations (including the difference equations fundamental to the difference engine) and operator (D-module) methods for differential equations. The analogy of difference and differential equations was notationally changing \u0394 to D, as a \"finite\" difference becomes \"infinitesimal\". These symbolic directions became popular, as operational calculus, and pushed to the point of diminishing returns. The Cauchy concept of limit was kept at bay. Woodhouse had already founded this second \"British Lagrangian School\" with its treatment of Taylor series as formal.\nIn this context function composition is complicated to express, because the chain rule is not simply applied to second and higher derivatives. This matter was known to Woodhouse by 1803, who took from Louis Fran\u00e7ois Antoine Arbogast what is now called Fa\u00e0 di Bruno's formula. In essence it was known to Abraham De Moivre (1697). Herschel found the method impressive, Babbage knew of it, and it was later noted by Ada Lovelace as compatible with the analytical engine. In the period to 1820 Babbage worked intensively on functional equations in general, and resisted both conventional finite differences and Arbogast's approach (in which \u0394 and D were related by the simple additive case of the exponential map). But via Herschel he was influenced by Arbogast's ideas in the matter of iteration, i.e. composing a function with itself, possibly many times. Writing in a major paper on functional equations in the \"Philosophical Transactions\" (1815/6), Babbage said his starting point was work of Gaspard Monge.\nAcademic.\nFrom 1828 to 1839, Babbage was Lucasian Professor of Mathematics at Cambridge. Not a conventional resident don, and inattentive to his teaching responsibilities, he wrote three topical books during this period of his life. He was elected a Foreign Honorary Member of the American Academy of Arts and Sciences in 1832. Babbage was out of sympathy with colleagues: George Biddell Airy, his predecessor as Lucasian Professor of Mathematics at Trinity College, Cambridge, thought an issue should be made of his lack of interest in lecturing. Babbage planned to lecture in 1831 on political economy. Babbage's reforming direction looked to see university education more inclusive, universities doing more for research, a broader syllabus and more interest in applications; but William Whewell found the programme unacceptable. A controversy Babbage had with Richard Jones lasted for six years. He never did give a lecture.\nIt was during this period that Babbage tried to enter politics. Simon Schaffer writes that his views of the 1830s included disestablishment of the Church of England, a broader political franchise, and inclusion of manufacturers as stakeholders. He twice stood for Parliament as a candidate for the borough of Finsbury. In 1832 he came in third among five candidates, missing out by some 500 votes in the two-member constituency when two other reformist candidates, Thomas Wakley and Christopher Temple, split the vote. In his memoirs Babbage related how this election brought him the friendship of Samuel Rogers: his brother Henry Rogers wished to support Babbage again, but died within days. In 1834 Babbage finished last among four. In 1832, Babbage, Herschel and Ivory were appointed Knights of the Royal Guelphic Order, however they were not subsequently made knights bachelor to entitle them to the prefix \"Sir\", which often came with appointments to that foreign order (though Herschel was later created a baronet).\n\"Declinarians\", learned societies and the BAAS.\nBabbage now emerged as a polemicist. One of his biographers notes that all his books contain a \"campaigning element\". His \"Reflections on the Decline of Science and some of its Causes\" (1830) stands out, however, for its sharp attacks. It aimed to improve British science, and more particularly to oust Davies Gilbert as President of the Royal Society, which Babbage wished to reform. It was written out of pique, when Babbage hoped to become the junior secretary of the Royal Society, as Herschel was the senior, but failed because of his antagonism to Humphry Davy. Michael Faraday had a reply written, by Gerrit Moll, as \"On the Alleged Decline of Science in England\" (1831). On the front of the Royal Society Babbage had no impact, with the bland election of the Duke of Sussex to succeed Gilbert the same year. As a broad manifesto, on the other hand, his \"Decline\" led promptly to the formation in 1831 of the British Association for the Advancement of Science (BAAS).\nThe \"Mechanics' Magazine\" in 1831 identified as Declinarians the followers of Babbage. In an unsympathetic tone it pointed out David Brewster writing in the \"Quarterly Review\" as another leader; with the barb that both Babbage and Brewster had received public money.\nIn the debate of the period on statistics (\"qua\" data collection) and what is now statistical inference, the BAAS in its Statistical Section (which owed something also to Whewell) opted for data collection. This Section was the sixth, established in 1833 with Babbage as chairman and John Elliot Drinkwater as secretary. The foundation of the Statistical Society followed. Babbage was its public face, backed by Richard Jones and Robert Malthus.\n\"On the Economy of Machinery and Manufactures\".\nBabbage published \"On the Economy of Machinery and Manufactures\" (1832), on the organisation of industrial production. It was an influential early work of operational research. John Rennie the Younger in addressing the Institution of Civil Engineers on manufacturing in 1846 mentioned mostly surveys in encyclopaedias, and Babbage's book was first an article in the \"Encyclop\u00e6dia Metropolitana\", the form in which Rennie noted it, in the company of related works by John Farey Jr., Peter Barlow and Andrew Ure. From \"An essay on the general principles which regulate the application of machinery to manufactures and the mechanical arts\" (1827), which became the \"Encyclop\u00e6dia Metropolitana\" article of 1829, Babbage developed the schematic classification of machines that, combined with discussion of factories, made up the first part of the book. The second part considered the \"domestic and political economy\" of manufactures.\nThe book sold well, and quickly went to a fourth edition (1836). Babbage represented his work as largely a result of actual observations in factories, British and abroad. It was not, in its first edition, intended to address deeper questions of political economy; the second (late 1832) did, with three further chapters including one on piece rate. The book also contained ideas on rational design in factories, and profit sharing.\n\"Babbage principle\".\nIn \"Economy of Machinery\" was described what is now called the \"Babbage principle\". It pointed out commercial advantages available with more careful division of labour. As Babbage himself noted, it had already appeared in the work of Melchiorre Gioia in 1815. The term was introduced in 1974 by Harry Braverman. Related formulations are the \"principle of multiples\" of Philip Sargant Florence, and the \"balance of processes\".\nWhat Babbage remarked is that skilled workers typically spend parts of their time performing tasks that are below their skill level. If the labour process can be divided among several workers, labour costs may be cut by assigning only high-skill tasks to high-cost workers, restricting other tasks to lower-paid workers. He also pointed out that training or apprenticeship can be taken as fixed costs; but that returns to scale are available by his approach of standardisation of tasks, therefore again favouring the factory system. His view of human capital was restricted to minimising the time period for recovery of training costs.\nPublishing.\nAnother aspect of the work was its detailed breakdown of the cost structure of book publishing. Babbage took the unpopular line, from the publishers' perspective, of exposing the trade's profitability. He went as far as to name the organisers of the trade's restrictive practices. Twenty years later he attended a meeting hosted by John Chapman to campaign against the Booksellers Association, still a cartel.\nInfluence.\nIt has been written that \"what Arthur Young was to agriculture, Charles Babbage was to the factory visit and machinery\". Babbage's theories are said to have influenced the layout of the 1851 Great Exhibition, and his views had a strong effect on his contemporary George Julius Poulett Scrope. Karl Marx argued that the source of the productivity of the factory system was exactly the combination of the division of labour with machinery, building on Adam Smith, Babbage and Ure. Where Marx picked up on Babbage and disagreed with Smith was on the motivation for division of labour by the manufacturer: as Babbage did, he wrote that it was for the sake of profitability, rather than productivity, and identified an impact on the concept of a trade.\nJohn Ruskin went further, to oppose completely what manufacturing in Babbage's sense stood for. Babbage also affected the economic thinking of John Stuart Mill. George Holyoake saw Babbage's detailed discussion of profit sharing as substantive, in the tradition of Robert Owen and Charles Fourier, if requiring the attentions of a benevolent captain of industry, and ignored at the time.\nWorks by Babbage and Ure were published in French translation in 1830; \"On the Economy of Machinery\" was translated in 1833 into French by \u00c9douard Biot, and into German the same year by Gottfried Friedenberg. The French engineer and writer on industrial organisation L\u00e9on Lalanne was influenced by Babbage, but also by the economist Claude Lucien Bergery, in reducing the issues to \"technology\". William Jevons connected Babbage's \"economy of labour\" with his own labour experiments of 1870. The Babbage principle is an inherent assumption in Frederick Winslow Taylor's scientific management.\nMary Everest Boole claimed that there was profound influence \u2013 via her uncle George Everest \u2013 of Indian thought in general and Indian logic, in particular, on Babbage and on her husband George Boole, as well as on Augustus De Morgan:\nThink what must have been the effect of the intense Hinduizing of three such men as Babbage, De Morgan, and George Boole on the mathematical atmosphere of 1830\u201365. What share had it in generating the Vector Analysis and the mathematics by which investigations in physical science are now conducted? \nNatural theology.\nIn 1837, responding to the series of eight \"Bridgewater Treatises\", Babbage published his \"Ninth Bridgewater Treatise\", under the title \"On the Power, Wisdom and Goodness of God, as manifested in the Creation\". In this work Babbage weighed in on the side of uniformitarianism in a current debate. He preferred the conception of creation in which a God-given natural law dominated, removing the need for continuous \"contrivance\".\nThe book is a work of natural theology, and incorporates extracts from related correspondence of Herschel with Charles Lyell. Babbage put forward the thesis that God had the omnipotence and foresight to create as a divine legislator. In this book, Babbage dealt with relating interpretations between science and religion; on the one hand, he insisted that \"there exists no fatal collision between the words of Scripture and the facts of nature;\" on the other hand, he wrote that the Book of Genesis was not meant to be read literally in relation to scientific terms. Against those who said these were in conflict, he wrote \"that the contradiction they have imagined can have no real existence, and that whilst the testimony of Moses remains unimpeached, we may also be permitted to confide in the testimony of our senses.\"\nThe \"Ninth Bridgewater Treatise\" was quoted extensively in \"Vestiges of the Natural History of Creation\". The parallel with Babbage's computing machines is made explicit, as allowing plausibility to the theory that transmutation of species could be pre-programmed.\nJonar Ganeri, author of \"Indian Logic\", believes Babbage may have been influenced by Indian thought; one possible route would be through Henry Thomas Colebrooke. Mary Everest Boole argues that Babbage was introduced to Indian thought in the 1820s by her uncle George Everest:\nSome time about 1825, [Everest] came to England for two or three years, and made a fast and lifelong friendship with Herschel and with Babbage, who was then quite young. I would ask any fair-minded mathematician to read Babbage's Ninth Bridgewater Treatise and compare it with the works of his contemporaries in England; and then ask himself whence came the peculiar conception of the nature of miracle which underlies Babbage's ideas of Singular Points on Curves (Chap, viii) \u2013 from European Theology or Hindu Metaphysic? Oh! how the English clergy of that day hated Babbage's book!\nReligious views.\nBabbage was raised in the Protestant form of the Christian faith, his family having inculcated in him an orthodox form of worship. He explained:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;My excellent mother taught me the usual forms of my daily and nightly prayer; and neither in my father nor my mother was there any mixture of bigotry and intolerance on the one hand, nor on the other of that unbecoming and familiar mode of addressing the Almighty which afterwards so much disgusted me in my youthful years.\nRejecting the Athanasian Creed as a \"direct contradiction in terms\", in his youth he looked to Samuel Clarke's works on religion, of which \"Being and Attributes of God\" (1704) exerted a particularly strong influence on him. Later in life, Babbage concluded that \"the true value of the Christian religion rested, not on speculative [theology] \u2026 but \u2026 upon those doctrines of kindness and benevolence which that religion claims and enforces, not merely in favour of man himself but of every creature susceptible of pain or of happiness.\"\nIn his autobiography (1864), Babbage wrote a whole chapter on the topic of religion, where he identified three sources of divine knowledge:\nHe stated, on the basis of the design argument, that studying the works of nature had been the more appealing evidence, and the one which led him to actively profess the existence of God. Advocating for natural theology, he wrote:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the works of the Creator ever open to our examination, we possess a firm basis on which to raise the superstructure of an enlightened creed. The more man inquires into the laws which regulate the material universe, the more he is convinced that all its varied forms arise from the action of a few simple principles ... The works of the Creator, ever present to our senses, give a living and perpetual testimony of his power and goodness far surpassing any evidence transmitted through human testimony. The testimony of man becomes fainter at every stage of transmission, whilst each new inquiry into the works of the Almighty gives to us more exalted views of his wisdom, his goodness, and his power.\nLike Samuel Vince, Babbage also wrote a defence of the belief in divine miracles. Against objections previously posed by David Hume, Babbage advocated for the belief of divine agency, stating \"we must not measure the credibility or incredibility of an event by the narrow sphere of our own experience, nor forget that there is a Divine energy which overrides what we familiarly call the laws of nature.\" He alluded to the limits of human experience, expressing: \"all that we see in a miracle is an effect which is new to our observation, and whose cause is concealed. The cause may be beyond the sphere of our observation, and would be thus beyond the familiar sphere of nature; but this does not make the event a violation of any law of nature. The limits of man's observation lie within very narrow boundaries, and it would be arrogance to suppose that the reach of man's power is to form the limits of the natural world.\"\nLater life.\nThe British Association was consciously modelled on the Deutsche Naturforscher-Versammlung, founded in 1822. It rejected romantic science as well as metaphysics, and started to entrench the divisions of science from literature, and professionals from amateurs. Belonging as he did to the \"Wattite\" faction in the BAAS, represented in particular by James Watt the younger, Babbage identified closely with industrialists. He wanted to go faster in the same directions, and had little time for the more gentlemanly component of its membership. Indeed, he subscribed to a version of conjectural history that placed industrial society as the culmination of human development (and shared this view with Herschel). A clash with Roderick Murchison led in 1838 to his withdrawal from further involvement. At the end of the same year he sent in his resignation as Lucasian professor, walking away also from the Cambridge struggle with Whewell. His interests became more focussed, on computation and metrology, and on international contacts.\nMetrology programme.\nA project announced by Babbage was to tabulate all physical constants (referred to as \"constants of nature\", a phrase in itself a neologism), and then to compile an encyclopaedic work of numerical information. He was a pioneer in the field of \"absolute measurement\". His ideas followed on from those of Johann Christian Poggendorff, and were mentioned to Brewster in 1832. There were to be 19 categories of constants, and Ian Hacking sees these as reflecting in part Babbage's \"eccentric enthusiasms\". Babbage's paper \"On Tables of the Constants of Nature and Art\" was reprinted by the Smithsonian Institution in 1856, with an added note that the physical tables of Arnold Henry Guyot \"will form a part of the important work proposed in this article\".\nExact measurement was also key to the development of machine tools. Here again Babbage is considered a pioneer, with Henry Maudslay, William Sellers, and Joseph Whitworth.\nEngineer and inventor.\nThrough the Royal Society Babbage acquired the friendship of the engineer Marc Brunel. It was through Brunel that Babbage knew of Joseph Clement, and so came to encounter the artisans whom he observed in his work on manufactures. Babbage provided an introduction for Isambard Kingdom Brunel in 1830, for a contact with the proposed Bristol &amp; Birmingham Railway. He carried out studies, around 1838, to show the superiority of the broad gauge for railways, used by Brunel's Great Western Railway.\nIn 1838, Babbage invented the pilot (also called a cow-catcher), the metal frame attached to the front of locomotives that clears the tracks of obstacles; he also constructed a dynamometer car. His eldest son, Benjamin Herschel Babbage, worked as an engineer for Brunel on the railways before emigrating to Australia in the 1850s.\nBabbage also invented an ophthalmoscope, which he gave to Thomas Wharton Jones for testing. Jones, however, ignored it. The device only came into use after being independently invented by Hermann von Helmholtz.\nCryptography.\nBabbage achieved notable results in cryptography, though this was still not known a century after his death. Letter frequency was category 18 of Babbage's tabulation project. Joseph Henry later defended interest in it, in the absence of the facts, as relevant to the management of movable type.\nAs early as 1845, Babbage had solved a cipher that had been posed as a challenge by his nephew Henry Hollier, and in the process, he made a discovery about ciphers that were based on Vigen\u00e8re tables. Specifically, he realised that enciphering plain text with a keyword rendered the cipher text subject to modular arithmetic. During the Crimean War of the 1850s, Babbage broke Vigen\u00e8re's autokey cipher as well as the much weaker cipher that is called Vigen\u00e8re cipher today. His discovery was kept a military secret, and was not published. Credit for the result was instead given to Friedrich Kasiski, a Prussian infantry officer, who made the same discovery some years later. However, in 1854, Babbage published the solution of a Vigen\u00e8re cipher, which had been published previously in the \"Journal of the Society of Arts\". In 1855, Babbage also published a short letter, \"Cypher Writing\", in the same journal. Nevertheless, his priority was not established until 1985.\nPublic nuisances.\nBabbage involved himself in well-publicised but unpopular campaigns against public nuisances. He once counted all the broken panes of glass of a factory, publishing in 1857 a \"Table of the Relative Frequency of the Causes of Breakage of Plate Glass Windows\": Of 464 broken panes, 14 were caused by \"drunken men, women or boys\".\nBabbage's distaste for commoners (the Mob) included writing \"Observations of Street Nuisances\" in 1864, as well as tallying up 165 \"nuisances\" over a period of 80 days. He especially hated street music, and in particular the music of organ grinders, against whom he railed in various venues. The following quotation is typical:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;It is difficult to estimate the misery inflicted upon thousands of persons, and the absolute pecuniary penalty imposed upon multitudes of intellectual workers by the loss of their time, destroyed by organ-grinders and other similar nuisances.\nBabbage was not alone in his campaign. A convert to the cause was the MP Michael Thomas Bass.\nIn the 1860s, Babbage also took up the anti-hoop-rolling campaign. He blamed hoop-rolling boys for driving their iron hoops under horses' legs, with the result that the rider is thrown and very often the horse breaks a leg. Babbage achieved a certain notoriety in this matter, being denounced in debate in Commons in 1864 for \"commencing a crusade against the popular game of tip-cat and the trundling of hoops.\"\nComputing pioneer.\nBabbage's machines were among the first mechanical computers. That they were not actually completed was largely because of funding problems and clashes of personality, most notably with George Biddell Airy, the Astronomer Royal.\nBabbage directed the building of some steam-powered machines that achieved some modest success, suggesting that calculations could be mechanised. For more than ten years he received government funding for his project, which amounted to \u00a317,000, but eventually the Treasury lost confidence in him.\nWhile Babbage's machines were mechanical and unwieldy, their basic architecture was similar to that of a modern computer. The data and program memory were separated, operation was instruction-based, the control unit could make conditional jumps, and the machine had a separate I/O unit.\nBackground on mathematical tables.\nIn Babbage's time, printed mathematical tables were calculated by human computers; in other words, by hand. They were central to navigation, science and engineering, as well as mathematics. Mistakes were known to occur in transcription as well as calculation.\nAt Cambridge, Babbage saw the fallibility of this process, and the opportunity of adding mechanisation into its management. His own account of his path towards mechanical computation references a particular occasion: &lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In 1812 he was sitting in his rooms in the Analytical Society looking at a table of logarithms, which he knew to be full of mistakes, when the idea occurred to him of computing all tabular functions by machinery. The French government had produced several tables by a new method. Three or four of their mathematicians decided how to compute the tables, half a dozen more broke down the operations into simple stages, and the work itself, which was restricted to addition and subtraction, was done by eighty computers who knew only these two arithmetical processes. Here, for the first time, mass production was applied to arithmetic, and Babbage was seized by the idea that the labours of the unskilled computers [people] could be taken over completely by machinery which would be quicker and more reliable.\nThere was another period, seven years later, when his interest was aroused by the issues around computation of mathematical tables. The French official initiative by Gaspard de Prony, and its problems of implementation, were familiar to him. After the Napoleonic Wars came to a close, scientific contacts were renewed on the level of personal contact: in 1819 Charles Blagden was in Paris looking into the printing of the stalled de Prony project, and lobbying for the support of the Royal Society. In works of the 1820s and 1830s, Babbage referred in detail to de Prony's project.\nDifference engine.\nBabbage began in 1822 with what he called the difference engine, made to compute values of polynomial functions. It was created to calculate a series of values automatically. By using the method of finite differences, it was possible to avoid the need for multiplication and division.\nFor a prototype difference engine, Babbage brought in Joseph Clement to implement the design, in 1823. Clement worked to high standards, but his machine tools were particularly elaborate. Under the standard terms of business of the time, he could charge for their construction, and would also own them. He and Babbage fell out over costs around 1831.\nSome parts of the prototype survive in the Museum of the History of Science, Oxford. This prototype evolved into the \"first difference engine\". It remained unfinished and the finished portion is located at the Science Museum in London. This first difference engine would have been composed of around 25,000 parts, weighed , and would have been tall. Although Babbage received ample funding for the project, it was never completed. He later (1847\u20131849) produced detailed drawings for an improved version,\"Difference Engine No. 2\", but did not receive funding from the British government. His design was finally constructed in 1989\u20131991, using his plans and 19th-century manufacturing tolerances. It performed its first calculation at the Science Museum, London, returning results to 31 digits.\nNine years later, in 2000, the Science Museum completed the printer Babbage had designed for the difference engine.\nCompleted models.\nThe Science Museum has constructed two Difference Engines according to Babbage's plans for the Difference Engine No 2. One is owned by the museum. The other, owned by the technology multimillionaire Nathan Myhrvold, went on exhibition at the Computer History Museum in Mountain View, California on 10 May 2008. The two models that have been constructed are not replicas.\nAnalytical Engine.\nAfter the attempt at making the first difference engine fell through, Babbage worked to design a more complex machine called the Analytical Engine. He hired C. G. Jarvis, who had previously worked for Clement as a draughtsman. The Analytical Engine marks the transition from mechanised arithmetic to fully-fledged general purpose computation. It is largely on it that Babbage's standing as computer pioneer rests.\nThe major innovation was that the Analytical Engine was to be programmed using punched cards: the Engine was intended to use loops of Jacquard's punched cards to control a mechanical calculator, which could use as input the results of preceding computations. The machine was also intended to employ several features subsequently used in modern computers, including sequential control, branching and looping. It would have been the first mechanical device to be, in principle, Turing-complete. The Engine was not a single physical machine, but rather a succession of designs that Babbage tinkered with until his death in 1871.\nAda Lovelace and Italian followers.\nAda Lovelace, who corresponded with Babbage during his development of the Analytical Engine, is credited with developing an algorithm that would enable the Engine to calculate a sequence of Bernoulli numbers. Despite documentary evidence in Lovelace's own handwriting, some scholars dispute to what extent the ideas were Lovelace's own. For this achievement, she is often described as the first computer programmer; though no programming language had yet been invented.\nLovelace also translated and wrote literature supporting the project. Describing the engine's programming by punch cards, she wrote: \"We may say most aptly that the Analytical Engine weaves algebraical patterns just as the Jacquard loom weaves flowers and leaves.\"\nBabbage visited Turin in 1840 at the invitation of Giovanni Plana, who had developed in 1831 an analog computing machine that served as a perpetual calendar. Here in 1840 in Turin, Babbage gave the only public explanation and lectures about the Analytical Engine. In 1842 Charles Wheatstone approached Lovelace to translate a paper of Luigi Menabrea, who had taken notes of Babbage's Turin talks; and Babbage asked her to add something of her own. Fortunato Prandi who acted as interpreter in Turin was an Italian exile and follower of Giuseppe Mazzini.\nSwedish followers.\nPer Georg Scheutz wrote about the difference engine in 1830, and experimented in automated computation. After 1834 and Lardner's \"Edinburgh Review\" article he set up a project of his own, doubting whether Babbage's initial plan could be carried out. This he pushed through with his son, Edvard Scheutz. Another Swedish engine was that of Martin Wiberg (1860).\nLegacy.\nIn 2011, researchers in Britain proposed a multimillion-pound project, \"Plan 28\", to construct Babbage's Analytical Engine. Since Babbage's plans were continually being refined and were never completed, they intended to engage the public in the project and crowd-source the analysis of what should be built. It would have the equivalent of 675 bytes of memory, and run at a clock speed of about 7\u00a0Hz. They hoped to complete it by the 150th anniversary of Babbage's death, in 2021.\nAdvances in MEMS and nanotechnology have led to recent high-tech experiments in mechanical computation. The benefits suggested include operation in high radiation or high temperature environments. These modern versions of mechanical computation were highlighted in \"The Economist\" in its special \"end of the millennium\" black cover issue in an article entitled \"Babbage's Last Laugh\".\nDue to his association with the town Babbage was chosen in 2007 to appear on the 5 Totnes pound note. An image of Babbage features in the British cultural icons section of the newly designed British passport in 2015.\nFamily.\nOn 25 July 1814, Babbage married Georgiana Whitmore, sister of British parliamentarian William Wolryche-Whitmore, at St. Michael's Church in Teignmouth, Devon. The couple lived at Dudmaston Hall, Shropshire (where Babbage engineered the central heating system), before moving to 5 Devonshire Street, London in 1815.\nCharles and Georgiana had eight children, but only four \u2013 Benjamin Herschel, Georgiana Whitmore, Dugald Bromhead and Henry Prevost \u2013 survived childhood. Charles' wife Georgiana died in Worcester on 1 September 1827, the same year as his father, their second son (also named Charles) and their newborn son Alexander.\nHis youngest surviving son, Henry Prevost Babbage (1824\u20131918), went on to create six small demonstration pieces for Difference Engine No. 1 based on his father's designs, one of which was sent to Harvard University where it was later discovered by Howard H. Aiken, pioneer of the Harvard Mark I. Henry Prevost's 1910 Analytical Engine Mill, previously on display at Dudmaston Hall, is now on display at the Science Museum.\nDeath.\nBabbage lived and worked for over 40 years at 1 Dorset Street, Marylebone, where he died, at the age of 79, on 18 October 1871; he was buried in London's Kensal Green Cemetery. According to Horsley, Babbage died \"of renal inadequacy, secondary to cystitis.\" He had declined both a knighthood and baronetcy. He also argued against hereditary peerages, favouring life peerages instead.\nAutopsy report.\nIn 1983, the autopsy report for Charles Babbage was discovered and later published by his great-great-grandson. A copy of the original is also available. Half of Babbage's brain is preserved at the Hunterian Museum in the Royal College of Surgeons in London. The other half of Babbage's brain is on display in the Science Museum, London.\nMemorials.\nThere is a black plaque commemorating the 40 years Babbage spent at 1 Dorset Street, London. Locations, institutions and other things named after Babbage include:\nIn fiction and film.\nBabbage frequently appears in steampunk works; he has been called an iconic figure of the genre. Other works in which Babbage appears include:\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5700", "revid": "39524300", "url": "https://en.wikipedia.org/wiki?curid=5700", "title": "Cross-dressing", "text": "Practice of dressing like a different gender\nCross-dressing is the act of wearing clothes traditionally or stereotypically associated with a different gender. From as early as pre-modern history, cross-dressing has been practiced in order to disguise, comfort, entertain, and express oneself.\nAlmost every human society throughout history has had expected norms for each gender relating to style, color, or type of clothing they are expected to wear, and likewise most societies have had a set of guidelines, views or even laws defining what type of clothing is appropriate for each gender. Therefore, cross-dressing allows individuals to express themselves by acting beyond guidelines, views, or even laws defining what type of clothing is expected and appropriate for each gender.\nThe term \"cross-dressing\" refers to an action or a behavior, without attributing or implying any specific causes or motives for that behavior. Cross-dressing is not synonymous with being transgender.\nTerminology.\nThe phenomenon of cross-dressing is seen throughout recorded history, being referred to as far back as the Hebrew Bible. The terms used to describe it have changed throughout history; the Anglo-Saxon-rooted term \"cross-dresser\" is viewed more favorably than the Latin-origin term \"transvestite\" in some circles, where it has come to be seen as outdated and derogatory. Its first mention originated in Magnus Hirschfeld's \"Die Transvestiten\" (The Transvestites) in 1910, originally associating cross-dressing with non-heterosexual behavior or derivations of sexual intent. Its connotations largely changed in the 20th century as its use was more frequently associated with sexual excitement, otherwise known as transvestic disorder. This term was historically used to diagnose psychiatric disorders (e.g. transvestic fetishism), but the former (cross-dressing) was coined by the transgender community. The Oxford English Dictionary gives 1911 as the earliest citation of the term \"cross-dressing\", by Edward Carpenter: \"Cross-dressing must be taken as a general indication of, and a cognate phenomenon to, homosexuality\". In 1928, Havelock Ellis used the two terms \"cross-dressing\" and \"transvestism\" interchangeably. The earliest citations for \"cross-dress\" and \"cross-dresser\" are 1966 and 1976, respectively.\nHistory.\nNon-Western History.\nCross-dressing has been practiced throughout much of recorded history, in many societies, and for many reasons. Examples exist in Greek, Norse, and Hindu mythology. Cross-dressing can be found in theater and religion, such as kabuki, Noh, and Korean shamanism, as well as in folklore, literature, and music. For instance, in examining kabuki culture during Japan's edo period, cross-dressing was not only used for theater purposes but also because current societal trends: cross-dressing and the switching of genders was a familiar concept to the Japanese at the time which allowed them to interchange characters's genders easily and incorporate geisha fashion into men's wear. This was especially common in the story-telling of ancient stories such as the character Benten from Benten Koz\u014d. He was a thief in the play cross-dressing as a woman. Cross-dressing was also exhibited in Japanese Noh for similar reasons. Societal standards at the time broke boundaries between gender. For example, ancient Japanese portraits of aristocrats have no clear differentiation in characteristics between male and female beauty. Thus, in Noh performance, the cross-dressing of actors was common; especially given the ease of disguising biological sex with the use of masks and heavy robes. In a non-entertainment context, cross-dressing is also exhibited in Korean shamanism for religious purposes. Specifically, this is displayed in chaesu-gut, a shamanistic rite gut in which a shaman offers a sacrifice to the spirits to intermediate in the fortunes of the intended humans for the gut. Here, cross-dressing serves many purposes. Firstly, the shaman (typically a woman) would cross-dress as both male and female spirits can occupy her. This allows her to represent the opposite sex and become a cross-sex icon in 75% of the time of the ritual. This also allows her to become a sexually liminal being. It is clear that in entertainment, literature, art, and religion, different civilizations have utilized cross-dressing for many different purposes.\nWestern History.\nIn the British and European context, theatrical troupes (\"playing companies\") were all-male, with the female parts undertaken by boy players.\nThe Rebecca Riots took place between 1839 and 1843 in West and Mid Wales. They were a series of protests undertaken by local farmers and agricultural workers in response to unfair taxation. The rioters, often men dressed as women, took their actions against toll-gates, as they were tangible representations of high taxes and tolls. The riots ceased prior to 1844 due to several factors, including increased troop levels, a desire by the protestors to avoid violence and the appearance of criminal groups using the guise of the biblical character Rebecca for their own purposes. In 1844 an Act of Parliament to consolidate and amend the laws relating to turnpike trusts in Wales was passed.\nA variety of historical figures are known to have cross-dressed to varying degrees. Many women found they had to disguise themselves as men in order to participate in the wider world. For example, Margaret King cross-dressed in the early 19th century to attend medical school, as none would accept female students. A century later, Vita Sackville-West dressed as a young soldier in order to \"walk out\" with her girlfriend Violet Keppel, to avoid the street harassment that two women would have faced. The prohibition on women wearing male garb, once strictly applied, still has echoes today in some Western societies which require girls and women to wear skirts, for example as part of school uniform or office dress codes. In some countries, even in casual settings, women are still prohibited from wearing traditionally male clothing. Sometimes all trousers, no matter how loose and long, are automatically considered \"indecent\", which may render their wearer subject to severe punishment, as in the case of Lubna al-Hussein in Sudan in 2009.\nLegal issues.\nIn many countries, cross-dressing was illegal under laws that identified it as indecent or immoral. Many such laws were challenged in the late 1900s giving people the right to freedom of gender expression with regard to their clothing.\nFor instance, from 1840 forward, United States saw state and city laws forbidding people from appearing in public while dressed in clothes that do not associate with their assigned sex. The goal of this wave of policies was to create a tool that would enforce a normative gender narrative, targeting multiple gender identities across the gender spectrum. With the progression of time, styles, and societal trends, it became even more difficult to draw the line between what was cross-dressing or not. Only recently have these laws changed. As recently as 2011, it was still possible for a man to get arrested for \"impersonating a woman\" \u2014 a vestige of the 19th century laws. Even with this, legal issues surrounding cross-dressing perpetuated all throughout the mid 20th century. During this time period, police would often reference laws that did not exist or laws that have been repealed in order to target the LGBTQ+ community.\nThis extends beyond the United States: There still remains 13 UN member States that explicitly criminalize transgender individuals, and there exist even more countries that use a great deal of diverse laws to target them. The third edition of the Trans Legal Mapping Report, done by the International Lesbian, Gay, Bisexual, Trans, and Intersex Association found that an especially common method to target these individuals is through cross-dressing regulations. For instance, only in 2014 did an appeal court in Malaysia finally overturned a state law prohibiting Muslim men from cross-dressing as women.\nIn the Australian state of Tasmania, cross-dressing in public was made a criminal offence in 1935, and this law was only repealed in 2000.\nVarieties.\nThere are many different kinds of cross-dressing and many different reasons why an individual might engage in cross-dressing behavior. Some people cross-dress as a matter of comfort or style, a personal preference for clothing associated with the opposite gender. Some people cross-dress to shock others or challenge social norms; others will limit their cross-dressing to underwear, so that it is not apparent. Some people attempt to pass as a member of the opposite gender in order to gain access to places or resources they would not otherwise be able to reach. \nGender disguise.\nGender disguise has been used by women and girls to pass as male, and by men and boys to pass as female. Gender disguise has also been used as a plot device in storytelling, particularly in narrative ballads, and is a recurring motif in literature, theater, and film. Historically, some women have cross-dressed to take up male-dominated or male-exclusive professions, such as military service. Conversely, some men have cross-dressed to escape from mandatory military service or as a disguise to assist in political or social protest, as men in Wales did in the Rebecca Riots and when conducting Ceffyl Pren as a form of mob justice.\nUndercover journalism may require cross-dressing, as with Norah Vincent's project \"Self-Made Man\".\nOne famous case of gender disguise was when Bernard Boursicot, a French diplomat, was caught in a honeypot trap (seducing him to participate in Chinese espionage) by Shi Pei Pu, a male Peking opera singer who performed female roles, whom Boursicot believed to be female. This espionage case became something of a cause c\u00e9l\u00e8bre in France in 1986, as Boursicot and Shi were brought to trial, owing to the nature of the unusual sexual subterfuge alleged. \nSome girls in Afghanistan, even after the fall of the Taliban, were still disguised by their families as boys. This is known as \"bacha posh\".\nTheater and performance.\nSingle-sex theatrical troupes often have some performers who cross-dress to play roles written for members of the opposite sex (travesti and trouser roles). Cross-dressing, particularly the depiction of males wearing dresses, is often used for comic effect onstage and on-screen.\nBoy player refers to children who performed in Medieval and English Renaissance playing companies. Some boy players worked for the adult companies and performed the female roles as women did not perform on the English stage in this period. Others worked for children's companies in which all roles, not just the female ones, were played by boys.\u202f\nIn an effort to clamp down on kabuki's popularity, women's kabuki, known as , was banned in 1629 in Japan for being too erotic. Following this ban, young boys began performing in , which was also soon banned. Thus adult men play female roles in kabuki.\nDan is the general name for female roles in Chinese opera, often referring to leading roles. They may be played by male or female actors. In the early years of Peking opera, all roles were played by men, but this practice is no longer common in any Chinese opera genre.\nWomen have often been excluded from Noh, and men often play female characters in it. \nDrag is a special form of performance art based on the act of cross-dressing. A drag queen is usually a male-assigned person who performs as an exaggeratedly feminine character, in heightened costuming sometimes consisting of a showy dress, high-heeled shoes, obvious make-up, and wig. A drag queen may imitate famous female film or pop-music stars. A faux queen is a female-assigned person employing the same techniques. A drag king is a counterpart of the drag queen \u2013 a female-assigned person who adopts a masculine persona in performance or imitates a male film or pop-music star. Some female-assigned people undergoing gender reassignment therapy also self-identify as 'drag kings'.\nThe modern activity of battle reenactments has raised the question of women passing as male soldiers. In 1989, Lauren Burgess dressed as a male soldier in a U.S. National Park Service reenactment of the Battle of Antietam, and was ejected after she was discovered to be a woman. Burgess sued the Park Service for sexual discrimination. The case spurred spirited debate among Civil War buffs. In 1993, a federal judge ruled in Burgess's favor.\n\"Wigging\" refers to the practice of male stunt doubles taking the place of an actress, parallel to \"paint downs\", where white stunt doubles are made up to resemble black actors. Female stunt doubles have begun to protest this norm of \"historical sexism\", saying that it restricts their already limited job possibilities.\nBritish pantomime, television and comedy.\nCross-dressing is a traditional popular trope in British comedy. The pantomime dame in British pantomime dates from the 19th century, which is part of the theatrical tradition of female characters portrayed by male actors in drag. Widow Twankey (Aladdin's mother) is a popular pantomime dame: in 2004 Ian McKellen played the role.\nThe Monty Python comedy troupe donned frocks and makeup, playing female roles while speaking in falsetto. Character comics such as Benny Hill and Dick Emery drew upon several female identities. In the BBC's long-running sketch show \"The Dick Emery Show\" (broadcast from 1963 to 1981), Emery played Mandy, a busty peroxide blonde whose catchphrase, \"Ooh, you are awful ... but I like you!\", was given in response to a seemingly innocent remark made by her interviewer, but perceived by her as ribald double entendre. The popular tradition of cross dressing in British comedy extended to the 1984 music video for Queen's \"I Want to Break Free\" where the band parody several female characters from the soap opera \"Coronation Street\".\nSexual fetishes.\nA transvestic fetishist is a person who cross-dresses as part of a sexual fetish. According to the fourth edition of \"Diagnostic and Statistical Manual of Mental Disorders\", this fetishism was limited to heterosexual men; however, DSM-5 does not have this restriction, and opens it to women and men, regardless of their sexual orientation.\nSometimes either member of a heterosexual couple will cross-dress in order to arouse the other. For example, the male might wear skirts or lingerie and/or the female will wear boxers or other male clothing. (See also forced feminization)\nPassing.\nSome people who cross-dress may endeavor to project a complete impression of belonging to another gender, including mannerisms, speech patterns, and emulation of sexual characteristics. This is referred to as passing or \"trying to pass\", depending how successful the person is. An observer who sees through the cross-dresser's attempt to pass is said to have \"read\" or \"clocked\" them. There are videos, books, and magazines on how a man may look more like a woman.\nOthers may choose to take a mixed approach, adopting some feminine traits and some masculine traits in their appearance. For instance, a man might wear both a dress and a beard. This is sometimes known as \"genderfuck\". In a broader context, cross-dressing may also refer to other actions undertaken to pass as a particular sex, such as packing (accentuating the male crotch bulge) or, the opposite, tucking (concealing the male crotch bulge).\nClothes.\nThe actual determination of cross-dressing is largely socially constructed. For example, in Western society, trousers have long been adopted for usage by women, and it is no longer regarded as cross-dressing. In cultures where men have traditionally worn skirt-like garments such as the kilt or sarong, these are not seen as women's clothing, and wearing them is not seen as cross-dressing for men. As societies are becoming more global in nature, both men's and women's clothing are adopting styles of dress associated with other cultures.\nCosplaying may also involve cross-dressing, for some females may wish to dress as a male, and vice versa (see Crossplay (cosplay)). Breast binding (for females) is not uncommon and is one of the things likely needed to cosplay a male character.\nIn most parts of the world, it remains socially disapproved for men to wear clothes traditionally associated with women. Attempts are occasionally made, e.g. by fashion designers, to promote the acceptance of skirts as everyday wear for men. Cross-dressers have complained that society permits women to wear pants or jeans and other masculine clothing, while condemning any man who wants to wear clothing sold for women.\nWhile creating a more feminine figure, male cross-dressers will often utilize different types and styles of breast forms, which are silicone prostheses traditionally used by women who have undergone mastectomies to recreate the visual appearance of a breast.\nWhile most male cross-dressers utilize clothing associated with modern women, some are involved in subcultures that involve dressing as little girls or in vintage clothing. Some such men have written that they enjoy dressing as femininely as possible, so they wear frilly dresses with lace and ribbons, bridal gowns complete with veils, as well as multiple petticoats, corsets, girdles and/or garter belts with nylon stockings.\nThe term \"underdressing\" is used by male cross-dressers to describe wearing female undergarments such as panties under their male clothes. The famous low-budget film-maker Edward D. Wood, Jr. (who also went out in public dressed in drag as \"Shirley\", his female alter ego) said he often wore women's underwear under his military uniform as a Marine during World War II. \"Female masking\" is a form of cross-dressing in which men wear masks that present them as female.\nSocial issues.\nCross-dressers may begin wearing clothing associated with the opposite sex in childhood, using the clothes of a sibling, parent, or friend. Some parents have said they allowed their children to cross-dress and, in many cases, the child stopped when they became older. The same pattern often continues into adulthood, where there may be confrontations with a spouse, partner, family member or friend. Married cross-dressers can experience considerable anxiety and guilt if their spouse objects to their behavior.\nSometimes because of guilt or other reasons cross-dressers dispose of all their clothing, a practice called \"purging\", only to start collecting the other gender's clothing again.\nFestivals.\nCelebrations of cross-dressing occur in widespread cultures. The Abissa festival in C\u00f4te d'Ivoire, Ofudamaki in Japan, and Kottankulangara Festival in India are all examples of this.\nAnalysis.\nAdvocacy for social change has done much to relax the constrictions of gender roles on men and women, but they are still subject to prejudice from some people. It is noticeable that as being transgender becomes more socially accepted as a normal human condition, the prejudices against cross-dressing are changing quite quickly, just as the similar prejudices against homosexuals have changed rapidly in recent decades.\nThe reason it is so hard to have statistics for female cross-dressers is that the line where cross-dressing stops and cross-dressing begins has become blurred, whereas the same line for men is as well defined as ever. This is one of the many issues being addressed by third wave feminism as well as the modern-day masculist movement.\nThe general culture has very mixed views about cross-dressing. A woman who wears her husband's shirt to bed is considered attractive, while a man who wears his wife's nightgown to bed may be considered transgressive. Marlene Dietrich in a tuxedo was considered very erotic; Jack Lemmon in a dress was considered ridiculous. All this may result from an overall gender role rigidity for males; that is, because of the prevalent gender dynamic throughout the world, men frequently encounter discrimination when deviating from masculine gender norms, particularly violations of heteronormativity. A man's adoption of feminine clothing is often considered a going down in the gendered social order whereas a woman's adoption of what are traditionally men's clothing (at least in the English-speaking world) has less of an impact because women have been traditionally subordinate to men, unable to affect serious change through style of dress. Thus when a male cross-dresser puts on his clothes, he transforms into the quasi-female and thereby becomes an embodiment of the conflicted gender dynamic. Following the work of Judith Butler, gender proceeds along through ritualized performances, but in male cross-dressing it becomes a performative \"breaking\" of the masculine and a \"subversive repetition\" of the feminine.\nPsychoanalysts today do not regard cross-dressing by itself as a psychological problem, unless it interferes with a person's life. \"For instance,\" said Joseph Merlino, senior editor of \"Freud at 150: 21st Century Essays on a Man of Genius\", \"[suppose that]...I'm a cross-dresser and I don't want to keep it confined to my circle of friends, or my party circle, and I want to take that to my wife and I don't understand why she doesn't accept it, or I take it to my office and I don't understand why they don't accept it, then it's become a problem because it's interfering with my relationships and environment.\"\nCross-dressing in the 21st century.\nFashion trends.\nCross-dressing today is much more common and normalized thanks to trends such as camp fashion and androgynous fashion. These trends have long histories but have recently been popularized thanks to major designers, fashion media, and celebrities today. \nCamp is a style of fashion that has had a long history extending all the way back to the Victorian era to the modern era. During the Victorian era up until the mid-20th century, it was defined as an exaggerated and flamboyant style of dressing. This was typically associated with ideas of effeminacy, de-masculization, and homosexuality. As the trend entered the 20th century, it also developed an association with a lack of conduct, creating the connotation that those who engaged in Camp are unrefined, improper, distasteful, and, essentially, undignified. Though this was its former understanding, Camp has now developed a new role in the fashion industry. It is considered a fashion style that has \"failed seriousness\" and has instead become a fun way of self-expression. Thanks to its integration with high fashion and extravagance, Camp is now seen as a high art form of absurdity: including loud, vibrant, bold, fun, and empty frivolity.\nCamp is often used in drag culture as a method of exaggerating or inversing traditional conceptions of what it means to be feminine. In actuality, the QTPOC community has had a large impact on Camp. This is exhibited by ballroom culture, camp/glamour queens, Black '70s funk, Caribbean Carnival costumes, Blaxploitation movies, \"pimp/player fashion\", and more. This notion has also been materialized by camp icons such as Josephine Baker and RuPaul.\nAndrogynous fashion is described as neither masculine nor feminine rather it is the embodiment of a gender inclusive and sexually neutral fashion of expression. The general understanding of androgynous fashion is mixing both masculine and feminine pieces with the goal of producing a look that has no visual differentiations between one gender or another. This look is achieved by masking the general body so that one cannot identify the biological sex of an individual given the silhouette of the clothing pieces: Therefore, many androgynous looks include looser, baggier clothing that can conceal curves in the female body or using more \"feminine\" fabrics and prints for men.\nBoth of these style forms have been normalized and popularized by celebrities such as Harry Styles, Timoth\u00e9e Chalamet, Billie Eilish, Princess Diana, and more. These styles have also been adopted by fashion designers as well including Telfar, One DNA, Toogood, and more.\nSocietal changes.\nBeyond fashion, cross-dressing in non-Western countries have not fully outgrown the negative connotations that is has in the West. For instance, many Eastern and Southeastern Asian countries have a narrative of discrimination and stigma against LGBTQ and cross-dressing individuals. This is especially evident in the post-pandemic world. During this time, it was clear to see the failures of these governments to provide sufficient support to these individuals due to a lack of legal services, lack of job opportunity, and more. For instance, to be able to receive government aid, these individuals need to be able to quickly change their legal name, gender, and other information on official ID documents. This fault augmented the challenges of income loss, food insecurity, safe housing, healthcare, and more for many trans and cross-dressing individuals. This was especially pertinent as many of these individuals relied on entertainment and sex work for income. With the pandemic removing these job opportunities, the stigmatisation and discrimination against these individuals only increased, especially in Southeast Asian countries.\nOn the other hand, some Asian countries have grown to be more accepting of cross-dressing as modernization has increased. For instance, among Japan's niche communities there exists the otokonoko. This is a group of male-assigned individuals who engage in female cross-dressing as a form of gender expression. This trend originated with manga and grew with an increase in maid cafes, cosplaying, and more in the 2010s. With the normalization of this through cosplay, cross-dressing has become a large part of otaku and anime culture.\nAcross media.\nWomen dressed as men, and less often men dressed as women, is a common trope in fiction and folklore. For example, in Norse myth, Thor disguised himself as Freya. These disguises were also popular in Gothic fiction, such as in works by Charles Dickens, Alexandre Dumas, p\u00e8re, and Eug\u00e8ne Sue, and in a number of Shakespeare's plays, such as \"Twelfth Night\". In \"The Wind in the Willows\", Toad dresses as a washerwoman, and in \"The Lord of the Rings\", \u00c9owyn pretends to be a man.\nIn science fiction, fantasy and women's literature, this literary motif is occasionally taken further, with literal transformation of a character from male to female or vice versa. Virginia Woolf's \"\" focuses on a man who becomes a woman, as does a warrior in Peter S. Beagle's \"The Innkeeper's Song\"; while in Geoff Ryman's \"The Warrior Who Carried Life\", Cara magically transforms herself into a man.\nOther popular examples of gender disguise include \"Madame Doubtfire\" (published as \"Alias Madame Doubtfire\" in the United States) and its movie adaptation \"Mrs. Doubtfire\", featuring a man disguised as a woman. Similarly, the movie \"Tootsie\" features Dustin Hoffman disguised as a woman, while the movie \"The Associate\" features Whoopi Goldberg disguised as a man.\nMedical views.\nThe 10th edition of the International Statistical Classification of Diseases and Related Health Problems lists \"dual-role transvestism\" (non-sexual cross-dressing) and \"fetishistic transvestism\" (cross-dressing for sexual pleasure) as disorders. Both listings were removed for the 11th edition. \nTransvestic fetishism is a paraphilia and a psychiatric diagnosis in the DSM-5 version of the \"Diagnostic and Statistical Manual of Mental Disorders\".\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nNotes.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5702", "revid": "40310821", "url": "https://en.wikipedia.org/wiki?curid=5702", "title": "Channel Tunnel", "text": "Undersea rail tunnel linking France and England\n&lt;templatestyles src=\"Routemap/styles.css\"/&gt;\nThe Channel Tunnel (), also known as the Chunnel, is a underwater railway tunnel that connects Folkestone (Kent, England) with Coquelles (Pas-de-Calais, France) beneath the English Channel at the Strait of Dover. It is the only fixed link between the island of Great Britain and the European mainland. At its lowest point, it is deep below the sea bed and below sea level. At , it has the longest underwater section of any tunnel in the world and is the third longest railway tunnel in the world. The speed limit for trains through the tunnel is . The tunnel is owned and operated by the company Getlink, formerly \"Groupe Eurotunnel\".\nThe tunnel carries high-speed Eurostar passenger trains, LeShuttle services for road vehicles and freight trains. It connects end-to-end with high-speed railway lines: the LGV Nord in France and High Speed 1 in England. In 2017, rail services carried 10.3 million passengers and 1.22 million tonnes of freight, and the Shuttle carried 10.4 million passengers, 2.6 million cars, 51,000 coaches, and 1.6 million lorries (equivalent to 21.3 million tonnes of freight), compared with 11.7 million passengers, 2.6 million lorries and 2.2 million cars by sea through the Port of Dover.\nPlans to build a cross-Channel fixed link appeared as early as 1802, but British political and media pressure over the compromising of national security had disrupted attempts to build one. An early unsuccessful attempt was made in the late 19th century, on the English side, \"in the hope of forcing the hand of the English Government\". The eventual successful project, organised by Eurotunnel, began construction in 1988 and opened in 1994. Estimated to cost \u00a35.5 billion in 1985, it was at the time the most expensive construction project ever proposed. The cost finally amounted to \u00a39 billion (equivalent to \u00a3\u00a0billion in 2021), well over budget.\nSince its opening, the tunnel has had some mechanical problems. Both fires and cold weather have temporarily disrupted its operation.\nSince at least 1997, aggregations of migrants around Calais seeking irregular, undocumented and/or otherwise illegal entry to the United Kingdom, such as through the tunnel, have prompted deterrence and countermeasures.\nOrigins.\nEarlier proposals.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\nKey dates\nIn 1802, Albert Mathieu-Favier, a French mining engineer, put forward a proposal to tunnel under the English Channel, with illumination from oil lamps, horse-drawn coaches, and an artificial island positioned mid-Channel for changing horses. His design envisaged a bored two-level tunnel with the top tunnel used for transport and the bottom one for groundwater flows.\nIn 1839, Aim\u00e9 Thom\u00e9 de Gamond, a Frenchman, performed the first geological and hydrographical surveys on the Channel between Calais and Dover. He explored several schemes and, in 1856, presented a proposal to Napoleon III for a mined railway tunnel from Cap Gris-Nez to East Wear Point with a port/airshaft on the Varne sandbank at a cost of 170\u00a0million francs, or less than \u00a37\u00a0million.\nIn 1865, a deputation led by George Ward Hunt proposed the idea of a tunnel to the Chancellor of the Exchequer of the day, William Ewart Gladstone.\nIn 1866, Henry Marc Brunel made a survey of the floor of the Strait of Dover. By his results, he proved that the floor was composed of chalk, like the adjoining cliffs, and thus a tunnel was feasible. For this survey, he invented the gravity corer, which is still used in geology.\nAround 1866, William Low and Sir John Hawkshaw promoted tunnel ideas, but apart from preliminary geological studies, none were implemented.\nAn official Anglo-French protocol was established in 1876 for a cross-Channel railway tunnel.\nIn 1881, British railway entrepreneur Sir Edward Watkin and Alexandre Lavalley, a French Suez Canal contractor, were in the Anglo-French Submarine Railway Company that conducted exploratory work on both sides of the Channel. From June 1882 to March 1883, the British tunnel boring machine tunneled, through chalk, a total of , while Lavalley used a similar machine to drill from Sangatte on the French side. However, the cross-Channel tunnel project was abandoned in 1883, despite this success, after fears raised by the British military that an underwater tunnel might be used as an invasion route. Nevertheless, in 1883, this TBM was used to bore a railway ventilation tunnel\u2014 in diameter and long\u2014between Birkenhead and Liverpool, England, through sandstone under the Mersey River. These early works were encountered more than a century later during the TML project.\nA 1907 film, \"Tunnelling the English Channel\" by pioneer filmmaker Georges M\u00e9li\u00e8s, depicts King Edward VII and President Armand Falli\u00e8res dreaming of building a tunnel under the English Channel.\nIn 1919, during the Paris Peace Conference, British prime minister David Lloyd George repeatedly brought up the idea of a Channel tunnel as a way of reassuring France about British willingness to defend against another German attack. The French did not take the idea seriously, and nothing came of the proposal.\nIn the 1920s, Winston Churchill advocated for the Channel Tunnel, using that exact name in his essay \"Should Strategists Veto The Tunnel?\" It was published on 27 July 1924 in the \"Weekly Dispatch\", and argued vehemently against the idea that the tunnel could be used by a Continental enemy in an invasion of Britain. Churchill expressed his enthusiasm for the project again in an article for the \"Daily Mail\" on 12 February 1936, \"Why Not A Channel Tunnel?\"\nThere was another proposal in 1929, but nothing came of this discussion and the idea was shelved. Proponents estimated the construction cost at US$150 million. The engineers had addressed the concerns of both nations' military leaders by designing two sumps\u2014one near the coast of each country\u2014that could be flooded at will to block the tunnel. But this did not appease military leaders, or dispel concerns about hordes of tourists who would disrupt English life. Military fears continued during the Second World War. After the fall of France, as Britain prepared for an expected German invasion, a Royal Navy officer in the Directorate of Miscellaneous Weapons Development calculated that Hitler could use slave labour to build two Channel tunnels in 18 months. The estimate caused rumours that Germany had already begun digging.\nA British film from Gaumont Studios, \"The Tunnel\" (also called \"TransAtlantic Tunnel\"), was released in 1935 as a science-fiction project concerning the creation of a transatlantic tunnel. It referred briefly to its protagonist, a Mr. McAllan, as having completed a British Channel tunnel successfully in 1940, five years into the future of the film's release.\nBy 1955, defence arguments had become less relevant due to the dominance of air power, and both the British and French governments supported technical and geological surveys. In 1958 the 1881 workings were cleared in preparation for a \u00a3100,000 geological survey by the Channel Tunnel Study Group. 30% of the funding came from Channel Tunnel Co Ltd, the largest shareholder of which was the British Transport Commission, as successor to the South Eastern Railway. A detailed geological survey was carried out in 1964 and 1965.\nAlthough the two countries agreed to build a tunnel in 1964, the phase 1 initial studies and signing of a second agreement to cover phase 2 took until 1973. The plan described a government-funded project to create two tunnels to accommodate car shuttle wagons on either side of a service tunnel. Construction started on both sides of the Channel in 1974.\nOn 20 January 1975, to the dismay of their French partners, the then-governing Labour Party in Britain cancelled the project due to uncertainty about EEC membership, doubling cost estimates and the general economic crisis at the time. By this time the British tunnel boring machine was ready and the Ministry of Transport had conducted a experimental drive. (This short tunnel, called Adit A1, was eventually reused as the starting and access point for tunnelling operations from the British side, and remains an access point to the service tunnel.) The cancellation costs were estimated at \u00a317 million. On the French side, a tunnel-boring machine had been installed underground in a stub tunnel. It lay there for 14 years until 1988, when it was sold, dismantled, refurbished and shipped to Turkey, where it was used to drive the Moda tunnel for the Istanbul Sewerage Scheme.\nInitiation of project.\nIn 1979, the \"Mouse-hole Project\" was suggested when the Conservatives came to power in Britain. The concept was a single-track rail tunnel with a service tunnel but without shuttle terminals. The British government took no interest in funding the project, but the British Prime Minister Margaret Thatcher did not object to a privately funded project, although she said she assumed it would be for cars rather than trains. In 1981, Thatcher and French president Fran\u00e7ois Mitterrand agreed to establish a working group to evaluate a privately funded project. In June 1982 the Franco-British study group favoured a twin tunnel to accommodate conventional trains and a vehicle shuttle service. In April 1985 promoters were invited to submit scheme proposals. Four submissions were shortlisted:\nThe cross-Channel ferry industry protested under the name \"Flexilink\". In 1975 there was no campaign protesting a fixed link, with one of the largest ferry operators (Sealink) being state-owned. Flexilink continued rousing opposition throughout 1986 and 1987. Public opinion strongly favoured a drive-through tunnel, but concerns about ventilation, accident management and driver mesmerisation led to the only shortlisted rail submission, CTG/F-M, being awarded the project in January 1986. Reasons given for the selection included that it caused least disruption to shipping in the Channel and least environmental disruption, was the best protected against terrorism, and was the most likely to attract sufficient private finance.\nArrangement.\nThe British \"Channel Tunnel Group\" consisted of two banks and five construction companies, while their French counterparts, \"France\u2013Manche\", consisted of three banks and five construction companies. The banks' role was to advise on financing and secure loan commitments. On 2 July 1985, the groups formed Channel Tunnel Group/France\u2013Manche (CTG/F\u2013M). Their submission to the British and French governments was drawn from the 1975 project, including 11\u00a0volumes and a substantial environmental impact statement.\nThe Anglo-French Treaty on the Channel Tunnel was signed by both governments in Canterbury Cathedral. The Treaty of Canterbury (1986) prepared the Concession for the construction and operation of the Fixed Link by privately owned companies and outlined arbitration methods to be used in the event of disputes. It set up the Intergovernmental Commission (IGC), responsible for monitoring all matters associated with the Tunnel's construction and operation on behalf of the British and French governments, and a Safety Authority to advise the IGC.\nIt drew a land frontier between the two countries in the middle of the Channel tunnel\u2014the first of its kind.\nDesign and construction were done by the ten construction companies in the CTG/F-M group. The French terminal and boring from Sangatte were done by the five French construction companies in the joint venture group \"GIE Transmanche Construction\". The English Terminal and boring from Shakespeare Cliff were done by the five British construction companies in the \"Translink Joint Venture\". The two partnerships were linked by a bi-national project organisation, TransManche Link (TML). The \"Ma\u00eetre d'Oeuvre\" was a supervisory engineering body employed by Eurotunnel under the terms of the concession that monitored the project and reported to the governments and banks.\nIn France, with its long tradition of infrastructure investment, the project had widespread approval. The French National Assembly approved it unanimously in April 1987, and after a public inquiry, the Senate approved it unanimously in June. In Britain, select committees examined the proposal, making history by holding hearings away from Westminster, in Kent. In February 1987, the third reading of the Channel Tunnel Bill took place in the House of Commons, and passed by 94\u00a0votes to 22. The Channel Tunnel Act gained Royal assent and passed into law in July. Parliamentary support for the project came partly from provincial members of Parliament on the basis of promises of regional Eurostar through train services that never materialised; the promises were repeated in 1996 when the contract for construction of the Channel Tunnel Rail Link was awarded.\nCost.\nThe tunnel is a build-own-operate-transfer (BOOT) project with a concession.&lt;ref name=\"Flyvbjerg p. 96/97\"&gt;Flyvbjerg et al. pp.\u00a096\u201397&lt;/ref&gt; TML would design and build the tunnel, but financing was through a separate legal entity, Eurotunnel. Eurotunnel absorbed CTG/F-M and signed a construction contract with TML, but the British and French governments controlled final engineering and safety decisions, now in the hands of the Channel Tunnel Safety Authority. The British and French governments gave Eurotunnel a 55-year operating concession (from 1987; extended by 10 years to 65 years in 1993) to repay loans and pay dividends. A Railway Usage Agreement was signed between Eurotunnel, British Rail and SNCF guaranteeing future revenue in exchange for the railways obtaining half of the tunnel's capacity.\nPrivate funding for such a complex infrastructure project was of unprecedented scale. Initial equity of \u00a345\u00a0million was raised by CTG/F-M, increased by \u00a3206\u00a0million private institutional placement, \u00a3770\u00a0million was raised in a public share offer that included press and television advertisements, a syndicated bank loan and letter of credit arranged \u00a35\u00a0billion. Privately financed, the total investment costs at 1985 prices were \u00a32.6\u00a0billion. At the 1994 completion actual costs were, in 1985 prices, \u00a34.65\u00a0billion: an 80% cost overrun. The cost overrun was partly due to enhanced safety, security, and environmental demands. Financing costs were 140% higher than forecast.\nConstruction.\nWorking from both the English and French sides of the Channel, eleven tunnel boring machines or TBMs cut through chalk marl to construct two rail tunnels and a service tunnel. The vehicle shuttle terminals are at Cheriton (part of Folkestone) and Coquelles, and are connected to the English M20 and French A16 motorways respectively.\nTunnelling commenced in 1988, and the tunnel began operating in 1994. In 1985 prices, the total construction cost was \u00a34.65\u00a0billion (equivalent to \u00a3\u00a0billion in 2015), an 80% cost overrun. At the peak of construction 15,000\u00a0people were employed with daily expenditure over \u00a33\u00a0million. Ten workers, eight of them British, were killed during construction between 1987 and 1993, most in the first few months of boring.\nCompletion.\nA diameter pilot hole allowed the service tunnel to break through without ceremony on 30 October 1990. On 1 December 1990, Englishman Graham Fagg and Frenchman Phillippe Cozette broke through the service tunnel with the media watching. Eurotunnel completed the tunnel on time. (A BBC TV television commentator called Graham Fagg \"the first man to cross the Channel by land for 8000 years\".) The two tunnelling efforts met each other with an offset of only . A Paddington Bear soft toy was chosen by British tunnellers as the first item to pass through to their French counterparts when the two sides met.\nThe tunnel was officially opened, one year later than originally planned, by Queen Elizabeth II and the French president, Fran\u00e7ois Mitterrand, in a ceremony held in Calais on 6 May 1994. The Queen travelled through the tunnel to Calais on a Eurostar train, which stopped nose to nose with the train that carried President Mitterrand from Paris. Following the ceremony President Mitterrand and the Queen travelled on Le Shuttle to a similar ceremony in Folkestone. A full public service did not start for several months. The first freight train, however, ran on 1 June 1994 and carried Rover and Mini cars being exported to Italy.\nThe Channel Tunnel Rail Link (CTRL), now called High Speed 1, runs from St Pancras railway station in London to the tunnel portal at Folkestone in Kent. It cost \u00a35.8\u00a0billion. On 16 September 2003 the prime minister, Tony Blair, opened the first section of High Speed 1, from Folkestone to north Kent. On 6 November 2007, the Queen officially opened High Speed 1 and St Pancras International station, replacing the original slower link to Waterloo International railway station. High Speed 1 trains travel at up to , the journey from London to Paris taking 2\u00a0hours 15\u00a0minutes, to Brussels 1\u00a0hour 51\u00a0minutes.\nIn 1994, the American Society of Civil Engineers elected the tunnel as one of the seven modern Wonders of the World. In 1995, the American magazine \"Popular Mechanics\" published the results.\nOpening dates.\nThe opening was phased for various services offered as the Channel Tunnel Safety Authority, the IGC, gave permission for various services to begin at several dates over the period 1994/1995 but start-up dates were a few days later.\nEngineering.\nSite investigation undertaken in the 20 years before construction confirmed earlier speculations that a tunnel could be bored through a chalk marl stratum. The chalk marl is conducive to tunnelling, with impermeability, ease of excavation and strength. The chalk marl runs along the entire length of the English side of the tunnel, but on the French side a length of has variable and difficult geology. The tunnel consists of three bores: two diameter rail tunnels, apart, in length with a diameter service tunnel in between. The three bores are connected by cross-passages and piston relief ducts. The service tunnel was used as a pilot tunnel, boring ahead of the main tunnels to determine the conditions. English access was provided at Shakespeare Cliff and French access from a shaft at Sangatte. The French side used five tunnel boring machines (TBMs), and the English side six. The service tunnel uses Service Tunnel Transport System (STTS) and Light Service Tunnel Vehicles (LADOGS). Fire safety was a critical design issue.\nBetween the portals at Beussingue and Castle Hill the tunnel is long, with under land on the French side and on the UK side, and under sea. It is the third-longest rail tunnel in the world, behind the Gotthard Base Tunnel in Switzerland and the Seikan Tunnel in Japan, but with the longest under-sea section. The average depth is below the seabed. On the UK side, of the expected of spoil approximately was used for fill at the terminal site, and the remainder was deposited at Lower Shakespeare Cliff behind a seawall, reclaiming of land. This land was then made into the Samphire Hoe Country Park. Environmental impact assessment did not identify any major risks for the project, and further studies into safety, noise, and air pollution were overall positive. However, environmental objections were raised over a high-speed link to London.\nGeology.\nSuccessful tunnelling required a sound understanding of topography and geology and the selection of the best rock strata through which to dig. The geology of this site generally consists of northeasterly dipping Cretaceous strata, part of the northern limb of the Wealden-Boulonnais dome. Characteristics include:\nOn the English side, the stratum dip is less than 5\u00b0; on the French side, this increases to 20\u00b0. Jointing and faulting are present on both sides. On the English side, only minor faults of displacement less than exist; on the French side, displacements of up to are present owing to the Quenocs anticlinal fold. The faults are of limited width, filled with calcite, pyrite and remolded clay. The increased dip and faulting restricted the selection of routes on the French side. To avoid confusion, microfossil assemblages were used to classify the chalk marl. On the French side, particularly near the coast, the chalk was harder, more brittle and more fractured than on the English side. This led to the adoption of different tunnelling techniques on the two sides.\nThe Quaternary undersea valley Fosse Dangeard, and Castle Hill landslip at the English portal, caused concerns. Identified by the 1964\u201365 geophysical survey, the Fosse Dangeard is an infilled valley system extending below the seabed, south of the tunnel route in mid-channel. A 1986 survey showed that a tributary crossed the path of the tunnel, and so the tunnel route was made as far north and deep as possible. The English terminal had to be located in the Castle Hill landslip, which consists of displaced and tipping blocks of lower chalk, glauconitic marl and gault debris. Thus the area was stabilised by buttressing and inserting drainage adits. The service tunnel acted as a pilot preceding the main ones, so that the geology, areas of crushed rock, and zones of high water inflow could be predicted. Exploratory probing took place in the service tunnel, in the form of extensive forward probing, vertical downward probes and sideways probing.\nSite investigation.\nMarine soundings and samplings by Thom\u00e9 de Gamond were carried out during 1833\u201367, establishing the seabed depth at a maximum of and the continuity of geological strata (layers). Surveying continued over many years, with 166\u00a0marine and 70\u00a0land-deep boreholes being drilled and over 4,000 line kilometres of the marine geophysical survey completed. Surveys were undertaken in 1958\u20131959, 1964\u20131965, 1972\u20131974 and 1986\u20131988.\nThe surveying in 1958\u201359 catered for immersed tube and bridge designs, as well as a bored tunnel, and thus a wide area, was investigated. At this time, marine geophysics surveying for engineering projects was in its infancy, with poor positioning and resolution from seismic profiling. The 1964\u201365 surveys concentrated on a northerly route that left the English coast at Dover harbour; using 70\u00a0boreholes, an area of deeply weathered rock with high permeability was located just south of Dover harbour.\nGiven the previous survey results and access constraints, a more southerly route was investigated in the 1972\u201373 survey, and the route was confirmed to be feasible. Information for the tunnelling project also came from work before the 1975 cancellation. On the French side at Sangatte, a deep shaft with adits was made. On the English side at Shakespeare Cliff, the government allowed of diameter tunnel to be driven. The actual tunnel alignment, method of excavation and support were essentially the same as the 1975 attempt. In the 1986\u201387 survey, previous findings were reinforced, and the characteristics of the gault clay and the tunnelling medium (chalk marl that made up 85% of the route) were investigated. Geophysical techniques from the oil industry were employed.\nTunnelling.\nTunnelling was a major engineering challenge, with the only precedent being the undersea Seikan Tunnel in Japan, which opened in 1988. A serious health and safety risk with building tunnels underwater is major water inflow due to the high hydrostatic pressure from the sea above, under weak ground conditions. The tunnel also had the challenge of time: being privately funded, the early financial return was paramount.\nThe objective was to construct two rail tunnels, apart, in length; a service tunnel between the two main ones; pairs of -diameter cross-passages linking the rail tunnels to the service one at spacing; piston relief ducts in diameter connecting the rail tunnels apart; two undersea crossover caverns to connect the rail tunnels, with the service tunnel always preceding the main ones by at least to ascertain the ground conditions. There was plenty of experience with excavating through chalk in the mining industry, while the undersea crossover caverns were a complex engineering problem. The French one was based on the Mount Baker Ridge freeway tunnel in Seattle; the UK cavern was dug from the service tunnel ahead of the main ones, to avoid delay.\nPrecast segmental linings in the main TBM drives were used, but two different solutions were used. On the French side, neoprene and grout sealed bolted linings made of cast iron or high-strength reinforced concrete were used; on the English side, the main requirement was for speed so bolting of cast-iron lining segments was only carried out in areas of poor geology. In the UK rail tunnels, eight lining segments plus a key segment were used; in the French side, five segments plus a key. On the French side, a diameter deep grout-curtained shaft at Sangatte was used for access. On the English side, a marshalling area was below the top of Shakespeare Cliff, the New Austrian Tunnelling method (NATM) was first applied in the chalk marl here. On the English side, the land tunnels were driven from Shakespeare Cliff\u2014the same place as the marine tunnels\u2014not from Folkestone. The platform at the base of the cliff was not large enough for all of the drives and, despite environmental objections, tunnel spoil was placed behind a reinforced concrete seawall, on condition of placing the chalk in an enclosed lagoon, to avoid wide dispersal of chalk fines. Owing to limited space, the precast lining factory was on the Isle of Grain in the Thames estuary, which used Scottish granite aggregate delivered by ship from the Foster Yeoman coastal super quarry at Glensanda in Loch Linnhe on the west coast of Scotland.\nOn the French side, owing to the greater permeability to water, earth pressure balance TBMs with open and closed modes was used. The TBMs were of a closed nature during the initial , but then operated as open, boring through the chalk marl stratum. This minimised the impact to the ground, allowed high water pressures to be withstood and it also alleviated the need to grout ahead of the tunnel. The French effort required five TBMs: two main marine machines, one mainland machine (the short land drives of allowed one TBM to complete the first drive then reverse direction and complete the other), and two service tunnel machines. On the English side, the simpler geology allowed faster open-faced TBMs. Six machines were used; all commenced digging from Shakespeare Cliff, three marine-bound and three for the land tunnels. Towards the completion of the undersea drives, the UK TBMs were driven steeply downwards and buried clear of the tunnel. These buried TBMs were then used to provide an electrical earth. The French TBMs then completed the tunnel and were dismantled. A gauge railway was used on the English side during construction.\nIn contrast to the English machines, which were given technical names, the French tunnelling machines were all named after women: Brigitte, Europa, Catherine, Virginie, Pascaline, S\u00e9verine.\nAt the end of the tunnelling, one machine was on display at the side of the M20 motorway in Folkestone until Eurotunnel sold it on eBay for \u00a339,999 to a scrap metal merchant. Another machine (T4 \"Virginie\") still survives on the French side, adjacent to Junction 41 on the A16, in the middle of the D243E3/D243E4 roundabout. On it are the words \"hommage aux b\u00e2tisseurs du tunnel\", meaning \"tribute to the builders of the tunnel\".\nTunnel boring machines.\nThe eleven tunnel boring machines were designed and manufactured through a joint venture between the Robbins Company of Kent, Washington, United States; Markham &amp; Co. of Chesterfield, England; and Kawasaki Heavy Industries of Japan. The TBMs for the service tunnels and main tunnels on the UK side were designed and manufactured by James Howden &amp; Company Ltd, Scotland.\nRailway design.\nLoading gauge.\nThe loading gauge height is .\nCommunications.\nThere are three communication systems: concession radio (CR) for mobile vehicles and personnel within Eurotunnel's Concession (terminals, tunnels, coastal shafts); track-to-train radio (TTR) for secure speech and data between trains and the railway control centre; Shuttle internal radio (SIR) for communication between shuttle crew and to passengers over car radios.\nPower supply.\nPower is delivered to the locomotives via an overhead line (catenary) at 25 kV 50 Hz. with a normal overhead clearance of . All tunnel services run on electricity, shared equally from English and French sources. There are two sub-stations fed at 400 kV at each terminal, but in an emergency, the tunnel's lighting (about 20,000 light fittings) and the plant can be powered solely from either England or France.\nThe traditional railway south of London uses a 750\u00a0V\u00a0DC third rail to deliver electricity, but since the opening of High Speed 1 there is no longer any need for tunnel trains to use the third rail system. High Speed 1, the tunnel and the LGV Nord all have power provided via overhead catenary at 25\u00a0kV\u00a050\u00a0Hz. The railways on \"classic\" lines in Belgium are also electrified by overhead wires, but at 3000\u00a0V\u00a0DC.\nSignalling.\nA cab signalling system gives information directly to train drivers on a display. There is a train protection system that stops the train if the speed exceeds that indicated on the in-cab display. TVM430, as used on LGV Nord and High Speed 1, is used in the tunnel. The TVM signalling is interconnected with the signalling on the high-speed lines on either side, allowing trains to enter and exit the tunnel system without stopping. The maximum speed is .\nSignalling in the tunnel is coordinated from two control centres: The main control centre at the Folkestone terminal, and a backup at the Calais terminal, which is staffed at all times and can take over all operations in the event of a breakdown or emergency.\nTrack system.\nConventional ballasted tunnel track was ruled out owing to the difficulty of maintenance and lack of stability and precision. The Sonneville International Corporation's track system was chosen based on reliability and cost-effectiveness based on a good performance in Swiss tunnels and worldwide. The type of track used is known as Low Vibration Track (LVT). Like a ballasted track, the LVT is of the free-floating type, held in place by gravity and friction. Reinforced concrete blocks of 100\u00a0kg support the rails every 60\u00a0cm and are held by 12\u00a0mm thick closed-cell polymer foam pads placed at the bottom of rubber boots. The latter separates the blocks' mass movements from the lean encasement concrete. The ballastless track provides extra overhead clearance necessary for the passage of larger trains. The corrugated rubber walls of the boots add a degree of isolation of horizontal wheel-rail vibrations and are insulators of the track signal circuit in the humid tunnel environment. UIC60 (60\u00a0kg/m) rails of 900A grade rest on rail pads, which fit the RN/Sonneville bolted dual leaf-springs. The rails, LVT-blocks and their boots with pads were assembled outside the tunnel, in a fully automated process developed by the LVT inventor, Mr. Roger Sonneville. About 334,000 Sonneville blocks were made on the Sangatte site.\nMaintenance activities are less than projected. Initially, the rails were ground on a yearly basis or after approximately 100MGT of traffic. Ride quality continues to be noticeably smooth and of low noise. Maintenance is facilitated by the existence of two tunnel junctions or crossover facilities, allowing for two-way operation in each of the six tunnel segments thereby creating, and thus providing safe access for maintenance of one isolated tunnel segment at a time. The two crossovers are the largest artificial undersea caverns ever built; 150 m long, 10 m high and 18 m wide. The English crossover is from Shakespeare Cliff, and the French crossover is from Sangatte.\nVentilation, cooling and drainage.\nThe ventilation system maintains the air pressure in the service tunnel higher than in the rail tunnels, so that in the event of a fire, smoke does not enter the service tunnel from the rail tunnels. Two cooling water pipes in each rail tunnel circulate chilled water to remove heat generated by the rail traffic. Pumping stations remove water in the tunnels from rain, seepage, and so on.\nDuring the design stage of the tunnel, engineers found that its aerodynamic properties and the heat generated by high-speed trains as they passed through it would raise the temperature inside the tunnel to . As well as making the trains \"unbearably warm\" for passengers this also presented a risk of equipment failure and track distortion. To cool the tunnel to below , engineers installed of diameter cooling pipes carrying of water. The network\u2014Europe's largest cooling system\u2014was supplied by eight York Titan chillers running on R22, a Hydrochlorofluorocarbon (HCFC) refrigerant gas.\nDue to R22's ozone depletion potential (ODP) and high global warming potential (GWP), its use is being phased out in developed countries, and since 1 January 2015, it has been illegal in Europe to use HCFCs to service air-conditioning equipment\u2014broken equipment that used HCFCs must instead be replaced with equipment that does not use it. In 2016, Trane was selected to provide replacement chillers for the tunnel's cooling network. The York chillers were decommissioned and four \"next generation\" Trane Series E CenTraVac large-capacity (2600\u00a0kW to 14,000\u00a0kW) chillers were installed\u2014two located in Sangatte, France, and two at Shakespeare Cliff, UK. The energy-efficient chillers, using Honeywell's non-flammable, ultra-low GWP R1233zd(E) refrigerant, maintain temperatures at , and in their first year of operation generated savings of 4.8 GWh\u2014approximately 33%, equating to \u20ac500,000 ($585,000)\u2014for tunnel operator Getlink.\nOperators.\nLeShuttle.\nGetlink operates the LeShuttle, a vehicle shuttle service, through the tunnel.\nCar shuttle sets have two separate halves: single and double deck. Each half has two loading/unloading wagons and 12 carrier wagons. Eurotunnel's original order was for nine car shuttle sets.\nHeavy goods vehicle (HGV) shuttle sets also have two halves, with each half containing one loading wagon, one unloading wagon and 14\u00a0carrier wagons. There is a club car behind the leading locomotive, where drivers must stay during the journey. Eurotunnel originally ordered six HGV shuttle sets.\nInitially 38\u00a0LeShuttle locomotives were commissioned, with one at each end of a shuttle train.\nFreight locomotives.\nForty-six Class 92 locomotives for hauling freight trains and overnight passenger trains (the Nightstar project, which was abandoned) were commissioned, running on both overhead AC and third-rail DC power. However, RFF does not let these run on French railways, so there are plans to certify Alstom Prima II locomotives for use in the tunnel.\nInternational passenger.\nThirty-one Eurostar trains, based on the French TGV, built to UK loading gauge with many modifications for safety within the tunnel, were commissioned, with ownership split between British Rail, French national railways (SNCF) and Belgian national railways (NMBS/SNCB). British Rail ordered seven more for services north of London. Around 2010, Eurostar ordered ten trains from Siemens based on its Velaro product. The Class 374 entered service in 2016 and has been operating through the Channel Tunnel ever since alongside the current Class 373.\nGermany (DB) has since around 2005 tried to get permission to run train services to London. At the end of 2009, extensive fire-proofing requirements were dropped and DB received permission to run German Intercity-Express (ICE) test trains through the tunnel. In June 2013 DB was granted access to the tunnel, but these plans were ultimately dropped.\nIn October 2021, Renfe, the Spanish state railway company, expressed interest in operating a cross-Channel route between Paris and London using some of their existing trains with the intention of competing with Eurostar. No details have been revealed as to which trains would be used.\nService locomotives.\nDiesel locomotives for rescue and shunting work are Eurotunnel Class 0001 and Eurotunnel Class 0031.\nOperation.\nThe following chart presents the estimated number of passengers and tonnes of freight, respectively, annually transported through the Channel Tunnel since 1994 (M = million).\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Passengers\n&lt;templatestyles src=\"Legend/styles.css\" /&gt;\u00a0\u00a0Tonnes of freight\nUsage and services.\nTransport services offered by the tunnel are as follows:\nBoth the freight and passenger traffic forecasts that led to the construction of the tunnel were overestimated; in particular, Eurotunnel's commissioned forecasts were over-predictions. Although the captured share of Channel crossings was forecast correctly, high competition (especially from budget airlines which expanded rapidly in the 1990s and 2000s) and reduced tariffs led to low revenue. Overall cross-Channel traffic was overestimated.\nWith the EU's liberalisation of international rail services, the tunnel and High Speed 1 have been open to competition since 2010. There have been a number of operators interested in running trains through the tunnel and along High Speed 1 to London. In June 2013, after several years, DB obtained a license to operate Frankfurt \u2013 London trains, not expected to run before 2016 because of delivery delays of the custom-made trains.\nPlans for the service to Frankfurt seem to have been shelved in 2018.\nPassenger traffic volumes.\nCross-tunnel passenger traffic volumes peaked at 18.4\u00a0million in 1998, dropped to 14.9\u00a0million in 2003 and has increased substantially since then.\nAt the time of the decision about building the tunnel, 15.9\u00a0million passengers were predicted for Eurostar trains in the opening year. In 1995, the first full year, actual numbers were a little over 2.9\u00a0million, growing to 7.1\u00a0million in 2000, then dropping to 6.3\u00a0million in 2003. Eurostar was initially limited by the lack of a high-speed connection on the British side. After the completion of High Speed 1 in two stages in 2003 and 2007, traffic increased. In 2008, Eurostar carried 9,113,371 passengers, a 10% increase over the previous year, despite traffic limitations due to the 2008 Channel Tunnel fire. Eurostar passenger numbers continued to increase.\nFreight traffic volumes.\nFreight volumes have been erratic, with a major decrease during 1997 due to a closure caused by a fire in a freight shuttle. Freight crossings increased over the period, indicating the substitutability of the tunnel by sea crossings. The tunnel has achieved a market share close to or above Eurotunnel's 1980s predictions but Eurotunnel's 1990 and 1994 predictions were overestimates.\nFor through freight trains, the first year prediction was 7.2\u00a0million tonnes; the actual 1995 figure was 1.3M tonnes. Through freight volumes peaked in 1998 at 3.1M tonnes. This fell back to 1.21M tonnes in 2007, increasing slightly to 1.24M tonnes in 2008. Together with that carried on freight shuttles, freight growth has occurred since opening, with 6.4M tonnes carried in 1995, 18.4M tonnes recorded in 2003 and 19.6M tonnes in 2007. Numbers fell back in the wake of the 2008 fire.\nEurotunnel's freight subsidiary is Europorte 2. In September 2006 EWS, the UK's largest rail freight operator, announced that owing to the cessation of UK-French government subsidies of \u00a352\u00a0million per annum to cover the tunnel \"Minimum User Charge\" (a subsidy of around \u00a313,000 per train, at a traffic level of 4,000\u00a0trains per annum), freight trains would stop running after 30 November.\nEconomic performance.\nShares in Eurotunnel were issued at \u00a33.50 per share on 9 December 1987. By mid-1989 the price had risen to \u00a311.00. Delays and cost overruns led to the price dropping; during demonstration runs in October 1994, it reached an all-time low. Eurotunnel suspended payment on its debt in September 1995 to avoid bankruptcy. In December 1997 the British and French governments extended Eurotunnel's operating concession by 34\u00a0years, to 2086. The financial restructuring of Eurotunnel occurred in mid-1998, reducing debt and financial charges. Despite the restructuring, \"The Economist\" reported in 1998 that to break even Eurotunnel would have to increase fares, traffic and market share for sustainability. A cost-benefit analysis of the tunnel indicated that there were few impacts on the wider economy and few developments associated with the project and that the British economy would have been better off if it had not been constructed.\nUnder the terms of the Concession, Eurotunnel was obliged to investigate a cross-Channel road tunnel. In December 1999 road and rail tunnel proposals were presented to the British and French governments, but it was stressed that there was not enough demand for a second tunnel. A three-way treaty between the United Kingdom, France and Belgium governs border controls, with the establishment of \"control zones\" wherein the officers of the other nation may exercise limited customs and law enforcement powers. For most purposes, these are at either end of the tunnel, with the French border controls on the UK side of the tunnel and vice versa. For some city-to-city trains, the train is a control zone. A binational emergency plan coordinates UK and French emergency activities.\nIn 1999 Eurostar posted its first net profit, having made a loss of \u00a3925m in 1995. In 2005 Eurotunnel was described as being in a serious situation. In 2013, operating profits rose 4 percent from 2012, to \u00a354 million.\nSecurity.\nThere is a need for full passport controls since this is the border between the Schengen Area and the Common Travel Area. There are juxtaposed controls, meaning that passports are checked before boarding first by officials belonging to the departing country and then officials of the destination country. These are placed only at the main Eurostar stations: French officials operate at London St Pancras, Ebbsfleet International and Ashford International, while British officials operate at Calais-Fr\u00e9thun, Lille-Europe, Marne-la-Vall\u00e9e\u2013Chessy, Brussels-South and Paris-Gare du Nord. There are security checks before boarding as well. For the shuttle road-vehicle trains, there are juxtaposed passport controls before boarding the trains.\nFor Eurostar trains travelling from places south of Paris, there is no passport and security check before departure, and those trains must stop in Lille at least 30 minutes to allow all passengers to be checked. No checks are done on board. There have been plans for services from Amsterdam, Frankfurt and Cologne to London, but a major reason to cancel them was the need for a stop in Lille. Direct service from London to Amsterdam started on 4 April 2018; following the building of check-in terminals at Amsterdam and Rotterdam and the intergovernmental agreement, a direct service from the two Dutch cities to London started on 30 April 2020.\nTerminals.\nThe terminals' sites are at Cheriton (near Folkestone in the United Kingdom) and Coquelles (near Calais in France). The UK site uses the M20 motorway for access. The terminals are organised with the frontier controls juxtaposed with the entry to the system to allow travellers to go onto the motorway at the destination country immediately after leaving the shuttle.\nTo achieve design output at the French terminal, the shuttles accept cars on double-deck wagons; for flexibility, ramps were placed inside the shuttles to provide access to the top decks. At Folkestone there are of the main-line track, 45\u00a0turnouts and eight platforms. At Calais there are of track and 44\u00a0turnouts. At the terminals, the shuttle trains traverse a figure eight to reduce uneven wear on the wheels. There is a freight marshalling yard west of Cheriton at Dollands Moor Freight Yard.\nRegional impact.\nA 1996 report from the European Commission predicted that Kent and Nord-Pas de Calais had to face increased traffic volumes due to the general growth of cross-Channel traffic and traffic attracted by the tunnel. In Kent, a high-speed rail line to London would transfer traffic from road to rail. Kent's regional development would benefit from the tunnel, but being so close to London restricts the benefits. Gains are in the traditional industries and are largely dependent on the development of Ashford International railway station, without which Kent would be totally dependent on London's expansion. Nord-Pas-de-Calais enjoys a strong internal symbolic effect of the Tunnel which results in significant gains in manufacturing.\nThe removal of a bottleneck by means like the tunnel does not necessarily induce economic gains in all adjacent regions. The image of a region being connected to European high-speed transport and active political response is more important for regional economic development. Some small-medium enterprises located in the immediate vicinity of the terminal have used the opportunity to re-brand the profile of their business with positive effects, such as \"The New Inn\" at Etchinghill which was able to commercially exploit its unique selling point as being 'the closest pub to the Channel Tunnel'. Tunnel-induced regional development is small compared to general economic growth. The South East of England is likely to benefit developmentally and socially from faster and cheaper transport to continental Europe, but the benefits are unlikely to be equally distributed throughout the region. The overall environmental impact is almost certainly negative.\nSince the opening of the tunnel, small positive impacts on the wider economy have been felt, but it is difficult to identify major economic successes directly attributed to the tunnel.&lt;ref name=\"Flyvbjerg p. 68/69\"&gt;Flyvbjerg et al. p.\u00a068\u201369&lt;/ref&gt; The Eurotunnel does operate profitably, offering an alternative transportation mode unaffected by poor weather. High costs of construction did delay profitability, however, and companies involved in the tunnel's construction and operation early in operation relied on government aid to deal with the accumulated debt.\nIllegal immigration.\nIllegal immigrants and would-be asylum seekers have used the tunnel to attempt to enter Britain. By 1997, the problem had attracted international press attention, and by 1999, the French Red Cross opened the first migrant centre at Sangatte, using a warehouse once used for tunnel construction; by 2002, it housed up to 1,500\u00a0people at a time, most of them trying to get to the UK. In 2001, most came from Afghanistan, Iraq, and Iran, but African countries were also represented.\nEurotunnel, the company that operates the crossing, said that more than 37,000 migrants were intercepted between January and July 2015. Approximately 3,000 migrants, mainly from Ethiopia, Eritrea, Sudan and Afghanistan, were living in the temporary camps erected in Calais at the time of an official count in July 2015. An estimated 3,000 to 5,000 migrants were waiting in Calais for a chance to get to England.\nBritain and France operate a system of juxtaposed controls on immigration and customs, where investigations happen before travel. France is part of the Schengen immigration zone, removing border checks in normal times between most EU member states; Britain and the Republic of Ireland form their own separate Common Travel Area immigration zone.\nMost illegal immigrants and would-be asylum seekers who got into Britain found some way to ride a freight train. Trucks are loaded onto freight trains. In a few instances, migrants stowed away in a liquid chocolate tanker and managed to survive, spread across several attempts. Although the facilities were fenced, airtight security was deemed impossible; migrants would even jump from bridges onto moving trains. In several incidents people were injured during the crossing; others tampered with railway equipment, causing delays and requiring repairs. Eurotunnel said it was losing \u00a35m per month because of the problem.\nIn 2001 and 2002, several riots broke out at Sangatte, and groups of migrants (up to 550 in a December 2001 incident) stormed the fences and attempted to enter \"en masse\".\nOther migrants seeking permanent UK settlement use the Eurostar passenger train. They may purport to be visitors (whether to be issued with a required visit visa or deny and falsify their true intentions to obtain a maximum of 6-months-in-a-year at-port stamp); purport to be someone else whose documents they hold or used forged or counterfeit passports. Such breaches will result in refusal of permission to enter the UK, affected by Border Force after such a person's identity is fully established assuming they persist in their application to enter the UK.\nDiplomatic efforts.\nLocal authorities in both France and the UK called for the closure of the Sangatte migrant camp, and Eurotunnel twice sought an injunction against the centre. As of 2006 the United Kingdom blamed France for allowing Sangatte to open, and France blamed both the UK for its then lax asylum rules/law, and the EU for not having a uniform immigration policy. The \"cause c\u00e9l\u00e8bre\" nature of the problem even included journalists detained as they followed migrants onto railway property.\nIn 2002, after the European Commission told France that it was in breach of European Union rules on the free transfer of goods because of the delays and closures as a result of its poor security, a double fence was built at a cost of \u00a35\u00a0million, reducing the numbers of migrants detected each week reaching Britain on goods trains from 250 to almost none. Other measures included CCTV cameras and increased police patrols. At the end of 2002, the Sangatte centre was closed after the UK agreed to absorb some migrants.\nOn 23 and 30 June 2015, striking workers associated with MyFerryLink damaged the sections of track by burning car tires, leading to all trains being cancelled and a backlog of vehicles. Hundreds seeking to reach Britain made use of the situation to attempt to stow away inside and underneath transport trucks destined for the United Kingdom. Extra security measures included a \u00a32 million upgrade of detection technology, \u00a31 million extra for dog searches, and \u00a312 million (over three years) towards a joint fund with France for security surrounding the Port of Calais.\nIllegal attempts to cross and deaths.\nIn 2002, a dozen migrants died in crossing attempts. In the two months from June to July 2015, ten migrants died near the French tunnel terminal, during a period when 1,500 attempts to evade security precautions were being made each day.\nOn 6 July 2015, a migrant died while attempting to climb onto a freight train while trying to reach Britain from the French side of the Channel. The previous month an Eritrean man was killed under similar circumstances.\nDuring the night of 28 July 2015, one person, aged 25\u201330, was found dead after a night in which 1,500\u20132,000 migrants had attempted to enter the Eurotunnel terminal. The body of a Sudanese migrant was subsequently found inside the tunnel. On 4 August 2015, another Sudanese migrant walked nearly the entire length of one of the tunnels. He was arrested close to the British side, after having walked about through the tunnel.\nMechanical incidents.\nFires.\nThere have been three fires in the tunnel, all on the heavy goods vehicle (HGV) shuttles, that were significant enough to close the tunnel, as well as other minor incidents.\nOn 9 December 1994, during an \"invitation only\" testing phase, a fire broke out in a Ford Escort car while its owner was loading it onto the upper deck of a tourist shuttle. The fire started at about 10:00, with the shuttle train stationary in the Folkestone terminal, and was put out about 40 minutes later with no passenger injuries.\nOn 18 November 1996, a fire broke out on an HGV shuttle wagon in the tunnel, but nobody was seriously hurt. The exact cause is unknown, although it was neither a Eurotunnel equipment nor rolling stock problem; it may have been due to arson of a heavy goods vehicle. It is estimated that the heart of the fire reached , with the tunnel severely damaged over , with some affected to some extent. Full operation recommenced six months after the fire.\nOn 21 August 2006, the tunnel was closed for several hours when a truck on an HGV shuttle train caught fire.\nOn 11 September 2008, a fire occurred in the Channel Tunnel at 13:57 GMT. The incident started on an HGV shuttle train travelling towards France. The event occurred from the French entrance to the tunnel. No one was killed but several people were taken to hospitals suffering from smoke inhalation, and minor cuts and bruises. The tunnel was closed to all traffic, with the undamaged South Tunnel reopening for limited services two days later. Full service resumed on 9 February 2009 after repairs costing \u20ac60\u00a0million.\nOn 29 November 2012, the tunnel was closed for several hours after a truck on an HGV shuttle caught fire.\nOn 17 January 2015, both tunnels were closed following a lorry fire that filled the midsection of Running Tunnel North with smoke. Eurostar cancelled all services. The shuttle train had been heading from Folkestone to Coquelles and stopped adjacent to cross-passage CP 4418 just before 12:30 UTC. Thirty-eight passengers and four members of Eurotunnel staff were evacuated into the service tunnel and then transported to France using special STTS road vehicles in the Service Tunnel. The passengers and crew were taken to the Eurotunnel Fire/Emergency Management Centre close to the French portal.\nTrain failures.\nOn the night of 19/20 February 1996, about 1,000 passengers became trapped in the Channel Tunnel when Eurostar trains from London broke down owing to failures of electronic circuits caused by snow and ice being deposited and then melting on the circuit boards.\nOn 3 August 2007, an electrical failure lasting six hours caused passengers to be trapped in the tunnel on a shuttle.\nOn the evening of 18 December 2009, during the December 2009 European snowfall, five London-bound Eurostar trains failed inside the tunnel, trapping 2,000 passengers for approximately 16 hours, during the coldest temperatures in eight years. A Eurotunnel spokesperson explained that snow had evaded the train's winterisation shields, and the transition from cold air outside to the tunnel's warm atmosphere had melted the snow, resulting in electrical failures. One train was turned back before reaching the tunnel; two trains were hauled out of the tunnel by Eurotunnel Class 0001 diesel locomotives. The blocking of the tunnel led to the implementation of Operation Stack, the transformation of the M20 motorway into a linear car park.\nThe occasion was the first time that a Eurostar train was evacuated inside the tunnel; the failing of four at once was described as \"unprecedented\". The Channel Tunnel reopened the following morning. Nirj Deva, Member of the European Parliament for South East England, had called for Eurostar chief executive Richard Brown to resign over the incidents. An independent report by Christopher Garnett (former CEO of Great North Eastern Railway) and Claude Gressier (a French transport expert) on the 18/19 December 2009 incidents was issued in February 2010, making 21 recommendations.\nOn 7 January 2010, a Brussels\u2013London Eurostar broke down in the tunnel. The train had 236 passengers on board and was towed to Ashford; other trains that had not yet reached the tunnel were turned back.\nSafety.\nThe Channel Tunnel Safety Authority is responsible for some aspects of safety regulation in the tunnel; it reports to the Intergovernmental Commission (IGC).\n&lt;templatestyles src=\"Routemap/styles.css\"/&gt;\nThe service tunnel is used for access to technical equipment in cross-passages and equipment rooms, to provide fresh-air ventilation and for emergency evacuation. The Service Tunnel Transport System (STTS) allows fast access to all areas of the tunnel. The service vehicles are rubber-tired with a buried wire guidance system. The 24 STTS vehicles are used mainly for maintenance but also for firefighting and emergencies. \"Pods\" with different purposes, up to a payload of , are inserted into the side of the vehicles. The vehicles cannot turn around within the tunnel and are driven from either end. The maximum speed is when the steering is locked. A fleet of 15 Light Service Tunnel Vehicles (LADOGS) was introduced to supplement the STTSs. The LADOGS has a short wheelbase with a turning circle, allowing two-point turns within the service tunnel. Steering cannot be locked like the STTS vehicles, and maximum speed is . Pods up to can be loaded onto the rear of the vehicles. Drivers in the tunnel sit on the right, and the vehicles drive on the left. Owing to the risk of French personnel driving on their native right side of the road, sensors in the vehicles alert the driver if the vehicle strays to the right side.\nThe three tunnels contain of air that needs to be conditioned for comfort and safety. Air is supplied from ventilation buildings at Shakespeare Cliff and Sangatte, with each building capable of providing 100% standby capacity. Supplementary ventilation also exists on either side of the tunnel. In the event of a fire, ventilation is used to keep smoke out of the service tunnel and move smoke in one direction in the main tunnel to give passengers clean air. The tunnel was the first main-line railway tunnel to have special cooling equipment. Heat is generated from traction equipment and drag. The design limit was set at , using a mechanical cooling system with refrigeration plants on both sides that run chilled water circulating in pipes within the tunnel.\nTrains travelling at high speed create piston effect pressure changes that can affect passenger comfort, ventilation systems, tunnel doors, fans and the structure of the trains, and which drag on the trains. Piston relief ducts of diameter were chosen to solve the problem, with 4\u00a0ducts per kilometre to give close to optimum results. However, this design led to extreme lateral forces on the trains, so a reduction in train speed was required and restrictors were installed in the ducts.\nThe safety issue of a possible fire on a passenger-vehicle shuttle garnered much attention, with Eurotunnel noting that fire was the risk attracting the most attention in a 1994 safety case for three reasons: the opposition of ferry companies to passengers being allowed to remain with their cars; Home Office statistics indicating that car fires had doubled in ten years; and the long length of the tunnel. Eurotunnel commissioned the UK Fire Research Station\u2014now part of the Building Research Establishment\u2014to give reports of vehicle fires, and liaised with Kent Fire Brigade to gather vehicle fire statistics over one year. Fire tests took place at the French Mines Research Establishment with a mock wagon used to investigate how cars burned. The wagon door systems are designed to withstand fire inside the wagon for 30\u00a0minutes, longer than the transit time of 27\u00a0minutes. Wagon air conditioning units help to purge dangerous fumes from inside the wagon before travel. Each wagon has a fire detection and extinguishing system, with sensing of ions or ultraviolet radiation, smoke and gases that can trigger halon gas to quench a fire. Since the HGV wagons are not covered, fire sensors are located on the loading wagon and in the tunnel. A water main in the service tunnel provides water to the main tunnels at intervals. The ventilation system can control smoke movement. Special arrival sidings accept a train that is on fire, as the train is not allowed to stop whilst on fire in the tunnel unless continuing its journey would lead to a worse outcome. Eurotunnel has banned a wide range of hazardous goods from travelling in the tunnel. Two STTS (Service Tunnel Transportation System) vehicles with firefighting pods are on duty at all times, with a maximum delay of 10\u00a0minutes before they reach a burning train.\nUnusual traffic.\nTrains.\nIn 1999, the \"Kosovo Train for Life\" passed through the tunnel en route to Pristina, in Kosovo.\nOther.\nIn 2009, former F1 racing champion John Surtees drove a Ginetta G50 EV electric sports car prototype from England to France, using the service tunnel, as part of a charity event. He was required to keep to the speed limit. To celebrate the 2014 Tour de France's transfer from its opening stages in Britain to France in July of that year, Chris Froome of Team Sky rode a bicycle through the service tunnel, becoming the first solo rider to do so. The crossing took under an hour, reaching speeds of \u2014faster than most cross-channel ferries.\nMobile network coverage.\nSince 2012, French operators Bouygues Telecom, Orange and SFR have covered Running Tunnel South, the tunnel bore normally used for travel from France to Britain.\nIn January 2014, UK operators EE and Vodafone signed ten-year contracts with Eurotunnel for Running Tunnel North. The agreements will enable both operators' subscribers to use 2G and 3G services. Both EE and Vodafone planned to offer LTE services on the route; EE said it expected to cover the route with LTE connectivity by the summer of 2014. EE and Vodafone will offer Channel Tunnel network coverage for travellers from the UK to France. Eurotunnel said it also held talks with Three UK but has yet to reach an agreement with the operator.\nIn May 2014, Eurotunnel announced that they had installed equipment from Alcatel-Lucent to cover Running Tunnel North and simultaneously to provide mobile service (GSM 900/1800\u00a0MHz and UMTS 2100\u00a0MHz) by EE, O2 and Vodafone. The service of EE and Vodafone commenced on the same date as the announcement. O2 service was expected to be available soon afterwards.\nIn November 2014, EE announced that it had previously switched on LTE earlier in September 2014. O2 turned on 2G, 3G and 4G services in November 2014, whilst Vodafone's 4G was due to go live later.\nOther (non-transport) services.\nThe tunnel also houses the 1,000\u00a0MW ElecLink interconnector to transfer power between the British and French electricity networks. During the night 31 August/1 September 2021, The 51km-long 320\u00a0kV DC cable was switched into service for the first time.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;\nFurther reading.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5703", "revid": "1274395", "url": "https://en.wikipedia.org/wiki?curid=5703", "title": "Cyberpunk", "text": "Postmodern science fiction genre in a futuristic dystopian setting\nCyberpunk is a subgenre of science fiction in a dystopian futuristic setting that tends to focus on a \"combination of lowlife and high tech\", featuring futuristic technological and scientific achievements, such as artificial intelligence and cybernetics, juxtaposed with societal collapse, dystopia or decay. Much of cyberpunk is rooted in the New Wave science fiction movement of the 1960s and 1970s, when writers like Philip K. Dick, Michael Moorcock, Roger Zelazny, John Brunner, J. G. Ballard, Philip Jos\u00e9 Farmer and Harlan Ellison examined the impact of drug culture, technology, and the sexual revolution while avoiding the utopian tendencies of earlier science fiction.\nComics exploring cyberpunk themes began appearing as early as Judge Dredd, first published in 1977. Released in 1984, William Gibson's influential debut novel \"Neuromancer\" helped solidify cyberpunk as a genre, drawing influence from punk subculture and early hacker culture. Frank Miller's Ronin is an example of a cyberpunk graphic novel. Other influential cyberpunk writers included Bruce Sterling and Rudy Rucker. The Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation (also directed by Otomo) later popularizing the subgenre.\nEarly films in the genre include Ridley Scott's 1982 film \"Blade Runner\", one of several of Philip K. Dick's works that have been adapted into films (in this case, \"Do Androids Dream of Electric Sheep?\"). The \"first cyberpunk television series\" was the TV series \"Max Headroom\" from 1987, playing in a futuristic dystopia ruled by an oligarchy of television networks, and where computer hacking played a central role in many story lines. The films \"Johnny Mnemonic\" (1995) and \"New Rose Hotel\" (1998), both based upon short stories by William Gibson, flopped commercially and critically, while \"The Matrix trilogy\" (1999\u20132003) and \"Judge Dredd\" (1995) were some of the most successful cyberpunk films.\nNewer cyberpunk media includes \"Blade Runner 2049\" (2017), a sequel to the original 1982 film; \"Dredd\" (2012), which was not a sequel to the original movie; \"Upgrade\" (2018); \"\" (2019), based on the 1990s Japanese manga \"Battle Angel Alita\"; the 2018 Netflix TV series \"Altered Carbon\", based on Richard K. Morgan's 2002 novel of the same name; the 2020 remake of 1997 role-playing video game \"Final Fantasy VII\"; and the video game \"Cyberpunk 2077\" (2020), based on R. Talsorian Games's 1988 tabletop role-playing game \"Cyberpunk\".\nBackground.\nLawrence Person has attempted to define the content and ethos of the cyberpunk literary movement stating: \n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Classic cyberpunk characters were marginalized, alienated loners who lived on the edge of society in generally dystopic futures where daily life was impacted by rapid technological change, an ubiquitous datasphere of computerized information, and invasive modification of the human body.\nCyberpunk plots often center on conflict among artificial intelligences, hackers, and megacorporations, and tend to be set in a near-future Earth, rather than in the far-future settings or galactic vistas found in novels such as Isaac Asimov's \"Foundation\" or Frank Herbert's \"Dune\". The settings are usually post-industrial dystopias but tend to feature extraordinary cultural ferment and the use of technology in ways never anticipated by its original inventors (\"the street finds its own uses for things\"). Much of the genre's atmosphere echoes film noir, and written works in the genre often use techniques from detective fiction. There are sources who view that cyberpunk has shifted from a literary movement to a mode of science fiction due to the limited number of writers and its transition to a more generalized cultural formation.\nHistory and origins.\nThe origins of cyberpunk are rooted in the New Wave science fiction movement of the 1960s and 1970s, where \"New Worlds\", under the editorship of Michael Moorcock, began inviting and encouraging stories that examined new writing styles, techniques, and archetypes. Reacting to conventional storytelling, New Wave authors attempted to present a world where society coped with a constant upheaval of new technology and culture, generally with dystopian outcomes. Writers like Roger Zelazny, J. G. Ballard, Philip Jos\u00e9 Farmer, Samuel R. Delany, and Harlan Ellison often examined the impact of drug culture, technology, and the sexual revolution with an avant-garde style influenced by the Beat Generation (especially William S. Burroughs' science fiction writing), Dadaism, and their own ideas. Ballard attacked the idea that stories should follow the \"archetypes\" popular since the time of Ancient Greece, and the assumption that these would somehow be the same ones that would call to modern readers, as Joseph Campbell argued in \"The Hero with a Thousand Faces\". Instead, Ballard wanted to write a new myth for the modern reader, a style with \"more psycho-literary ideas, more meta-biological and meta-chemical concepts, private time systems, synthetic psychologies and space-times, more of the sombre half-worlds one glimpses in the paintings of schizophrenics.\"\nThis had a profound influence on a new generation of writers, some of whom would come to call their movement \"cyberpunk\". One, Bruce Sterling, later said:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;In the circle of American science fiction writers of my generation\u2014cyberpunks and humanists and so forth\u2014[Ballard] was a towering figure. We used to have bitter struggles over who was more Ballardian than whom. We knew we were not fit to polish the man's boots, and we were scarcely able to understand how we could get to a position to do work which he might respect or stand, but at least we were able to see the peak of achievement that he had reached.\nBallard, Zelazny, and the rest of New Wave was seen by the subsequent generation as delivering more \"realism\" to science fiction, and they attempted to build on this.\nSamuel R. Delany's 1968 novel \"Nova\" is also considered one of the major forerunners of the cyberpunk movement. It prefigures, for instance, cyberpunk's staple trope of humans interfacing with computers via implants. Writer William Gibson claimed to be greatly influenced by Delany, and his novel \"Neuromancer\" includes allusions to \"Nova\".\nSimilarly influential, and generally cited as proto-cyberpunk, is the Philip K. Dick novel \"Do Androids Dream of Electric Sheep?\", first published in 1968. Presenting precisely the general feeling of dystopian post-economic-apocalyptic future as Gibson and Sterling later deliver, it examines ethical and moral problems with cybernetic, artificial intelligence in a way more \"realist\" than the Isaac Asimov \"Robot\" series that laid its philosophical foundation. Dick's protege and friend K. W. Jeter wrote a novel called \"Dr. Adder\" in 1972 that, Dick lamented, might have been more influential in the field had it been able to find a publisher at that time. It was not published until 1984, after which Jeter made it the first book in a trilogy, followed by \"The Glass Hammer\" (1985) and \"Death Arms\" (1987). Jeter wrote other standalone cyberpunk novels before going on to write three authorized sequels to \"Do Androids Dream of Electric Sheep?\", named \"Blade Runner 2: The Edge of Human\" (1995), \"Blade Runner 3: Replicant Night\" (1996), and \"Blade Runner 4: Eye and Talon\".\n\"Do Androids Dream of Electric Sheep?\" was made into the seminal movie \"Blade Runner\", released in 1982. This was one year after William Gibson's story, \"Johnny Mnemonic\" helped move proto-cyberpunk concepts into the mainstream. That story, which also became a film years later in 1995, involves another dystopian future, where human couriers deliver computer data, stored cybernetically in their own minds.\nThe term \"cyberpunk\" first appeared as the title of a short story written by Bruce Bethke, written in 1980 and published in \"Amazing Stories\" in 1983. It was picked up by Gardner Dozois, editor of \"Isaac Asimov's Science Fiction Magazine\", and popularized in his editorials.\nBethke says he made two lists of words, one for technology, one for troublemakers, and experimented with combining them variously into compound words, consciously attempting to coin a term that encompassed both punk attitudes and high technology. He described the idea thus:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;The kids who trashed my computer; their kids were going to be Holy Terrors, combining the ethical vacuity of teenagers with a technical fluency we adults could only guess at. Further, the parents and other adult authority figures of the early 21st Century were going to be terribly ill-equipped to deal with the first generation of teenagers who grew up truly \"speaking computer\".\nAfterward, Dozois began using this term in his own writing, most notably in a \"Washington Post\" article where he said \"About the closest thing here to a self-willed esthetic 'school' would be the purveyors of bizarre hard-edged, high-tech stuff, who have on occasion been referred to as 'cyberpunks'\u2014Sterling, Gibson, Shiner, Cadigan, Bear.\"\nAbout that time in 1984, William Gibson's novel \"Neuromancer\" was published, delivering a glimpse of a future encompassed by what became an archetype of cyberpunk \"virtual reality\", with the human mind being fed light-based worldscapes through a computer interface. Some, perhaps ironically including Bethke himself, argued at the time that the writers whose style Gibson's books epitomized should be called \"Neuromantics\", a pun on the name of the novel plus \"New Romantics\", a term used for a New Wave pop music movement that had just occurred in Britain, but this term did not catch on. Bethke later paraphrased Michael Swanwick's argument for the term: \"the movement writers should properly be termed neuromantics, since so much of what they were doing was clearly imitating \"Neuromancer\".\nSterling was another writer who played a central role, often consciously, in the cyberpunk genre, variously seen as either keeping it on track, or distorting its natural path into a stagnant formula. In 1986, he edited a volume of cyberpunk stories called \", an attempt to establish what cyberpunk was, from Sterling's perspective.\nIn the subsequent decade, the motifs of Gibson's \"Neuromancer\" became formulaic, climaxing in the satirical extremes of Neal Stephenson's \"Snow Crash\" in 1992.\nBookending the cyberpunk era, Bethke himself published a novel in 1995 called \"Headcrash\", like \"Snow Crash\" a satirical attack on the genre's excesses. Fittingly, it won an honor named after cyberpunk's spiritual founder, the Philip K. Dick Award.\nIt satirized the genre in this way:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;...full of young guys with no social lives, no sex lives and no hope of ever moving out of their mothers' basements ... They're total wankers and losers who indulge in Messianic fantasies about someday getting even with the world through almost-magical computer skills, but whose actual use of the Net amounts to dialing up the scatophilia forum and downloading a few disgusting pictures. You know, cyberpunks.\nThe impact of cyberpunk, though, has been long-lasting. Elements of both the setting and storytelling have become normal in science fiction in general, and a slew of sub-genres now have -punk tacked onto their names, most obviously steampunk, but also a host of other cyberpunk derivatives.\nStyle and ethos.\nPrimary figures in the cyberpunk movement include William Gibson, Neal Stephenson, Bruce Sterling, Bruce Bethke, Pat Cadigan, Rudy Rucker, and John Shirley. Philip K. Dick (author of \"Do Androids Dream of Electric Sheep?\", from which the film \"Blade Runner\" was adapted) is also seen by some as prefiguring the movement.\n\"Blade Runner\" can be seen as a quintessential example of the cyberpunk style and theme. Video games, board games, and tabletop role-playing games, such as \"Cyberpunk 2020\" and \"Shadowrun\", often feature storylines that are heavily influenced by cyberpunk writing and movies. Beginning in the early 1990s, some trends in fashion and music were also labeled as cyberpunk. Cyberpunk is also featured prominently in anime and manga (Japanese cyberpunk), with \"Akira\", \"Ghost in the Shell\" and \"Cowboy Bebop\" being among the most notable.\nSetting.\nCyberpunk writers tend to use elements from crime fiction\u2014particularly hardboiled detective fiction and film noir\u2014and postmodernist prose to describe an often nihilistic underground side of an electronic society. The genre's vision of a troubled future is often called the antithesis of the generally utopian visions of the future popular in the 1940s and 1950s. Gibson defined cyberpunk's antipathy towards utopian science fiction in his 1981 short story \"The Gernsback Continuum,\" which pokes fun at and, to a certain extent, condemns utopian science fiction.\nIn some cyberpunk writing, much of the action takes place online, in cyberspace, blurring the line between actual and virtual reality. A typical trope in such work is a direct connection between the human brain and computer systems. Cyberpunk settings are dystopias with corruption, computers, and computer networks. Giant, multinational corporations have for the most part replaced governments as centers of political, economic, and even military power.\nThe economic and technological state of Japan is a regular theme in the cyberpunk literature of the 1980s. Of Japan's influence on the genre, William Gibson said, \"Modern Japan simply was cyberpunk.\" Cyberpunk is often set in urbanized, artificial landscapes, and \"city lights, receding\" was used by Gibson as one of the genre's first metaphors for cyberspace and virtual reality. The cityscapes of Hong Kong has had major influences in the urban backgrounds, ambiance and settings in many cyberpunk works such as \"Blade Runner\" and \"Shadowrun\". Ridley Scott envisioned the landscape of cyberpunk Los Angeles in \"Blade Runner\" to be \"Hong Kong on a very bad day\". The streetscapes of the \"Ghost in the Shell\" film were based on Hong Kong. Its director Mamoru Oshii felt that Hong Kong's strange and chaotic streets where \"old and new exist in confusing relationships\", fit the theme of the film well. Hong Kong's Kowloon Walled City is particularly notable for its disorganized hyper-urbanization and breakdown in traditional urban planning to be an inspiration to cyberpunk landscapes. Portrayals of East Asia and Asians in Western cyberpunk have been criticized as Orientalist and promoting racist tropes playing on American and European fears of East Asian dominance; this has been referred to as \"techno-Orientalism\".\nProtagonists.\nOne of the cyberpunk genre's prototype characters is Case, from Gibson's \"Neuromancer\". Case is a \"console cowboy,\" a brilliant drug addicted hacker who has betrayed his organized criminal partners. Robbed of his talent through a crippling injury inflicted by the vengeful partners, Case unexpectedly receives a once-in-a-lifetime opportunity to be healed by expert medical care but only if he participates in another criminal enterprise with a new crew.\nLike Case, many cyberpunk protagonists are manipulated or forced into situations where they have little or no control. They often begin their story with little to no power, starting in roles of subordinates or burnouts. The story usually involves them breaking out of these lowly roles early on. They typically have bittersweet or negative endings, rarely make great gains by the end of the story.\nProtagonists often fit into the role of outcasts, criminals, misfits and malcontents, expressing the \"punk\" component of cyberpunk. Due to the morally ambiguous nature of the worlds they inhabit, cyberpunk protagonists are usually antiheroes. They often engage with their society's drug subcultures or some other vice. Though they may morally or ethically oppose some of the more bleak aspects of their worlds, they are often too pragmatic or defeated to change them.\nSociety and government.\nCyberpunk can be intended to disquiet readers and call them to action. It often expresses a sense of rebellion, suggesting that one could describe it as a type of cultural revolution in science fiction. In the words of author and critic David Brin:\n...a closer look [at cyberpunk authors] reveals that they nearly always portray future societies in which governments have become wimpy and pathetic ...Popular science fiction tales by Gibson, Williams, Cadigan and others \"do\" depict Orwellian accumulations of power in the next century, but nearly always clutched in the secretive hands of a wealthy or corporate elite.\nCyberpunk stories have also been seen as fictional forecasts of the evolution of the Internet. The earliest descriptions of a global communications network came long before the World Wide Web entered popular awareness, though not before traditional science-fiction writers such as Arthur C. Clarke and some social commentators such as James Burke began predicting that such networks would eventually form.\nSome observers cite that cyberpunk tends to marginalize sectors of society such as women and people of colour. It is claimed that, for instance, cyberpunk depicts fantasies that ultimately empower masculinity using fragmentary and decentered aesthetic that culminate in a masculine genre populated by male outlaws. Critics also note the absence of any reference to Africa or black characters in the quintessential cyberpunk film \"Blade Runner\" while other films reinforce stereotypes.\nMedia.\nLiterature.\nMinnesota writer Bruce Bethke coined the term in 1983 for his short story \"Cyberpunk\", which was published in an issue of \"Amazing Science Fiction Stories\". The term was quickly appropriated as a label to be applied to the works of William Gibson, Bruce Sterling, Pat Cadigan and others. Of these, Sterling became the movement's chief ideologue, thanks to his fanzine \"Cheap Truth\". John Shirley wrote articles on Sterling and Rucker's significance. John Brunner's 1975 novel \"The Shockwave Rider\" is considered by many to be the first cyberpunk novel with many of the tropes commonly associated with the genre, some five years before the term was popularized by Dozois.\nWilliam Gibson with his novel \"Neuromancer\" (1984) is arguably the most famous writer connected with the term cyberpunk. He emphasized style, a fascination with surfaces, and atmosphere over traditional science-fiction tropes. Regarded as ground-breaking and sometimes as \"the archetypal cyberpunk work\", \"Neuromancer\" was awarded the Hugo, Nebula, and Philip K. Dick Awards. \"Count Zero\" (1986) and \"Mona Lisa Overdrive\" (1988) followed after Gibson's popular debut novel. According to the Jargon File, \"Gibson's near-total ignorance of computers and the present-day hacker culture enabled him to speculate about the role of computers and hackers in the future in ways hackers have since found both irritatingly na\u00efve and tremendously stimulating.\"\nEarly on, cyberpunk was hailed as a radical departure from science-fiction standards and a new manifestation of vitality. Shortly thereafter, however, some critics arose to challenge its status as a revolutionary movement. These critics said that the science fiction New Wave of the 1960s was much more innovative as far as narrative techniques and styles were concerned. Furthermore, while \"Neuromancer\"'s narrator may have had an unusual \"voice\" for science fiction, much older examples can be found: Gibson's narrative voice, for example, resembles that of an updated Raymond Chandler, as in his novel \"The Big Sleep\" (1939). Others noted that almost all traits claimed to be uniquely cyberpunk could in fact be found in older writers' works\u2014often citing J. G. Ballard, Philip K. Dick, Harlan Ellison, Stanis\u0142aw Lem, Samuel R. Delany, and even William S. Burroughs. For example, Philip K. Dick's works contain recurring themes of social decay, artificial intelligence, paranoia, and blurred lines between objective and subjective realities. The influential cyberpunk movie \"Blade Runner\" (1982) is based on his book, \"Do Androids Dream of Electric Sheep?\". Humans linked to machines are found in Pohl and Kornbluth's \"Wolfbane\" (1959) and Roger Zelazny's \"Creatures of Light and Darkness\" (1968).\nIn 1994, scholar Brian Stonehill suggested that Thomas Pynchon's 1973 novel \"Gravity's Rainbow\" \"not only curses but precurses what we now glibly dub cyberspace.\" Other important predecessors include Alfred Bester's two most celebrated novels, \"The Demolished Man\" and \"The Stars My Destination\", as well as Vernor Vinge's novella \"True Names\".\nReception and impact.\nScience-fiction writer David Brin describes cyberpunk as \"the finest free promotion campaign ever waged on behalf of science fiction\". It may not have attracted the \"real punks\", but it did ensnare many new readers, and it provided the sort of movement that postmodern literary critics found alluring. Cyberpunk made science fiction more attractive to academics, argues Brin; in addition, it made science fiction more profitable to Hollywood and to the visual arts generally. Although the \"self-important rhetoric and whines of persecution\" on the part of cyberpunk fans were irritating at worst and humorous at best, Brin declares that the \"rebels did shake things up. We owe them a debt.\"\nFredric Jameson considers cyberpunk the \"supreme literary expression if not of postmodernism, then of late capitalism itself\".\nCyberpunk further inspired many later writers to incorporate cyberpunk ideas into their own works, such as George Alec Effinger's \"When Gravity Fails\". \"Wired\" magazine, created by Louis Rossetto and Jane Metcalfe, mixes new technology, art, literature, and current topics in order to interest today's cyberpunk fans, which Paula Yoo claims \"proves that hardcore hackers, multimedia junkies, cyberpunks and cellular freaks are poised to take over the world\"\nFilm and television.\nThe film \"Blade Runner\" (1982) is set in 2019 in a dystopian future in which manufactured beings called replicants are slaves used on space colonies and are legal prey on Earth to various bounty hunters who \"retire\" (kill) them. Although \"Blade Runner\" was largely unsuccessful in its first theatrical release, it found a viewership in the home video market and became a cult film. Since the movie omits the religious and mythical elements of Dick's original novel (e.g. empathy boxes and Wilbur Mercer), it falls more strictly within the cyberpunk genre than the novel does. William Gibson would later reveal that upon first viewing the film, he was surprised at how the look of this film matched his vision for \"Neuromancer\", a book he was then working on. The film's tone has since been the staple of many cyberpunk movies, such as \"The Matrix trilogy\" (1999\u20132003), which uses a wide variety of cyberpunk elements.\nThe number of films in the genre or at least using a few genre elements has grown steadily since \"Blade Runner\". Several of Philip K. Dick's works have been adapted to the silver screen. The films \"Johnny Mnemonic\" and \"New Rose Hotel\", both based upon short stories by William Gibson, flopped commercially and critically. These box offices misses significantly slowed the development of cyberpunk as a literary or cultural form although a sequel to the 1982 film \"Blade Runner\" was released in October 2017 with Harrison Ford reprising his role from the original film. A rigorous implementation of all core cyberpunk hallmarks is the TV series Max Headroom from 1987, playing in a futuristic dystopia ruled by an oligarchy of television networks, and where computer hacking played a central role in many story lines. \"Max Headroom\" has been called \"the first cyberpunk television series\", with \"deep roots in the Western philosophical tradition\".\nIn addition, \"tech-noir\" film as a hybrid genre, means a work of combining neo-noir and science fiction or cyberpunk. It includes many cyberpunk films such as \"Blade Runner\", \"Burst City\", \"Robocop\", \"12 Monkeys\", \"The Lawnmower Man\", \"Hackers\", \"Hardware\", and \"Strange Days\", \"Total Recall\".\nAnime and manga.\nThe Japanese cyberpunk subgenre began in 1982 with the debut of Katsuhiro Otomo's manga series \"Akira\", with its 1988 anime film adaptation, which Otomo directed, later popularizing the subgenre. \"Akira\" inspired a wave of Japanese cyberpunk works, including manga and anime series such as \"Ghost in the Shell\", \"Battle Angel Alita\", and \"Cowboy Bebop\". Other early Japanese cyberpunk works include the 1982 film \"Burst City\", the 1985 original video animation \"Megazone 23\", and the 1989 film \"\".\nIn contrast to Western cyberpunk which has roots in New Wave science fiction literature, Japanese cyberpunk has roots in underground music culture, specifically the Japanese punk subculture that arose from the Japanese punk music scene in the 1970s. The filmmaker Sogo Ishii introduced this subculture to Japanese cinema with the punk film \"Panic High School\" (1978) and the punk biker film \"Crazy Thunder Road\" (1980), both portraying the rebellion and anarchy associated with punk, and the latter featuring a punk biker gang aesthetic. Ishii's punk films paved the way for Otomo's seminal cyberpunk work \"Akira\".\nCyberpunk themes are widely visible in anime and manga. In Japan, where cosplay is popular and not only teenagers display such fashion styles, cyberpunk has been accepted and its influence is widespread. William Gibson's \"Neuromancer,\" whose influence dominated the early cyberpunk movement, was also set in Chiba, one of Japan's largest industrial areas, although at the time of writing the novel Gibson did not know the location of Chiba and had no idea how perfectly it fit his vision in some ways. The exposure to cyberpunk ideas and fiction in the 1980s has allowed it to seep into the Japanese culture.\nCyberpunk anime and manga draw upon a futuristic vision which has elements in common with Western science fiction and therefore have received wide international acceptance outside Japan. \"The conceptualization involved in cyberpunk is more of forging ahead, looking at the new global culture. It is a culture that does not exist right now, so the Japanese concept of a cyberpunk future, seems just as valid as a Western one, especially as Western cyberpunk often incorporates many Japanese elements.\" William Gibson is now a frequent visitor to Japan, and he came to see that many of his visions of Japan have become a reality:\nModern Japan simply was cyberpunk. The Japanese themselves knew it and delighted in it. I remember my first glimpse of Shibuya, when one of the young Tokyo journalists who had taken me there, his face drenched with the light of a thousand media-suns\u2014all that towering, animated crawl of commercial information\u2014said, \"You see? You see? It is \"Blade Runner\" town.\" And it was. It so evidently was.\nCyberpunk themes have appeared in many anime and manga, including the ground-breaking \"Appleseed\", \"Ghost in the Shell\", \"Ergo Proxy\", \"Megazone 23\", \"Goku Midnight Eye\", \"Cyber City Oedo 808\", ', \"Bubblegum Crisis\", ', \"Angel Cop\", \"Blame!\", \"Armitage III\", \"Texhnolyze\", \"Psycho-Pass\" and \"No Guns Life\".\nInfluence.\n\"Akira\" (1982 manga) and its 1988 anime film adaptation have influenced numerous works in animation, comics, film, music, television and video games. \"Akira\" has been cited as a major influence on Hollywood films such as \"The Matrix\", \"Chronicle\", \"Looper\", \"Midnight Special\", and \"Inception\", as well as cyberpunk-influenced video games such as Hideo Kojima's \"Snatcher\" and \"Metal Gear Solid\", Valve's \"Half-Life\" series and Dontnod Entertainment's \"Remember Me\". \"Akira\" has also influenced the work of musicians such as Kanye West, who paid homage to \"Akira\" in the \"Stronger\" music video, and Lupe Fiasco, whose album \"Tetsuo &amp; Youth\" is named after Tetsuo Shima. The popular bike from the film, Kaneda's Motorbike, appears in \"Steven Spielberg\"'s film \"Ready Player One\", and CD Projekt's video game \"Cyberpunk 2077\".\n\"Ghost in the Shell\" (1995) influenced a number of prominent filmmakers, most notably the Wachowskis in \"The Matrix\" (1999) and its sequels. \"The Matrix\" series took several concepts from the film, including the Matrix digital rain, which was inspired by the opening credits of \"Ghost in the Shell\" and a sushi magazine the wife of the senior designer of the animation, Simon Witheley, had in the kitchen at the time., and the way characters access the Matrix through holes in the back of their necks. Other parallels have been drawn to James Cameron's \"Avatar\", Steven Spielberg's \"A.I. Artificial Intelligence\", and Jonathan Mostow's \"Surrogates\". James Cameron cited \"Ghost in the Shell\" as a source of inspiration, citing it as an influence on \"Avatar\".\nThe original video animation \"Megazone 23\" (1985) has a number of similarities to \"The Matrix\". \"Battle Angel Alita\" (1990) has had a notable influence on filmmaker James Cameron, who was planning to adapt it into a film since 2000. It was an influence on his TV series \"Dark Angel\", and he is the producer of the 2019 film adaptation \"\".\nComics.\nIn 1975, artist Moebius collaborated with writer Dan O'Bannon on a story called \"The Long Tomorrow\", published in the French magazine \"M\u00e9tal Hurlant\". One of the first works featuring elements now seen as exemplifying cyberpunk, it combined influences from film noir and hardboiled crime fiction with a distant sci-fi environment. Author William Gibson stated that Moebius' artwork for the series, along with other visuals from \"M\u00e9tal Hurlant\", strongly influenced his 1984 novel \"Neuromancer\". The series had a far-reaching impact in the cyberpunk genre, being cited as an influence on Ridley Scott's \"Alien\" (1979) and \"Blade Runner\". Moebius later expanded upon \"The Long Tomorrow\"'s aesthetic with \"The Incal\", a graphic novel collaboration with Alejandro Jodorowsky published from 1980 to 1988. The story centers around the exploits of a detective named John Difool in various science fiction settings, and while not confined to the tropes of cyberpunk, it features many elements of the genre.\nConcurrently with many other foundational cyberpunk works, DC Comics published Frank Miller's six-issue miniseries \"R\u014dnin\" from 1983 to 1984. The series, incorporating aspects of Samurai culture, martial arts films and manga, is set in a dystopian near-future New York. It explores the link between an ancient Japanese warrior and the apocalyptic, crumbling cityscape he finds himself in. The comic also bears several similarities to \"Akira\", with highly powerful telepaths playing central roles, as well as sharing many key visuals.\n\"R\u014dnin\" would go on to influence many later works, including \"Samurai Jack\" and the \"Teenage Mutant Ninja Turtles\", as well as video games such as \"Cyberpunk 2077\". Two years later, Miller himself would incorporate several toned-down elements of \"R\u014dnin\" into his acclaimed 1986 miniseries \"The Dark Knight Returns\", in which a retired Bruce Wayne once again takes up the mantle of Batman in a Gotham that is increasingly becoming more dystopian.\nPaul Pope's \"\", published in 2006, also exhibits several traits typical of cyberpunk fiction, such as a rebel protagonist opposing a future authoritarian state, and a distinct retrofuturist aesthetic that makes callbacks to both \"The Dark Knight Returns\" and Batman's original appearances in the 1940s.\nGames.\nThere are cyberpunk video games. Popular series include \"Final Fantasy VII\" and its spin-offs and remake, the \"Megami Tensei\" series, Kojima's \"Snatcher\" and \"Metal Gear\" series, \"Deus Ex\" series, \"Syndicate\" series, and \"System Shock\" and its sequel. Other games, like \"Blade Runner\", \"Ghost in the Shell\", and the \"Matrix\" series, are based upon genre movies, or role-playing games (for instance the various \"Shadowrun\" games).\nSeveral RPGs called \"Cyberpunk\" exist: \"Cyberpunk\", \"Cyberpunk 2020\", \"Cyberpunk v3.0\" and \"Cyberpunk Red\" written by Mike Pondsmith and published by R. Talsorian Games, and \"GURPS Cyberpunk\", published by Steve Jackson Games as a module of the GURPS family of RPGs. \"Cyberpunk 2020\" was designed with the settings of William Gibson's writings in mind, and to some extent with his approval, unlike the approach taken by FASA in producing the transgenre \"Shadowrun\" game and its various sequels, which mixes cyberpunk with fantasy elements such as magic and fantasy races such as orcs and elves. Both are set in the near future, in a world where cybernetics are prominent. In addition, Iron Crown Enterprises released an RPG named \"Cyberspace\", which was out of print for several years until recently being re-released in online PDF form. CD Projekt Red released \"Cyberpunk 2077,\" a cyberpunk open world first-person shooter/role-playing video game (RPG) based on the tabletop RPG \"Cyberpunk 2020\", on December 10, 2020. In 1990, in a convergence of cyberpunk art and reality, the United States Secret Service raided Steve Jackson Games's headquarters and confiscated all their computers. Officials denied that the target had been the \"GURPS Cyberpunk\" sourcebook, but Jackson would later write that he and his colleagues \"were never able to secure the return of the complete manuscript; [...] The Secret Service at first flatly refused to return anything \u2013 then agreed to let us copy files, but when we got to their office, restricted us to one set of out-of-date files \u2013 then agreed to make copies for us, but said \"tomorrow\" every day from March 4 to March 26. On March 26 we received a set of disks which purported to be our files, but the material was late, incomplete and well-nigh useless.\" Steve Jackson Games won a lawsuit against the Secret Service, aided by the new Electronic Frontier Foundation. This event has achieved a sort of notoriety, which has extended to the book itself as well. All published editions of \"GURPS Cyberpunk\" have a tagline on the front cover, which reads \"The book that was seized by the U.S. Secret Service!\" Inside, the book provides a summary of the raid and its aftermath.\nCyberpunk has also inspired several tabletop, miniature and board games such as \"Necromunda\" by Games Workshop. \"Netrunner\" is a collectible card game introduced in 1996, based on the \"Cyberpunk 2020\" role-playing game. \"Tokyo NOVA\", debuting in 1993, is a cyberpunk role-playing game that uses playing cards instead of dice.\n\"Cyberpunk 2077\" set a new record for the largest number of simultaneous players in a single player game, with a record 1,003,262 playing just after the December 10th launch, according to Steam Database. That tops the previous Steam record of 472,962 players set by \"Fallout 4\" back in 2015.\nMusic.\n&lt;templatestyles src=\"Template:Quote_box/styles.css\" /&gt;\n\"Much of the industrial/dance heavy 'Cyberpunk'\u2014recorded in Billy Idol's Macintosh-run studio\u2014revolves around Idol's theme of the common man rising up to fight against a faceless, soulless, corporate world.\"\n\u2014Julie Romandetta\nInvariably the origin of cyberpunk music lies in the synthesizer-heavy scores of cyberpunk films such as \"Escape from New York\" (1981) and \"Blade Runner\" (1982). Some musicians and acts have been classified as cyberpunk due to their aesthetic style and musical content. Often dealing with dystopian visions of the future or biomechanical themes, some fit more squarely in the category than others. Bands whose music has been classified as cyberpunk include Psydoll, Front Line Assembly, Clock DVA, Angelspit and Sigue Sigue Sputnik.\nSome musicians not normally associated with cyberpunk have at times been inspired to create concept albums exploring such themes. Albums such as the British musician and songwriter Gary Numan's \"Replicas\", \"The Pleasure Principle\" and \"Telekon\" were heavily inspired by the works of Philip K. Dick. Kraftwerk's \"The Man-Machine\" and \"Computer World\" albums both explored the theme of humanity becoming dependent on technology. Nine Inch Nails' concept album \"Year Zero\" also fits into this category. Fear Factory concept albums are heavily based upon future dystopia, cybernetics, clash between man and machines, virtual worlds. Billy Idol's \"Cyberpunk\" drew heavily from cyberpunk literature and the cyberdelic counter culture in its creation. \"1. Outside\", a cyberpunk narrative fueled concept album by David Bowie, was warmly met by critics upon its release in 1995. Many musicians have also taken inspiration from specific cyberpunk works or authors, including Sonic Youth, whose albums \"Sister\" and \"Daydream Nation\" take influence from the works of Philip K. Dick and William Gibson respectively. Madonna's 2001 Drowned World Tour opened with a cyberpunk section, where costumes, asethetics and stage props were used to accentuate the dystopian nature of the theatrical concert. Lady Gaga used a cyberpunk-persona and visual style for her sixth studio album Chromatica (2020).\nVaporwave and synthwave are also influenced by cyberpunk. The former has been inspired by one of the messages of cyberpunk and is interpreted as a dystopian critique of capitalism in the vein of cyberpunk and the latter is more surface-level, inspired only by the aesthetic of cyberpunk as a nostalgic retrofuturistic revival of aspects of cyberpunk's origins.\nSocial impact.\nArt and architecture.\nWriters David Suzuki and Holly Dressel describe the cafes, brand-name stores and video arcades of the Sony Center in the Potsdamer Platz public square of Berlin, Germany, as \"a vision of a cyberpunk, corporate urban future\".\nSociety and counterculture.\nSeveral subcultures have been inspired by cyberpunk fiction. These include the cyberdelic counter culture of the late 1980s and early 1990s. Cyberdelic, whose adherents referred to themselves as \"cyberpunks\", attempted to blend the psychedelic art and drug movement with the technology of cyberculture. Early adherents included Timothy Leary, Mark Frauenfelder and R. U. Sirius. The movement largely faded following the dot-com bubble implosion of 2000.\nCybergoth is a fashion and dance subculture which draws its inspiration from cyberpunk fiction, as well as rave and Gothic subcultures. In addition, a distinct cyberpunk fashion of its own has emerged in recent years which rejects the raver and goth influences of cybergoth, and draws inspiration from urban street fashion, \"post apocalypse\", functional clothing, high tech sports wear, tactical uniform and multifunction. This fashion goes by names like \"tech wear\", \"goth ninja\" or \"tech ninja\".\nThe Kowloon Walled City in Hong Kong (demolished in 1994) is often referenced as the model cyberpunk/dystopian slum as, given its poor living conditions at the time coupled with the city's political, physical, and economic isolation has caused many in academia to be fascinated by the ingenuity of its spawning.\nRelated genres.\nAs a wider variety of writers began to work with cyberpunk concepts, new subgenres of science fiction emerged, some of which could be considered as playing off the cyberpunk label, others which could be considered as legitimate explorations into newer territory. These focused on technology and its social effects in different ways. One prominent subgenre is \"steampunk,\" which is set in an alternate history Victorian era that combines anachronistic technology with cyberpunk's bleak film noir world view. The term was originally coined around 1987 as a joke to describe some of the novels of Tim Powers, James P. Blaylock, and K.W. Jeter, but by the time Gibson and Sterling entered the subgenre with their collaborative novel \"The Difference Engine\" the term was being used earnestly as well.\nAnother subgenre is \"biopunk\" (cyberpunk themes dominated by biotechnology) from the early 1990s, a derivative style building on biotechnology rather than informational technology. In these stories, people are changed in some way not by mechanical means, but by genetic manipulation.\nCyberpunk works have been described as well situated within postmodern literature.\nRegistered trademark status.\nIn the United States, the term \"Cyberpunk\" is a registered trademark by R. Talsorian Games Inc. for its tabletop role-playing game.\nWithin the European Union, the \"Cyberpunk\" trademark is owned by two parties: CD Projekt SA for \"games and online gaming services\" (particularly for the video game adaptation of the former) and by Sony Music for use outside games.\nSee also.\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5704", "revid": "46051363", "url": "https://en.wikipedia.org/wiki?curid=5704", "title": "Comic strip", "text": "Short serialized comics\nA comic strip is a sequence of cartoons, arranged in interrelated panels to display brief humor or form a narrative, often serialized, with text in balloons and captions. Traditionally, throughout the 20th and into the 21st century, these have been published in newspapers and magazines, with daily horizontal strips printed in black-and-white in newspapers, while Sunday papers offered longer sequences in special color comics sections. With the advent of the internet, online comic strips began to appear as webcomics.\nStrips are written and drawn by a comics artist, known as a cartoonist. As the word \"comic\" implies, strips are frequently humorous. Examples of these gag-a-day strips are \"Blondie\", \"Bringing Up Father\", \"Marmaduke\", and \"Pearls Before Swine\". In the late 1920s, comic strips expanded from their mirthful origins to feature adventure stories, as seen in \"Popeye\", \"Captain Easy\", \"Buck Rogers\", \"Tarzan\", and \"Terry and the Pirates\". In the 1940s, soap-opera-continuity strips such as \"Judge Parker\" and \"Mary Worth\" gained popularity. Because \"comic\" strips are not always funny, cartoonist Will Eisner has suggested that sequential art would be a better genre-neutral name.\nEvery day in American newspapers, for most of the 20th century, there were at least 200 different comic strips and cartoon panels, which makes 73,000 per year. Comic strips have appeared inside American magazines such as \"Liberty\" and \"Boys' Life\", but also on the front covers, such as the \"Flossy Frills\" series on \"The American Weekly\" Sunday newspaper supplement. In the UK and the rest of Europe, comic strips are also serialized in \"comic book magazines\", with a strip's story sometimes continuing over three pages.\nHistory.\nStorytelling using a sequence of pictures has existed through history. One medieval European example in textile form is the Bayeux Tapestry. Printed examples emerged in 19th-century Germany and in 18th-century England, where some of the first satirical or humorous sequential narrative drawings were produced. William Hogarth's 18th-century English cartoons include both narrative sequences, such as \"A Rake's Progress\", and single panels.\nThe \"Biblia pauperum\" (\"Paupers' Bible\"), a tradition of picture Bibles beginning in the later Middle Ages, sometimes depicted Biblical events with words spoken by the figures in the miniatures written on scrolls coming out of their mouths\u2014which makes them to some extent ancestors of the modern cartoon strips.\nIn China, with its traditions of block printing and of the incorporation of text with image, experiments with what became \"lianhuanhua\" date back to 1884.\nNewspapers.\nThe first newspaper comic strips appeared in North America in the late 19th century. \"The Yellow Kid\" is usually credited as one of the first newspaper strips. However, the art form combining words and pictures developed gradually and there are many examples which led up to the comic strip.\nThe Glasgow Looking Glass was the first mass-produced publication to tell stories using illustrations and is regarded as the worlds first comic strip. It satirised the political and social life of Scotland in the 1820s. It was conceived and illustrated by William Heath. \nSwiss author and caricature artist Rodolphe T\u00f6pffer (Geneva, 1799\u20131846) is considered the father of the modern comic strips. His illustrated stories such as \"Histoire de M. Vieux Bois\" (1827), first published in the US in 1842 as \"The Adventures of Obadiah Oldbuck\" or \"Histoire de Monsieur Jabot\" (1831), inspired subsequent generations of German and American comic artists. In 1865, German painter, author, and caricaturist Wilhelm Busch created the strip \"Max and Moritz\", about two trouble-making boys, which had a direct influence on the American comic strip. \"Max and Moritz\" was a series of seven severely moralistic tales in the vein of German children's stories such as \"Struwwelpeter\" (\"Shockheaded Peter\"). In the story's final act, the boys, after perpetrating some mischief, are tossed into a sack of grain, run through a mill, and consumed by a flock of geese (without anybody mourning their demise). \"Max and Moritz\" provided an inspiration for German immigrant Rudolph Dirks, who created the \"Katzenjammer Kids\" in 1897\u2014a strip starring two German-American boys visually modelled on \"Max and Moritz\". Familiar comic-strip iconography such as stars for pain, sawing logs for snoring, speech balloons, and thought balloons originated in Dirks' strip.\nHugely popular, \"Katzenjammer Kids\" occasioned one of the first comic-strip copyright ownership suits in the history of the medium. When Dirks left William Randolph Hearst for the promise of a better salary under Joseph Pulitzer, it was an unusual move, since cartoonists regularly deserted Pulitzer for Hearst. In a highly unusual court decision, Hearst retained the rights to the name \"Katzenjammer Kids\", while creator Dirks retained the rights to the characters. Hearst promptly hired Harold Knerr to draw his own version of the strip. Dirks renamed his version \"Hans and Fritz\" (later, \"The Captain and the Kids\"). Thus, two versions distributed by rival syndicates graced the comics pages for decades. Dirks' version, eventually distributed by United Feature Syndicate, ran until 1979.\nIn the United States, the great popularity of comics sprang from the . \"The Little Bears\" (1893\u201396) was the first American comic strip with recurring characters, while the first color comic supplement was published by the \"Chicago Inter-Ocean\" sometime in the latter half of 1892, followed by the \"New York Journal\"'s first color Sunday comic pages in 1897. On January 31, 1912, Hearst introduced the nation's first full daily comic page in his \"New York Evening Journal\". The history of this newspaper rivalry and the rapid appearance of comic strips in most major American newspapers is discussed by Ian Gordon. Numerous events in newspaper comic strips have reverberated throughout society at large, though few of these events occurred in recent years, owing mainly to the declining use of continuous storylines on newspaper comic strips, which since the 1970s had been waning as an entertainment form. From 1903 to 1905 Gustave Verbeek, wrote his comic series \"The UpsideDowns of Old Man Muffaroo and Little Lady Lovekins\". These comics were made in such a way that one could read the 6 panel comic, flip the book and keep reading. He made 64 such comics in total.\nThe longest-running American comic strips are:\nMost newspaper comic strips are syndicated; a syndicate hires people to write and draw a strip and then distributes it to many newspapers for a fee. Some newspaper strips begin or remain exclusive to one newspaper. For example, the \"Pogo\" comic strip by Walt Kelly originally appeared only in the \"New York Star\" in 1948 and was not picked up for syndication until the following year.\nNewspaper comic strips come in two different types: daily strips and Sunday strips. In the United States, a daily strip appears in newspapers on weekdays, Monday through Saturday, as contrasted with a Sunday strip, which typically only appears on Sundays. Daily strips usually are printed in black and white, and Sunday strips are usually in color. However, a few newspapers have published daily strips in color, and some newspapers have published Sunday strips in black and white.\nPopularity.\nMaking his first appearance in the British magazine \"Judy\" by writer and fledgling artist Charles H. Ross in 1867, Ally Sloper is one of the earliest comic strip characters and he is regarded as the first recurring character in comics. The highly popular character was spun off into his own comic, \"Ally Sloper's Half Holiday\", in 1884. \nWhile in the early 20th century comic strips were a frequent target for detractors of \"yellow journalism\", by the 1920s the medium became wildly popular. While radio, and later, television surpassed newspapers as a means of entertainment, most comic strip characters were widely recognizable until the 1980s, and the \"funny pages\" were often arranged in a way they appeared at the front of Sunday editions. In 1931, George Gallup's first poll had the comic section as the most important part of the newspaper, with additional surveys pointing out that the comic strips were the second most popular feature after the picture page. During the 1930s, many comic sections had between 12 and 16 pages, although in some cases, these had up to 24 pages.\nThe popularity and accessibility of strips meant they were often clipped and saved; authors including John Updike and Ray Bradbury have written about their childhood collections of clipped strips. Often posted on bulletin boards, clipped strips had an ancillary form of distribution when they were faxed, photocopied or mailed. \"The Baltimore Sun\"'s Linda White recalled, \"I followed the adventures of \"Winnie Winkle\", \"Moon Mullins\" and \"Dondi\", and waited each fall to see how Lucy would manage to trick Charlie Brown into trying to kick that football. (After I left for college, my father would clip out that strip each year and send it to me just to make sure I didn't miss it.)\"\nProduction and format.\nThe two conventional formats for newspaper comics are strips and single gag panels. The strips are usually displayed horizontally, wider than they are tall. Single panels are square, circular or taller than they are wide. Strips usually, but not always, are broken up into several smaller panels with continuity from panel to panel. A horizontal strip can also be used for a single panel with a single gag, as seen occasionally in Mike Peters' \"Mother Goose and Grimm\".\nEarly daily strips were large, often running the entire width of the newspaper, and were sometimes three or more inches high. Initially, a newspaper page included only a single daily strip, usually either at the top or the bottom of the page. By the 1920s, many newspapers had a comics page on which many strips were collected together. During the 1930s, the original art for a daily strip could be drawn as large as 25\u00a0inches wide by six inches high. Over decades, the size of daily strips became smaller and smaller, until by 2000, four standard daily strips could fit in an area once occupied by a single daily strip. As strips have become smaller, the number of panels have been reduced.\nProof sheets were the means by which syndicates provided newspapers with black-and-white line art for the reproduction of strips (which they arranged to have colored in the case of Sunday strips). Michigan State University Comic Art Collection librarian Randy Scott describes these as \"large sheets of paper on which newspaper comics have traditionally been distributed to subscribing newspapers. Typically each sheet will have either six daily strips of a given title or one Sunday strip. Thus, a week of \"Beetle Bailey\" would arrive at the \"Lansing State Journal\" in two sheets, printed much larger than the final version and ready to be cut apart and fitted into the local comics page.\" Comic strip historian Allan Holtz described how strips were provided as mats (the plastic or cardboard trays in which molten metal is poured to make plates) or even plates ready to be put directly on the printing press. He also notes that with electronic means of distribution becoming more prevalent printed sheets \"are definitely on their way out.\"\nNEA Syndicate experimented briefly with a two-tier daily strip, \"Star Hawks\", but after a few years, \"Star Hawks\" dropped down to a single tier.\nIn Flanders, the two-tier strip is the standard publication style of most daily strips like \"Spike and Suzy\" and \"Nero\". They appear Monday through Saturday; until 2003 there were no Sunday papers in Flanders. In the last decades, they have switched from black and white to color.\nCartoon panels.\nSingle panels usually, but not always, are not broken up and lack continuity. The daily \"Peanuts\" is a strip, and the daily \"Dennis the Menace\" is a single panel. J. R. Williams' long-run \"Out Our Way\" continued as a daily panel even after it expanded into a Sunday strip, \"Out Our Way with the Willets\". Jimmy Hatlo's \"They'll Do It Every Time\" was often displayed in a two-panel format with the first panel showing some deceptive, pretentious, unwitting or scheming human behavior and the second panel revealing the truth of the situation.\nSunday comics.\nSunday newspapers traditionally included a special color section. Early Sunday strips (known colloquially as \"the funny papers\", shortened to \"the funnies\"), such as \"Thimble Theatre\" and \"Little Orphan Annie\", filled an entire newspaper page, a format known to collectors as full page. Sunday pages during the 1930s and into the 1940s often carried a secondary strip by the same artist as the main strip. No matter whether it appeared above or below a main strip, the extra strip was known as the topper, such as \"The Squirrel Cage\" which ran along with \"Room and Board\", both drawn by Gene Ahern.\nDuring the 1930s, the original art for a Sunday strip was usually drawn quite large. For example, in 1930, Russ Westover drew his \"Tillie the Toiler\" Sunday page at a size of 17\" \u00d7 37\". In 1937, the cartoonist Dudley Fisher launched the innovative \"Right Around Home\", drawn as a huge single panel filling an entire Sunday page.\nFull-page strips were eventually replaced by strips half that size. Strips such as \"The Phantom\" and \"Terry and the Pirates\" began appearing in a format of two strips to a page in full-size newspapers, such as the \"New Orleans Times Picayune\", or with one strip on a tabloid page, as in the \"Chicago Sun-Times\". When Sunday strips began to appear in more than one format, it became necessary for the cartoonist to allow for rearranged, cropped or dropped panels. During World War II, because of paper shortages, the size of Sunday strips began to shrink. After the war, strips continued to get smaller and smaller because of increased paper and printing costs. The last full-page comic strip was the \"Prince Valiant\" strip for 11 April 1971.\nComic strips have also been published in Sunday newspaper magazines. Russell Patterson and Carolyn Wells' \"New Adventures of Flossy Frills\" was a continuing strip series seen on Sunday magazine covers. Beginning January 26, 1941, it ran on the front covers of Hearst's \"American Weekly\" newspaper magazine supplement, continuing until March 30 of that year. Between 1939 and 1943, four different stories featuring Flossy appeared on \"American Weekly\" covers.\nSunday comics sections employed offset color printing with multiple print runs imitating a wide range of colors. Printing plates were created with four or more colors\u2014traditionally, the CMYK color model: cyan, magenta, yellow and \"K\" for black. With a screen of tiny dots on each printing plate, the dots allowed an image to be printed in a halftone that appears to the eye in different gradations. The semi-opaque property of ink allows halftone dots of different colors to create an optical effect of full-color imagery.\nUnderground comic strips.\nThe decade of the 1960s saw the rise of underground newspapers, which often carried comic strips, such as \"Fritz the Cat\" and \"The Fabulous Furry Freak Brothers\". \"Zippy the Pinhead\" initially appeared in underground publications in the 1970s before being syndicated. \"Bloom County\" and \"Doonesbury\" began as strips in college newspapers under different titles, and later moved to national syndication. Underground comic strips covered subjects that are usually taboo in newspaper strips, such as sex and drugs. Many underground artists, notably Vaughn Bode, Dan O'Neill, Gilbert Shelton, and Art Spiegelman went on to draw comic strips for magazines such as \"Playboy\", \"National Lampoon\", and Pete Millar's \"CARtoons\". Jay Lynch graduated from undergrounds to alternative weekly newspapers to \"Mad\" and children's books.\nWebcomics.\n\"Webcomics\", also known as \"online comics\" and \"internet comics\", are comics that are available to read on the Internet. Many are exclusively published online, but the majority of traditional newspaper comic strips have some Internet presence. King Features Syndicate and other syndicates often provide archives of recent strips on their websites. Some, such as Scott Adams, creator of \"Dilbert\", include an email address in each strip.\nConventions and genres.\nMost comic strip characters do not age throughout the strip's life, but in some strips, like Lynn Johnston's award-winning \"For Better or For Worse\", the characters age as the years pass. The first strip to feature aging characters was \"Gasoline Alley\".\nThe history of comic strips also includes series that are not humorous, but tell an ongoing dramatic story. Examples include \"The Phantom\", \"Prince Valiant\", \"Dick Tracy\", \"Mary Worth\", \"Modesty Blaise\", \"Little Orphan Annie\", \"Flash Gordon\", and \"Tarzan\". Sometimes these are spin-offs from comic books, for example \"Superman\", \"Batman\", and \"The Amazing Spider-Man\".\nA number of strips have featured animals as main characters. Some are non-verbal (\"Marmaduke\", \"The Angriest Dog in the World\"), some have verbal thoughts but are not understood by humans, (\"Garfield\", Snoopy in \"Peanuts\"), and some can converse with humans (\"Bloom County\", \"Calvin and Hobbes\", \"Mutts\", \"Citizen Dog\", \"Buckles\", \"Get Fuzzy\", \"Pearls Before Swine\", and \"Pooch Cafe\"). Other strips are centered entirely on animals, as in \"Pogo\" and \"Donald Duck\". Gary Larson's \"The Far Side\" was unusual, as there were no central characters. Instead \"The Far Side\" used a wide variety of characters including humans, monsters, aliens, chickens, cows, worms, amoebas, and more. John McPherson's \"Close to Home\" also uses this theme, though the characters are mostly restricted to humans and real-life situations. Wiley Miller not only mixes human, animal, and fantasy characters, but also does several different comic strip continuities under one umbrella title, \"Non Sequitur\". Bob Thaves's \"Frank &amp; Ernest\" began in 1972 and paved the way for some of these strips, as its human characters were manifest in diverse forms\u2014as animals, vegetables, and minerals.\nSocial and political influence.\nThe comics have long held a distorted mirror to contemporary society, and almost from the beginning have been used for political or social commentary. This ranged from the conservative slant of Harold Gray's \"Little Orphan Annie\" to the unabashed liberalism of Garry Trudeau's \"Doonesbury\". Al Capp's \"Li'l Abner\" espoused liberal opinions for most of its run, but by the late 1960s, it became a mouthpiece for Capp's repudiation of the counterculture.\n\"Pogo\" used animals to particularly devastating effect, caricaturing many prominent politicians of the day as animal denizens of Pogo's Okeefenokee Swamp. In a fearless move, Pogo's creator Walt Kelly took on Joseph McCarthy in the 1950s, caricaturing him as a bobcat named Simple J. Malarkey, a megalomaniac who was bent on taking over the characters' birdwatching club and rooting out all undesirables. Kelly also defended the medium against possible government regulation in the McCarthy era. At a time when comic books were coming under fire for supposed sexual, violent, and subversive content, Kelly feared the same would happen to comic strips. Going before the Congressional subcommittee, he proceeded to charm the members with his drawings and the force of his personality. The comic strip was safe for satire.\nDuring the early 20th century, comic strips were widely associated with publisher William Randolph Hearst, whose papers had the largest circulation of strips in the United States. Hearst was notorious for his practice of yellow journalism, and he was frowned on by readers of \"The New York Times\" and other newspapers which featured few or no comic strips. Hearst's critics often assumed that all the strips in his papers were fronts for his own political and social views. Hearst did occasionally work with or pitch ideas to cartoonists, most notably his continued support of George Herriman's \"Krazy Kat\". An inspiration for Bill Watterson and other cartoonists, \"Krazy Kat\" gained a considerable following among intellectuals during the 1920s and 1930s.\nSome comic strips, such as \"Doonesbury\" and \"Mallard Fillmore\", may be printed on the editorial or op-ed page rather than the comics page because of their regular political commentary. For example, the August 12, 1974 \"Doonesbury\" strip was awarded a 1975 Pulitzer Prize for its depiction of the Watergate scandal. \"Dilbert\" is sometimes found in the business section of a newspaper instead of the comics page because of the strip's commentary about office politics, and \"Tank McNamara\" often appears on the sports page because of its subject matter. Lynn Johnston's \"For Better or For Worse\" created an uproar when Lawrence, one of the strip's supporting characters, came out of the closet.\nPublicity and recognition.\nThe world's longest comic strip is long and on display at Trafalgar Square as part of the London Comedy Festival. The London Cartoon Strip was created by 15 of Britain's best known cartoonists and depicts the history of London.\nThe Reuben, named for cartoonist Rube Goldberg, is the most prestigious award for U.S. comic strip artists. Reuben awards are presented annually by the National Cartoonists Society (NCS).\nIn 1995, the United States Postal Service issued a series of commemorative stamps, Comic Strip Classics, marking the comic-strip centennial.\nToday's strip artists, with the help of the NCS, enthusiastically promote the medium, which since the 1970s (and particularly the 1990s) has been considered to be in decline due to numerous factors such as changing tastes in humor and entertainment, the waning relevance of newspapers in general and the loss of most foreign markets outside English-speaking countries. One particularly humorous example of such promotional efforts is the Great Comic Strip Switcheroonie, held in 1997 on April Fool's Day, an event in which dozens of prominent artists took over each other's strips. \"Garfield\"'s Jim Davis, for example, switched with \"Blondie\"'s Stan Drake, while Scott Adams (\"Dilbert\") traded strips with Bil Keane (\"The Family Circus\").\nWhile the 1997 Switcheroonie was a one-time publicity stunt, an artist taking over a feature from its originator is an old tradition in newspaper cartooning (as it is in the comic book industry). In fact, the practice has made possible the longevity of the genre's more popular strips. Examples include \"Little Orphan Annie\" (drawn and plotted by Harold Gray from 1924 to 1944 and thereafter by a succession of artists including Leonard Starr and Andrew Pepoy), and \"Terry and the Pirates\", started by Milton Caniff in 1934 and picked up by George Wunder.\nA business-driven variation has sometimes led to the same feature continuing under a different name. In one case, in the early 1940s, Don Flowers' \"Modest Maidens\" was so admired by William Randolph Hearst that he lured Flowers away from the Associated Press and to King Features Syndicate by doubling the cartoonist's salary, and renamed the feature \"Glamor Girls\" to avoid legal action by the AP. The latter continued to publish \"Modest Maidens\", drawn by Jay Allen in Flowers' style.\nIssues in U.S. newspaper comic strips.\nAs newspapers have declined, the changes have affected comic strips. Jeff Reece, lifestyle editor of \"The Florida Times-Union\", wrote, \"Comics are sort of the 'third rail' of the newspaper.\"\nSize.\nIn the early decades of the 20th century, all Sunday comics received a full page, and daily strips were generally the width of the page. The competition between papers for having more cartoons than the rest from the mid-1920s, the growth of large-scale newspaper advertising during most of the thirties, paper rationing during World War II, the decline on news readership (as television newscasts began to be more common) and inflation (which has caused higher printing costs) beginning during the fifties and sixties led to Sunday strips being published on smaller and more diverse formats. As newspapers have reduced the page count of Sunday comic sections since the late 1990s (by the 2010s, most sections have only four pages, with the back page not always being destined for comics) has also led to further downsizes.\nDaily strips have suffered as well. Before the mid-1910s, there wasn't a \"standard\" size\", with strips running the entire width of a page or having more than one tier. By the 1920s, strips often covered six of the eight columns occupied by a traditional broadsheet paper. During the 1940s, strips were reduced to four columns wide (with a \"transition\" width of five columns). As newspapers became narrower beginning in the 1970s, strips have gotten even smaller, often being just three columns wide, a similar width to the one most daily panels occupied before the 1940s.\nIn an issue related to size limitations, Sunday comics are often bound to rigid formats that allow their panels to be rearranged in several different ways while remaining readable. Such formats usually include throwaway panels at the beginning, which some newspapers will omit for space. As a result, cartoonists have less incentive to put great efforts into these panels. \"Garfield\" and \"Mutts\" were known during the mid-to-late 80s and 1990s respectively for their throwaways on their Sunday strips, however both strips now run \"generic\" title panels.\nSome cartoonists have complained about this, with Walt Kelly, creator of \"Pogo,\" openly voicing his discontent about being forced to draw his Sunday strips in such rigid formats from the beginning. Kelly's heirs opted to end the strip in 1975 as a form of protest against the practice. Since then, \"Calvin and Hobbes\" creator Bill Watterson has written extensively on the issue, arguing that size reduction and dropped panels reduce both the potential and freedom of a cartoonist. After a lengthy battle with his syndicate, Watterson won the privilege of making half page-sized Sunday strips where he could arrange the panels any way he liked. Many newspaper publishers and a few cartoonists objected to this, and some papers continued to print \"Calvin and Hobbes\" at small sizes. \"Opus\" won that same privilege years after \"Calvin and Hobbes\" ended, while Wiley Miller circumvented further downsizes by making his \"Non Sequitur\" Sunday strip available only in a vertical arrangement. Actually, most strips created since 1990 are drawn in the unbroken \"third-page\" format. Few newspapers still run half-page strips, as with \"Prince Valiant\" and \"H\u00e4gar the Horrible\" in the front page of the \"Reading Eagle\" Sunday comics section until the mid-2010s.\nFormat.\nWith the success of \"The Gumps\" during the 1920s, it became commonplace for strips (comedy- and adventure-laden alike) to have lengthy stories spanning weeks or months. The \"Monarch of Medioka\" story in Floyd Gottfredson's \"Mickey Mouse\" comic strip ran from September 8, 1937 to May 2, 1938. Between the 1960s and the late 1980s, as television news relegated newspaper reading to an occasional basis rather than daily, syndicators were abandoning long stories and urging cartoonists to switch to simple daily gags, or week-long \"storylines\" (with six consecutive (mostly unrelated) strips following a same subject), with longer storylines being used mainly on adventure-based and dramatic strips. Strips begun during the mid-1980s or after (such as \"Get Fuzzy\", \"Over the Hedge\", \"Monty\", and others) are known for their heavy use of storylines, lasting between one and three weeks in most cases.\nThe writing style of comic strips changed as well after World War II. With an increase in the number of college-educated readers, there was a shift away from slapstick comedy and towards more cerebral humor. Slapstick and visual gags became more confined to Sunday strips, because as \"Garfield\" creator Jim Davis put it, \"Children are more likely to read Sunday strips than dailies.\"\nSecond author.\nMany older strips are no longer drawn by the original cartoonist, who has either died or retired. Such strips are known as \"zombie strips\". A cartoonist, paid by the syndicate or sometimes a relative of the original cartoonist, continues writing the strip, a tradition that became commonplace in the early half of the 20th century. \"H\u00e4gar the Horrible\" and \"Frank and Ernest\" are both drawn by the sons of the creators. Some strips which are still in affiliation with the original creator are produced by small teams or entire companies, such as Jim Davis' \"Garfield\", however there is some debate if these strips fall in this category.\nThis act is commonly criticized by modern cartoonists including Watterson and \"Pearls Before Swine\"'s Stephan Pastis. The issue was addressed in six consecutive \"Pearls\" strips in 2005. Charles Schulz, of \"Peanuts\" fame, requested that his strip not be continued by another cartoonist after his death. He also rejected the idea of hiring an inker or letterer, comparing it to a golfer hiring a man to make his putts. Schulz's family has honored his wishes and refused numerous proposals by syndicators to continue \"Peanuts\" with a new author.\nAssistants.\nSince the consolidation of newspaper comics by the first quarter of the 20th century, most cartoonists have used a group of assistants (with usually one of them credited). However, quite a few cartoonists (e.g.: George Herriman and Charles Schulz, among others) have done their strips almost completely by themselves; often criticizing the use of assistants for the same reasons most have about their editors hiring anyone else to continue their work after their retirement.\nRights to the strips.\nHistorically, syndicates owned the creators' work, enabling them to continue publishing the strip after the original creator retired, left the strip, or died. This practice led to the term \"legacy strips\", or more pejoratively \"zombie strips\". Most syndicates signed creators to 10- or even 20-year contracts. (There have been exceptions, however, such as Bud Fisher's \"Mutt and Jeff\" being an early\u2014if not the earliest\u2014case in which the creator retained ownership of his work.) Both these practices began to change with the 1970 debut of Universal Press Syndicate, as the company gave cartoonists a 50-percent ownership share of their work. Creators Syndicate, founded in 1987, granted artists full rights to the strips, something that Universal Press did in 1990, followed by King Features in 1995. By 1999 both Tribune Media Services and United Feature had begun granting ownership rights to creators (limited to new and/or hugely popular strips).\nCensorship.\nStarting in the late 1940s, the national syndicates which distributed newspaper comic strips subjected them to very strict censorship. \"Li'l Abner\" was censored in September 1947 and was pulled from the Pittsburgh Press by Scripps-Howard. The controversy, as reported in \"Time\", centered on Capp's portrayal of the U.S. Senate. Said Edward Leech of Scripps, \"We don't think it is good editing or sound citizenship to picture the Senate as an assemblage of freaks and crooks... boobs and undesirables.\"\nAs comics are easier for children to access compared to other types of media, they have a significantly more rigid censorship code than other media. Stephan Pastis has lamented that the \"unwritten\" censorship code is still \"stuck somewhere in the 1950s\". Generally, comics are not allowed to include such words as \"damn\", \"sucks\", \"screwed\", and \"hell\", although there have been exceptions such as the September 22, 2010 \"Mother Goose and Grimm\" in which an elderly man says, \"This nursing home food sucks,\" and a pair of \"Pearls Before Swine\" comics from January 11, 2011 with a character named Ned using the word \"crappy\". Naked backsides and shooting guns cannot be shown, according to \"Dilbert\" cartoonist Scott Adams. Such comic strip taboos were detailed in Dave Breger's book \"But That's Unprintable\" (Bantam, 1955).\nMany issues such as sex, narcotics, and terrorism cannot or can very rarely be openly discussed in strips, although there are exceptions, usually for satire, as in \"Bloom County\". This led some cartoonists to resort to double entendre or dialogue children do not understand, as in Greg Evans' \"Luann\". Another example of wordplay to get around censorship is a July 27, 2016 Pearls Before Swine strip that features Pig talking to his sister, and says the phrase \"I SIS!\" repeatedly after correcting his sister's grammar. The strip then cuts to a scene of a NSA wiretap agent, following a scene of Pig being arrested by the FBI saying \"Never correct your sister's grammar\", implying that the CIA mistook the phrase \"I SIS\" with \"ISIS\". Younger cartoonists have claimed commonplace words, images, and issues should be allowed in the comics, considering that the pressure on \"clean\" humor has been a chief factor for the declining popularity of comic strips since the 1990s (Aaron McGruder, creator of \"The Boondocks\", decided to end his strip partly because of censorship issues, while the \"Popeye\" daily comic strip ended in 1994 after newspapers objected to a storyline they considered to be a satire on abortion). Some of the taboo words and topics are mentioned daily on television and other forms of visual media. Webcomics and comics distributed primarily to college newspapers are much freer in this respect.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5705", "revid": "43025706", "url": "https://en.wikipedia.org/wiki?curid=5705", "title": "Continuum hypothesis", "text": "Proposition in mathematical logic\nIn mathematics, specifically set theory, the continuum hypothesis (abbreviated CH) is a hypothesis about the possible sizes of infinite sets. It states that\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;there is no set whose cardinality is strictly between that of the integers and the real numbers,\nor equivalently, that\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;any subset of the real numbers is finite, is countably infinite, or has the same cardinality as the real numbers.\nIn Zermelo\u2013Fraenkel set theory with the axiom of choice (ZFC), this is equivalent to the following equation in aleph numbers: formula_1, or even shorter with beth numbers: formula_2.\nThe continuum hypothesis was advanced by Georg Cantor in 1878, and establishing its truth or falsehood is the first of Hilbert's 23\u00a0problems presented in 1900. The answer to this problem is independent of ZFC, so that either the continuum hypothesis or its negation can be added as an axiom to ZFC set theory, with the resulting theory being consistent if and only if ZFC is consistent. This independence was proved in 1963 by Paul Cohen, complementing earlier work by Kurt G\u00f6del in 1940.\nThe name of the hypothesis comes from the term \"the continuum\" for the real numbers.\nHistory.\nCantor believed the continuum hypothesis to be true and for many years tried in vain to prove it. It became the first on David Hilbert's list of important open questions that was presented at the International Congress of Mathematicians in the year 1900 in Paris. Axiomatic set theory was at that point not yet formulated. \nKurt G\u00f6del proved in 1940 that the negation of the continuum hypothesis, i.e., the existence of a set with intermediate cardinality, could not be proved in standard set theory. The second half of the independence of the continuum hypothesis \u2013 i.e., unprovability of the nonexistence of an intermediate-sized set \u2013 was proved in 1963 by Paul Cohen.\nCardinality of infinite sets.\nTwo sets are said to have the same \"cardinality\" or \"cardinal number\" if there exists a bijection (a one-to-one correspondence) between them. Intuitively, for two sets \"S\" and \"T\" to have the same cardinality means that it is possible to \"pair off\" elements of \"S\" with elements of \"T\" in such a fashion that every element of \"S\" is paired off with exactly one element of \"T\" and vice versa. Hence, the set {banana, apple, pear} has the same cardinality as {yellow, red, green}.\nWith infinite sets such as the set of integers or rational numbers, the existence of a bijection between two sets becomes more difficult to demonstrate. The rational numbers seemingly form a counterexample to the continuum hypothesis: the integers form a proper subset of the rationals, which themselves form a proper subset of the reals, so intuitively, there are more rational numbers than integers and more real numbers than rational numbers. However, this intuitive analysis is flawed; it does not take proper account of the fact that all three sets are infinite. It turns out the rational numbers can actually be placed in one-to-one correspondence with the integers, and therefore the set of rational numbers is the same size (\"cardinality\") as the set of integers: they are both countable sets.\nCantor gave two proofs that the cardinality of the set of integers is strictly smaller than that of the set of real numbers (see Cantor's first uncountability proof and Cantor's diagonal argument). His proofs, however, give no indication of the extent to which the cardinality of the integers is less than that of the real numbers. Cantor proposed the continuum hypothesis as a possible solution to this question.\nThe continuum hypothesis states that the set of real numbers has minimal possible cardinality which is greater than the cardinality of the set of integers. That is, every set, \"S\", of real numbers can either be mapped one-to-one into the integers or the real numbers can be mapped one-to-one into \"S\". As the real numbers are equinumerous with the powerset of the integers, formula_3 and the continuum hypothesis says that there is no set formula_4 for which formula_5.\nAssuming the axiom of choice, there is a unique smallest cardinal number formula_6 greater than formula_7, and the continuum hypothesis is in turn equivalent to the equality formula_8.\nIndependence from ZFC.\nThe independence of the continuum hypothesis (CH) from Zermelo\u2013Fraenkel set theory (ZF) follows from combined work of Kurt G\u00f6del and Paul Cohen.\nG\u00f6del showed that CH cannot be disproved from ZF, even if the axiom of choice (AC) is adopted (making ZFC). G\u00f6del's proof shows that CH and AC both hold in the constructible universe L, an inner model of ZF set theory, assuming only the axioms of ZF. The existence of an inner model of ZF in which additional axioms hold shows that the additional axioms are consistent with ZF, provided ZF itself is consistent. The latter condition cannot be proved in ZF itself, due to G\u00f6del's incompleteness theorems, but is widely believed to be true and can be proved in stronger set theories.\nCohen showed that CH cannot be proven from the ZFC axioms, completing the overall independence proof. To prove his result, Cohen developed the method of forcing, which has become a standard tool in set theory. Essentially, this method begins with a model of ZF in which CH holds, and constructs another model which contains more sets than the original, in a way that CH does not hold in the new model. Cohen was awarded the Fields Medal in 1966 for his proof.\nThe independence proof just described shows that CH is independent of ZFC. Further research has shown that CH is independent of all known \"large cardinal axioms\" in the context of ZFC. Moreover, it has been shown that the cardinality of the continuum can be any cardinal consistent with K\u00f6nig's theorem. A result of Solovay, proved shortly after Cohen's result on the independence of the continuum hypothesis, shows that in any model of ZFC, if formula_9 is a cardinal of uncountable cofinality, then there is a forcing extension in which formula_10. However, per K\u00f6nig's theorem, it is not consistent to assume formula_11 is formula_12 or formula_13 or any cardinal with cofinality formula_14.\nThe continuum hypothesis is closely related to many statements in analysis, point set topology and measure theory. As a result of its independence, many substantial conjectures in those fields have subsequently been shown to be independent as well.\nThe independence from ZFC means that proving or disproving the CH within ZFC is impossible. However, G\u00f6del and Cohen's negative results are not universally accepted as disposing of all interest in the continuum hypothesis. Hilbert's problem remains an active topic of research; see Woodin and Peter Koellner for an overview of the current research status.\nThe continuum hypothesis was not the first statement shown to be independent of ZFC. An immediate consequence of G\u00f6del's incompleteness theorem, which was published in 1931, is that there is a formal statement (one for each appropriate G\u00f6del numbering scheme) expressing the consistency of ZFC that is independent of ZFC, assuming that ZFC is consistent. The continuum hypothesis and the axiom of choice were among the first mathematical statements shown to be independent of ZF set theory.\nArguments for and against the continuum hypothesis.\nG\u00f6del believed that CH is false, and that his proof that CH is consistent with ZFC only shows that the Zermelo\u2013Fraenkel axioms do not adequately characterize the universe of sets. G\u00f6del was a platonist and therefore had no problems with asserting the truth and falsehood of statements independent of their provability. Cohen, though a formalist, also tended towards rejecting CH.\nHistorically, mathematicians who favored a \"rich\" and \"large\" universe of sets were against CH, while those favoring a \"neat\" and \"controllable\" universe favored CH. Parallel arguments were made for and against the axiom of constructibility, which implies CH. More recently, Matthew Foreman has pointed out that ontological maximalism can actually be used to argue in favor of CH, because among models that have the same reals, models with \"more\" sets of reals have a better chance of satisfying CH.\nAnother viewpoint is that the conception of set is not specific enough to determine whether CH is true or false. This viewpoint was advanced as early as 1923 by Skolem, even before G\u00f6del's first incompleteness theorem. Skolem argued on the basis of what is now known as Skolem's paradox, and it was later supported by the independence of CH from the axioms of ZFC since these axioms are enough to establish the elementary properties of sets and cardinalities. In order to argue against this viewpoint, it would be sufficient to demonstrate new axioms that are supported by intuition and resolve CH in one direction or another. Although the axiom of constructibility does resolve CH, it is not generally considered to be intuitively true any more than CH is generally considered to be false.\nAt least two other axioms have been proposed that have implications for the continuum hypothesis, although these axioms have not currently found wide acceptance in the mathematical community. In 1986, Chris Freiling presented an argument against CH by showing that the negation of CH is equivalent to Freiling's axiom of symmetry, a statement derived by arguing from particular intuitions about probabilities. Freiling believes this axiom is \"intuitively true\" but others have disagreed.\nA difficult argument against CH developed by W. Hugh Woodin has attracted considerable attention since the year 2000. Foreman does not reject Woodin's argument outright but urges caution. Woodin proposed a new hypothesis that he labeled the (*)-axiom\", or \"Star axiom\". The Star axiom would imply that formula_11 is formula_16, thus falsifying CH. The Star axiom was bolstered by an independent May 2021 proof showing the Star axiom can be derived from a variation of Martin's maximum. However, Woodin stated in the 2010s that he now instead believes CH to be true, based on his belief in his new \"ultimate L\" conjecture.\nSolomon Feferman has argued that CH is not a definite mathematical problem. He proposes a theory of \"definiteness\" using a semi-intuitionistic subsystem of ZF that accepts classical logic for bounded quantifiers but uses intuitionistic logic for unbounded ones, and suggests that a proposition formula_17 is mathematically \"definite\" if the semi-intuitionistic theory can prove formula_18. He conjectures that CH is not definite according to this notion, and proposes that CH should, therefore, be considered not to have a truth value. Peter Koellner wrote a critical commentary on Feferman's article.\nJoel David Hamkins proposes a multiverse approach to set theory and argues that \"the continuum hypothesis is settled on the multiverse view by our extensive knowledge about how it behaves in the multiverse, and, as a result, it can no longer be settled in the manner formerly hoped for\". In a related vein, Saharon Shelah wrote that he does \"not agree with the pure Platonic view that the interesting problems in set theory can be decided, that we just have to discover the additional axiom. My mental picture is that we have many possible set theories, all conforming to ZFC\".\nThe generalized continuum hypothesis.\nThe generalized continuum hypothesis (GCH) states that if an infinite set's cardinality lies between that of an infinite set \"S\" and that of the power set formula_19 of \"S\", then it has the same cardinality as either \"S\" or formula_19. That is, for any infinite cardinal formula_21 there is no cardinal formula_9 such that formula_23. GCH is equivalent to:\nformula_24 for every ordinal formula_25 (occasionally called Cantor's aleph hypothesis).\nThe beth numbers provide an alternate notation for this condition: formula_26 for every ordinal formula_25. The continuum hypothesis is the special case for the ordinal formula_28. GCH was first suggested by Philip Jourdain. For the early history of GCH, see Moore.\nLike CH, GCH is also independent of ZFC, but Sierpi\u0144ski proved that ZF + GCH implies the axiom of choice (AC) (and therefore the negation of the axiom of determinacy, AD), so choice and GCH are not independent in ZF; there are no models of ZF in which GCH holds and AC fails. To prove this, Sierpi\u0144ski showed GCH implies that every cardinality n is smaller than some aleph number, and thus can be ordered. This is done by showing that n is smaller than formula_29 which is smaller than its own Hartogs number\u2014this uses the equality formula_30; for the full proof, see Gillman.\nKurt G\u00f6del showed that GCH is a consequence of ZF + V=L (the axiom that every set is constructible relative to the ordinals), and is therefore consistent with ZFC. As GCH implies CH, Cohen's model in which CH fails is a model in which GCH fails, and thus GCH is not provable from ZFC. W.\u00a0B.\u00a0Easton used the method of forcing developed by Cohen to prove Easton's theorem, which shows it is consistent with ZFC for arbitrarily large cardinals formula_31 to fail to satisfy formula_32. Much later, Foreman and Woodin proved that (assuming the consistency of very large cardinals) it is consistent that formula_33 holds for every infinite cardinal formula_9. Later Woodin extended this by showing the consistency of formula_35 for every formula_9. Carmi Merimovich showed that, for each \"n\"\u00a0\u2265\u00a01, it is consistent with ZFC that for each \u03ba, 2\u03ba is the \"n\"th successor of \u03ba. On the other hand, L\u00e1szl\u00f3 Patai proved that if \u03b3 is an ordinal and for each infinite cardinal \u03ba, 2\u03ba is the \u03b3th successor of \u03ba, then \u03b3 is finite.\nFor any infinite sets A and B, if there is an injection from A to B then there is an injection from subsets of A to subsets of B. Thus for any infinite cardinals A and B, formula_37 . If A and B are finite, the stronger inequality formula_38 holds. GCH implies that this strict, stronger inequality holds for infinite cardinals as well as finite cardinals.\nImplications of GCH for cardinal exponentiation.\nAlthough the generalized continuum hypothesis refers directly only to cardinal exponentiation with 2 as the base, one can deduce from it the values of cardinal exponentiation formula_39 in all cases. GCH implies that:\nformula_40 when \"\u03b1\" \u2264 \"\u03b2\"+1;\nformula_41 when \"\u03b2\"+1 &lt; \"\u03b1\" and formula_42, where cf is the cofinality operation; and\nformula_43 when \"\u03b2\"+1 &lt; \"\u03b1\" and formula_44.\nThe first equality (when \"\u03b1\" \u2264 \"\u03b2\"+1) follows from:\nformula_45 , while:\nformula_46 ;\nThe third equality (when \"\u03b2\"+1 &lt; \"\u03b1\" and formula_47) follows from:\nformula_48, by K\u00f6nig's theorem, while:\nformula_49\nWhere, for every \u03b3, GCH is used for equating formula_50 and formula_51; formula_52 is used as it is equivalent to the axiom of choice.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5706", "revid": "1301192", "url": "https://en.wikipedia.org/wiki?curid=5706", "title": "\u00c7evik Bir", "text": "Turkish general\n\u00c7evik Bir (born 1939) is a retired Turkish army general. He was a member of the Turkish General Staff in the 1990s. He took a major part in several important international missions in the Middle East and North Africa. He was born in Buca, Izmir Province, in 1939 and is married with one child.\nHe graduated from the Turkish Military Academy as an engineer officer in 1958, from the Army Staff College in 1970 and from the Armed Forces College in 1971. He graduated from NATO Defense College, Rome, Italy in 1973.\nFrom 1973 to 1985, he served at SHAPE, NATO's headquarters in Belgium. He was promoted to brigadier general and commanded an armed brigade and division in Turkey. From 1987 to 1991, he served as major general, and then was promoted to lieutenant general.\nAfter the dictator Siad Barre\u2019s ousting, conflicts between the General Mohammed Farah Aidid party and other clans in Somalia had led to famine and lawlessness throughout the country. An estimated 300,000 people had died from starvation. A combined military force of United States and United Nations (under the name \"UNOSOM\") were deployed to Mogadishu, to monitor the ceasefire and deliver food and supplies to the starving people of Somali. \u00c7evik Bir, who was then a lieutenant-general of Turkey, became the force commander of UNOSOM II in April 1993. Despite the retreat of US and UN forces after several deaths due to local hostilities mainly led by Aidid, the introduction of a powerful military force opened the transportation routes, enabling the provision of supplies and ended the famine quickly. He was succeeded as Force Commander by a Malaysian general in January 1994.\nHe became a four-star general and served three years as vice chairman of the Turkish Armed Forces, then appointed commander of the Turkish First Army, in Istanbul. While he was vice chairman of the TAF, he signed the Turkish-Israeli Military Coordination agreement in 1996.\n\u00c7evik Bir became the Turkish army's deputy chief of general staff shortly after the Somali operation and played a vital role in establishing a Turkish-Israeli entente.\n\u00c7evik Bir retired from the army on August 30, 1999. He is a former member of the Association for the Study of the Middle East and Africa (ASMEA).\nOn April 12, 2012, Bir and 30 other officers were taken in custody for their role in the 1997 military memorandum that forced the then Turkish government, led by the Refah Partisi (Welfare Party), to step down. On September 11, 2021, the General Staff Personnel Presidency reported to the Ankara 5th High Criminal Court, where the case was heard, that the administrative action was taken to demolish the 13 retired generals convicted in the February 28 trial. Thus, \u00c7evik Bir was demoted. \n\u00c7evik Bir, one of the generals who planned the process, said \"In Turkey we have a marriage of Islam and democracy. (\u2026) The child of this marriage is secularism. Now this child gets sick from time to time. The Turkish Armed Forces is the doctor which saves the child. Depending on how sick the kid is, we administer the necessary medicine to make sure the child recuperates\". H\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5708", "revid": "46016588", "url": "https://en.wikipedia.org/wiki?curid=5708", "title": "Collectivism (disambiguation)", "text": "Collectivism is the type of social organization.\nCollectivism may also refer to:\nSee also.\nTopics referred to by the same term\n&lt;templatestyles src=\"Dmbox/styles.css\" /&gt;\n This page lists associated with the title ."}
{"id": "5711", "revid": "7903804", "url": "https://en.wikipedia.org/wiki?curid=5711", "title": "Nepeta", "text": "Genus of flowering plants, known for effect on cats (catnip) in the mint family (Lamiaceae)\nNepeta is a genus of flowering plants in the family Lamiaceae. The genus name is reportedly in reference to Nepete, an ancient Etruscan city. There are about 250 species.\nThe genus is native to Europe, Asia, and Africa, and has also naturalized in North America.\nSome members of this group are known as catnip or catmint because of their effect on house cats \u2013 the nepetalactone contained in some \"Nepeta\" species binds to the olfactory receptors of cats, typically resulting in temporary euphoria.\nDescription.\nMost of the species are herbaceous perennial plants, but some are annuals. They have sturdy stems with opposite heart-shaped, green to gray-green leaves. \"Nepeta\" plants are usually aromatic in foliage and flowers.\nThe tubular flowers can be lavender, blue, white, pink, or lilac, and spotted with tiny lavender-purple dots. The flowers are located in verticillasters grouped on spikes; or the verticillasters are arranged in opposite cymes, racemes, or panicles \u2013 toward the tip of the stems.\nThe calyx is tubular or campanulate, they are slightly curved or straight, and the limbs are often 2-lipped with five teeth. The lower lip is larger, with 3-lobes, and the middle lobe is the largest. The flowers have 4 hairless stamens that are nearly parallel, and they ascend under the upper lip of the corolla. Two stamen are longer and stamens of pistillate flowers are rudimentary. The style protrudes outside of the mouth of the flowers.\nThe fruits are nutlets, which are oblong-ovoid, ellipsoid, ovoid, or obovoid in shape. The surfaces of the nutlets can be slightly ribbed, smooth or warty.\nSelected species.\nSpecies include:\n&lt;templatestyles src=\"Div col/styles.css\"/&gt;\nUses.\nCultivation.\nSome \"Nepeta\" species are cultivated as ornamental plants. They can be drought tolerant \u2013 water conserving, often deer repellent, with long bloom periods from late spring to autumn. Some species also have repellent properties to insect pests, including aphids and squash bugs, when planted in a garden.\n\"Nepeta\" species are used as food plants by the larvae of some Lepidoptera (butterfly and moth) species including \"Coleophora albitarsella\", and as nectar sources for pollinators, such as honey bees and hummingbirds.\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5714", "revid": "32005", "url": "https://en.wikipedia.org/wiki?curid=5714", "title": "Cornish Nationalist Party", "text": "The Cornish Nationalist Party (CNP; ) is a political party, founded by Dr James Whetter, who campaigned for independence for Cornwall.\nHistory.\nIt was formed by people who left Cornwall's main nationalist party Mebyon Kernow on 28 May 1975, but it is no longer for independence.\nA separate party with a similar name (Cornish National Party) existed from 1969.\nThe split with Mebyon Kernow was based on the same debate that was occurring in most of the other political parties campaigning for autonomy from the United Kingdom at the time (such as the Scottish National Party and Plaid Cymru): whether to be a centre-left party, appealing to the electorate on a social democratic line, or whether to appeal emotionally on a centre-right cultural line. Originally, another subject of the split was whether to embrace devolution as a first step to full independence (or as the sole step if this was what the electorate wished) or for it to be \"all or nothing\".\nThe CNP essentially represented a more right-wing outlook from those who disagree that economic arguments were more likely to win votes than cultural. The CNP worked to preserve the Celtic identity of Cornwall and improve its economy, and encouraged links with Cornish people overseas and with other regions with distinct identities. It also gave support to the Cornish language and commemorated Thomas Flamank, a leader of the Cornish Rebellion in 1497, at an annual ceremony at Bodmin on 27 June each year.\nThe CNP was for some time seen as more of a pressure group, as it did not put up candidates for any elections, although its visibility and influence within Cornwall is negligible. As of 2012[ [update]], it is now registered on the UK political parties register, and so Mebyon Kernow is no longer the only registered political party based in Cornwall. In April 2009, a news story reported that the CNP had re-formed following a conference in Bodmin; however, it did not contest any elections that year.\nDr Whetter was the founder and editor of the CNP quarterly journal, \"The Cornish Banner\" (\"An Baner Kernewek\"), within the actions of the Roseland Institute. Since his death in 2018 the CNP has been led by Androw Hawke.\nA newspaper article and a revamp of the party website in October 2014 state that the party is now to contest elections once more.\nJohn Le Bretton, vice-chairman of the party, said: \"The CNP supports the retention of Cornwall Council as a Cornwall-wide authority running Cornish affairs and we call for the British government in Westminster to devolve powers to the council so that decisions affecting Cornwall can be made in Cornwall\".\nThe CNP polled 227 (0.4) votes in Truro during the 1979 UK General Election, 364 (0.67) in North Cornwall in the 1983 UK General Election, and 1,892 (1.0) at the European Parliament elections in the Cornwall and Plymouth constituency in 1984. The candidate on all three occasions was the founder and first leader of the CNP, Dr James Whetter.\nThe CNP had one parish councillor, CNP leader Androw Hawke who was elected to Polperro Community Council for the second time on 4 May 2017.\nThe reformed party was registered with the Electoral Commission in 2014, but ceased to be registered in 2017.\nPolicy.\nThe Policy Statement and Programme of the CNP were published in 1975 and included the following points:\nThe party's policies include the following:\nImage.\nThere have been perceived image problems as the CNP has been seen as similarly styled to the BNP and NF (the nativist British National Party and National Front), and during the 1970s letters were published in the party magazine \"The Cornish Banner\" (\"An Baner Kernewek\") sympathetic to the NF and critical of \"Zionist\" politicians. The CNP also formed a controversial uniformed wing known as the \"Greenshirts\" led by the CNP Youth Movement leader and Public Relations Officer, Wallace Simmons who also founded the pro-NF \"Cornish Front\". (Although the CNP and CF were sympathetic to Irish republicanism while the NF was supportive of Ulster loyalism, with the exception of leading NF figures like Patrick Harrington, who refused to condemn the IRA during an interview for the Channel 4 TV documentary \"Disciples of Chaos\").\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
{"id": "5715", "revid": "2066186", "url": "https://en.wikipedia.org/wiki?curid=5715", "title": "Cryptanalysis", "text": "Study of analyzing information systems in order to discover their hidden aspects\nCryptanalysis (from the Greek \"krypt\u00f3s\", \"hidden\", and \"anal\u00fdein\", \"to analyze\") refers to the process of analyzing information systems in order to understand hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.\nIn addition to mathematical analysis of cryptographic algorithms, cryptanalysis includes the study of side-channel attacks that do not target weaknesses in the cryptographic algorithms themselves, but instead exploit weaknesses in their implementation.\nEven though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.\nOverview.\nIn encryption, confidential information (called the \"plaintext\") is sent securely to a recipient by the sender first converting it into an unreadable form (\"ciphertext\") using an encryption algorithm. The ciphertext is sent through an insecure channel to the recipient. The recipient decrypts the ciphertext by applying an inverse decryption algorithm, recovering the plaintext. To decrypt the ciphertext, the recipient requires a secret knowledge from the sender, usually a string of letters, numbers, or bits, called a \"cryptographic key\". The concept is that even if an unauthorized person gets access to the ciphertext during transmission, without the secret key they cannot convert it back to plaintext.\nEncryption has been used throughout history to send important military, diplomatic and commercial messages, and today is very widely used in computer networking to protect email and internet communication.\nThe goal of cryptanalysis is for a third party, a cryptanalyst, to gain as much information as possible about the original (\"plaintext\"), attempting to \"break\" the encryption to read the ciphertext and learning the secret key so future messages can be decrypted and read. A mathematical technique to do this is called a \"cryptographic attack\". Cryptographic attacks can be characterized in a number of ways:\nAmount of information available to the attacker.\nAttacks can be classified based on what type of information the attacker has available. As a basic starting point it is normally assumed that, for the purposes of analysis, the general algorithm is known; this is Shannon's Maxim \"the enemy knows the system\" \u2013 in its turn, equivalent to Kerckhoffs' principle. This is a reasonable assumption in practice \u2013 throughout history, there are countless examples of secret algorithms falling into wider knowledge, variously through espionage, betrayal and reverse engineering. (And on occasion, ciphers have been broken through pure deduction; for example, the German Lorenz cipher and the Japanese Purple code, and a variety of classical schemes):\nComputational resources required.\nAttacks can also be characterised by the resources they require. Those resources include:\nIt is sometimes difficult to predict these quantities precisely, especially when the attack is not practical to actually implement for testing. But academic cryptanalysts tend to provide at least the estimated \"order of magnitude\" of their attacks' difficulty, saying, for example, \"SHA-1 collisions now 252.\"\nBruce Schneier notes that even computationally impractical attacks can be considered breaks: \"Breaking a cipher simply means finding a weakness in the cipher that can be exploited with a complexity less than brute force. Never mind that brute-force might require 2128 encryptions; an attack requiring 2110 encryptions would be considered a break...simply put, a break can just be a certificational weakness: evidence that the cipher does not perform as advertised.\"\nPartial breaks.\nThe results of cryptanalysis can also vary in usefulness. Cryptographer Lars Knudsen (1998) classified various types of attack on block ciphers according to the amount and quality of secret information that was discovered:\nAcademic attacks are often against weakened versions of a cryptosystem, such as a block cipher or hash function with some rounds removed. Many, but not all, attacks become exponentially more difficult to execute as rounds are added to a cryptosystem, so it's possible for the full cryptosystem to be strong even though reduced-round variants are weak. Nonetheless, partial breaks that come close to breaking the original cryptosystem may mean that a full break will follow; the successful attacks on DES, MD5, and SHA-1 were all preceded by attacks on weakened versions.\nIn academic cryptography, a \"weakness\" or a \"break\" in a scheme is usually defined quite conservatively: it might require impractical amounts of time, memory, or known plaintexts. It also might require the attacker be able to do things many real-world attackers can't: for example, the attacker may need to choose particular plaintexts to be encrypted or even to ask for plaintexts to be encrypted using several keys related to the secret key. Furthermore, it might only reveal a small amount of information, enough to prove the cryptosystem imperfect but too little to be useful to real-world attackers. Finally, an attack might only apply to a weakened version of cryptographic tools, like a reduced-round block cipher, as a step towards breaking the full system.\nHistory.\nCryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography\u2014new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.\nClassical ciphers.\nAlthough the actual word \"cryptanalysis\" is relatively recent (it was coined by William Friedman in 1920), methods for breaking codes and ciphers are much older. David Kahn notes in \"The Codebreakers\" that Arab scholars were the first people to systematically document cryptanalytic methods.\nThe first known recorded explanation of cryptanalysis was given by Al-Kindi (c. 801\u2013873, also known as \"Alkindus\" in Europe), a 9th-century Arab polymath, in \"Risalah fi Istikhraj al-Mu'amma\" (\"A Manuscript on Deciphering Cryptographic Messages\"). This treatise contains the first description of the method of frequency analysis. Al-Kindi is thus regarded as the first codebreaker in history. His breakthrough work was influenced by Al-Khalil (717\u2013786), who wrote the \"Book of Cryptographic Messages\", which contains the first use of permutations and combinations to list all possible Arabic words with and without vowels.\nFrequency analysis is the basic tool for breaking most classical ciphers. In natural languages, certain letters of the alphabet appear more often than others; in English, \"E\" is likely to be the most common letter in any sample of plaintext. Similarly, the digraph \"TH\" is the most likely pair of letters in English, and so on. Frequency analysis relies on a cipher failing to hide these statistics. For example, in a simple substitution cipher (where each letter is simply replaced with another), the most frequent letter in the ciphertext would be a likely candidate for \"E\". Frequency analysis of such a cipher is therefore relatively easy, provided that the ciphertext is long enough to give a reasonably representative count of the letters of the alphabet that it contains.\nAl-Kindi's invention of the frequency analysis technique for breaking monoalphabetic substitution ciphers was the most significant cryptanalytic advance until World War II. Al-Kindi's \"Risalah fi Istikhraj al-Mu'amma\" described the first cryptanalytic techniques, including some for polyalphabetic ciphers, cipher classification, Arabic phonetics and syntax, and most importantly, gave the first descriptions on frequency analysis. He also covered methods of encipherments, cryptanalysis of certain encipherments, and statistical analysis of letters and letter combinations in Arabic. An important contribution of Ibn Adlan (1187\u20131268) was on sample size for use of frequency analysis.\nIn Europe, Italian scholar Giambattista della Porta (1535\u20131615) was the author of a seminal work on cryptanalysis, \"De Furtivis Literarum Notis\".\nSuccessful cryptanalysis has undoubtedly influenced history; the ability to read the presumed-secret thoughts and plans of others can be a decisive advantage. For example, in England in 1587, Mary, Queen of Scots was tried and executed for treason as a result of her involvement in three plots to assassinate Elizabeth I of England. The plans came to light after her coded correspondence with fellow conspirators was deciphered by Thomas Phelippes.\nIn Europe during the 15th and 16th centuries, the idea of a polyalphabetic substitution cipher was developed, among others by the French diplomat Blaise de Vigen\u00e8re (1523\u201396). For some three centuries, the Vigen\u00e8re cipher, which uses a repeating key to select different encryption alphabets in rotation, was considered to be completely secure (\"le chiffre ind\u00e9chiffrable\"\u2014\"the indecipherable cipher\"). Nevertheless, Charles Babbage (1791\u20131871) and later, independently, Friedrich Kasiski (1805\u201381) succeeded in breaking this cipher. During World War I, inventors in several countries developed rotor cipher machines such as Arthur Scherbius' Enigma, in an attempt to minimise the repetition that had been exploited to break the Vigen\u00e8re system.\nCiphers from World War I and World War II.\nIn World War I, the breaking of the Zimmermann Telegram was instrumental in bringing the United States into the war. In World War II, the Allies benefitted enormously from their joint success cryptanalysis of the German ciphers \u2013 including the Enigma machine and the Lorenz cipher \u2013 and Japanese ciphers, particularly 'Purple' and JN-25. 'Ultra' intelligence has been credited with everything between shortening the end of the European war by up to two years, to determining the eventual result. The war in the Pacific was similarly helped by 'Magic' intelligence.\nCryptanalysis of enemy messages played a significant part in the Allied victory in World War II. F. W. Winterbotham, quoted the western Supreme Allied Commander, Dwight D. Eisenhower, at the war's end as describing Ultra intelligence as having been \"decisive\" to Allied victory. Sir Harry Hinsley, official historian of British Intelligence in World War II, made a similar assessment about Ultra, saying that it shortened the war \"by not less than two years and probably by four years\"; moreover, he said that in the absence of Ultra, it is uncertain how the war would have ended.\nIn practice, frequency analysis relies as much on linguistic knowledge as it does on statistics, but as ciphers became more complex, mathematics became more important in cryptanalysis. This change was particularly evident before and during World War II, where efforts to crack Axis ciphers required new levels of mathematical sophistication. Moreover, automation was first applied to cryptanalysis in that era with the Polish Bomba device, the British Bombe, the use of punched card equipment, and in the Colossus computers \u2013 the first electronic digital computers to be controlled by a program.\nIndicator.\nWith reciprocal machine ciphers such as the Lorenz cipher and the Enigma machine used by Nazi Germany during World War II, each message had its own key. Usually, the transmitting operator informed the receiving operator of this message key by transmitting some plaintext and/or ciphertext before the enciphered message. This is termed the \"indicator\", as it indicates to the receiving operator how to set his machine to decipher the message.\nPoorly designed and implemented indicator systems allowed first Polish cryptographers and then the British cryptographers at Bletchley Park to break the Enigma cipher system. Similar poor indicator systems allowed the British to identify \"depths\" that led to the diagnosis of the Lorenz SZ40/42 cipher system, and the comprehensive breaking of its messages without the cryptanalysts seeing the cipher machine.\nDepth.\nSending two or more messages with the same key is an insecure process. To a cryptanalyst the messages are then said to be \"in depth.\" This may be detected by the messages having the same \"indicator\" by which the sending operator informs the receiving operator about the key generator initial settings for the message.\nGenerally, the cryptanalyst may benefit from lining up identical enciphering operations among a set of messages. For example, the Vernam cipher enciphers by bit-for-bit combining plaintext with a long key using the \"exclusive or\" operator, which is also known as \"modulo-2 addition\" (symbolized by \u2295 ):\nPlaintext \u2295 Key = Ciphertext\nDeciphering combines the same key bits with the ciphertext to reconstruct the plaintext:\nCiphertext \u2295 Key = Plaintext\n(In modulo-2 arithmetic, addition is the same as subtraction.) When two such ciphertexts are aligned in depth, combining them eliminates the common key, leaving just a combination of the two plaintexts:\nCiphertext1 \u2295 Ciphertext2 = Plaintext1 \u2295 Plaintext2\nThe individual plaintexts can then be worked out linguistically by trying \"probable words\" (or phrases), also known as \"cribs,\" at various locations; a correct guess, when combined with the merged plaintext stream, produces intelligible text from the other plaintext component:\n(Plaintext1 \u2295 Plaintext2) \u2295 Plaintext1 = Plaintext2\nThe recovered fragment of the second plaintext can often be extended in one or both directions, and the extra characters can be combined with the merged plaintext stream to extend the first plaintext. Working back and forth between the two plaintexts, using the intelligibility criterion to check guesses, the analyst may recover much or all of the original plaintexts. (With only two plaintexts in depth, the analyst may not know which one corresponds to which ciphertext, but in practice this is not a large problem.) When a recovered plaintext is then combined with its ciphertext, the key is revealed:\nPlaintext1 \u2295 Ciphertext1 = Key\nKnowledge of a key then allows the analyst to read other messages encrypted with the same key, and knowledge of a set of related keys may allow cryptanalysts to diagnose the system used for constructing them.\nDevelopment of modern cryptography.\nGovernments have long recognized the potential benefits of cryptanalysis for intelligence, both military and diplomatic, and established dedicated organizations devoted to breaking the codes and ciphers of other nations, for example, GCHQ and the NSA, organizations which are still very active today.\nEven though computation was used to great effect in the cryptanalysis of the Lorenz cipher and other systems during World War II, it also made possible new methods of cryptography orders of magnitude more complex than ever before. Taken as a whole, modern cryptography has become much more impervious to cryptanalysis than the pen-and-paper systems of the past, and now seems to have the upper hand against pure cryptanalysis. The historian David Kahn notes:\n&lt;templatestyles src=\"Template:Blockquote/styles.css\" /&gt;Many are the cryptosystems offered by the hundreds of commercial vendors today that cannot be broken by any known methods of cryptanalysis. Indeed, in such systems even a chosen plaintext attack, in which a selected plaintext is matched against its ciphertext, cannot yield the key that unlock[s] other messages. In a sense, then, cryptanalysis is dead. But that is not the end of the story. Cryptanalysis may be dead, but there is \u2013 to mix my metaphors \u2013 more than one way to skin a cat.\nKahn goes on to mention increased opportunities for interception, bugging, side channel attacks, and quantum computers as replacements for the traditional means of cryptanalysis. In 2010, former NSA technical director Brian Snow said that both academic and government cryptographers are \"moving very slowly forward in a mature field.\"\nHowever, any postmortems for cryptanalysis may be premature. While the effectiveness of cryptanalytic methods employed by intelligence agencies remains unknown, many serious attacks against both academic and practical cryptographic primitives have been published in the modern era of computer cryptography:\nThus, while the best modern ciphers may be far more resistant to cryptanalysis than the Enigma, cryptanalysis and the broader field of information security remain quite active.\nAsymmetric ciphers.\nAsymmetric cryptography (or public-key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on \"hard\" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.\nAsymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie\u2013Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization \u2013 a breakthrough in factoring would impact the security of RSA.\nIn 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.\nAnother distinguishing feature of asymmetric schemes is that, unlike attacks on symmetric cryptosystems, any cryptanalysis has the opportunity to make use of knowledge gained from the public key.\nQuantum computing applications for cryptanalysis.\nQuantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.\nBy using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.\nReferences.\nCitations.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;\nSources.\n&lt;templatestyles src=\"Refbegin/styles.css\" /&gt;"}
{"id": "5716", "revid": "20925888", "url": "https://en.wikipedia.org/wiki?curid=5716", "title": "Chicano", "text": "Subculture, chosen identity of some Mexican Americans in the United States\nChicano (masculine form), Chicana (feminine form), is an identity for Mexican Americans who have a non-Anglo self-image. \"Chicano\" was originally a classist and racist slur used toward low-income Mexicans that was reclaimed in the 1940s among youth who belonged to the Pachuco and Pachuca subculture. In the 1960s, \"Chicano\" was widely reclaimed in the building of a movement toward political empowerment, ethnic solidarity, and pride in being of indigenous descent (with many using the Nahuatl language or names). \"Chicano\" developed its own meaning separate from \"Mexican American\" identity. Youth in \"barrios\" rejected cultural assimilation into whiteness and embraced their own identity and worldview as a form of empowerment and resistance. The community forged an independent political and cultural movement, sometimes working alongside the Black power movement. \nThe Chicano Movement faltered by the mid-1970s as a result of external and internal pressures. It was under state surveillance, infiltration, and repression by U.S. government agencies, informants, and agent provocateurs, such as through COINTELPRO. The Chicano Movement also had a fixation on masculine pride and machismo that fractured the community through sexism toward Chicanas and homophobia toward queer Chicana/os. In the 1980s, assimilation and economic mobility motivated many to embrace \"Hispanic\" identity in an era of conservatism. The term \"Hispanic\" emerged from a collaboration between the U.S. government and Mexican-American political elites in the Hispanic Caucus of Congress. They used the term to identify themselves and the community with mainstream American culture, depart from \"Chicanismo,\" and distance themselves from what they perceived as the \"militant\" Black Caucus.At the grassroots level, Chicana/os continued to build the feminist, gay and lesbian, and anti-apartheid movements, which kept the identity politically relevant. After a decade of \"Hispanic\" dominance, Chicana/o student activism in the early 1990s recession and the anti-Gulf War movement revived the identity with a demand to expand Chicana/o studies programs. Chicanas were active at the forefront, despite facing critiques from \"movement loyalists\", as they did in the Chicano Movement. Chicana feminists addressed employment discrimination, environmental racism, healthcare, sexual violence, and exploitation in their communities and in solidarity with the Third World. Chicanas worked to \"liberate her \"entire people\"\"; not to oppress men, but to be equal partners in the movement. \"Xicanisma\", coined by Ana Castillo in 1994, called for Chicana/os to \"reinsert the forsaken feminine into our consciousness\", to embrace one's Indigenous roots, and support Indigenous sovereignty.\nIn the 2000s, earlier traditions of anti-imperialism in the Chicano Movement were expanded. Building solidarity with undocumented immigrants became more important, despite issues of legal status and economic competitiveness sometimes maintaining distance between groups. U.S. foreign interventions abroad were connected with domestic issues concerning the rights of undocumented immigrants in the United States. Chicano/a consciousness increasingly became transnational and transcultural, thinking beyond and bridging with communities over political borders. The identity was renewed based on Indigenous and decolonial consciousness, cultural expression, resisting gentrification, defense of immigrants, and the rights of women and queer people. \"Xicanx\" identity also emerged in the 2010s, based on the Chicana feminist intervention of \"Xicanisma\".\nEtymology.\nThe etymology of the term \"Chicano\" is the subject of some debate by historians. Some believe \"Chicano\" is a Spanish language derivative of an older Nahuatl word \"Mexitli\" (\"Meh-shee-tlee\"). Mexitli formed part of the expression \"Huitzilopochtlil Mexitli\"\u2014a reference to the historic migration of the Mexica people from their homeland of Aztl\u00e1n to the Oaxaca Valley. Mexitli is the root of the word \"Mexica\", which refers to the Mexica people, and its singular form \"Mexihcatl\" (/me\u02d0\u02c8\u0283i\u0294kat\u0361\u026c/). The \"x\" in Mexihcatl represents an /\u0283/ or \"sh\" sound in both Nahuatl and early modern Spanish, while the glottal stop in the middle of the Nahuatl word disappeared.\nThe word \"Chicano\" may derive from the loss of the initial syllable of \"Mexicano\" (Mexican). According to Villanueva, \"given that the velar (x) is a palatal phoneme (S) with the spelling (sh),\" in accordance with the Indigenous phonological system of the Mexicas (\"Meshicas\"), it would become \"Meshicano\" or \"Mechicano.\" In this explanation, \"Chicano\" comes from the \"xicano\" in \"Mexicano.\" Some Chicanos replace the \"Ch\" with the letter \"X\", or \"Xicano\", to reclaim the Nahuatl \"sh\" sound that the Spanish did not have a letter for and marked with the letter \"x\". The first two syllables of \"Xicano\" are therefore in Nahuatl while the last syllable is Castilian.\nIn Mexico's Indigenous regions, Indigenous people refer to members of the non-indigenous majority as \"mexicanos\", referring to the modern nation of Mexico. Among themselves, the speaker identifies by their ' (village or tribal) identity, such as Mayan, Zapotec, Mixtec, Huastec, or any of the other hundreds of indigenous groups. A newly emigrated Nahuatl speaker in an urban center might have referred to his cultural relatives in this country, different from himself, as ', shortened to \"Chicanos\" or \"Xicanos.\"\nUsage of terms.\nEarly recorded use.\nThe town of \"Chicana\" was shown on the Guti\u00e9rrez 1562 New World map near the mouth of the Colorado River, and is probably pre-Columbian in origin. The town was again included on , a 1566 French map by Paolo Forlani. Roberto Cintli Rodr\u00edguez places the location of \"Chicana\" at the mouth of the Colorado River, near present-day Yuma, Arizona. An 18th century map of the Nayarit Missions used the name \"Xicana\" for a town near the same location of \"Chicana\", which is considered to be the oldest recorded usage of that term.\nA gunboat, the \"Chicana\", was sold in 1857 to Jose Maria Carvajal to ship arms on the Rio Grande. The King and Kenedy firm submitted a voucher to the Joint Claims Commission of the United States in 1870 to cover the costs of this gunboat's conversion from a passenger steamer. No explanation for the boat's name is known.\nThe Chicano poet and writer Tino Villanueva traced the first documented use of the term as an ethnonym to 1911, as referenced in a then-unpublished essay by University of Texas anthropologist Jos\u00e9 Lim\u00f3n. Linguists Edward R. Simmen and Richard F. Bauerle report the use of the term in an essay by Mexican-American writer, Mario Su\u00e1rez, published in the \"Arizona Quarterly\" in 1947. There is ample literary evidence to substantiate that \"Chicano\" is a long-standing endonym, as a large body of Chicano literature pre-dates the 1950s.\nReclaiming the term.\nIn the 1940s, \"Chicano\" was reclaimed by Pachuco youth as an expression of defiance to Anglo-American society. At the time, \"Chicano\" was used among English and Spanish speakers as a classist and racist slur to refer to working class Mexican Americans in Spanish-speaking neighborhoods. In Mexico, the term was used with \"Pocho\" \"to deride Mexicans living in the United States, and especially their U.S.-born children, for losing their culture, customs, and language.\" Mexican anthropologist Manuel Gamio reported in 1930 that \"Chicamo\" (with an \"m\") was used as a derogatory term by Hispanic Texans for recently arrived Mexican immigrants displaced during the Mexican Revolution in the beginning of the early 20th century.\nBy the 1950s, \"Chicano\" referred to those who resisted total assimilation, while \"Pocho\" referred (often pejoratively) to those who strongly advocated for assimilation. In his essay \"Chicanismo\" in \"The Oxford Encyclopedia of Mesoamerican Cultures\" (2002), Jos\u00e9 Cu\u00e9llar, dates the transition from derisive to positive to the late 1950s, with increasing use by young Mexican-American high school students. These younger, politically aware Mexican Americans adopted the term \"as an act of political defiance and ethnic pride\", similar to the reclaiming of \"Black\" by African Americans. The Chicano Movement during the 1960s and early 1970s played a significant role in reclaiming \"Chicano,\" challenging those who used it as a term of derision on both sides of the Mexico-U.S. border.\nDemographic differences in the adoption of \"Chicano\" occurred at first. It was more likely to be used by males than females, and less likely to be used among those of higher socioeconomic status. Usage was also generational, with third-generation men more likely to use the word. This group was also younger, more political, and different from traditional Mexican cultural heritage. \"Chicana\" was a similar classist term to refer to \"[a] marginalized, brown woman who is treated as a foreigner and is expected to do menial labor and ask nothing of the society in which she lives.\" Among Mexican Americans, \"Chicano\" and \"Chicana\" began to be viewed as a positive identity of self-determination and political solidarity. In Mexico, \"Chicano\" may still be associated with a Mexican American person of low importance, class, and poor morals (similar to the terms \"Cholo\", \"Chulo\" and \"Majo\"), indicating a difference in cultural views.\nChicano Movement.\n\"Chicano\" was widely reclaimed in the 1960s and 1970s during the Chicano Movement to assert a distinct ethnic, political, and cultural identity that resisted assimilation into whiteness, systematic racism and stereotypes, colonialism, and the American nation-state. Chicano identity formed around seven themes: unity, economy, education, institutions, self-defense, culture, and political liberation, in an effort to bridge regional and class divisions. The notion of Aztl\u00e1n, a mythical homeland claimed to be located in the southwestern United States, mobilized Mexican Americans to take social and political action. \"Chicano\" became a unifying term for \"mestizos\". \"Xicano\" was also used in the 1970s.\nIn the 1970s, Chicanos developed a reverence for machismo while also maintaining the values of their original platform. For instance, Oscar Zeta Acosta defined machismo as the source of Chicano identity, claiming that this \"instinctual and mystical source of manhood, honor and pride... alone justifies all behavior.\" Armando Rend\u00f3n wrote in \"Chicano Manifesto\" (1971) that machismo was \"in fact an underlying drive of the gathering identification of Mexican Americans... the essence of \"machismo\", of being \"macho\", is as much a symbolic principle for the Chicano revolt as it is a guideline for family life.\"\nFrom the beginning of the Chicano Movement, some Chicanas criticized the idea that machismo must guide the people and questioned if machismo was \"indeed a genuinely Mexican cultural value or a kind of distorted view of masculinity generated by the psychological need to compensate for the indignities suffered by Chicanos in a white supremacist society.\" Angie Chabram-Dernersesian found that most of the literature on the Chicano Movement focused on men and boys, while almost none focused on Chicanas. The omission of Chicanas and the machismo of the Chicano Movement led to a shift by the 1990s.\nXicanisma.\n\"Xicanisma\" was coined by Ana Castillo in \"Massacre of the Dreamers\" (1994) as a recognition of a shift in consciousness since the Chicano Movement and to reinvigorate Chicana feminism. The aim of Xicanisma is not to replace patriarchy with matriarchy, but to create \"a nonmaterialistic and nonexploitive society in which feminine principles of nurturing and community prevail\"; where the feminine is reinserted into our consciousness rather than subordinated by colonization. The \"X\" reflects the \"Sh\" sound in Mesoamerican languages that the Spanish could not pronounce (such as \"Tlaxcala\", which is pronounced \"Tlash-KAH-lah\"), and so marked this sound with a letter X. More than a letter, the \"X\" in Xicanisma is also a symbol to represent being at a literal crossroads or otherwise embodying hybridity.\n\"Xicanisma\" acknowledges Indigenous survival after hundreds of years of colonization and the need to reclaim one's Indigenous roots while also being \"committed to the struggle for liberation of all oppressed people\", wrote Francesca A. L\u00f3pez. Activists like Guillermo G\u00f3mez-Pe\u00f1a, issued \"a call for a return to the Amerindian roots of most Latinos as well as a call for a strategic alliance to give agency to Native American groups.\" This can include one's Indigenous roots from Mexico \"as well as those with roots centered in Central and South America,\" wrote Francisco Rios. Castillo argued that this shift in language was important because \"language is the vehicle by which we perceive ourselves in relation to the world\".\nAmong a minority of Mexican Americans, the term \"Xicanx\" may be used to refer to gender non-conformity. Luis J. Rodriguez states that \"even though most US Mexicans may not use this term,\" that it can be important for gender non-conforming Mexican Americans. \"Xicanx\" may destabilize aspects of the coloniality of gender in Mexican American communities. Artist Roy Martinez states that it is not \"bound to the feminine or masculine aspects\" and that it may be \"inclusive to anyone who identifies with it\". Some prefer the -e suffix \"Xicane\" in order to be more in-line with Spanish-speaking language constructs.\nDistinction from other terms.\n\"Mexican American\".\nIn the 1930s, \"community leaders promoted the term \"Mexican American\" to convey an assimilationist ideology stressing white identity,\" as noted by legal scholar Ian Haney L\u00f3pez. Lisa Y. Ramos argues that \"this phenomenon demonstrates why no Black-Brown civil rights effort emerged prior to the 1960s.\" Chicano youth rejected the previous generation's racial aspirations to assimilate into Anglo-American society and developed a \"Pachuco culture that fashioned itself neither as Mexican nor American.\"\nIn the Chicano Movement, possibilities for Black\u2013brown unity arose: \"Chicanos defined themselves as proud members of a brown race, thereby rejecting, not only the previous generation's assimilationist orientation, but their racial pretensions as well.\" Chicano leaders collaborated with Black Power movement leaders and activists. \"Mexican Americans\" insisted that Mexicans were white, while \"Chicanos\" embraced being non-white and the development of \"brown pride\".\n\"Mexican American\" continued to be used by a more assimilationist faction who wanted to define Mexican Americans \"as a white ethnic group that had little in common with African Americans.\" Carlos Mu\u00f1oz argues that the desire to separate themselves from Blackness and political struggle was rooted in an attempt to minimize \"the existence of racism toward their own people, [believing] they could \"deflect\" anti-Mexican sentiment in society\" through affiliating with whiteness.\n\"Hispanic\".\nFollowing the decline of the Chicano Movement, \"Hispanic\" was first defined by the U.S. Federal Office of Management and Budget's (OMB) Directive No. 15 in 1977 as \"a person of Mexican, Dominican, Puerto Rican, Cuban, Central or South America or other Spanish culture or origin, regardless of race.\" The term was promoted by Mexican American political elites to encourage cultural assimilation into whiteness and move away from \"Chicanismo\". The rise of Hispanic identity paralleled the emerging era of political and cultural conservatism in the United States during the 1980s.\nKey members of the Mexican American political elite, all of whom were middle-aged men, helped popularize the term \"Hispanic\" among Mexican Americans. The term was picked up by electronic and print media. Laura E. G\u00f3mez conducted a series of interviews with these elites and found that one of the main reasons \"Hispanic\" was promoted was to move away from \"Chicano\": \"The Chicano label reflected the more radical political agenda of Mexican-Americans in the 1960s and 1970s, and the politicians who call themselves Hispanic today are the harbingers of a more conservative, more accomadationist politics.\"\nG\u00f3mez found that some of these elites promoted \"Hispanic\" to appeal to white American sensibilities, particularly in regard to separating themselves from Black political consciousness. G\u00f3mez records:Another respondent agreed with this position, contrasting his white colleagues' perceptions of the Congressional Hispanic Caucus with their perception of the Congressional Black Caucus. 'We certainly haven't been militant like the Black Caucus. We're seen as a power bloc\u2014an ethnic power bloc striving to deal with mainstream issues.'In 1980, \"Hispanic\" was first made available as a self-identification on U.S. census forms. While \"Chicano\" also appeared on the 1980 U.S. census, it was only permitted to be selected as a subcategory underneath \"Spanish/Hispanic descent\", which erased the possibility of Afro-Chicanos and of Chicanos being of Indigenous descent. \"Chicano\" did not appear on any subsequent census forms and \"Hispanic\" has remained. Since then, \"Hispanic\" has widely been used by politicians and the media. For this reason, many Chicanos reject the term \"Hispanic\".\nOther terms.\nInstead of or in addition to identifying as Chicano or any of its variations, some may prefer:\nIdentity.\nChicano and Chicana identity reflects elements of ethnic, political, cultural and Indigenous hybridity. These qualities of what constitutes Chicano identity may be expressed by Chicanos differently. Armando Rend\u00f3n wrote in the \"Chicano Manifesto\" (1971), \"I am Chicano. What it means to me may be different than what it means to you.\" Benjamin Alire S\u00e1enz wrote \"There is no such thing as the Chicano voice: there are only Chicano and Chicana \"voices\".\" The identity can be somewhat ambiguous (e.g. in the 1991 Culture Clash play \"A Bowl of Beings\", in response to Che Guevara's demand for a definition of \"Chicano\", an \"armchair activist\" cries out, \"I still don't know!\").\nMany Chicanos understand themselves as being \"neither from here, nor from there\", as neither from the United States or Mexico. Juan Bruce-Novoa wrote in 1990: \"A Chicano lives in the space between the hyphen in Mexican-American.\" Being Chicano/a may represent the struggle of being institutionally acculturated to assimilate into the Anglo-dominated society of the United States, yet maintaining the cultural sense developed as a Latin-American cultured U.S.-born Mexican child. Rafael P\u00e9rez-Torres wrote, \"one can no longer assert the wholeness of a Chicano subject ... It is illusory to deny the nomadic quality of the Chicano community, a community in flux that yet survives and, through survival, affirms itself.\"\nEthnic identity.\n\"Chicano\" is a way for Mexican Americans to assert ethnic solidarity and \"Brown Pride.\" Boxer Rodolfo Gonzales was one of the first to reclaim the term in this way. This \"Brown Pride\" movement established itself alongside the \"Black is Beautiful\" movement. Chicano identity emerged as a symbol of pride in having a non-white and non-European image of oneself. It challenged the U.S. census designation \"Whites with Spanish Surnames\" that was used in the 1950s. Chicanos asserted ethnic pride at a time when Mexican assimilation into whiteness was being promoted by the U.S. government. Ian Haney L\u00f3pez argues that this was to \"serve Anglo self-interest\", who claimed Mexicans were white to try to deny racism against them.\nAlfred Arteaga argues that Chicano as an ethnic identity is born out of the European colonization of the Americas. He states that Chicano arose as hybrid ethnicity or race amidst colonial violence. This hybridity extends beyond a previously generalized \"Aztec\" ancestry, since the Indigenous peoples of Mexico are a diverse group of nations and peoples. A 2011 study found that 85 to 90% of maternal mtDNA lineages in Mexican Americans are Indigenous. Chicano ethnic identity may involve more than just Indigenous and Spanish ancestry. It may also include African ancestry (as a result of Spanish slavery or runaway slaves from Anglo-Americans). Arteaga concluded that \"the physical manifestation of the Chicano, is itself a product of hybridity.\"Robert Quintana Hopkins argues that Afro-Chicanos are sometimes erased from the ethnic identity \"because so many people uncritically apply the 'one drop rule' in the U.S. [which] ignores the complexity of racial hybridity.\" Black and Chicano communities have engaged in close political movements and struggles for liberation, yet there have also been tensions between Black and Chicano communities. This has been attributed to racial capitalism and anti-Blackness in Chicano communities. Afro-Chicano rapper Choosey stated \"there's a stigma that Black and Mexican cultures don't get along, but I wanted to show the beauty in being a product of both.\"\nPolitical identity.\nChicano political identity developed from a reverence of Pachuco resistance in the 1940s. Luis Valdez wrote that \"Pachuco determination and pride grew through the 1950s and gave impetus to the Chicano Movement of the 1960s ... By then the political consciousness stirred by the 1943 Zoot Suit Riots had developed into a movement that would soon issue the Chicano Manifesto\u2014a detailed platform of political activism.\" By the 1960s, the Pachuco figure \"emerged as an icon of resistance in Chicano cultural production.\" The Pachuca was not regarded with the same status. Catherine Ram\u00edrez credits this to the Pachuca being interpreted as a symbol of \"dissident femininity, female masculinity, and, in some instances, lesbian sexuality\".\nThe political identity was founded on the principle that the U.S. nation-state had impoverished and exploited the Chicano people and communities. Alberto Varon argued that this brand of Chicano nationalism focused on the machismo subject in its calls for political resistance. Chicano machismo was both a unifying and fracturing force. Cherr\u00ede Moraga argued that it fostered homophobia and sexism, which became obstacles to the Movement. As the Chicano political consciousness developed, Chicanas, including Chicana lesbians of color brought attention to \"reproductive rights, especially sterilization abuse [sterilization of Latinas], battered women's shelters, rape crisis centers, [and] welfare advocacy.\" Chicana texts like \"Essays on La Mujer\" (1977), \"Mexican Women in the United States\" (1980), and \"This Bridge Called My Back\" (1981) have been relatively ignored even in Chicano Studies. Sonia Sald\u00edvar-Hull argued that even when Chicanas have challenged sexism, their identities have been invalidated.\nChicano political activist groups like the Brown Berets (1967\u20131972; 1992\u2013Present) gained support in their protests of educational inequalities and demanding an end to police brutality. They collaborated with the Black Panthers and Young Lords, which were founded in 1966 and 1968 respectively. Membership in the Brown Berets was estimated to have reached five thousand in over 80 chapters (mostly centered in California and Texas). The Brown Berets helped organize the Chicano Blowouts of 1968 and the national Chicano Moratorium, which protested the high rate of Chicano casualties in the Vietnam War. Police harassment, infiltration by federal agents provacateur via COINTELPRO, and internal disputes led to the decline and disbandment of the Berets in 1972. S\u00e1nchez, then a professor at East Los Angeles College, revived the Brown Berets in 1992 prompted by the high number of Chicano homicides in Los Angeles County, hoping to replace the gang life with the Brown Berets.\nReies Tijerina, who was a vocal claimant to the rights of Latin Americans and Mexican Americans and a major figure of the early Chicano Movement, wrote: \"The Anglo press degradized the word 'Chicano.' They use it to divide us. We use it to unify ourselves with our people and with Latin America.\"\nCultural identity.\n\"Chicano\" represents a cultural identity that is neither fully \"American\" or \"Mexican.\" Chicano culture embodies the \"in-between\" nature of cultural hybridity. Central aspects of Chicano culture include lowriding, hip hop, rock, graffiti art, theater, muralism, visual art, literature, poetry, and more. Notable subcultures include the Cholo, Pachuca, Pachuco, and Pinto subcultures. Chicano culture has had international influence in the form of lowrider car clubs in Brazil and England, music and youth culture in Japan, M\u0101ori youth enhancing lowrider bicycles and taking on cholo style, and intellectuals in France \"embracing the deterritorializing qualities of Chicano subjectivity.\"\nAs early as the 1930s, the precursors to Chicano cultural identity were developing in Los Angeles, California and the Southwestern United States. Former zoot suiter Salvador \"El Chava\" reflects on how racism and poverty forged a hostile social environment for Chicanos which led to the development of gangs: \"we had to protect ourselves\". \"Barrios\" and \"colonias\" (rural \"barrios\") emerged throughout southern California and elsewhere in neglected districts of cities and outlying areas with little infrastructure. Alienation from public institutions made some Chicano youth susceptible to gang channels, who became drawn to their rigid hierarchical structure and assigned social roles in a world of government-sanctioned disorder.\nPachuco culture, which probably originated in the El Paso-Juarez area, spread to the borderland areas of California and Texas as \"Pachuquismo\", which would eventually evolve into \"Chicanismo\". Chicano zoot suiters on the west coast were influenced by Black zoot suiters in the jazz and swing music scene on the East Coast. Chicano zoot suiters developed a unique cultural identity, as noted by Charles \"Chaz\" Boj\u00f3rquez, \"with their hair done in big pompadours, and \"draped\" in tailor-made suits, they were swinging to their own styles. They spoke \"C\u00e1lo\", their own language, a cool jive of half-English, half-Spanish rhythms. [...] Out of the zootsuiter experience came lowrider cars and culture, clothes, music, tag names, and, again, its own graffiti language.\" San Antonio-based Chicano artist Adan Hernandez regarded pachucos as \"the coolest thing to behold in fashion, manner, and speech.\u201d As described by artist Carlos Jackson, \"Pachuco culture remains a prominent theme in Chicano art because the contemporary urban \"cholo\" culture\" is seen as its heir.\nMany aspects of Chicano culture like lowriding cars and bicycles have been stigmatized and policed by Anglo Americans who perceive Chicanos as \"juvenile delinquents or gang members\" for their embrace of nonwhite style and cultures, much as they did Pachucos. These negative societal perceptions of Chicanos were amplified by media outlets such as the \"Los Angeles Times\". Luis Alvarez remarks how negative portrayals in the media served as a tool to advocate for increased policing of Black and Brown male bodies in particular: \"Popular discourse characterizing nonwhite youth as animal-like, hypersexual, and criminal marked their bodies as \"other\" and, when coming from city officials and the press, served to help construct for the public a social meaning of African Americans and Mexican American youth [as, in their minds, justifiably criminalized].\"\nChicano rave culture in southern California provided a space for Chicanos to partially escape criminalization in the 1990s. Artist and archivist Guadalupe Rosales states that \"a lot of teenagers were being criminalized or profiled as criminals or gangsters, so the party scene gave access for people to escape that\". Numerous party crews, such as Aztek Nation, organized events and parties would frequently take place in neighborhood backyards, particularly in East and South Los Angeles, the surrounding valleys, and Orange County. By 1995, it was estimated that over 500 party crews were in existence. They laid the foundations for \"an influential but oft-overlooked Latin dance subculture that offered community for Chicano ravers, queer folk, and other marginalized youth.\" Ravers used map points techniques to derail police raids. Rosales states that a shift occurred around the late 1990s and increasing violence affected the Chicano party scene.\nIndigenous identity.\nChicano identity functions as a way to reclaim one's Indigenous American, and often Indigenous Mexican, ancestry\u2014to form an identity distinct from European identity, despite some Chicanos being of partial European descent\u2014as a way to resist and subvert colonial domination. Rather than part of European American culture, Alicia Gasper de Alba referred to \"Chicanismo\" as an \"\"alter-Native\" culture, an Other American culture Indigenous to the land base now known as the West and Southwest of the United States.\" While influenced by settler-imposed systems and structures, Alba refers to Chicano culture as \"not immigrant but native, not foreign but colonized, not alien but different from the overarching hegemony of white America.\"\nThe Plan Espiritual de Aztl\u00e1n (1969) drew from Frantz Fanon's \"The Wretched of the Earth\" (1961). In \"Wretched\", Fanon stated: \"the past existence of an Aztec civilization does not change anything very much in the diet of the Mexican peasant today\", elaborating that \"this passionate search for a national culture which existed before the colonial era finds its legitimate reason in the anxiety shared by native intellectuals to shrink away from that of Western culture in which they all risk being swamped ... the native intellectuals, since they could not stand wonderstruck before the history of today's barbarity, decided to go back further and to delve deeper down; and, let us make no mistake, it was with the greatest delight that they discovered that there was nothing to be ashamed of in the past, but rather dignity, glory, and solemnity.\"\nThe Chicano Movement adopted this perspective through the notion of Aztl\u00e1n\u2014a mythic Aztec homeland which Chicanos used as a way to connect themselves to a precolonial past, before the time of the \"'gringo' invasion of our lands.\" Chicano scholars have described how this functioned as a way for Chicanos to reclaim a diverse or imprecise Indigenous past; while recognizing how Aztl\u00e1n promoted divisive forms of Chicano nationalism that \"did little to shake the walls and bring down the structures of power as its rhetoric so firmly proclaimed\". As stated by Chicano historian Juan G\u00f3mez-Qui\u00f1ones, the Plan Espiritual de Aztl\u00e1n was \"stripped of what radical element it possessed by stressing its alleged romantic idealism, reducing the concept of Aztl\u00e1n to a psychological ploy ... all of which became possible because of the Plan's incomplete analysis which, in turn, allowed it ... to degenerate into reformism.\"\nWhile acknowledging its romanticized and exclusionary foundations, Chicano scholars like Rafael P\u00e9rez-Torres state that Aztl\u00e1n opened a subjectivity which stressed a connection to Indigenous peoples and cultures at a critical historical moment in which \"Mexican-Americans\" and Mexicans were \"under pressure to assimilate particular standards\u2014of beauty, of identity, of aspiration. In a Mexican context, the pressure was to urbanize and Europeanize ... \"Mexican-Americans\" were expected to accept anti-indigenous discourses as their own.\" As P\u00e9rez-Torres concludes, Aztl\u00e1n allowed \"for another way of aligning one's interests and concerns with community and with history ... though hazy as to the precise means in which agency would emerge, Aztl\u00e1n valorized a Chicanismo that rewove into the present previously devalued lines of descent.\" Romanticized notions of \"Aztl\u00e1n\" have declined among some Chicanos, who argue for a need to reconstruct the place of Indigeneity in relation to Chicano identity.\nDanza Azteca grew popular in the U.S. with the rise of the Chicano Movement, which inspired some \"Latinos to embrace their ethnic heritage and question the Eurocentric norms forced upon them.\" The use of pre-contact Aztec cultural elements has been critiqued by some Chicanos who stress a need to represent the diversity of Indigenous ancestry among Chicanos. Patrisia Gonzales portrays Chicanos as descendants of the Indigenous peoples of Mexico who have been displaced by colonial violence, positioning them as \"detribalized Indigenous peoples and communities.\" Roberto Cintli Rodr\u00edguez describes Chicanos as \"de-Indigenized,\" which he remarks occurred \"in part due to religious indoctrination and a violent uprooting from the land\", detaching millions of people from ma\u00edz-based cultures throughout the greater Mesoamerican region. Rodr\u00edguez asks how and why \"peoples who are clearly red or brown and undeniably Indigenous to this continent have allowed ourselves, historically, to be framed by bureaucrats and the courts, by politicians, scholars, and the media as alien, illegal, and less than human.\"\nGloria E. Anzald\u00faa has addressed Chicano's detribalization: \"In the case of Chicanos, being 'Mexican' is not a tribe. So in a sense Chicanos and Mexicans are 'detribalized'. We don't have tribal affiliations but neither do we have to carry ID cards establishing tribal affiliation.\" Anzald\u00faa recognized that \"Chicanos, people of color, and 'whites'\" have often chosen \"to ignore the struggles of Native people even when it's right in our \"caras\" (faces),\" expressing disdain for this \"willful ignorance\". She concluded that \"though both \"detribalized urban mixed bloods\" and Chicanos are recovering and reclaiming, this society is killing off urban mixed bloods through cultural genocide, by not allowing them equal opportunities for better jobs, schooling, and health care.\" In\u00e9s Hern\u00e1ndez-\u00c1vila argued that Chicanos should recognize and reconnect with their roots \"respectfully and humbly\" while also validating \"those peoples who still maintain their identity as original peoples of this continent\" in order to create radical change capable of \"transforming our world, our universe, and our lives\".\nPolitical aspects.\nAnti-imperialism and international solidarity.\nDuring World War II, Chicano youth were targeted by white servicemen, who despised their \"cool, measured indifference to the war, as well as an increasingly defiant posture toward whites in general\". Historian Robin Kelley states that this \"annoyed white servicemen to no end\". During the Zoot Suit Riots (1943), white rage erupted in Los Angeles, which \"became the site of racist attacks on Black and Chicano youth, during which white soldiers engaged in what amounted to a ritualized stripping of the zoot.\" Zoot suits were a symbol of collective resistance among Chicano and Black youth against city segregation and fighting in the war. Many Chicano and Black zoot-suiters engaged in draft evasion because they felt it was hypocritical for them to be expected to \"fight for democracy\" abroad yet face racism and oppression daily in the U.S.\nThis galvanized Chicano youth to focus on anti-war activism, \"especially influenced by the Third World movements of liberation in Asia, Africa, and Latin America.\" Historian Mario T. Garc\u00eda reflects that \"these anti-colonial and anti-Western movements for national liberation and self-awareness touched a historical nerve among Chicanos as they began to learn that they shared some similarities with these Third World struggles.\" Chicano poet Alurista argued that \"Chicanos cannot be truly free until they recognize that the struggle in the United States is intricately bound with the anti-imperialist struggle in other countries.\" The Cuban Revolution (1953\u20131959) led by Fidel Castro and Che Guevara was particularly influential to Chicanos, as noted by Garc\u00eda, who notes that Chicanos viewed the revolution as \"a nationalist revolt against 'Yankee imperialism' and neo-colonialism.\"\nIn the 1960s, the Chicano Movement brought \"attention and commitment to local struggles with an analysis and understanding of international struggles\". Chicano youth organized with Black, Latin American, and Filipino activists to form the Third World Liberation Front (TWLF), which fought for the creation of a Third World college. During the Third World Liberation Front strikes of 1968, Chicano artists created posters to express solidarity. Chicano poster artist Rupert Garc\u00eda referred to the place of artists in the movement: \"I was critical of the police, of capitalist exploitation. I did posters of Che, of Zapata, of other Third World leaders. As artists, we climbed down from the ivory tower.\" Learning from Cuban poster makers of the post-revolutionary period, Chicano artists \"incorporated international struggles for freedom and self-determination, such as those of Angola, Chile, and South Africa\", while also promoting the struggles of Indigenous people and other civil rights movements through Black-brown unity. Chicanas organized with women of color activists to create the Third World Women's Alliance (1968-1980), representing \"visions of liberation in third world solidarity that inspired political projects among racially and economically marginalized communities\" against U.S. capitalism and imperialism.\nThe Chicano Moratorium (1969\u20131971) against the Vietnam War was one of the largest demonstrations of Mexican-Americans in history, drawing over 30,000 supporters in East L.A. Draft evasion was a form of resistance for Chicano anti-war activists such as Rosalio Mu\u00f1oz, Ernesto Vigil, and Salomon Baldengro. They faced a felony charge\u2014a minimum of five years prison time, $10,000, or both. In response, Munoz wrote \"I declare my independence of the Selective Service System. I accuse the government of the United States of America of genocide against the Mexican people. Specifically, I accuse the draft, the entire social, political, and economic system of the United States of America, of creating a funnel which shoots Mexican youth into Vietnam to be killed and to kill innocent men, women, and children...\" Rodolfo Corky Gonzales expressed a similar stance: \"My feelings and emotions are aroused by the complete disregard of our present society for the rights, dignity, and lives of not only people of other nations but of our own unfortunate young men who die for an abstract cause in a war that cannot be honestly justified by any of our present leaders.\"\nAnthologies such as \"This Bridge Called My Back\": \"Writings by Radical Women of Color\" (1981) were produced in the late 1970s and early 80s by lesbian of color writers Cherr\u00ede Moraga, Pat Parker, Toni Cade Bambara, Chrystos, Audre Lorde, Gloria E. Anzald\u00faa, Cheryl Clarke, Jewelle Gomez, Kitty Tsui, and Hattie Gossett, who developed a poetics of liberation. and Third Woman Press, founded in 1979 by Chicana feminist Norma Alarc\u00f3n, provided sites for the production of women of color and Chicana literatures and critical essays. While first world feminists focused \"on the liberal agenda of political rights\", Third World feminists \"linked their agenda for women's rights with economic and cultural rights\" and unified together \"under the banner of Third World solidarity\". Maylei Blackwell identifies that this internationalist critique of capitalism and imperialism forged by women of color has yet to be fully historicized and is \"usually dropped out of the false historical narrative\".\nIn the 1980s and 90s, Central American activists influenced Chicano leaders. The Mexican American Legislative Caucus (MALC) supported the Esquipulas Peace Agreement in 1987, standing in opposition to Contra aid. Al Luna criticized Reagan and American involvement while defending Nicaragua's Sandinista-led government: \"President Reagan cannot credibly make public speeches for peace in Central America while at the same time advocating for a three-fold increase in funding to the Contras.\" The Southwest Voter Research Initiative (SVRI), launched by Chicano leader Willie Vel\u00e1squez, intended to educate Chicano youth about Central and Latin American political issues. In 1988, \"there was no significant urban center in the Southwest where Chicano leaders and activists had not become involved in lobbying or organizing to change U.S. policy in Nicaragua.\" In the early 1990s, Cherr\u00ede Moraga urged Chicano activists to recognize that \"the Anglo invasion of Latin America [had] extended well beyond the Mexican/American border\" while Gloria E. Anzald\u00faa positioned Central America as the primary target of a U.S. interventionism that had murdered and displaced thousands. However, Chicano solidarity narratives of Central Americans in the 1990s tended to center themselves, stereotype Central Americans, and filter their struggles \"through Chicana/o struggles, histories, and imaginaries.\"\nChicano activists organized against the Gulf War (1990\u201391). Raul Ruiz of the Chicano Mexican Committee against the Gulf War stated that U.S. intervention was \"to support U.S. oil interests in the region.\" Ruiz expressed, \"we were the only Chicano group against the war. We did a lot of protesting in L.A. even though it was difficult because of the strong support for the war and the anti-Arab reaction that followed ... we experienced racist attacks [but] we held our ground.\" The end of the Gulf War, along with the Rodney King Riots, were crucial in inspiring a new wave of Chicano political activism. In 1994, one of the largest demonstrations of Mexican Americans in the history of the United States occurred when 70,000 people, largely Chicanos and Latinos, marched in Los Angeles and other cities to protest Proposition 187, which aimed to cut educational and welfare benefits for undocumented immigrants.\nIn 2004, Mujeres against Militarism and the Raza Unida Coalition sponsored a Day of the Dead vigil against militarism within the Latino community, addressing the War in Afghanistan (2001\u2013) and the Iraq War (2003\u20132011) They held photos of the dead and chanted \"no blood for oil.\" The procession ended with a five-hour vigil at Tia Chucha's Centro Cultural. They condemned \"the Junior Reserve Officers Training Corps (JROTC) and other military recruitment programs that concentrate heavily in Latino and African American communities, noting that JROTC is rarely found in upper-income Anglo communities.\" Rub\u00e9n Funkahuatl Guevara organized a benefit concert for Latin@s Against the War in Iraq and \"Mexam\u00e9rica por la Paz\" at Self-Help Graphics against the Iraq War. Although the events were well-attended, Guevara stated that \"the Feds know how to manipulate fear to reach their ends: world military dominance and maintaining a foothold in an oil-rich region were their real goals.\"\nLabor organizing against capitalist exploitation.\nChicano and Mexican labor organizers played an active role in notable labor strikes since the early 20th century including the Oxnard strike of 1903, Pacific Electric Railway strike of 1903, 1919 Streetcar Strike of Los Angeles, Cantaloupe strike of 1928, California agricultural strikes (1931\u20131941), and the Ventura County agricultural strike of 1941, endured mass deportations as a form of strikebreaking in the Bisbee Deportation of 1917 and Mexican Repatriation (1929\u20131936), and experienced tensions with one another during the Bracero program (1942\u20131964). Although organizing laborers were harassed, sabotaged, and repressed, sometimes through warlike tactics from capitalist owners who engaged in coervice labor relations and collaborated with and received support from local police and community organizations, Chicano and Mexican workers, particularly in agriculture, have been engaged in widespread unionization activities since the 1930s.\nPrior to unionization, agricultural workers, many of whom were undocumented aliens, worked in dismal conditions. Historian F. Arturo Rosales recorded a Federal Project Writer of the period, who stated: \"It is sad, yet true, commentary that to the average landowner and grower in California the Mexican was to be placed in much the same category with ranch cattle, with this exception\u2013the cattle were for the most part provided with comparatively better food and water and immeasurably better living accommodations.\" Growers used cheap Mexican labor to reap bigger profits and, until the 1930s, perceived Mexicans as docile and compliant with their subjugated status because they \"did not organize troublesome labor unions, and it was held that he was not educated to the level of unionism\". As one grower described, \"We want the Mexican because we can treat them as we cannot treat any other living man ... We can control them by keeping them at night behind bolted gates, within a stockade eight feet high, surrounded by barbed wire ... We can make them work under armed guards in the fields.\"\nUnionization efforts were initiated by the Confederaci\u00f3n de Uniones Obreras (Federation of Labor Unions) in Los Angeles, with twenty-one chapters quickly extending throughout southern California, and La Uni\u00f3n de Trabajadores del Valle Imperial (Imperial Valley Workers' Union). The latter organized the Cantaloupe strike of 1928, in which workers demanded better working conditions and higher wages, but \"the growers refused to budge and, as became a pattern, local authorities sided with the farmers and through harassment broke the strike\". Communist-led organizations such as the Cannery and Agricultural Workers' Industrial Union (CAWIU) supported Mexican workers, renting spaces for cotton pickers during the cotton strikes of 1933 after they were thrown out of company housing by growers. Capitalist owners used \"red-baiting\" techniques to discredit the strikes through associating them with communists. Chicana and Mexican working women showed the greatest tendency to organize, particularly in the Los Angeles garment industry with the International Ladies' Garment Workers' Union, led by anarchist Rose Pesotta.\nDuring World War II, the government-funded Bracero program (1942\u20131964) hindered unionization efforts. In response to the California agricultural strikes and the 1941 Ventura County strike of Chicano and Mexican, as well as Filipino, lemon pickers/packers, growers organized the Ventura County Citrus Growers Committee (VCCGC) and launched a lobbying campaign to pressure the U.S. government to pass laws to prohibit labor organizing. VCCGC joined with other grower associations, forming a powerful lobbying bloc in Congress, and worked to legislate for (1) a Mexican guest workers program, which would become the Bracero program, (2) laws prohibiting strike activity, and (3) military deferments for pickers. Their lobbying efforts were successful: unionization among farmworkers was made illegal, farmworkers were excluded from minimum wage laws, and the usage of child labor by growers was ignored. In formerly active areas, such as Santa Paula, union activity stopped for over thirty years as a result.\nWhen World War II ended, the Bracero program continued. Legal anthropologist Martha Menchaca states that this was \"regardless of the fact that massive quantities of crops were no longer needed for the war effort ... after the war, the braceros were used for the benefit of the large-scale growers and not for the nation's interest.\" The program was extended for an indefinite period in 1951. In the mid-1940s, labor organizer Ernesto Galarza founded the National Farm Workers Union (NFWU) in opposition to the Bracero Program, organizing a large-scale 1947 strike against the Di Giorgio Fruit Company in Arvin, California. Hundreds of Mexican, Filipino, and white workers walked out and demanded higher wages. The strike was broken by the usual tactics, with law enforcement on the side of the owners, evicting strikers and bringing in undocumented workers as strikebreakers. The NFWU folded, but served as a precursor to the United Farm Workers Union led by C\u00e9sar Ch\u00e1vez. By the 1950s, opposition to the Bracero program had grown considerably, as unions, churches, and Mexican-American political activists raised awareness about the effects it had on American labor standards. On December 31, 1964, the U.S. government conceded and terminated the program.\nFollowing the closure of the Bracero program, domestic farmworkers began to organize again because \"growers could not longer maintain the peonage system\" with the end of imported laborers from Mexico. Labor organizing formed part of the Chicano Movement via the struggle of farmworkers against depressed wages and working conditions. C\u00e9sar Ch\u00e1vez began organizing Chicano farmworkers in the early 1960s, first through the National Farm Workers Association (NFWA) and then merging the association with the Agricultural Workers Organizing Committee (AWOC), an organization of mainly Filipino workers, to form the United Farm Workers. The labor organizing of Ch\u00e1vez was central to the expansion of unionization throughout the United States and inspired the Farm Labor Organizing Committee (FLOC), under the leadership of Baldemar Vel\u00e1squez, which continues today. Farmworkers collaborated with local Chicano organizations, such as in Santa Paula, California, where farmworkers attended Brown Berets meetings in the 1970s and Chicano youth organized to improve working conditions and initiate an urban renewal project on the eastside of the city.\nAlthough Mexican and Chicano workers, organizers, and activists organized for decades to improve working conditions and increase wages, some scholars characterize these gains as minimal. As described by Ronald Mize and Alicia Swords, \"piecemeal gains in the interests of workers have had very little impact on the capitalist agricultural labor process, so picking grapes, strawberries, and oranges in 1948 is not so different from picking those same crops in 2008.\" U.S. agriculture today remains totally reliant on Mexican labor, with Mexican-born individuals now constituting about 90% of the labor force.\nStruggles in the education system.\nChicanos often endure struggles in the U.S. education system, such as being erased in curriculums and devalued as students. Some Chicanos identify schools as colonial institutions that exercise control over colonized students by teaching Chicanos to idolize whiteness and develop a negative self-image of themselves and their worldviews. School segregation between Mexican and white students was not legally ended until the late 1940s. In Orange County, California, 80% of Mexican students could only attend schools that taught Mexican children manual education, or gardening, bootmaking, blacksmithing, and carpentry for Mexican boys and sewing and homemaking for Mexican girls. White schools taught academic preparation. When Sylvia Mendez was told to attend a Mexican school, her parents brought suit against the court in Mendez vs. Westminster (1947) and won.\nAlthough legal segregation had been successfully challenged, \"de facto\" or segregation-in-practice continued in many areas. Schools with primarily Mexican American enrollment were still treated as \"Mexican schools\" much as before the legal overturning of segregation. Mexican American students were still treated poorly in schools. Continued bias in the education system motivated Chicanos to protest and use direct action, such as walkouts, in the 1960s. On March 5, 1968, the Chicano Blowouts at East Los Angeles High School occurred as a response to the racist treatment of Chicano students, an unresponsive school board, and a high dropout rate. It became known as \"the first major mass protest against racism undertaken by Mexican-Americans in the history of the United States.\" \nSal Castro, a Chicano social science teacher at the school was arrested and fired for inspiring the walkouts. It was led by Harry Gamboa Jr. who was named \"one of the hundred most dangerous and violent subversives in the United States\" for organizing the student walkouts. The day prior, FBI director J. Edgar Hoover sent out a memo to law enforcement to place top priority on \"political intelligence work to prevent the development of nationalist movements in minority communities\". Chicana activist Alicia Escalante protested Castro's dismissal: \"We in the Movement will at least be able to hold our heads up and say that we haven't submitted to the gringo or to the pressures of the system. We are brown and we are proud. I am at least raising my children to be proud of their heritage, to demand their rights, and as they become parents they too will pass this on until justice is done.\"\nIn 1969, Plan de Santa B\u00e1rbara was drafted as a 155-page document that outlined the foundation of Chicano Studies programs in higher education. It called for students, faculty, employees and the community to come together as \"central and decisive designers and administrators of these programs\". Chicano students and activists asserted that universities should exist to serve the community. However, by the mid-1970s, much of the radicalism of earlier Chicano studies became deflated by the education system, aimed to alter Chicano Studies programs from within. Mario Garc\u00eda argued that one \"encountered a deradicalization of the radicals\". Some opportunistic faculty avoided their political responsibilities to the community. University administrators co-opted oppositional forces within Chicano Studies programs and encouraged tendencies that led \"to the loss of autonomy of Chicano Studies programs.\" At the same time, \"a domesticated Chicano Studies provided the university with the facade of being tolerant, liberal, and progressive.\"\nSome Chicanos argued that the solution was to create \"publishing outlets that would challenge Anglo control of academic print culture with its rules on peer review and thereby publish alternative research,\" arguing that a Chicano space in the colonial academy could \"avoid colonization in higher education\". In an attempt to establish educational autonomy, they worked with institutions like the Ford Foundation, but found that \"these organizations presented a paradox\". Rodolfo Acu\u00f1a argued that such institutions \"quickly became content to only acquire funding for research and thereby determine the success or failure of faculty\". Chicano Studies became \"much closer [to] the mainstream than its practitioners wanted to acknowledge.\" Others argued that Chicano Studies at UCLA shifted from its earlier interests in serving the Chicano community to gaining status within the colonial institution through a focus on academic publishing, which alienated it from the community.In 2012, the Mexican American Studies Department Programs (MAS) in Tucson Unified School District were banned after a campaign led by Anglo-American politician Tom Horne accused it of working to \"promote the overthrow of the U.S. government, promote resentment toward a race or class of people, are designed primarily for pupils of a particular ethnic group or advocate ethnic solidarity instead of the treatment of pupils as individuals.\" Classes on Latino literature, American history/Mexican-American perspectives, Chicano art, and an American government/social justice education project course were banned. Readings of In Lak'ech from Luis Valdez's poem \"Pensamiento Serpentino\" were also banned.\nSeven books, including Paulo Friere's \"Pedagogy of the Oppressed\" and works covering Chicano history and critical race theory, were banned, taken from students, and stored away. The ban was overturned in 2017 by Judge A. Wallace Tashima, who ruled that it was unconstitutional and motivated by racism by depriving Chicano students of knowledge, thereby violating their Fourteenth Amendment right. The Xicanx Institute for Teaching &amp; Organizing (XITO) emerged to carry on the legacy of the MAS programs. Chicanos continue to support the institution of Chicano studies programs. In 2021, students at Southwestern College, the closest college to the Mexico-United States Border urged for the creation of a Chicanx Studies Program to service the predominately Latino student body.\nRejection of borders.\nThe Chicano concept of \"sin fronteras\" rejects the idea of borders. Some argued that the 1848 Treaty of Guadalupe Hidalgo transformed the Rio Grande region from a rich cultural center to a rigid border poorly enforced by the United States government. At the end of the Mexican-American War, 80,000 Spanish-Mexican-Indian people were forced into sudden U.S. habitation. Some Chicanos identified with the idea of Aztl\u00e1n as a result, which celebrated a time preceding land division and rejected the \"immigrant/foreigner\" categorization by Anglo society. Chicano activists have called for unionism between both Mexicans and Chicanos on both sides of the border.\nIn the early 20th century, the border crossing had become a site of dehumanization for Mexicans. Protests in 1910 arose along the Santa Fe Bridge from abuses committed against Mexican workers while crossing the border. The 1917 Bath riots erupted after Mexicans crossing the border were required to strip naked and be disinfected with chemical agents like gasoline, kerosene, sulfuric acid, and Zyklon B, the latter of which was the fumigation of choice and would later notoriously be used in the gas chambers of Nazi Germany. Chemical dousing continued into the 1950s. During the early 20th century, Chicanos used \"corridos\" \"to counter Anglocentric hegemony.\" Ram\u00f3n Saldivar stated that \"\"corridos\" served the symbolic function of empirical events and for creating counterfactual worlds of lived experience (functioning as a substitute for fiction writing).\"Newspaper \"Sin Fronteras\" (1976\u20131979) openly rejected the Mexico-United States border. The newspaper considered it \"to be only an artificial creation that in time would be destroyed by the struggles of Mexicans on both sides of the border\" and recognized that \"Yankee political, economic, and cultural colonialism victimized all Mexicans, whether in the U.S. or in Mexico.\" Similarly, the General Brotherhood of Workers (CASA), important to the development of young Chicano intellectuals and activists, identified that, as \"victims of oppression, \"Mexicanos\" could achieve liberation and self-determination only by engaging in a borderless struggle to defeat American international capitalism.\"\nChicana theorist Gloria E. Anzald\u00faa notably emphasized the border as a \"1,950 mile-long wound that does not heal\". In referring to the border as a wound, writer Catherine Leen suggests that Anzald\u00faa recognizes \"the trauma and indeed physical violence very often associated with crossing the border from Mexico to the US, but also underlies the fact that the cyclical nature of this immigration means that this process will continue and find little resolution.\" Anzald\u00faa writes that \"la frontera\" signals \"the coming together of two self-consistent but habitually incompatible frames of reference [which] cause \"un choque\", a cultural collision\" because \"the U.S.-Mexican border \"es una herida abierta\" where the Third World grates against the first and bleeds.\" Chicano and Mexican artists and filmmakers continue to address \"the contentious issues of exploitation, exclusion, and conflict at the border and attempt to overturn border stereotypes\" through their work. Luis Alberto Urrea writes \"the border runs down the middle of me. I have a barbed wire fence neatly bisecting my heart.\"\nSociological aspects.\nCriminalization.\nThe 19th-century and early-20th-century image of the Mexican in the U.S. was \"that of the greasy Mexican bandit or \"bandito,\"\" who was perceived as criminal because of Mestizo ancestry and \"Indian blood.\" This rhetoric fueled anti-Mexican sentiment among whites, which led to many lynchings of Mexicans in the period as an act of racist violence. One of the largest massacres of Mexicans was known as \"La Matanza\" in Texas, where hundreds of Mexicans were lynched by white mobs. Many whites viewed Mexicans as inherently criminal, which they connected to their Indigenous ancestry. White historian Walter P. Webb wrote in 1935, \"there is a cruel streak in the Mexican nature ... this cruelty may be a heritage from the Spanish and of the Inquisition; it may, and doubtless should be, attributed partly to Indian blood.\" The \"greasy bandito\" stereotype of the old West evolved into images of \"crazed Zoot-Suiters and pachuco killers in the 1940s, to contemporary \"cholos\", gangsters, and gang members.\" Pachucos were portrayed as violent criminals in American mainstream media, which fueled the Zoot Suit Riots; initiated by off-duty policemen conducting a vigilante-hunt, the riots targeted Chicano youth who wore the zoot suit as a symbol of empowerment. On-duty police supported the violence against Chicano zoot suiters; they \"escorted the servicemen to safety and arrested their Chicano victims.\" Arrest rates of Chicano youth rose during these decades, fueled by the \"criminal\" image portrayed in the media, by politicians, and by the police. Not aspiring to assimilate in Anglo-American society, Chicano youth were criminalized for their defiance to cultural assimilation: \"When many of the same youth began wearing what the larger society considered outlandish clothing, sporting distinctive hairstyles, speaking in their own language (\"Cal\u00f3\"), and dripping with attitude, law enforcement redoubled their efforts to rid [them from] the streets.\"\nIn the 1970s and subsequent decades, there was a wave of police killings of Chicanos. One of the most prominent cases was Luis \"Tato\" Rivera, who was a 20-year-old Chicano shot in the back by officer Craig Short in 1975. 2,000 Chicano demonstrators showed up to the city hall of National City, California in protest. Short was indicted for manslaughter by district attorney Ed Miller and was acquitted of all charges. Short was later appointed acting chief of police of National City in 2003. Another high-profile case was the murder of Ricardo Falc\u00f3n, a student at the University of Colorado and leader of the United Latin American Students (UMAS), by Perry Brunson, a member of the far-right American Independent Party, at a gas station. Bruson was tried for manslaughter and was \"acquitted by an all-White jury\". Falc\u00f3n became a martyr for the Chicano Movement as police violence increased in the subsequent decades. Similar cases led sociologist Alfredo Mirand\u00e9 to refer to the U.S. criminal justice system as \"gringo justice\", because \"it reflected one standard for Anglos and another for Chicanos.\"\nThe criminalization of Chicano youth in the \"barrio\" remains omnipresent. Chicano youth who adopt a \"cholo\" or \"chola\" identity endure hyper-criminalization in what has been described by Victor Rios as the youth control complex. While older residents initially \"embraced the idea of a \"chola\" or \"cholo\" as a larger subculture not necessarily associated with crime and violence (but rather with a youthful temporary identity), law enforcement agents, ignorant or disdainful of \"barrio\" life, labeled youth who wore clean white tennis shoes, shaved their heads, or long socks, as deviant.\" Community members were convinced by the police of cholo criminality, which led to criminalization and surveillance \"reminiscent of the criminalization of Chicana and Chicano youth during the Zoot-Suit era in the 1940s.\"\nSociologist Jos\u00e9 S. Plascencia-Castillo refers to the \"barrio\" as a panopticon that leads to intense self-regulation, as Cholo youth are both scrutinized by law enforcement to \"stay in their side of town\" and by the community who in some cases \"call the police to have the youngsters removed from the premises\". The intense governance of Chicano youth, especially of cholo identity, has deep implications on youth experience, affecting their physical and mental health as well as their outlook on the future. Some youth feel they \"can either comply with the demands of authority figures, and become obedient and compliant, and suffer the accompanying loss of identity and self-esteem, or, adopt a resistant stance and contest social invisibility to command respect in the public sphere.\"\nGender and sexuality.\nChicanas.\nChicanas often confront objectification in Anglo society, being perceived as \"exotic\", \"lascivious\", and \"hot\" at a very young age while also facing denigration as \"barefoot\", \"pregnant\", \"dark\", and \"low-class\". These perceptions in society create numerous negative sociological and psychological effects, such as excessive dieting and eating disorders. Social media may enhance these stereotypes of Chicana women and girls. Numerous studies have found that Chicanas experience elevated levels of stress as a result of sexual expectations by society, as well as their parents and families.\nAlthough many Chicana youth desire open conversation of these gender roles and sexuality, as well as mental health, these issues are often not discussed openly in Chicano families, which perpetuates unsafe and destructive practices. While young Chicanas are objectified, middle-aged Chicanas discuss feelings of being invisible, saying they feel trapped in balancing family obligations to their parents and children while attempting to create a space for their own sexual desires. The expectation that Chicanas should be \"protected\" by Chicanos may also constrict the agency and mobility of Chicanas.\nChicanas are often relegated to a secondary and subordinate status in families. Cherrie Moraga argues that this issue of patriarchal ideology in Chicano and Latino communities runs deep, as the great majority of Chicano and Latino men believe in and uphold male supremacy. Moraga argues that this ideology is not only upheld by men in Chicano families, but also by mothers in their relationship to their children: \"the daughter must constantly earn the mother's love, prove her fidelity to her. The son\u2014he gets her love for free.\"\nChicanos.\nChicanos develop their manhood within a context of marginalization in white society. Some argue that \"Mexican men and their Chicano brothers suffer from an inferiority complex due to the conquest and genocide inflicted upon their Indigenous ancestors,\" which leaves Chicano men feeling trapped between identifying with the so-called \"superior\" European and the so-called \"inferior\" Indigenous sense of self. This conflict may manifest itself in the form of hypermasculinity or machismo, in which a \"quest for power and control over others in order to feel better\" about oneself is undertaken. This may result in men developing abusive behaviors, the development of an impenetrable \"cold\" persona, alcohol abuse, and other destructive and self-isolating behaviors.\nThe lack of discussion of what it means to be a Chicano man between Chicano male youth and their fathers or their mothers creates a search for identity that often leads to self-destructive behaviors. Chicano male youth tend to learn about sex from their peers as well as older male family members who perpetuate the idea that as men they have \"a right to engage in sexual activity without commitment\". The looming threat of being labeled a \"joto\" (gay) for not engaging in sexual activity also conditions many Chicanos to \"use\" women for their own sexual desires. Gabriel S. Estrada argues that the criminalization of Chicanos proliferates further homophobia among Chicano boys and men who may adopt hypermasculine personas to escape such association.\nHeteronormativity.\nHeteronormative gender roles are typically enforced in Chicano families. Any deviation from gender and sexual conformity is commonly perceived as a weakening or attack of \"la familia\". However, Chicano men who retain a masculine or machismo performance are afforded some mobility to discreetly engage in homosexual behaviors, as long as it remains on the fringes. Effeminacy in Chicanos, Chicana lesbianism, and any deviation is understood as an attack on the family.\nQueer Chicana/os may seek refuge in their families, if possible, because it is difficult for them to find spaces where they feel safe in the dominant and hostile white gay culture. Chicano machismo, religious traditionalism, and homophobia creates challenges for them to feel accepted by their families. Gabriel S. Estrada argues that upholding \"Judeo-Christian mandates against homosexuality that are not native to [Indigenous Mexico],\" exiles queer Chicana/o youth.\nMental health.\nChicanos may seek out both Western biomedical healthcare and Indigenous health practices when dealing with trauma or illness. The effects of colonization are proven to produce psychological distress among Indigenous communities. Intergenerational trauma, along with racism and institutionalized systems of oppression, have been shown to adversely impact the mental health of Chicanos and Latinos. Mexican Americans are three times more likely than European Americans to live in poverty. Chicano adolescent youth experience high rates of depression and anxiety. Chicana adolescents have higher rates of depression and suicidal ideation than their European-American and African-American peers. Chicano adolescents experience high rates of homicide, and suicide. Chicanos ages ten to seventeen are at a greater risk for mood and anxiety disorders than their European-American and African-American peers. Scholars have determined that the reasons for this are unclear due to the scarcity of studies on Chicano youth, but that intergenerational trauma, acculturative stress, and family factors are believed to contribute.\nAmong Mexican immigrants who have lived in the United States for less than thirteen years, lower rates of mental health disorders were found in comparison to Mexican-Americans and Chicanos born in the United States. Scholar Yvette G. Flores concludes that these studies demonstrate that \"factors associated with living in the United States are related to an increased risk of mental disorders.\" Risk factors for negative mental health include historical and contemporary trauma stemming from colonization, marginalization, discrimination, and devaluation. The disconnection of Chicanos from their Indigeneity has been cited as a cause of trauma and negative mental health:Loss of language, cultural rituals, and spiritual practices creates shame and despair. The loss of culture and language often goes unmourned, because it is silenced and denied by those who occupy, conquer, or dominate. Such losses and their psychological and spiritual impact are passed down across generations, resulting in depression, disconnection, and spiritual distress in subsequent generations, which are manifestations of historical or intergenerational trauma.Psychological distress may emerge from Chicanos being \"othered\" in society since childhood and is linked to psychiatric disorders and symptoms which are culturally bound\u2014\"susto\" (fright), \"nervios\" (nerves), \"mal de ojo\" (evil eye), and \"ataque de nervios\" (an attack of nerves resembling a panic attack). Dr. Manuel X. Zamarripa discusses how mental health and spirituality are often seen as disconnected subjects in Western perspectives. Zamarripa states \"in our community, spirituality is key for many of us in our overall wellbeing and in restoring and giving balance to our lives\". For Chicanos, Zamarripa recognizes that identity, community, and spirituality are three core aspects which are essential to maintaining good mental health.\nSpirituality.\nChicano spirituality has been described as a process of engaging in a journey to unite one's consciousness for the purposes of cultural unity and social justice. It brings together many elements and is therefore hybrid in nature. Scholar Regina M Marchi states that Chicano spirituality \"emphasizes elements of struggle, process, and politics, with the goal of creating a unity of consciousness to aid social development and political action\". Lara Medina and Martha R. Gonzales explain that \"reclaiming and reconstructing our spirituality based on non-Western epistemologies is central to our process of decolonization, particularly in these most troubling times of incessant Eurocentric, heteronormative patriarchy, misogyny, racial injustice, global capitalist greed, and disastrous global climate change.\" As a result, some scholars state that Chicano spirituality must involve a study of Indigenous Ways of Knowing (IWOK). The \"Circulo de Hombres\" group in San Diego, California spiritually heals Chicano, Latino, and Indigenous men \"by exposing them to Indigenous-based frameworks, men of this cultural group heal and rehumanize themselves through Maya-Nahua Indigenous-based concepts and teachings\", helping them process intergenerational trauma and dehumanization that has resulted from colonization. A study on the group reported that reconnecting with Indigenous worldviews was overwhelmingly successful in helping Chicano, Latino, and Indigenous men heal. As stated by Jesus Mendoza, \"our bodies remember our indigenous roots and demand that we open our mind, hearts, and souls to our reality\".\nChicano spirituality is a way for Chicanos to listen, reclaim, and survive while disrupting coloniality. While historically Catholicism was the primary way for Chicanos to express their spirituality, this is changing rapidly. According to a Pew Research Center report in 2015, \"the primary role of Catholicism as a conduit to spirituality has declined and some Chicanos have changed their affiliation to other Christian religions and many more have stopped attending church altogether.\" Increasingly, Chicanos are considering themselves spiritual rather than religious or part of an organized religion. A study on spirituality and Chicano men in 2020 found that many Chicanos indicated the benefits of spirituality through connecting with Indigenous spiritual beliefs and worldviews instead of Christian or Catholic organized religion in their lives. Dr. Lara Medina defines spirituality as (1) Knowledge of oneself\u2014one's gifts and one's challenges, (2) Co-creation or a relationship with communities (others), and (3) A relationship with sacred sources of life and death 'the Great Mystery' or Creator. Jesus Mendoza writes that, for Chicanos, \"spirituality is our connection to the earth, our pre-Hispanic history, our ancestors, the mixture of pre-Hispanic religion with Christianity ... a return to a non-Western worldview that understands all life as sacred.\" In her writing on Gloria Anzaldua's idea of \"spiritual activism\", AnaLouise Keating states that spirituality is distinct from organized religion and New Age thinking. Leela Fernandes defines spirituality as follows:When I speak of spirituality, at the most basic level I am referring to an understanding of the self as encompassing body and mind, as well as spirit. I am also referring to a transcendent sense of interconnection that moves beyond the knowable, visible material world. This sense of interconnection has been described variously as divinity, the sacred, spirit, or simply the universe. My understanding is also grounded in a form of lived spirituality, which is directly accessible to all and which does not need to be mediated by religious experts, institutions or theological texts; this is what is often referred to as the mystical side of spirituality... Spirituality can be as much about practices of compassion, love, ethics, and truth defined in nonreligious terms as it can be related to the mystical reinterpretations of existing religious traditions.\nDavid Carrasco states that Mesoamerican spiritual or religious beliefs have historically always been evolving in response to the conditions of the world around them: \"These ritual and mythic traditions were not mere repetitions of ancient ways. New rituals and mythic stories were produced to respond to ecological, social, and economic changes and crises.\" This was represented through the art of the Olmecs, Maya, and Mexica. European colonizers sought and worked to destroy Mesoamerican worldviews regarding spirituality and replace these with a Christian model. The colonizers used syncreticism in art and culture, exemplified through practices such as the idea presented in the Testerian Codices that \"Jesus ate tortillas with his disciples at the last supper\" or the creation of the Virgen de Guadalupe (mirroring the Christian Mary) in order to force Christianity into Mesoamerican cosmology.\nChicanos can create new spiritual traditions by recognizing this history or \"by observing the past and creating a new reality\". Gloria Anzaldua states that this can be achieved through nepantla spirituality or a space where, as stated by Jesus Mendoza, \"all religious knowledge can coexist and create a new spirituality ... where no one is above the other ... a place where all is useful and none is rejected.\" Anzaldua and other scholars acknowledge that this is a difficult process that involves navigating many internal contradictions in order to find a path towards spiritual liberation. Cherrie Moraga calls for a deeper self-exploration of who Chicanos are in order to reach \"a place of deeper inquiry into ourselves as a people ... possibly, we must turn our eyes away from racist America and take stock at the damages done to us. Possibly, the greatest risks yet to be taken are entre nosotros, where we write, paint, dance, and draw the wound for one another to build a stronger pueblo. The women artist seemed disposed to do this, their work often mediating the delicate area between cultural affirmation and criticism.\" Laura E. P\u00e9rez states in her study of Chicana art that \"the artwork itself [is] altar-like, a site where the disembodied\u2014divine, emotional, or social\u2014[is] acknowledged, invoked, meditated upon, and released as a shared offering.\"\nCultural aspects.\nThe diversity of Chicano cultural production is vast. Guillermo G\u00f3mez-Pe\u00f1a wrote that the complexity and diversity of the Chicano community includes influences from Central American, Caribbean, Asian, and African Americans who have moved into Chicano communities as well as queer people of color. Many Chicano artists continue to question \"conventional, static notions of \"Chicanismo\",\" while others conform to more conventional cultural traditions.\nFilm.\nChicano film has been marginalized since its inception and was established in the 1960s. The generally marginal status of Chicanos in the film industry has meant that many Chicano films are not released with wide theatrical distribution. Chicano film emerged from the creation of political plays and documentaries. This included El Teatro Campesino's \"Yo Soy Joaqu\u00edn\" (1969), Luis Valdez's \"El Corrido\" (1976), and Efra\u00edn Guti\u00e9rrez's \"Please, Don't Bury Me Alive!\" (1976), the latter of which is referred to as the first full-length Chicano film.\nDocudramas then emerged like Esperanza Vasquez's \"\" (1977), Jes\u00fas Salvador Trevi\u00f1o's \"Ra\u00edces de Sangre\" (1977), and Robert M. Young's \"\u00a1Alambrista!\" (1977). Luis Valdez's \"Zoot Suit\" (1981), Young's \"The Ballad of Gregorio Cortez\" (1982), Gregory Nava's, \"My Family/Mi familia\" (1995) and \"Selena\" (1997), and Josefina L\u00f3pez's \"Real Women Have Curves\" (2002). Chicana/o films continue to be regarded as a small niche in the film industry that has yet to receive mainstream commercial success. However, Chicana/o films have been influential in shaping how Chicana/os see themselves.\nLiterature.\nChicano literature tends to focus on challenging the dominant narrative, while embracing notions of hybridity, including the use of Spanglish, as well as the blending of genre forms, such as fiction and autobiography. Jos\u00e9 Antonio Villarreal's \"Pocho\" (1959) is widely recognized as the first major Chicano novel. Poet Alurista wrote that Chicano literature served an important role to push back against narratives by white Anglo-Saxon Protestant culture that sought to \"keep Mexicans in their place.\"\nRodolfo \"Corky\" Gonzales's \"Yo Soy Joaquin\" is one of the first examples of explicitly Chicano poetry. Other early influential poems included \"El Louie\" by Jos\u00e9 Montoya and Abelardo \"Lalo\" Delgado's poem \"Stupid America.\" In 1967, Octavio Romano founded Tonatiuh-Quinto Sol Publications, which was the first dedicated Chicano publication houses. The novel \"Chicano\" (1970) by Richard Vasquez, was the first novel about Mexican Americans to be released by a major publisher. It was widely read in high schools and universities during the 1970s and is now recognized as a breakthrough novel.\nChicana feminist writers have tended to focus on themes of identity, questioning how identity is constructed, who constructs it, and for what purpose in a racist, classist, and patriarchal structure. Characters in books such as \"Victuum\" (1976) by Isabella R\u00edos, \"The House on Mango Street\" (1983) by Sandra Cisneros, \"Loving in the War Years: lo que nunca pas\u00f3 por sus labios\" (1983) by Cherr\u00ede Moraga, \"The Last of the Menu Girls\" (1986) by Denise Ch\u00e1vez, \"Margins\" (1992) by Terri de la Pe\u00f1a, and \"Gulf Dreams\" (1996) by Emma P\u00e9rez have also been read regarding how they intersect with themes of gender and sexuality. Catri\u00f3na Rueda Esquibel performs a queer reading of Chicana literature in \"With Her Machete in Her Hand\" (2006) to demonstrate how some of the intimate relationships between girls and women contributed to a discourse on homoeroticism and queer sexuality in Chicana/o literature.\nChicano characters who were gay tended to be removed from the \"barrio\" and were typically portrayed with negative attributes, such as the character of \"Joe Pete\" in \"Pocho\" and the unnamed protagonist of John Rechy's \"City of Night\" (1963). Other characters in the Chicano canon may also be read as queer, including the unnamed protagonist of Tom\u00e1s Rivera's \"...y no se lo trag\u00f3 la tierra\" (1971), and \"Antonio M\u00e1rez\" in Rudolfo Anaya's \"Bless Me, Ultima\" (1972). Juan Bruce-Novoa wrote that homosexuality was \"far from being ignored during the 1960s and 1970s,\" despite homophobia restricting representations: \"our community is less sexually repressive than we might expect\".\nMusic.\nLalo Guerrero has been lauded as the \"father of Chicano music.\" Beginning in the 1930s, he wrote songs in the big band and swing genres and expanded into traditional genres of Mexican music. During the farmworkers' rights campaign, he wrote music in support of C\u00e9sar Ch\u00e1vez and the United Farm Workers. Other notable musicians include Selena, who sang a mixture of Mexican, Tejano, and American popular music, and died in 1995 at the age of 23; Zack de la Rocha, social activist and lead vocalist of Rage Against the Machine; and Los Lonely Boys, a Texas-style country rock band.\nChicano electro.\nChicano techno and electronic music artists DJ Rolando, Santiago Salazar, DJ Tranzo, and Esteban Adame have released music through independent labels like Underground Resistance, Planet E, Krown Entertainment, and Rush Hour. In the 1990s, house music artists such as DJ Juanito (Johnny Loopz), Rudy \"Rude Dog\" Gonzalez, and Juan V. released numerous tracks through Los Angeles-based house labels Groove Daddy Records and Bust A Groove.\nDJ Rolando's techno track \"Knights of the Jaguar,\" released on the UR label in 1999, became the most well-known Chicano techno track after charting at #43 in the UK in 2000. Mixmag commented: \"after it was released, it spread like wildfire all over the world. It's one of those rare tracks that feels like it can play for an eternity without anyone batting an eyelash.\" It's consistently placed on Best Songs lists. The official video for the track features various portraits of Chicana/os in Detroit among several Chicano murals, lowrider cars and lowrider bicycles, and lifestyle.\nSalazar and Adame are also affiliated with Underground Resistance and have collaborated with Nomadico. Salazar founded music labels Major People, Ican (as in \"Mex-Ican\", with Esteban Adame) and Historia y Violencia (with Juan Mendez a.k.a. Silent Servant) and released his debut album \"Chicanismo\" in 2015 to positive reviews. Nomadico's label Yaxteq, founded in 2015, has released tracks by veteran Los Angeles techno producer Xavier De Enciso and Honduran producer Ritmos.\nChicano folk.\nA growing Tex-Mex polka band trend influenced by the ' and ' music of Mexican immigrants, has in turn influenced much new Chicano folk music, especially on large-market Spanish language radio stations and on television music video programs in the U.S. Some of these artists, like the band Quetzal, are known for the political content of political songs.\nChicano rap.\nHip hop culture, which is cited as having formed in the 1980s street culture of African American, West Indian (especially Jamaican), and Puerto Rican New York City Bronx youth and characterized by DJing, rap music, graffiti, and breakdancing, was adopted by many Chicano youth by the 1980s as its influence moved westward across the United States. Chicano artists were beginning to develop their own style of hip hop. Rappers such as Ice-T and Eazy-E shared their music and commercial insights with Chicano rappers in the late 1980s. Chicano rapper Kid Frost, who is often cited as \"the godfather of Chicano rap\" was highly influenced by Ice-T and was even cited as his prot\u00e9g\u00e9. Chicano rap is a unique style of hip hop music which started with Kid Frost, who saw some mainstream exposure in the early 1990s. While Mellow Man Ace was the first mainstream rapper to use Spanglish, Frost's song \"La Raza\" paved the way for its use in American hip hop. Chicano rap tends to discuss themes of importance to young urban Chicanos. Some of the most prominent Chicano artists include A.L.T., Lil Rob, Psycho Realm, Baby Bash, Serio, A Lighter Shade of Brown, and Funky Aztecs. Chicano rap artists with less mainstream exposure, yet with popular underground followings include Cali Life Style, Ese 40'z, Sleepy Loka, Ms. Sancha, Mac Rockelle, Sir Dyno, and Choosey.\nChicano R&amp;B artists include Paula DeAnda, Amanda Perez, Frankie J, and Victor Ivan Santos (early member of the Kumbia Kings and associated with Baby Bash).\nChicano jazz.\nAlthough Latin jazz is most popularly associated with artists from the Caribbean (particularly Cuba) and Brazil, young Mexican Americans have played a role in its development over the years, going back to the 1930s and early 1940s, the era of the zoot suit, when young Mexican-American musicians in Los Angeles and San Jose, such as Jenni Rivera, began to experiment with \"\", a jazz-like fusion genre that has grown recently in popularity among Mexican Americans\nChicano rock.\nIn the 1950s, 1960s and 1970s, a wave of Chicano pop music surfaced through innovative musicians Carlos Santana, Johnny Rodriguez, Ritchie Valens and Linda Ronstadt. Joan Baez, who is also of Mexican-American descent, included Hispanic themes in some of her protest folk songs. Chicano rock is rock music performed by Chicano groups or music with themes derived from Chicano culture.\nThere are two undercurrents in Chicano rock. One is a devotion to the original rhythm and blues roots of Rock and roll including Ritchie Valens, Sunny and the Sunglows, and ? and the Mysterians. Groups inspired by this include Sir Douglas Quintet, Thee Midniters, Los Lobos, War, Tierra, and El Chicano, and, of course, the Chicano Blues Man himself, the late Randy Garribay. The second theme is the openness to Latin American sounds and influences. Trini Lopez, Santana, Malo, Azteca, Toro, Ozomatli and other Chicano Latin rock groups follow this approach. Chicano rock crossed paths of other Latin rock genres (Rock en espa\u00f1ol) by Cubans, Puerto Ricans, such as Joe Bataan and Ralphi Pagan and South America (Nueva canci\u00f3n). Rock band The Mars Volta combines elements of progressive rock with traditional Mexican folk music and Latin rhythms along with Cedric Bixler-Zavala's Spanglish lyrics.\nChicano punk is a branch of Chicano rock. There were many bands that emerged from the California punk scene, including The Zeros, Bags, Los Illegals, The Brat, The Plugz, Manic Hispanic, and the Cruzados; as well as others from outside of California including Mydolls from Houston, Texas and Los Crudos from Chicago, Illinois. Some music historians argue that Chicanos of Los Angeles in the late 1970s might have independently co-founded punk rock along with the already-acknowledged founders from European sources when introduced to the US in major cities. The rock band ? and the Mysterians, which was composed primarily of Mexican-American musicians, was the first band to be described as punk rock. The term was reportedly coined in 1971 by rock critic Dave Marsh in a review of their show for \"Creem\" magazine.\nPerformance arts.\nEl Teatro Campesino (The Farmworkers' Theater) was founded by Luis Valdez and Agustin Lira in 1965 as the cultural wing of the United Farm Workers (UFW) as a result of the Great Delano Grape Strike in 1965. All of the actors were farmworkers and involved in organizing for farmworkers' rights. Its first performances sought to recruit members for the UFW and dissuade strikebreakers. Many early performances were not scripted and were rather conceived through the direction of Valdez and others through \"actos\", in which a scenario would be proposed for a scene and then dialogue would simply be improvised.Chicano performance art continued with the work of Los Angeles' comedy troupe Culture Clash, Guillermo G\u00f3mez-Pe\u00f1a, and Nao Bustamante, known internationally for her conceptual art pieces and as a participant in \"\". Chicano performance art became popular in the 1970s, blending humor and pathos for tragicomic effect. Groups such as Asco and the Royal Chicano Air Force illustrated this aspect of performance art through their work. Asco (Spanish for \"naseau\" or \"disgust\"), composed of Willie Her\u00f3n, Gronk, Harry Gamboa Jr., and Patssi Valdez, created performance pieces such as the \"Walking Mural\", walking down Whittier Boulevard dressed as \"a multifaceted mural, a Christmas tree, and the Virgin of Guadalupe. Asco continued its conceptual performance piece until 1987.\nIn the 1990s, San Diego-based artist cooperative of David Avalos, Louis Hock, and Elizabeth Sisco used their National Endowment for the Arts $5,000 fellowship subversively, deciding to circulate money back to the community: \"handing 10-dollar bills to undocumented workers to spend as they please.\" Their piece Arte Reembolsa (Art Rebate) created controversy among the art establishment, with the documentation of the piece featuring \"footage of U.S. House and Senate members questioning whether the project was, in fact, art.\"\nOne of the most well-known performance art troupes is La Pocha Nostra, which has been covered in numerous articles for various performance art pieces. The troupe has been active since 1993 yet has remained relevant into the 2010s and 2020s due to its political commentary, including anti-corporate stances. The troupe regularly uses parody and humor in their performances to make complex commentary on various social issues. Creating thought-provoking performances that challenge the audience to think differently is often their intention with each performance piece.\nVisual arts.\nThe Chicano visual art tradition, like the identity, is grounded in community empowerment and resisting assimilation and oppression. Prior to the introduction of spray cans, paint brushes were used by Chicano \"shoeshine boys [who] marked their names on the walls with their daubers to stake out their spots on the sidewalk\" in the early 20th century. Pachuco graffiti culture in Los Angeles was already \"in full bloom\" by the 1930s and 1940s, pachucos developed their \"placa\", \"a distinctive calligraphic writing style\" which went on to influence contemporary graffiti tagging. Pa\u00f1o, a form of \"pinto arte\" (a \"cal\u00f3\" term for male prisoner) using pen and pencil, developed in the 1930s, first using bed sheets and pillowcases as canvases. Pa\u00f1o has been described as \"rasquachismo\", a Chicano worldview and artmaking method which makes the most from the least.\nGraffiti artists, such as Charles \"Chaz\" Boj\u00f3rquez, developed an original style of graffiti art known as West Coast Cholo style influenced by Mexican murals and pachuco \"placas\" (tags which indicate territorial boundaries) in the mid-20th century. In the 1960s, Chicano graffiti artists from San Antonio to L.A. (especially in East LA, Whittier, and Boyle Heights) used the art form to challenge authority, tagging police cars, buildings, and subways as \"a demonstration of their bravado and anger\", understanding their work as \"individual acts of pride or protest, gang declarations of territory or challenge, and weapons in a class war.\" Chicano graffiti artists wrote C/S as an abbreviation for \"con safos\" or the variant \"con safo\" (loosely meaning \"don't touch this\" and expressing a \"the same to you\" attitude)\u2014a common expression among Chicanos on the eastside of Los Angeles and throughout the Southwest. \nThe Chicano Movement and political identity had heavily influenced Chicano artists by the 1970s. Alongside the Black arts movement, this led to the development of institutions such as Self-Help Graphics, Los Angeles Contemporary Exhibitions, and Plaza de la Raza. Artists such as Harry Gamboa Jr., Gronk, and Judith Baca created art which \"stood in opposition to the commercial galleries, museums, and civic institutional mainstream\". This was exemplified with Asco's tagging of LACMA after \"a curator refused to even entertain the idea of a Chicano art show within its walls\" in 1972. Chicano art collectives such as the Royal Chicano Air Force, founded in 1970 by Ricardo Favela, Jos\u00e9 Montoya and Esteban Villa, supported the United Farm Workers movement through art activism, using art to create and inspire social change. Favela believed that it was important to keep the culture alive through their artwork. Favela stated \"I was dealing with art forms very foreign to me, always trying to do western art, but there was always something lacking... it was very simple: it was just my Chicano heart wanting to do Chicano art.\" Other Chicano visual art collectives included Con Safo in San Antonio, which included Felipe Reyes, Jos\u00e9 Esquivel, Roberto R\u00edos, Jesse Almaz\u00e1n, Jesse \"Chista\" Cant\u00fa, Jose Garza, Mel Casas, Rudy Trevi\u00f1o, C\u00e9sar Mart\u00ednez, Kathy Vargas, Amado Pe\u00f1a, Jr., Robando Brise\u00f1o, and Roberto Gonzalez. The Mujeres Muralistas in the Mission District, San Francisco included Patricia Rodriguez, Graciela Carrillo, Consuelo Mendez, and Irene Perez.\nChicano muralism, which began in the 1960s, became a state-sanctioned artform in the 1970s as an attempt by outsiders to \"prevent gang violence and dissuade graffiti practices\". This led to the creation of murals at Estrada Courts and other sites throughout Chicano communities. In some instances, these murals were covered with the \"placas\" they were instituted by the state to prevent. Marcos Sanchez-Tranquilino states that \"rather than vandalism, the tagging of one's own murals points toward a complex sense of wall ownership and a social tension created by the uncomfortable yet approving attentions of official cultural authority.\" This created a division between established Chicano artists who celebrated inclusion and acceptance by the dominant culture and younger Chicano artists who \"saw greater power in renegade muralism and \"barrio\" calligraphy than in state-sanctioned pieces.\" Chicano poster art became prominent in the 1970s as a way to challenge political authority, with pieces such as Rupert Garc\u00eda's \"Save Our Sister\" (1972), depicting Angela Davis, and Yolanda M. L\u00f3pez's \"Who's the Illegal Alien, Pilgrim?\" (1978) addressing settler colonialism.\nThe oppositional current of Chicano art was bolstered in the 1980s by a rising hip hop culture. The Olympic freeway murals, including Frank Romero's \"Going to the Olympics\", created for the 1984 Olympic Games in Los Angeles became another site of contestation, as Chicano and other graffiti artists tagged the state-sanctioned public artwork. Government officials, muralists, and some residents were unable to understand the motivations for this, described it \"as \"mindless\", \"animalistic\" vandalism perpetrated by \"kids\" who simply lack respect.\" L.A. had developed a distinct graffiti culture by the 1990s and, with the rise of drugs and violence, Chicano youth culture gravitated towards graffiti to express themselves and to mark their territory amidst state-sanctioned disorder. Following the Rodney King riots and the murder of Latasha Harlins, which exemplified an explosion of racial tensions bubbling under in American society, racialized youth in L.A., \"feeling forgotten, angry, or marginalized, [embraced] graffiti's expressive power [as] a tool to push back.\"\nChicano art, although accepted into some institutional art spaces in shows like , was still largely excluded from many mainstream art institutions in the 1990s. By the 2000s, attitudes towards graffiti by white hipster culture were changing, as it became known as \"street art\". In academic circles, \"street art\" was termed \"post-graffiti\". By the 2000s, where the LAPD once deployed CRASH (Community Resources Against Street Hoodlums) units in traditionally Chicano neighborhoods like Echo Park and \"often brutalized suspected taggers and gang members\", \"street art\" was now being mainstreamed by the white art world in those same neighborhoods.Despite this shift, Chicano artists continued to challenge what was acceptable to both insiders and outsiders of their communities. Controversy surrounding Chicana artist Alma L\u00f3pez's \"Our Lady\" at the Museum of International Folk Art in 2001 erupted when \"local demonstrators demanded the image be removed from the state-run museum\". Previously, L\u00f3pez's digital mural \"Heaven\" (2000), which depicted two Latina women embracing, had been vandalized. L\u00f3pez received homophobic slurs, threats of physical violence, and over 800 hate mail inquiries for \"Our Lady.\" Santa Fe Archbishop Michael J Sheehan referred to the woman in L\u00f3pez's piece as \"a tart or a street woman\". L\u00f3pez stated that the response came from the conservative Catholic Church, \"which finds women's bodies inherently sinful, and thereby promot[es] hatred of women's bodies.\" The art was again protested in 2011.\nManuel Paul's mural \"Por Vida\" (2015) at Galeria de la Raza in Mission District, San Francisco, which depicted queer and trans Chicanos, was targeted multiple times after its unveiling. Paul, a queer DJ and artist of the Maric\u00f3n Collective, received online threats for the work. Ani Rivera, director of Galeria de la Raza, attributed the anger towards the mural to gentrification, which has led \"some people [to] associate LGBT people with non-Latino communities.\" The mural was meant to challenge \"long-held assumptions regarding the traditional exclusivity of heterosexuality in lowrider culture\". Some credited the negative response to the mural's direct challenging of machismo and heteronormativity in the community.\nXandra Ibarra's video art \"Spictacle II: La Tortillera\" (2004) was censored by San Antonio's Department of Arts and Culture in 2020 from \"XicanX: New Visions\", a show which aimed to challenge \"previous and existing surveys of Chicano and Latino identity-based exhibitions\" through highlighting \"the womxn, queer, immigrant, indigenous and activist artists who are at the forefront of the movement\". Ibarra stated \"the video is designed to challenge normative ideals of Mexican womanhood and is in alignment with the historical lineage of LGBTQAI+ artists' strategies to intervene in homophobic and sexist violence.\"\nInternational influence.\nChicano culture has become popular in some areas internationally, most prominently in Japan, Brazil, and Thailand. Chicano ideas such as Chicano hybridity and have found influence as well, such as in decoloniality. In S\u00e3o Paulo, Chicano cultural influence has formed the \"Cho-Low\" (combination of \"Cholo\" and Lowrider) subculture that has formed a sense of cultural pride among youth.\nChicano cultural influence is strong in Japan, where Chicano culture took hold in the 1980s and continued to grow with contributions from Shin Miyata, Junichi Shimodaira, Miki Style, Night Tha Funksta, and MoNa (Sad Girl). Miyata owns a record label, Gold Barrio Records, that re-releases Chicano music. Chicano fashion and other cultural aspects have also been adopted in Japan. There has been debate over whether this is cultural appropriation, with most arguing that it is appreciation rather than appropriation. In an interview asking why Chicano culture is popular in Japan, two long-time proponents of Chicano culture in Japan agreed that \"it's not about Mexico or about America: it's an alluring quality unique to the hybrid nature of Chicano and imprinted in all its resulting art forms, from lowriders in the '80s to TikTok videos today, that people relate to and appreciate, not only in Japan but around the world.\"\nMost recently, Chicano culture has found influence in Thailand among working-class men and women that is called \"Thaino\" culture. They state that they have disassociated the violence that Hollywood portrays of Chicanos from the Chicano people themselves. They have adopted rules of no cocaine or amphetamines, and only marijuana, which is legal in Thailand. The leader of one group stated that he was inspired by how Chicanos created a culture out of defiance \"to fight against people who were racist toward them\" and that this inspired him, since he was born in a slum in Thailand. He also stated \"if you look closely at [Chicano] culture, you'll notice how gentle it is. You can see this in their Latin music, dances, clothes, and how they iron their clothes. It's both neat and gentle.\"\nReferences.\n&lt;templatestyles src=\"Reflist/styles.css\" /&gt;"}
